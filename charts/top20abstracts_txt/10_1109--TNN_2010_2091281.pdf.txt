Domain Adaptation via Transfer
Component Analysis

Domain adaptation allows knowledge from a source
domain to be transferred to a different but related target
domain. Intuitively, discovering a good feature representation
across domains is crucial. In this paper, we first propose to find
such a representation through a new learning method, transfer
component analysis (TCA), for domain adaptation. TCA tries to
learn some transfer components across domains in a reproducing
kernel Hilbert space using maximum mean miscrepancy. In the
subspace spanned by these transfer components, data properties
are preserved and data distributions in different domains are
close to each other. As a result, with the new representations in
this subspace, we can apply standard machine learning methods
to train classifiers or regression models in the source domain
for use in the target domain. Furthermore, in order to uncover
the knowledge hidden in the relations between the data labels
from the source and target domains, we extend TCA in a
semisupervised learning setting, which encodes label information
into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we
propose a novel dimensionality reduction framework for reducing
the distance between domains in a latent space for domain
adaptation. We propose both unsupervised and semisupervised
feature extraction approaches, which can dramatically reduce the
distance between domain distributions by projecting data onto the
learned transfer components. Finally, our approach can handle
large datasets and naturally lead to out-of-sample generalization.
The effectiveness and efficiency of our approach are verified by
experiments on five toy datasets and two real-world applications:
cross-domain indoor WiFi localization and cross-domain text
classification.
Dimensionality reduction, domain adaptation,
Hilbert space embedding of distributions, transfer learning.

