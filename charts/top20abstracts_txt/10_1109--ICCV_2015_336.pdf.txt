PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization


Inferring where you are, or localization, is crucial for
mobile robotics, navigation and augmented reality. This paper addresses the lost or kidnapped robot problem by introducing a novel relocalization algorithm. Our proposed system, PoseNet, takes a single 224x224 RGB image and regresses the camera’s 6-DoF pose relative to a scene. Fig. 1
demonstrates some examples. The algorithm is simple in
the fact that it consists of a convolutional neural network
(convnet) trained end-to-end to regress the camera’s orientation and position. It operates in real time, taking 5ms to
run, and obtains approximately 2m and 3 degrees accuracy
for large scale outdoor scenes (covering a ground area of up
to 50, 000m2 ).
Our main contribution is the deep convolutional neural
network camera pose regressor. We introduce two novel
techniques to achieve this. We leverage transfer learning from recognition to relocalization with very large scale
classification datasets. Additionally we use structure from
motion to automatically generate training labels (camera
poses) from a video of the scene. This reduces the human
labor in creating labeled video datasets to just recording the

We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains
a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It
obtains approximately 2m and 3◦ accuracy for large scale
outdoor scenes and 0.5m and 5◦ accuracy indoors. This is
achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out
of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show that the PoseNet localizes from high
level features and is robust to difficult lighting, motion blur
and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature
that is produced generalizes to other scenes allowing us to
regress pose with only a few dozen training examples.

