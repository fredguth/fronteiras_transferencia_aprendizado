Partial Transfer Learning with Selective Adversarial Networks
Zhangjie Cao† , Mingsheng Long†∗, Jianmin Wang† , Michael I. Jordan♯
†
KLiss, MOE; School of Software, Tsinghua University, China
†
National Engineering Laboratory for Big Data Software
♯
University of California, Berkeley, Berkeley, CA, USA
caozhangjie14@gmail.com, {mingsheng,jimwang}@tsinghua.edu.cn, jordan@cs.berkeley.edu

Abstract
Adversarial learning has been successfully embedded into
deep networks to learn transferable features, which reduce
distribution discrepancy between the source and target domains. Existing domain adversarial networks assume fully
shared label space across domains. In the presence of big
data, there is strong motivation of transferring both classification and representation models from existing large-scale
domains to unknown small-scale domains. This paper introduces partial transfer learning, which relaxes the shared
label space assumption to that the target label space is only
a subspace of the source label space. Previous methods typically match the whole source domain to the target domain,
which are prone to negative transfer for the partial transfer
problem. We present Selective Adversarial Network (SAN),
which simultaneously circumvents negative transfer by selecting out the outlier source classes and promotes positive
transfer by maximally matching the data distributions in
the shared label space. Experiments demonstrate that our
models exceed state-of-the-art results for partial transfer
learning tasks on several benchmark datasets.

1. Introduction
Deep networks have significantly improved the state of
the art for a wide variety of machine learning problems and
applications. At the moment, these impressive gains in performance come only when massive amounts of labeled data
are available. Since manual labeling of sufficient training
data for diverse application domains on-the-fly is often prohibitive, for problems short of labeled data, there is strong
motivation to establishing effective algorithms to reduce the
labeling consumption, typically by leveraging off-the-shelf
labeled data from a different but related source domain. This
promising transfer learning paradigm, however, suffers from
the shift in data distributions across different domains, which
∗ Corresponding

author: M. Long (mingsheng@tsinghua.edu.cn).

poses a major obstacle in adapting classification models to
target tasks [23].
Existing transfer learning methods assume shared label
space and different feature distributions across the source
and target domains. These methods bridge different domains
by learning domain-invariant feature representations without
using target labels, and the classifier learned from source
domain can be directly applied to target domain. Recent
studies have revealed that deep networks can learn more
transferable features for transfer learning [5, 33], by disentangling explanatory factors of variations behind domains.
The latest advances have been achieved by embedding transfer learning in the pipeline of deep feature learning to extract
domain-invariant deep representations [30, 16, 7, 31, 18].
In the presence of big data, we can readily access largescale labeled datasets such as ImageNet-1K. Thus, a natural
ambition is to directly transfer both the representation and
classification models from large-scale dataset to our target
dataset, such as Caltech-256, which are usually small-scale
and with unknown categories at training and testing time.
From big data viewpoint, we can assume that the large-scale
dataset is diverse enough to subsume all categories of the
small-scale dataset. Thus, we introduce a novel partial transfer learning problem, assuming that the target label space is
a subspace of the source label space. It is a prerequisite of
open set domain adaptation [3]. As shown in Figure 1, partial
transfer learning problem is more general and challenging
than standard transfer learning, since outlier source classes
(“sofa”) will result in negative transfer when discriminating
the target classes (“soccer-ball” and “binoculars”). Negative
transfer is the phenomenon that a transfer learner performs
even worse than a supervised classifier trained solely on
the source domain, which is the key challenge of transfer
learning [23]. Thus, matching the whole source and target
domains as previous methods is not an effective solution to
this new partial transfer learning scenario.
This paper presents Selective Adversarial Networks
(SAN), which largely extends the ability of deep adversarial
adaptation [7] to address partial transfer learning from large-

2724

target domain

soccer-ball
source domain

+ ++
+++ +++++
+ + +
+ + sofa
++ ++ ++ ++
++

binoculars

soccer-ball
binoculars

Figure 1. The partial transfer learning problem, where source label space subsumes target label space (should be prone to negative transfer).

scale domains to small-scale domains. SAN aligns the distributions of source and target data in the shared label space and
more importantly, selects out the source data in the outlier
source classes. A key improvement over previous methods is
the capability to simultaneously promote positive transfer of
relevant data and alleviate negative transfer of irrelevant data,
which can be trained in an end-to-end framework. Experiments show that our models exceed state-of-the-art results
for deep transfer learning on public datasets.

ing paradigm [7, 31]. While performance was significantly
improved, these state of the art methods may be restricted
by the assumption that the source and target domains share
the same label space. This assumption is violated in partial transfer learning, which transfers both representation
and classification models from existing large-scale domains
to unknown small-scale domains. To our knowledge, this
is the first work that addresses partial transfer learning in
adversarial networks.

2. Related Work

3. Partial Transfer Learning

Transfer learning [23] bridges different domains or tasks
to mitigate the burden of manual labeling for machine learning [22, 6, 34, 32], computer vision [26, 9, 14] and natural
language processing [4]. The main technical difficulty of
transfer learning is to formally reduce the distribution discrepancy across different domains. Deep networks can learn
abstract representations that disentangle different explanatory factors of variations behind data [2] and manifest invariant factors underlying different populations that transfer well
from original tasks to similar novel tasks [33]. Thus deep
networks have been explored for transfer learning [8, 21, 14],
multimodal and multi-task learning [4, 20], where significant performance gains have been witnessed relative to prior
shallow transfer learning methods.
However, recent advances show that deep networks can
learn abstract feature representations that can only reduce,
but not remove, the cross-domain discrepancy [8, 30], resulting in unbounded risk for target tasks [19, 1]. Some
recent work bridges deep learning and domain adaptation
[30, 16, 7, 31, 18], which extends deep convolutional networks (CNNs) to domain adaptation by adding adaptation
layers through which the mean embeddings of distributions
are matched [30, 16, 18], or by adding a subnetwork as
domain discriminator while the deep features are learned
to confuse the discriminator in a domain-adversarial train-

In this paper, we propose partial transfer learning, a
novel transfer learning paradigm where the target domain
label space Ct is a subspace of the source domain label space
Cs i.e. Ct ⊂ Cs . This new paradigm finds wide applications
in practice, as we usually need to transfer a model from a
large-scale dataset (e.g. ImageNet) to a small-scale dataset
(e.g. Caltech-256). Similar to standard transfer learning, in
partial transfer learning we are also provided with a source
s
domain Ds = {(xi , yi )}ni=1
of ns labeled examples associs +nt
ated with |Cs | classes and a target domain Dt = {xi }ni=n
s +1
of nt unlabeled examples associated with |Ct | classes, but
differently, we have |Cs | > |Ct | in partial transfer learning.
The source domain and target domain are sampled from probability distributions p and q respectively. In standard transfer
learning, we have p 6= q; and in partial transfer learning, we
further have pCt 6= q, where pCt denotes the distribution of
the source domain labeled data belonging to label space Ct .
The goal of this paper is to design a deep neural network that
enables learning of transfer features f = Gf (x) and adaptive classifier y = Gy (f ) to bridge the cross-domain discrepancy, such that the target risk Pr(x,y)∼q [Gy (Gf (x)) 6= y]
is minimized by leveraging the source domain supervision.
In standard transfer learning, one of the main challenges
is that the target domain has no labeled data and thus the
source classifier Gy trained on source domain Ds cannot be

2725

directly applied to target domain Dt due to the distribution
discrepancy of p 6= q. In partial transfer learning, another
more difficult challenge is that we even do not know which
part of the source domain label space Cs is shared with the
target domain label space Ct because Ct is not accessible
during training, which results in two technical difficulties.
On one hand, the source domain labeled data belonging to
outlier label space Cs \Ct will cause negative transfer effect
to the overall transfer performance. Existing deep transfer
learning methods [16, 7, 31, 18] generally assume source
domain and target domain have the same label space and
match the whole distributions p and q, which are prone to
negative transfer since the source and target label spaces
are different and thus cannot be matched in principle. Thus,
how to eliminate or at least decrease the influence of the
source labeled data in outlier label space Cs \Ct is the key
to alleviating negative transfer. On the other hand, reducing
the distribution discrepancy between pCt and q is crucial
to enabling knowledge transfer in the shared label space
Ct . These challenges should be tackled by filtering out the
negative influence of unrelated part of source domain and at
the same time enabling effective transfer learning between
related part of source domain and target domain.
We propose a novel selective adversarial network to enable partial transfer learning by addressing two challenges.
(1) Circumvent negative transfer by filtering out the unrelated source labeled data belonging to the outlier label space
Cs \Ct . (2) Promote positive transfer by maximally matching
the data distributions pCt and q in the shared label space Ct .

3.1. Domain Adversarial Network
Domain adversarial networks have been successfully applied to transfer learning [7, 31] by extracting transferable
features that can reduce the distribution shift between the
source domain and the target domain. The adversarial learning procedure is a two-player game, where the first player is
the domain discriminator Gd trained to distinguish the source
domain from the target domain, and the second player is the
feature extractor Gf fine-tuned simultaneously to confuse
the domain discriminator.
To extract domain-invariant features f , the parameters
θf of feature extractor Gf are learned by maximizing the
loss of domain discriminator Gd , while the parameters θd of
domain discriminator Gd are learned by minimizing the loss
of the domain discriminator. In addition, the loss of label
predictor Gy is also minimized. The objective of domain
adversarial network [7] is the following functional:
1 X
Ly (Gy (Gf (xi )) , yi )
C0 (θf , θy , θd ) =
ns
xi ∈Ds

λ
−
ns + n t

X

Ld (Gd (Gf (xi )) , di )

xi ∈Ds ∪Dt

(1)

where λ is a trade-off parameter between the two objectives
that shape the features during learning. After training convergence, the parameters θ̂f , θ̂y , θ̂d will deliver a saddle point
of the functional (1):
(θ̂f , θ̂y ) = arg min C0 (θf , θy , θd ) ,
θf ,θy

(θ̂d ) = arg max C0 (θf , θy , θd ) .

(2)

θd

Domain adversarial networks are particularly effective for
standard transfer learning where the source domain label
space and target domain label space are the same, Cs = Ct .

3.2. Selective Adversarial Network
In partial transfer learning, the target domain label space
is a subset of the source domain label space, Ct ⊂ Cs . Thus,
matching the whole source domain distribution p and target
domain distribution q will result in negative transfer caused
by the outlier label space Cs \Ct . The larger the outlier label
space Cs \Ct compared to the target label space Ct , the severer
the negative transfer effect will be. To combat negative
transfer, we should find a way to select out the outlier source
classes as well as the associated source labeled data in Cs \Ct
when performing domain adversarial adaptation.
To match the source and target domains of different label
spaces Cs 6= Ct , we need to split the domain discriminator
Gd in Equation (1) into |Cs | class-wise domain discriminators Gkd , k = 1, . . . , |Cs |, each is responsible for matching
the source and target domain data associated with label k,
as shown in Figure 2. Since the target label space Ct is
inaccessible during training while the target domain data
are fully unlabeled, it is not easy to decide which domain
discriminator Gkd is responsible for each target data point.
Fortunately, we observe that the output of the label predictor ŷi = Gy (xi ) to each data point xi is a probability
distribution over the source label space Cs . This distribution
well characterizes the probability of assigning xi to each
of the |Cs | classes. Therefore, it is natural to use ŷi as the
probability to assign each data point xi to the |Cs | domain
discriminators Gkd , k = 1, . . . , |Cs |. The assignment of each
point xi to different discriminators can be implemented by a
probability-weighted domain discriminator loss for all |Cs |
domain discriminators Gkd , k = 1, . . . , |Cs | as follows,
L′d =

|Cs |
X
1
ns + nt

X

k=1 xi ∈Ds ∪Dt


ŷik Lkd Gkd (Gf (xi )) , di ,

(3)
where Gkd is the k-th domain discriminator while Lkd is its
cross-entropy loss, and di is the domain label of point xi .
Compared with the single-discriminator domain adversarial
network in Equation (1), the proposed multi-discriminator
domain adversarial network enables fine-grained adaptation
where each data point xi is matched only by those relevant

2726

@Ly
@θf

@Ly
@θy

Gf
x

Gy
f

CNN

@Ld
−
@θf

@Lf
@θf

GRL

y^
Gd1

d^1

L1d

Gd2

d^2

L2d

GdK

d^K

LKd

@Ld
@θd

back-propagation

Ly

Ld

Figure 2. The architecture of the proposed Selective Adversarial Networks (SAN) for partial transfer learning, where f is the extracted deep
features, ŷ is the predicted data label, and d̂ is the predicted domain label; Gf is the feature extractor, Gy and Ly are the label predictor and
its loss, Gkd and Lkd are the domain discriminator and its loss; GRL stands for Gradient Reversal Layer. The blue part shows the class-wise
adversarial networks (|Cs | in total) designed in this paper. Best viewed in color.

domain discriminators according to its probability ŷi . This
fine-grained adaptation may introduce three benefits. (1)
It avoids the hard assignment of each point to only one
domain discriminator, which tends to be inaccurate for target
domain data. (2) It circumvents negative transfer since each
point is only aligned to one or several most relevant classes,
while the irrelevant classes are filtered out by the probabilityweighted domain discriminator loss. (3) The probabilityweighted domain discriminator loss puts different losses
to different domain discriminators, which naturally learns
multiple domain discriminators with different parameters θdk ;
these domain discriminators with different parameters can
promote positive transfer for each instance.
Besides the instance-level weighting mechanism described above, we introduce another class-level weighting
method to further remove the negative influence of outlier
source classes Cs \Ct and the associated source data. We observe that only the domain discriminators responsible for the
target classes Ct are effective for promoting positive transfer, while the other discriminators responsible for the outlier
source classes Cs \Ct only introduce noises and deteriorate
the positive transfer between the source domain and the target domain in the shared label space Ct . Therefore, we need
to down-weight the domain discriminators responsible for
the outlier source classes, which can be implemented by
class-level weighting of these domain discriminators. Since
target data are not likely to belong to the outlier source
classes, their probabilities yik , k ∈ Cs \Ct are also sufficiently
small. Thus, we can down-weight the domain discriminators

responsible for the outlier source classes as follows,
!
"
|Cs |
X
1 X k
1
ŷi
Ld =
ns + nt
nt
xi ∈Dt
k=1


X

×
ŷik Lkd Gkd (Gf (xi )) , di  ,

(4)

xi ∈(Ds ∪Dt )

where n1t xi ∈Dt ŷik is the class-level weight for class k,
which is small for the outlier source classes.
Although the multiple domain discriminators introduced
in Equation (4) can selectively transfer relevant knowledge
to target domain by decreasing the negative influence of
outlier source classes Cs \Ct and by effectively transferring
knowledge of shared label space Ct , it highly depends on the
probability ŷi = Gy (xi ). Thus, we further refine the label
predictor Gy by exploiting the entropy minimization principle [10] which encourages low-density separation between
classes. This criterion is implemented by minimizing the
entropy E over probability ŷik on target domain Dt as
1 X
H (Gy (Gf (xi )))
(5)
E=
nt
P

xi ∈Dt

where H(·) is the conditional-entropy loss functional
P|Cs | k
H (Gy (Gf (xi ))) = − k=1
ŷi log ŷik . By minimizing
the entropy functional (5), the label predictor Gy (xi ) can
directly access target unlabeled data and will amend itself
to pass through the target low-density regions to give more
accurate probability ŷi with minimal prediction uncertainty.

2727

Integrating all things together, the final objective of the
proposed Selective Adversarial Network (SAN) is


1 X
|Cs |
=
C θf , θy , θdk |k=1
Ly (Gy (Gf (xi )), yi )
ns x ∈D
i

s

1 X
H (Gy (Gf (xi )))
+
nt x ∈D
t
i


|Cs |
X k
X
1
1

−
ŷi 
ns + nt
nt x ∈D
k=1
t
i




X
ŷik Lkd Gkd (Gf (xi )) , di 
×
xi ∈Ds ∪Dt

(6)
where λ is a hyper-parameter that trade-offs the two objectives in the unified optimization problem. The optimization problem is to find the network parameters θ̂f , θ̂y and
θ̂dk (k = 1, 2, ..., |Cs |) that satisfy


|Cs |
,
(θ̂f , θ̂y ) = arg min C θf , θy , θdk |k=1
θf ,θy


(7)
|Cs |
|C |
.
(θ̂d1 , ..., θ̂d s ) = arg max C θf , θy , θdk |k=1
|Cs |

θd1 ,...,θd

The selective adversarial network (SAN) enables partial
transfer learning, which simultaneously circumvents negative transfer by filtering out outlier source classes Cs \Ct ,
and promotes positive transfer by maximally matching the
data distributions pCt and q in the shared label space Ct .

4. Experiments
We conduct experiments on three benchmark datasets to
evaluate the efficacy of our approach against several state-ofthe-art deep transfer learning methods. Codes and datasets
will be available at: https://github.com/thuml.

4.1. Setup
The evaluation is conducted on three public datasets:
Office-31, Caltech-Office and ImageNet-Caltech.
Office-31 [26] is a standard benchmark for domain adaptation in computer vision, consisting of 4,652 images and 31
categories collected from three distinct domains: Amazon
(A), which contains images downloaded from amazon.com,
Webcam (W) and DSLR (D), which contain images taken by
web camera and digital SLR camera with different settings,
respectively. We denote the three domains with 31 categories
as A 31, W 31 and D 31. Then we use the ten categories
shared by Office-31 and Caltech-256 and select images of
these ten categories in each domain of Office-31 as target
domains, denoted as A 10, W 10 and D 10. We evaluate
all methods across six transfer tasks A 31 → W 10, D 31
→ W 10, W 31 → D 10, A 31 → D 10, D 31 → A 10 and
W 31 → A 10. These tasks represent the performance on

the setting where both source and target domains have small
number of classes.
Caltech-Office [9] is built by using Caltech-256 (C
256) [12] as source domain and the three domains in Office 31 as target domains. We use the ten categories shared
by Caltech-256 and Office-31 and select images of these ten
categories in each domain of Office-31 as target domains
[9, 17, 28]. Denoting source domains as C 256, we can build
3 transfer tasks: C 256 → W 10, C 256 → A 10 and C
256 → D 10. This setting aims to test the performance of
different methods on the task setting where source domain
has much more classes than the target domain.
ImageNet-Caltech is built from ImageNet-1K [25]
dataset containing 1000 classes and Caltech-256 containing
256 classes. They share 84 common classes, thus we form
two transfer learning tasks: ImageNet 1000 → Caltech 84
and Caltech 256 → ImageNet 84. To prevent the effect
of the pre-trained model on ImageNet, we use ImageNet
validation set when ImageNet is used as target domain and
ImageNet training set when ImageNet is used as source domain. This setting represents the performance on tasks with
large number of classes in both source and target domains.
We compare the performance of SAN with state of the
art transfer learning and deep learning methods: Convolutional Neural Network (AlexNet [15]), Deep Adaptation
Network (DAN) [16], Reverse Gradient (RevGrad) [7],
Residual Transfer Networks (RTN) [18], and Adversarial
Discriminative Domain Adaptation (ADDA) [29]. DAN
learns transferable features by embedding deep features of
multiple task-specific layers to reproducing kernel Hilbert
spaces (RKHSs) and matching different distributions optimally using multi-kernel MMD. RevGrad improves domain
adaptation by making the source and target domains indistinguishable for a discriminative domain classifier via an
adversarial training paradigm. RTN jointly learns transferable features and adapts different source and target classifiers
via deep residual learning [13]. ADDA combines discriminative modeling, untied weight sharing, and a GAN loss to
yield much better results than RevGrad. All prior methods
do not address partial transfer learning where the target label
space is a subspace of the source label space. To test SAN on
different base-networks, we also compare different methods
on VGG-16 [27]. To go deeper with the efficacy of selective
mechanism and entropy minimization, we perform ablation
study by evaluating two variants of SAN: (1) SAN-selective
is the variant without selective mechanism, which has the
same model complexity as AlexNet; (2) SAN-entropy is the
variant without entropy minimization, which has the same
model complexity as SAN.
We follow standard protocols and use all labeled source
data and all unlabeled target data for unsupervised transfer
learning [26, 16]. We compare average classification accuracy of each transfer task using three random experiments.

2728

Table 1. Classification Accuracy (%) of Partial Transfer Learning Tasks on Office-31 (AlexNet as Base Network)

Method
AlexNet [15]
DAN [16]
RevGrad [7]
RTN [18]
ADDA [29]
SAN-selective
SAN-entropy
SAN
Upper Bound

Office-31
A 31 → W 10 D 31 → W 10 W 31 → D 10 A 31 → D 10 D 31 → A 10 W 31 → A 10
58.51
95.05
98.08
71.23
70.6
67.74
56.52
71.86
86.78
51.86
50.42
52.29
49.49
93.55
90.44
49.68
46.72
48.81
66.78
86.77
99.36
70.06
73.52
76.41
70.68
96.44
98.65
72.90
74.26
75.56
71.51
98.31
100.00
78.34
77.87
76.32
74.61
98.31
100.00
80.29
78.39
82.25
80.02
98.64
100.00
81.28
80.58
83.09
91.86
98.64
100.00
92.99
90.19
90.19

Avg
76.87
61.62
63.11
78.82
81.42
83.73
85.64
87.27
93.98

Table 2. Classification Accuracy (%) of Partial Transfer Learning Tasks on Caltech-Office and ImageNet-Caltech (AlexNet as Base Network)

Method
AlexNet [15]
DAN [16]
RevGrad [7]
RTN [18]
ADDA [29]
SAN-selective
SAN-entropy
SAN

C 256 → W 10
58.44
42.37
54.57
71.02
73.66
76.44
72.54
88.33

Caltech-Office
C 256 → A 10 C 256 → D 10
76.64
65.86
70.75
47.04
72.86
57.96
81.32
62.35
78.35
74.80
81.63
80.25
78.95
76.43
83.82
85.35

For MMD-based methods (DAN and RTN), we use Gaussian kernel with bandwidth b set to median pairwise squared
distances on training data, i.e. median heuristic [11]. For all
methods, we perform standard cross-validation on labeled
source data to select their hyper-parameters.
We implement all deep methods based on the Caffe deeplearning framework, and fine-tune from Caffe-provided models of AlexNet [15] pre-trained on ImageNet. We add a
bottleneck layer between the f c7 and f c8 layers as RevGrad
[7] except for the task ImageNet 1000 → Caltech 84 since
the pre-trained model is trained on ImageNet dataset and it
can fully exploit the advantage of pre-trained model with the
original f c7 and f c8 layer. For SAN, we fine-tune all the
feature layers and train the bottleneck layer, the classifier
layer and the adversarial networks. Since these new layers
and networks are trained from scratch, we set their learning
rate to be 10 times that of the other layers. We use minibatch stochastic gradient descent (SGD) with momentum of
0.9 and the learning rate annealing strategy implemented in
RevGrad [7]: the learning rate is adjusted during SGD using
η0
the following formula: ηp = (1+αp)
β , where p is the training
progress linearly changing from 0 to 1, η0 = 0.001, α = 10
and β = 0.75, which is optimized for low error on the source
domain. As SAN can work stably across different transfer
tasks, the penalty of adversarial networks is increased from
0 to 1 gradually as RevGrad [7]. All the hyper-parameters of

Avg
66.98
53.39
61.80
71.56
75.60
79.44
75.97
85.83

ImageNet-Caltech
I 1000 → C 84 C 256 → I 84
52.37
47.35
54.21
52.03
51.34
47.02
63.69
50.45
64.20
51.55
66.78
51.25
55.27
52.31
68.45
55.61

Avg
49.86
53.12
49.18
57.07
57.88
59.02
53.79
62.03

the learning rate and penalty strategies are selected through
standard cross-validation on the labeled source data.

4.2. Results
The classification results on the six tasks of Office-31, the
three tasks of Caltech-Office and the two tasks of ImageNetCaltech are shown in Table 1 and 2. The SAN model
outperforms all comparison methods on all the tasks. In
particular, SAN substantially improves the accuracy by huge
margins on tasks with small source domain and small target
domain, e.g. A 31 → W 10 , A 31 → D 10, and tasks
with large source domain and small target domain, e.g. C
31 → W 10. And it achieves considerable accuracy gains
on tasks with large-scale source domain and target domain,
e.g. I 1000 → C 84. These results suggest that SAN can
learn transferable features for partial transfer learning in all
the tasks under the setting where the target label space is a
subspace of the source label space.
The results reveal several interesting observations. (1)
Previous deep transfer learning methods including those
based on adversarial-network like RevGrad and those based
on MMD like DAN perform worse than standard AlexNet,
which demonstrates the influence of negative transfer effect.
These methods try to transfer knowledge from all classes
of source domain to target domain but there are classes in
source domain that do not exist in the target domain, a.k.a.

2729

Table 3. Classification Accuracy (%) of Partial Transfer Learning Tasks on Office-31 (VGG-16 as Base Network)

Method
VGG [27]
DAN [16]
RevGrad [7]
RTN [18]
ADDA [29]
SAN

A 31→W 10
60.34
58.78
50.85
69.35
72.85
83.39

D 31→W 10
97.97
85.86
95.23
98.42
98.42
99.32

W 31→D 10
99.36
92.78
94.27
99.59
99.59
100.00

A 31→D 10
76.43
54.76
57.96
75.43
77.96
90.70

D 31→A 10
72.96
55.42
51.77
81.45
84.77
87.16

W 31→A 10
79.12
67.29
62.32
82.98
85.32
91.85

Avg
81.03
69.15
68.73
84.54
86.49
92.07

outlier source data. Fooling the adversarial network to match
the distribution of outlier source data and target data will
make the classifier more likely to classify target data in these
outlier classes, which is prone to negative transfer. Thus
these previous methods perform even worse than standard
AlexNet. However, SAN outperforms them by large margins,
indicating that SAN can effectively avoid negative transfer
by eliminating the outlier source classes irrelevant to target
domain. (2) RTN performs better than AlexNet because it
executes entropy minimization criterion which can avoid the
impact of outlier source data to some degree. But comparing
RTN with SAN-selective which only has entropy minimization loss, we observe that SAN-selective outperforms RTN
in most tasks, demonstrating that RTN also suffers from
negative transfer effect and even the residual branch of RTN
cannot learn the large discrepancy between source and target
domain. (3) ADDA first learns a discriminative representation using the labels in the source domain and then a separate
encoding that maps the target data to the same space using an
asymmetric mapping learned through a domain-adversarial
loss. By combining discriminative modeling, untied weight
sharing, and a GAN loss, ADDA yields much better results
than RevGrad and RTN. SAN outperforms ADDA in all the
tasks, proving that our selective adversarial mechanism can
jointly promote positive transfer from relevant source domain data to target domain and circumvent negative transfer
from outlier source domain data to target domain. As a reference, the Upper Bound performance is achieved by manually removing the outlier classes (not in the target domain)
from the source domain. We apply this to Office-31 dataset.
As shown in Table 1, our SAN performs 6.71% worse than
the upper bound while best baseline ADDA 12.56% worse.

probability of predicting each point to irrelevant classes especially when there are a large number of irrelevant classes,
which can in turn boost the performance of the selective
adversarial mechanism. This explains the improvement from
SAN-entropy to SAN.
By going even deeper with convolutions, the very deep
convolutional networks have made breakthroughs in achieving new state of the art results in ImageNet Large-Scale
Visual Recognition Challenge [24]. Although the transferability of AlexNet features has been extensively quantified
[33], it remains unclear whether very deep neural networks
can learn more transferable features and how the feature
transferability may change with the depths of very deep networks. In this paper, we approach this goal by evaluating the
all methods based on VGG-16 network [27]. From Table 3,
we can observe that SAN outperforms all the other methods on VGG-16 network, which demonstrates that SAN can
generalize to different base networks.

We go deeper into different modules of SAN by comparing the results of SAN variants in Tables 1 and 2. (1)
SAN outperforms SAN-selective, proving that using selective adversarial mechanism can selectively transfer knowledge from source data to target data. It can successfully
select the source data belonging to the classes shared with
target classes by the corresponding domain discriminators.
(2) SAN outperforms SAN-entropy especially in tasks where
source and target domains have very large distribution gap
in terms of the different numbers of classes, e.g. I 1000 →
C 84. Entropy minimization can effectively decrease the

Accuracy for Different Numbers of Target Classes:
We investigate a wider spectrum of partial transfer learning by varying the number of target classes. Figure 3(a)
shows that when the number of target classes decreases, the
performance of RevGrad degrades quickly, meaning that
negative transfer becomes severer when the domain gap is
enlarged. The performance of SAN degenerates when the
number of target classes decreases from 31 to 20, where negative transfer problem arises but the transfer problem itself
is still hard; the performance of SAN increases when the
number of target classes decreases from 20 to 10, where the

100

RevGrad
SAN

80

Test Error

Accuracy

90

70
60
50
40

31 30

25

20

15

10

Number of Target Classes

(a) Accuracy w.r.t #Target Classes

0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
500

SAN
DAN
RTN
RevGrad
AlexNet

3000

6000

9000

12000

15000

Number of Iterations

(b) Test Error

Figure 3. Empirical analysis of SAN: (a) transfer performance vs.
#target labels, and (b) test error on target domain vs. #iterations.

4.3. Analysis

2730

30

20

40

40

40

30

30

30

20

20

10

10

0

0

-10

-10

20

10

10

0

0

-10

-20

-30
-40

source1
source2
source3
source4
source5
target1
target2
target3
target4
target5

-20

-20

0

20

40

-30
-40

(a) DAN

-10

-20

-20

0

20

40

60

-30
-40

-20

(b) RevGrad

0

20

40

-20
-40

(c) RTN

-20

0

20

(d) SAN

Figure 4. The t-SNE visualization of DAN, RevGrad, RTN, and SAN with class information (10 classes).
30

40

40

40

20

30

30

30

20

20

20

10

10

0

0

source
target

10

10

0

0
-10

-10
-10

-10

-20

-20

-30

-30

-20
-20
-30
-40
-60

-40

-20

0

(a) DAN

20

40

-40
-40

-20

0

20

40

60

-30
-40

-40
-40

-20

(b) RevGrad

0

20

40

-50

-50

(c) RTN

0

50

(d) SAN

Figure 5. The t-SNE visualization of DAN, RevGrad, RTN, and SAN with domain information (31 classes).

transfer problem itself becomes easier. The margin that SAN
outperforms RevGrad becomes larger when the number of
target classes decreases. SAN also outperforms RevGrad in
standard transfer learning setting when the number of target
classes is 31.
Convergence Performance: We examine the convergence of SAN by studying the test error through training
process. As shown in Figure 3(b), the test errors of DAN
and RevGrad are increasing due to negative transfer. RTN
converges very fast depending on the entropy minimization,
but converges to a higher test error than SAN. SAN converges fast and stably to a lowest test error, meaning it can
be trained efficiently and stably to enable positive transfer
and alleviate negative transfer simultaneously.
Feature Visualization: We visualize the t-SNE embeddings [5] of the bottleneck representations by DAN,
RevGrad, RTN and SAN on transfer task A 31 → W 10 in
Figures 4(a)–4(d) (with class information) and Figures 5(a)–
5(d) (with domain information). We randomly select five
classes in the source domain not shared with target domain
and five classes shared with target domain. We can make intuitive observations. (1) Figure 4(a) shows that the bottleneck
features are mixed together, meaning that DAN cannot discriminate both source and target data very well; Figure 5(a)
shows that the target data are aligned to all source classes
including those outlier ones, which embodies the negative
transfer issue. (2) Figures 4(b)– 4(c) show that both RevGrad
and RTN discriminate the source domain well but the features of most target data are very close to source data even to

the wrong source classes; Figures 5(b)– 5(c) further indicate
that both RevGrad and RTN tend to draw target data close to
all source classes even to those not existing in target domain.
Thus, their performance on target data degenerates due to
negative transfer. (3) Figures 4(d) and 5(d) demonstrate
that SAN can discriminate different classes in both source
and target while the target data are close to the right source
classes, while the outlier source classes cannot influence the
target classes. These results demonstrate the efficacy of both
selective adversarial adaptation and entropy minimization.

5. Conclusion
This paper presented a novel selective adversarial network approach to partial transfer learning. Unlike previous
adversarial adaptation methods that match the whole source
and target domains based on the shared label space assumption, the proposed approach simultaneously circumvents
negative transfer by selecting out the outlier source classes
and promotes positive transfer by maximally matching the
data distributions in the shared label space. Our approach
successfully tackles partial transfer learning where source
label space subsumes target label space, which is testified by
extensive experiments.

6. Acknowledgements
This work was supported by the National Key R&D Program of China (No. 2017YFC1502003), the National Natural Science Foundation of China (No. 61772299, 71690231,
61502265), and Tsinghua TNList Laboratory Key Project.

2731

References
[1] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,
and J. W. Vaughan. A theory of learning from different domains. MLJ, 79(1-2):151–175, 2010. 2
[2] Y. Bengio, A. Courville, and P. Vincent. Representation
learning: A review and new perspectives. TPAMI, 35(8):1798–
1828, 2013. 2
[3] P. P. Busto and J. Gall. Open set domain adaptation. In The
IEEE International Conference on Computer Vision (ICCV),
volume 1, page 3, 2017. 1
[4] R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. JMLR, 12:2493–2537, 2011.
2
[5] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014. 1,
8
[6] L. Duan, I. W. Tsang, and D. Xu. Domain transfer multiple
kernel learning. TPAMI, 34(3):465–479, 2012. 2
[7] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. S. Lempitsky. Domainadversarial training of neural networks. Journal of Machine
Learning Research, 17:59:1–59:35, 2016. 1, 2, 3, 5, 6, 7
[8] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for
large-scale sentiment classification: A deep learning approach.
In ICML, 2011. 2
[9] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow
kernel for unsupervised domain adaptation. In CVPR, 2012.
2, 5
[10] Y. Grandvalet and Y. Bengio. Semi-supervised learning by
entropy minimization. In NIPS, pages 529–536, 2004. 4
[11] A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann,
S. Balakrishnan, M. Pontil, and K. Fukumizu. Optimal kernel
choice for large-scale two-sample tests. In NIPS, 2012. 6
[12] G. Griffin, A. Holub, and P. Perona. Caltech-256 object
category dataset. Technical report, California Institute of
Technology, 2007. 5
[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016. 5
[14] J. Hoffman, S. Guadarrama, E. Tzeng, R. Hu, J. Donahue,
R. Girshick, T. Darrell, and K. Saenko. LSDA: Large scale
detection through adaptation. In NIPS, 2014. 2
[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classification with deep convolutional neural networks. In
NIPS, 2012. 5, 6
[16] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning transferable features with deep adaptation networks. In ICML,
2015. 1, 2, 3, 5, 6, 7
[17] M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer
feature learning with joint distribution adaptation. In ICCV,
2013. 5
[18] M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks. In Advances
in Neural Information Processing Systems, pages 136–144,
2016. 1, 2, 3, 5, 6, 7

[19] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009.
2
[20] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng.
Multimodal deep learning. In ICML, 2011. 2
[21] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and
transferring mid-level image representations using convolutional neural networks. In CVPR, June 2013. 2
[22] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. TNNLS, 22(2):199–
210, 2011. 2
[23] S. J. Pan and Q. Yang. A survey on transfer learning. TKDE,
22(10):1345–1359, 2010. 1, 2
[24] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,
and L. Fei-Fei. ImageNet Large Scale Visual Recognition
Challenge. 2014. 7
[25] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,
and L. Fei-Fei. ImageNet Large Scale Visual Recognition
Challenge. International Journal of Computer Vision (IJCV),
115(3):211–252, 2015. 5
[26] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual
category models to new domains. In ECCV, 2010. 2, 5
[27] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015
(arXiv:1409.1556v6), 2015. 5, 7
[28] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy
domain adaptation. In AAAI, 2016. 5
[29] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial
discriminative domain adaptation. In CVPR, 2017. 5, 6, 7
[30] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.
Deep domain confusion: Maximizing for domain invariance.
2014. 1, 2
[31] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell.
Simultaneous deep transfer across domains and tasks. In
ICCV, 2015. 1, 2, 3
[32] X. Wang and J. Schneider. Flexible transfer learning under
support and model shift. In NIPS, 2014. 2
[33] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In NIPS, 2014.
1, 2, 7
[34] K. Zhang, B. Schölkopf, K. Muandet, and Z. Wang. Domain
adaptation under target and conditional shift. In ICML, 2013.
2

2732

