2017 5th International Conference on Enterprise Systems

Fault Diagnosis Based on improved Deep Belief Network
Tianqi Yang

Shuangxi Huang

Department of Automation
Tsinghua University
Beijing, China
ytqthu@139.com

Department of Automation
Tsinghua University
Beijing, China
huangsx@mail.tsinghua.edu.cn
characteristics, it is a typical method to convert the signal
into frequency domain to analyze the amplitude spectrum
and power spectrum through fast Fourier transform (FFT) [6].
However, the non-stationary vibration signal in the actual
scene often does not accord with the stationarity assumption
of FFT leading to the effect of the diagnosis results [7]. Short
Time Fourier Transform (STFT), Wavelet Transform (WT),
Empirical Mode Decomposition (EMD) and other methods
have been proposed for signal time-frequency analysis to
solve the problem [8]. Variable selection is another difficulty
of feature extraction [9]. Because of the limitation of the
dimension in the traditional machine learning, it is necessary
to reduce the dimension of the feature or select the
characteristic information when the feature vector is
composed of a plurality of feature variables, leading to the
influence of subjective factor on the result of diagnosis [10].
Therefore, the new fault diagnosis method needs to extract
the fault feature adaptively, and get rid of the dependence on
the expert diagnosis and the complex signal processing.
In recent years, the rise of deep learning provides a new
way to solve the above problems [11]. As the predecessor of
deep learning, neural network is limited by the amount of
computation and the disappearance of the gradient in a long
period of time. In 2006, Hinton et al. successfully extracted
the image features using the Autoencoder (AE) [12], and
proposed the Deep Belief Network (DBN) [13]. Hinton et al.
summed up the training framework of unsupervised pretraining and fine-tuning, realizing the extraction of high
dimensional abstract features from deep network structure on
the one hand, overcoming the gradient fading and local
extremum problem of multilayer network structure through
layer by layer greedy training on the other hand, and deep
learning began to flourish. In the field of computer vision,
LeNet, AlexNet [14], GoogleNet [15], VGG [16] and other model
based on Convolutional Neural Network have proved to be
the mainstream of image recognition. In the field of Natural
Language Processing, Recurrent Neural Network (RNN) [17]
and LSTM [18], GRU and other models based on RNN
represent the latest results of the sequence analysis.
Nevertheless, the application of deep learning in the field of
equipment maintenance and fault diagnosis is still in the
exploratory stage, which has a very broad imagination for the
mining potential of deep data mining.
In the above background, this paper proposes an
improved deep belief network named Semi-DBN, and
applies this model to equipment fault diagnosis. This model
based on DBN and combined with Semi-Supervised
Learning and Transfer Learning Algorithms and applied
layer by layer greedy coding to achieve high level deep fault

Abstract—A fault diagnosis method based on improved Deep
Belief Network is proposed. This paper proposed an network
named Semi-DBN, which adapts an adaptive semi supervised
method to train the RBM structure to improve the
performance of DBN, replaces the Sigmoid activation function
with Rectified Linear Units to improve the performance of the
network, and combines the model with Transfer Learning to
solve the problem of lack of data. It can get rid of the
dependence of the traditional machine learning methods on the
extraction of the sample characteristics, and effectively
overcome the problem of gradient vanishing, local extremum
and so on. The identification and generalization ability of the
method is verified when we used it in the experiment of rolling
bearing data.
Keywords- DBN˗RBM˗Semi-supervised˗Rectified Linear
Units˗Transfer Learning

I.

INTRODUCTION (HEADING 1)

With the development of the social production, the
complication of the production equipment, and the growth of
the data scale, the Condition Based Maintenance has turned
to be the effective method to improve he production
efficiency and strengthen the production safety [1] Adopting
machine learning and other intelligent means to fully tap the
inherent information of production data to realize the databased fault diagnosis is the important foundation of
Condition Based Maintenance [2]. Machine learning has
become the mainstream of intelligent fault diagnosis
recently. The equipment fault diagnosis is essentially a
classification problem of equipment operation mode,
consisted of two main steps: feature extraction and
classification. A typical health monitoring and fault
diagnosis system usually includes signal acquisition, feature
extraction, and fault recognition module.
For the problem of bearing fault diagnosis, usually, the
original monitoring data is about the vibration acceleration or
vibration displacement of the transmission mechanism [3].
The performance of the feature affects the result of fault
diagnosis to a great extent. Signal processing is needed to
extract the fault features as the time domain signal is often
difficult to directly reflect the fault information [4]. The
difficulty of feature extraction shows in complex signal
processing. Peak, kurtosis, RMS, peak factor, margin factor
are common characteristics of the time domain, but the
expression of the fault information is shallow, and the ability
to resist disturbance is limited, therefore the diagnosis results
are often not ideal [5]. In view of the fact that the vibration
signal has obvious periodic characteristics and frequency
2572-6609/17 $31.00 © 2017 IEEE
DOI 10.1109/ES.2017.57

305

feature mining, finally significantly improve the fault
recognition ability and generalization ability. Compared with
the traditional machine learning methods, the proposed
method integrates the two steps of feature extraction and
state classification, and gets rid of the dependence on the
experience and the complex operation, it can effectively
overcome the problem of gradient vanishing, local extremum
and so on, able to adapt to complex diagnostic requirements.
II.

B. Deep belief networks
When RBMs are stacked to form a deep belief
network, a generative model consisting of many layers the
real power emerges. In a DBN, each layer comprises a set of
binary or real-valued units. Two adjacent layers have a full
set of connections between them, but no two units in the
same layer are connected. Hinton et al. (2006) proposed an
efficient algorithm for training deep belief networks, by
greedily training each layer (from lowest to highest) as an
RBM using the previous layer’s activations as inputs. This
procedure works well in practice. It includes the forward
learning from the bottom to the top and the backward
trimming learning from the top to the bottom, while
handling the details of the data set in the lower level, and
dealing with the attribute, class information of the data set
and so on. It is abstract by layer by layer from low level to
high level, so as to dig the essential characteristics of data
deeply.
The forward stack learning is unsupervised learning,
the parameters are obtained by the maximum likelihood
method according to the function (6), and the maximum
value of the likelihood function is obtained by the stochastic
gradient ascent method. However, it is difficult to obtain
unbiased samples in practical applications, as well to solve
parameters, therefore, we generally use CD (Contrastive
Divergence) fast algorithm to get a better estimate
of P (v, h) . Firstly, calculate the conditional probability of
the unit in the hidden layer according to function (3), and
use Gibbs sampling to determine the probability of hidden
units. Secondly, calculate the conditional probability of the
unit in the visual layer according to function (4) and use
Gibbs sampling to determine the state of visual units. The
update function for parameter θ is as (7) to (9):

DEEP BELIEF NETWORK

Deep Belief Network is formed of many RBMs, it is a
deep learning model effective for sequential data processing.
[22]

A. Restricted Boltzmann machines
The restricted Boltzmann machine (RBM) is a two layer,
bipartite, undirected graphical model with a set of binary
hidden units h, a set of (binary or real-valued) visible units v,
and symmetric connections between these two layers
represented by a weight matrix W. The probabilistic
semantics for an RBM is defined by its energy function as
follows:

P (v, h) = exp( − E(v, h)) / Z(θ ) ⋅ ⋅⋅ ⋅ ⋅ ⋅⋅ ⋅ ⋅ ⋅⋅ ⋅ ⋅ ⋅⋅ ⋅ ⋅ ⋅⋅ ⋅ ⋅ ⋅(1)
Where Z(θ ) is the partition function. If the visible units

are binary-valued, we define the energy function as:

E(v, h) = −¦ viWijhj − ¦ bjhj − ¦ civi ⋅⋅⋅⋅⋅⋅⋅⋅⋅ (2)
i, j

j

i

And the Conditional probability distribution of visible
units and hidden units are:

P (h j = 1| v;θ ) = λ (¦ Wijvj + bi ) ⋅⋅⋅⋅(3)
i =1

P (vi = 1| h;θ ) = λ (¦ Wijhj + ci ) ⋅⋅⋅⋅(4)

L(θ ; v) = ∏ v L(θ | v) = ∏ v P(v) ⋅⋅⋅ (6)

i =1

Where bj are hidden unit biases and ci are visible unit
biases, and λ (x) = 1/ (1 + exp(x) ) . If the visible units

Δwij = ε (< vi h j > P (h|v) − < vi h j > recon ) ⋅⋅⋅ (7)

Δbi = ε (< vi > P (h|v) − < vi > recon ) ⋅⋅⋅ (8)

are real-valued, we can define the energy function as:

E(v, h) =

1
viWijhj − ¦ bjhj − ¦ civi ⋅⋅⋅ (5)
¦ vi 2 − ¦
2 i
i, j
j
i

Δa j = ε (< h j > P (h|v) − < h j > recon ) ⋅⋅⋅ (9)
Where

From the energy function, it is clear that the hidden units are
conditionally independent of one another given the visible
layer, and vice versa. In particular, the units of a binary
layer (conditioned on the other layer) are independent
Bernoulli random variables. If the visible layer is realvalued, the visible units (conditioned on the hidden layer)
are Gaussian with diagonal covariance. Therefore, we can
perform efficient block Gibbs sampling by alternately
sampling each layer’s units (in parallel) given the other
layer. We will often refer to a unit’s expected value as its
activation.

ε

is learning rate and

< • > P (h|v)

is the limit

of partial derivative function under P (v | h) distribution
and < • > recon is the limit of partial derivative function
under the distribution of reconstructed model.
Backward fine-tuning learning is needed after the
forward stack RBM learning, the model parameters, which
are learned from unsupervised learning, are used as the
initial value of supervised learning, which provide a priori
knowledge. From the last layer of the DBN, fine tune the
model parameters according to the label from the high to the
end, and finally use the Softmax Classisfier to classify the
extracted information. Assuming that the DBN network

306

2. P ( Xs ) ≠ P ( Xt ) , where the marginal probability
distribution of source domain and target domain is different.
3. YS ≠ YT , where the label space of two tasks is
different.
4. P (Ys | Xs ) ≠ P (Yt | Xt ) , where the conditional
probability distribution of source task and target task is
different.

l RBM layers, the initial sample is x , then we can
l
define the final output vector u ( x ) as:
u l ( x) = 1 / [1 + exp)(bl + wl u l −1 ( x))] ⋅⋅⋅⋅(10)
The probability of the sample xi belonging to category
yi after learning from the l RBM layers is:

contains

p( yi = k | u l ( xi ),V l , cl ) = eV k u ( xi )+c / ¦ k =1 eV k u ( xi )+ c ⋅⋅⋅⋅(11)
l

l

c

l

l

l

l

l

l

A. Improved RBM
The difficulty of deep network embodies in the training
process of the network, the training parameters have a
decisive influence on the model. We proposed adaptive semi
supervised method to train the RBM structure to improve the
performance of DBN, by which we can adaptive adjustment
learning rate in the training process of each RBM structure,
and input the sample and the same sample with noise into the
RBM, minimize the error between the outputs after
unsupervised training to optimize parameters. In traditional
DBN, the network parameters are the propagation of errors
will decrease with the increase of depth, and lack of antinoise capability. We can improve the performance of the
forward stack training stage as follow:
First, adaptive adjustment the learning rate according to
the reconstruction error before and after, define the
difference between the limit of partial derivative function

l

1 m c
eV k u ( xi ) + c
J (λ ) = − [¦¦1{ yi = k}lg c V l u l ( x ) + cl ] ⋅⋅⋅⋅(12)
m i =1 k =1
¦ek i
l

k

The gradient descent method is used to optimize the
error, the following function is used:

∇ λ l J (λ l ) = −
~

^
1 m l ^
[u ( xi )(1{ yi = k}) − hl ( xi ))] ⋅⋅⋅⋅(13)
¦
m i =1

λ l = λ l − α∇ λ J (λ l ) ⋅⋅⋅⋅(14)
Where α is the learning rate, we can adjust parameter

from the first RBM layer to the last one.
III.

SEMI-DBN

IV.

Where V is the parameter coefficient.
The category of decision is the class with the maximum
probability value, and the error function is:

TRANSFER LEARNING

<•>

P (h|v)
under P (v | h) distribution
and the limit of
partial derivative function under the distribution of
reconstructed model < • > recon as:

A. Transfer learning
First, confirm that you have the correct template for your
paper size. This template has been tailored for output on the
US-letter paper size. If you are using A4-sized paper, please
close this template and download the file for A4 paper
format called “CPS_A4_format”.
Data mining and machine learning technologies have
already achieved significant success in many knowledge
engineering areas including classification, regression, and
clustering [23]. However, many machine learning methods
work well only under a common assumption: the training
and test data are drawn from the same feature space and the
same distribution. When the distribution changes, most
statistical models need to be rebuilt from scratch using newly
collected training data. In many real-world applications, it is
expensive or impossible to recollect the needed training data
and rebuild the models. It would be nice to reduce the need
and effort to recollect the training data. In such cases,
knowledge transfer or transfer learning between task
domains would be desirable [24]. Given source domain Ds
and target domain Dt , where D = { X , P ( X )} , and the

Ω =< • > P (h|v) − < • > recon ⋅⋅⋅⋅⋅⋅ (15)
Update the learning rate as:

­(1 + λ )ε
¯(1 − λ )ε

if (Ω > 0)

ε =®

if (Ω < 0)

⋅⋅⋅ (16)

Where λ is the
adjustment coefficient of
learning rate, and update the parameters as:

Δwijt = (1− γ )ε t (< vi hj >t P(h|v) − < vi hj >t recon ) + γε t −1Δwijt −1 ⋅⋅⋅ (17)

Δbit = (1− γ )ε t (< vi >t P(h|v) − < vi >t recon ) + γε t −1Δbit −1 ⋅⋅⋅ (18)
Δa j t = (1− γ )ε t (< hj >t P(h|v) − < hj >t recon ) + γε t −1Δa j t −1 ⋅⋅⋅ (19)

Where γ is inertial coefficient of the renewal process,
which can reduce the shock of learning process.
Second, as seen in figure 1, input the sample and the
same sample with noise into the RBM and minimize the
error between the outputs after unsupervised training to
optimize parameters. We define the input sample as xi and

source task Ts and target task Tt , where
T = {Y , P(Y | X )} , Transfer Learning works on the following
four situations:
1. XS ≠ XT , where the feature space of source
domain and target domain is different.

~

the noised sample as

xi = xi + σ , where σ is random
~

noise, and define the output as

307

yi and yi , and define the

~

difference of the output as:

¦ ( yi − yi )2 . Use the back

model adapt to new domain, which make sense in visual or
text processing.
As seen in figure 3, we combined Transfer Learning with
DBN due to lack of data in this paper. As the vibration signal
follows a certain law and has a strong correlation with the
data before and after, we can use LSTM to get synthetic
dataset to extend data set for training. The extended dataset
belongs to the same feature space as the original data but
doesn’t follow the same marginal probability distribution,
although the gap will decrease if the prediction model gets
better. With the help of Transfer Learning, we can get the
DBN network more fully trained.

i

propagation algorithm to optimal the parameters of the
RBM.

Figure 1. add noise for RBM trainigd

B. Rectified Linear Units
We replaced the traditional Sigmoid activation function
with Rectified Linear Units (Relu) in hidden layers to
improve the performance of the network. We can see the
contrast between Relu and Sigmoid from the figure2. On the
one hand Relu won’t get saturated with the increase of x
when x is positive, which helps to improve deep learning
multilayer network, on the other hand Relu get the hidden
layer a more sparse representation as it goes to zero when x
turns negative, which provides better performance at zero
and helps accelerate the training speed of the network.

Figure 3. rocess for Transfer Learning

V.

APPLICATION

A. Experiment data
We used the improved DBN network to process the data
provided by Case Western Reserve University in America
[25]
. Multi stage transmission mechanism is used as the
platform, and SKF6205-2RS rolling bearing is chosen as the
research object. The vibration acceleration signal is collected
in the motor box, and the performance of the vibration data
under different bearing failure modes is studied. In this
experiment, the artificial damage of the inner ring, rolling
body and outer ring of the bearing is simulated by means of
electric discharge machining (EDM) to simulate the bearing
failure of different positions and different degrees. We
selected drive end vibration signal of 1 horsepower load,
1750 rpm, and the sampling frequency is 12kHz. In this
condition, each of the 420 sampling points can contain
information about the rotation of the bearing on average, and
every 450 sampling point can be selected as a sample
sequence while take the fluctuation into consideration. In this
paper, the normal state and the other 9 kinds of working
states of failure modes are selected. For each state, the
samples of 3/4 were randomly selected as the training set,
and the remaining 1/4 samples were used as the test set.
Table 1 describes the sample data used in this paper.
We used the improved DBN described above as the
training model, time domain data of 450 dimensional as

Figure 2. curve for Relu and Sigmoid

C. DBN with Transfer Learning
We can apply Transfer Learning to many situations. For
example, it is difficult to collect data or lack of data in real
world generally, while learning from simulation makes it
easier to collect data. And Transfer Learning can help the

308

empirical mode decomposition (EMD), then get the envelope
spectrum by Hilbert transform, finally gets the 12 envelope
spectrum characteristic value as the input of SVM. We can
see from the table below, Semi-DBN can adaptively detect
the fault features, and has stronger diagnostic ability and
generalization performance.

input and the coding of ten kinds of categories as output. As
for the model architecture, there were one visual layer, four
hidden layers and one output layer which adapts Softmax as
classifier (450-800-500-300-50-10). As for the training set,
we used the original training data to get the synthetic dataset
with label, which could also be used to train the model.
TABLE I.

stat
e
No
rm
IF0
7
IF1
4
IF2
1
BF0
7
BF1
4
BF2
1
OF
3
OF
6
OF
12

tot
al

DESCRIPTION OF THE ORIGINAL SAMPLE DATA

fault
locatio
n
/

diam
eter

Valid
ation
size
90

Testin
g
size
90

categ
ory

/

Traini
ng
size
250

Inner
ring
Inner
ring
Inner
ring
Ball

7

140

40

40

1

14

140

40

40

2

21

140

40

40

3

7

140

40

40

4

Ball

14

140

40

40

5

Ball

21

140

40

40

6

0

Figure 4. structure for improved DBN
TABLE II.

Outer
ring at
3 clock
Outer
ring at
6 clock
Outer
ring at
12
clock
/

7

140

40

40

7

model

7

140

40

40

8

Semi-DBN

7

/

140

1570

40

450

40

450

9

/

DIAGNOSTIC PERFORMANCE

average
accuracy for
validation
100%

average
accuracy for
test
100%

DBN

100%

98.6%

BP

73.21%

65.89%

PCA+SVM

/

47.81%

EMD+SVM

/

94.32%

VI.

CONCLUSION

In this paper, a fault diagnosis method based on
improved DBN is proposed, which is applied to fault
diagnosis, and the result shows:
1. The proposed method can realize the adaptive
extraction and dimensionality reduction of the fault feature,
and get rid of the dependence of the expert diagnosis
experience and the complex signal processing, realize
feature extraction and classification in one single step.
2. The proposed method improves the original DBN
successfully and has stronger diagnostic ability and
generalization performance.
3. Depth learning has a better performance in fault
diagnosis compared to traditional method.
.

B.

Model architecture
In order to reduce the random effects, 10 diagnostic tests
were repeated and the diagnostic performance was compared
with other classical methods. In this paper, the original DBN
and three other diagnostic models were selected as control
experiment. The traditional DBN had the same structure with
the Semi-DBN we used, but used Sigmoid as active function,
didn’t adapt the improved training method for RBM or
combine with Transfer Learning. The BP model is the
traditional DBN without pre-training. The PCA+SVM
method selected ten feature components as input of SVM by
principal component analysis, EMD+SVM method to
improve the feature extraction, the original signal is
decomposed into several intrinsic mode functions by

309

ACKNOWLEDGMENT

[9]

I would like to express my gratitude to all those who
helped me during the writing of this thesis.
My deepest gratitude goes first and foremost to Ding, the
director of the China Special Equipment Inspection Institute
for his constant generous help. Without his consistent and
illuminating instruction, this thesis could not have reached
its present form.
Second, I would like to express my heartfelt gratitude
to the colleagues of the Whole Life Cycle of Large Complex
Lifting Equipment Data Integration Management and
Application Technology Research Project, who helped a lot
for my work.
Last my thanks would go to my beloved family and
mentor for their loving considerations and great confidence
in me all through these years. I also owe my sincere gratitude
to my friends and my fellow classmates who gave me their
help and time in listening to me and helping me work out my
problems during the difficult course of the thesis..

[10]

[11]
[12]
[13]
[14]

[15]

[16]

[17]

REFERENCES
[1]

[2]
[3]
[4]

[5]

[6]

[7]

[8]

Oppenheimer, C.H., Loparo, K.A. Physically based diagnosis and
prognosis of cracked rotor shafts[J]. Component and Systems
Diagnostics, Prognostics, and Health Management II, 2002, 4733:
122-132.
Qiu, J., Zhang,C., Seth, B.B., Liang, S.Y. Damage mechanics
approach for
baring lifetime prognostics[J]. Mechanical Systems and Signal
Processing, 2002, 16(5): 817-829.
Luo, J., Namburu, M., Pattipati, K., Liu, Q., Kawamoto, M., Chigusa,
S. Model-based prognostic techniques [maintenance applications][J].
Proceedings of AUTOTESTCON, IEEE Systems Readiness
Technology Conference, 2003: 330-340.
Kacprzynski, G.J., Sarlashkar, A., Roemer, M.J., Hess, A.,
Hardman, B. Predicting remaining life by fusing the physics of
failure modeling with diagnostics[J]. Journal of Metal, 2004, 56(3):
29-35.
Biagetti, T., Sciubba. E. Automatic diagnostics and prognostics of
energy conversion processes via knowledge based systems[J].
Energy, 2004, 29(12-15): 2553-2572.
Tran, V.T., Yang, B.S. Machine fault diagnosis and condition
prognosis using classification and regression trees and neurofuzzy inference systems[J]. Control and Cybernetics, 2010, 39(1):
25-54.
Rao, B.K.N., Pai, P.S., Nagabhushana, T.N. Failure diagnosis
and prognosis of rolling-element bearings using artificial neural
networks: a critical overview[J]. Journal of Physics: Conference
Series, 2012, 364(1): 012023.

[18]

[19]
[20]
[21]

[22]

[23]
[24]

[25]

310

Caesarendraa, W., Widodob, A., Yang, B.S.
Application of
relevance vector machine and logistic regression for machine
degradation assessment[J]. Mechanical Systems and Signal
Processing, 2010, 24(4): 1161-1171.
Bunks, C., McCarthy, D., Al-Ani, T., Condition-based maintenance
of machines using hidden Markov models[J]. Mechanical Systems
and Signal Processing, 2000, 14(4): 597-612.
Bengio Y, Goodfellow I J, Courville A. Deep learning[J]. Nature,
2015, 521: 436-444.
Hinton G E, Salakhutdinov R R. Reducing the Dimensionality of
Data with Neural Networks[J]. Science, 2006, 313(5786):504.
Hinton G E, Osindero S, Teh Y W. A fast learning algorithm for deep
belief nets[J]. Neural computation, 2006, 18(7): 1527-1554.
Simonyan K, Zisserman A. Very deep convolutional networks for
large-scale image recognition[J]. arXiv preprint arXiv:1409.1556,
2014..
Szegedy C, Liu W, Jia Y, et al. Going deeper with
convolutions[C]//Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 2015: 1-9.
Simonyan K, Zisserman A. Very deep convolutional networks for
large-scale image recognition[J]. arXiv preprint arXiv:1409.1556,
2014.
Mikolov T, Karafiát M, Burget L, et al. Recurrent neural network
based language model[C]//Interspeech. 2010, 2: 3.
Graves A, Schmidhuber J. Framewise phoneme classification with
bidirectional LSTM and other neural network architectures[J]. Neural
Networks, 2005, 18(5): 602-610.
LeCun Y, Bengio Y, Hinton G. Deep learning[J]. Nature, 2015,
521(7553): 436-444.
Bengio Y. Learning deep architectures for AI[J]. Foundations and
trends® in Machine Learning, 2009, 2(1): 1-127.
Bengio Y, Lamblin P, Popovici D, et al. Greedy layer-wise training of
deep networks[C]// Advances in Neural Information Processing
Systems 19, Proceedings of the Twentieth Conference on Neural
Information Processing Systems, Vancouver, British Columbia,
Canada, December. DBLP, 2007:153-160.
Lee H, Grosse R, Ranganath R, et al. Convolutional deep belief
networks for scalable unsupervised learning of hierarchical
representations[C]//Proceedings of the 26th annual international
conference on machine learning. ACM, 2009: 609-616.
Pan S J, Yang Q. A survey on transfer learning[J]. IEEE Transactions
on knowledge and data engineering, 2010, 22(10): 1345-1359.
Raina R, Battle A, Lee H, et al. Self-taught learning: transfer learning
from unlabeled data[C]//Proceedings of the 24th international
conference on Machine learning. ACM, 2007: 759-766.
Bearing data center. Case Western Reserve University[EB/OL].
[2016-12-31].
Available:
http://csegroups.case.edu/bearingdatacenter/pages/download-data-file

