2017 IEEE Conference on Computer Vision and Pattern Recognition

Borrowing Treasures from the Wealthy: Deep Transfer Learning through
Selective Joint Fine-Tuning
Weifeng Ge
Yizhou Yu
Department of Computer Science, The University of Hong Kong
Abstract

Large-scale image datasets, such as the ImageNet ILSVRC
dataset [36], Places [50], and MS COCO [23], have led to
a series of breakthroughs in visual recognition, including
image classiﬁcation [24], object detection [10], and semantic segmentation [25]. Many other related visual tasks have
beneﬁted from these breakthroughs.
Nonetheless, researchers face a dilemma when using
deep convolutional neural networks to perform visual tasks
that do not have sufﬁcient training data. Training a deep
network with insufﬁcient data might even give rise to inferior performance in comparison to traditional classiﬁers fed
with handcrafted features. Fine-grained classiﬁcation problems, such as Oxford Flowers 102 [29] and Stanford Dogs
120 [19], are such examples. The number of training samples in these datasets is far from being enough for training
large-scale deep neural networks, and the networks would
become overﬁt quickly.
Solving the overﬁtting problem for deep convolutional
neural networks on learning tasks without sufﬁcient training data is challenging [39]. Transfer learning techniques
that apply knowledge learnt from one task to other related
tasks have been proven helpful [30]. In the context of deep
learning, ﬁne-tuning a deep network pre-trained on the ImageNet or Places dataset is a common strategy to learn taskspeciﬁc deep features.This strategy is considered a simple
transfer learning technique for deep learning. However,
since the ratio between the number of learnable parameters
and the number of training samples still remains the same,
ﬁne-tuning needs to be terminated after a relatively small
number of iterations; otherwise, overﬁtting still occurs.
In this paper, we attempt to tackle the problem of training
deep neural networks for learning tasks that have insufﬁcient training data. We adopt the source-target joint training
methodology [45] when ﬁne-tuning deep neural networks.
The original learning task without sufﬁcient training data
is called the target learning task, T t . To boost its performance, the target learning task is teamed up with another
learning task with rich training data. The latter is called
the source learning task, T s . Suppose the source learning
task has a large-scale training set D s , and the target learning task has a small-scale training set D t . Since the target

Deep neural networks require a large amount of labeled
training data during supervised learning. However, collecting and labeling so much data might be infeasible in many
cases. In this paper, we introduce a deep transfer learning scheme, called selective joint ﬁne-tuning, for improving the performance of deep learning tasks with insufﬁcient
training data. In this scheme, a target learning task with
insufﬁcient training data is carried out simultaneously with
another source learning task with abundant training data.
However, the source learning task does not use all existing
training data. Our core idea is to identify and use a subset
of training images from the original source learning task
whose low-level characteristics are similar to those from
the target learning task, and jointly ﬁne-tune shared convolutional layers for both tasks. Speciﬁcally, we compute
descriptors from linear or nonlinear ﬁlter bank responses
on training images from both tasks, and use such descriptors to search for a desired subset of training samples for
the source learning task.
Experiments demonstrate that our deep transfer learning scheme achieves state-of-the-art performance on multiple visual classiﬁcation tasks with insufﬁcient training
data for deep learning. Such tasks include Caltech 256,
MIT Indoor 67, and ﬁne-grained classiﬁcation problems
(Oxford Flowers 102 and Stanford Dogs 120). In comparison to ﬁne-tuning without a source domain, the proposed method can improve the classiﬁcation accuracy
by 2% - 10% using a single model. Codes and models are available at https://github.com/ZYYSzj/
Selective-Joint-Fine-tuning.

1. Introduction
Convolutional neural networks (CNNs) have become
deeper and larger to pursue increasingly better performance
on classiﬁcation and recognition tasks [22, 18, 40, 12, 46].
Looking at the successes of deep learning in computer
vison, we ﬁnd that a large amount of training or pretraining data is essential in training deep neural networks.
1063-6919/17 $31.00 © 2017 IEEE
DOI 10.1109/CVPR.2017.9

10

more robustly to generate highly discriminative features for
the target learning task.
The proposed source-target selective joint ﬁne-tuning
scheme is easy to implement. Experimental results demonstrate state-of-the-art performance on multiple visual classiﬁcation tasks with many fewer training samples than what
is required by recent deep learning architectures. These visual classiﬁcation tasks include ﬁne-grained classiﬁcation
on Stanford Dogs 120 [19] and Oxford Flowers 102 [29],
image classiﬁcation on Caltech 256 [11], and scene classiﬁcation on MIT Indoor 67 [33].
In summary, this paper has the following contributions:

learning task is likely a specialized task, we envisage the
image signals in its dataset possess certain unique low-level
characteristics (e.g. fur textures in Stanford Dogs 120 [19]),
and the learned kernels in the convolutional layers of a deep
network need to grasp such characteristics in order to generate highly discriminative features. Thus supplying sufﬁcient training images with similar low-level characteristics
becomes the most important mission of the source learning task. Our core idea is to identify a subset of training
images from D s whose low-level characteristics are similar to those from D t , and then jointly ﬁne-tune a shared set
of convolutional layers for both source and target learning
tasks. The source learning task is ﬁne-tuned using the selected training images only. Hence, this process is called
selective joint ﬁne-tuning. The rationale behind this is that
the unique low-level characteristics of the images from D t
might be overwhelmed if all images from D s were taken as
training samples for the source learning task.
How do we select images from D s that share similar
low-level characteristics as those from D t ? Since kernels
followed with nonlinear activation in a deep convolutional
neural network (CNN) are actually nonlinear spatial ﬁlters,
to ﬁnd sufﬁcient data for training high-quality kernels, we
use the responses from existing linear or nonlinear ﬁlter
banks to deﬁne similarity in low-level characteristics. Gabor ﬁlters [28] form an example of a linear ﬁlter bank, and
the complete set of kernels from certain layers of a pretrained CNN form an example of a nonlinear ﬁlter bank. We
use histograms of ﬁlter bank responses as image descriptors
to search for images with similar low-level characteristics.
The motivation behind selecting images according to
their low-level characteristics is two fold. First, low-level
characteristics are extracted by kernels in the lower convolutional layers of a deep network. These lower convolutional layers form the foundation of an entire network,
and the quality of features extracted by these layers determines the quality of features at higher levels of the deep network. Sufﬁcient training images sharing similar low-level
characteristics could strength the kernels in these layers.
Second, images with similar low-level characteristics could
have very different high-level semantic contents. Therefore,
searching for images using low-level characteristics has less
restrictions and can return much more training images than
using high-level semantic contents.
The above source-target selective joint ﬁne-tuning
scheme is expected to beneﬁt the target learning task in two
different ways. First, since convolutional layers are shared
between the two learning tasks, the selected training samples for the source learning task prevent the deep network
from overﬁtting quickly. Second, since the selected training samples for the source learning task share similar lowlevel characteristics as those from the target learning task,
kernels in their shared convolutional layers can be trained

• We introduce a new deep transfer learning scheme, called
selective joint ﬁne-tuning, for improving the performance of
deep learning tasks with insufﬁcient training data. It is an
important step forward in the context of the widely adopted
strategy of ﬁne-tuning a pre-trained deep neural network.
• We develop a novel pipeline for implementing this deep
transfer learning scheme. Speciﬁcally, we compute descriptors from linear or nonlinear ﬁlter bank responses on training images from both tasks, and use such descriptors to
search for a desired subset of training samples for the source
learning task.
• Experiments demonstrate that our deep transfer learning
scheme achieves state-of-the-art performance on multiple
visual classiﬁcation tasks with insufﬁcient training data for
deep learning.

2. Related Work
Multi-Task Learning. Multi-task learning (MTL) obtains shared feature representations or classiﬁers for related
tasks [3]. In comparison to learning individual tasks independently, features and classiﬁers learned with MTL often have better generalization capability. In deep learning,
faster RCNN [34] jointly learns object locations and labels
using shared convolutional layers but different loss functions for these two tasks. In [7], the same multi-scale convolutional architecture was used to predict depth, surface
normals and semantic labels. This indicates that convolutional neural networks can be adapted to different tasks easily. While previous work [8, 34] attempts to ﬁnd a shared
feature space that beneﬁts multiple learning tasks, the proposed joint training scheme in this paper focuses on learning
a shared feature space that improves the performance of the
target learning task only.
Feature Extraction and Fine-tuning. Off-the-shelf CNN
features [37, 5] have been proven to be powerful in various computer vision problems. Pre-training convolutional
neural networks on ImageNet [36] or Places [50] has been
the standard practice for other vision problems. However,
features learnt in pre-trained models are not tailored for the

11

(a) Source and Target Domain
Training Data

(b) Search k Nearest Neighbors
in Shallow Feature Space

(c) Deep Convolutional
Neural Networks

(d) Joint Optimization in
Different Label Spaces
Linear classifier

Source Training Samples
in Shallow Feature Spaces

Sea Snake

Black Grouse
Chameleon

Ă
Dog
Barn Spider
Source Domain Loss Minimization
Source Domain Data
Linear classifier

Fire Lily
Clematis
Rose

Ă
Artichoke
Bee Balm

Target Domain Data

Target Training Samples
in Shallow Feature Spaces

Convolutional layers shared by the
source and target learning tasks

Target Domain Loss Minimization

Figure 1. Pipeline of the proposed selective joint ﬁne-tuning. From left to right: (a) Datasets in the source domain and the target domain. (b)
Select nearest neighbors of each target domain training sample in the source domain via a low-level feature space. (c) Deep convolutional
neural network initialized with weights pre-trained on ImageNet or Places. (d) Jointly optimize the source and target cost functions in their
own label spaces.

target learning task. Fine-tuning pre-trained models [10]
has become a commonly used method to learn task-speciﬁc
features. The transfer ability of different convolutional layers in CNNs has been investigated in [48]. However, for
tasks that do not have sufﬁcient training data, overﬁtting
occurs quickly during ﬁne-tuning. The proposed pipeline in
this paper not only alleviates overﬁtting, but also attempts
to ﬁnd a more discriminative feature space for the target
learning task.

to form a training set. In our method, we search for nearest neighbors in a large-scale labeled dataset using lowlevel features instead of high-level semantic information. It
has been shown in [27] that low-level features computed in
the bottom layers of a CNN encode very rich information,
which can completely reconstruct the original image. Our
experimental results show that nearest neighbor search using low-level features can outperform that using high-level
semantic information as in [21].

Transfer Learning. Different from MTL, transfer learning (or domain adaptation) [30] applies knowledge learnt in
one domain to other related tasks. Domain adaptation algorithms can be divided into three categories, including instance adaption [16, 1], feature adaption [26, 41], and model
adaption [6]. Hong et al. [15] transferred rich semantic information from source categories to target categories via
the attention model. Tzeng et al. [41] performed feature
adaptation using a shared convolutional neural network by
transferring the class relationship in the source domain to
the target domain. To make our pipeline more ﬂexible, this
paper does not assume the source and target label spaces
are the same as in [41]. Different from the work in [1]
which randomly resamples training classes or images in the
source domain, this paper conducts a special type of transfer
learning by selecting source training samples that are nearest neighbors of samples in the target domain in the space
of certain low-level image descriptor.

3. Selective Joint Fine-tuning
3.1. Overview
Fig. 1 shows the overall pipeline for our proposed
source-target selective joint ﬁne-tuning scheme. Given a
target learning task T t that has insufﬁcient training data,
we perform selective joint ﬁne-tuning as follows. The entire training dataset associated with the target learning task
is called the target domain. The source domain is deﬁned
similarly.
Source Domain : The minimum requirement is that
the number
ns of images in the source domain, D s =

xsi , yis i=1 , should be large enough to train a deep convolutional neural network from scratch. Ideally, these training images should present diversiﬁed low-level characteristics. That is, running a ﬁlter bank on them give rise
to as diversiﬁed responses as possible. There exist a few
large-scale visual recognition datasets that can serve as the
source domain, including ImageNet ILSVRC dataset [36],
Places [50], and MS COCO [23].

Krause et al. [21] directly performed Google image
search using keywords associated with categories from the
target domain, and download a noisy collection of images

12

Source Domain Training Images : In our selective joint
ﬁne-tuning, we do not use all images in the source domain
as training images. Instead, for each image from the target
domain, we search a certain number of images with similar low-level characteristics from the source domain. Only
images returned from these searches are used as training
images for the source learning task in selective joint ﬁnetuning. We apply a ﬁlter bank to all images in both source
domain and target domain. Histograms of ﬁlter bank responses are used as image descriptors during search. We
associate an adaptive number of source domain images with
each target domain image. Hard training samples in the target domain might be associated with a larger number of
source domain images. Two ﬁlter banks are used in our
experiments. One is the Gabor ﬁlter bank, and the other
consists of kernels in the convolutional layers of AlexNet
pre-trained on ImageNet [22].

Filter Bank We use the responses to a ﬁlter bank to describe the low-level characteristics of an image. The ﬁrst
ﬁlter bank we use is the Gabor ﬁlter bank. Gabor ﬁlters are
commonly used for feature description, especially texture
description [28]. Gabor ﬁlter responses are powerful lowlevel features for image and pattern analysis. We use the
parameter setting in [28] as a reference. For each of the real
and imaginary parts, we use 24 convolutional kernels with
4 scales and 6 orientations. Thus there are 48 Gabor ﬁlters
in total.
Kernels in a deep convolutional neural network are actually spatial ﬁlters. When there is nonlinear activation following a kernel, the combination of the kernel and nonlinear
activation is essentially a nonlinear ﬁlter. A deep CNN can
extract low/middle/high level features at different convolutional layers [48]. Convolutional layers close to the input
data focus on extract low-level features while those further
away from the input extract middle- and high-level features.
In fact, a subset of the kernels in the ﬁrst convolutional layer
of AlexNet trained on ImageNet exhibit oriented stripes,
similar to Gabor ﬁlters [22]. When trained on a large-scale
diverse dataset, such as ImageNet, such kernels can be used
for describing generic low-level image characteristics. In
practice, we use all kernels (and their following nonlinear
activation) from the ﬁrst and second convolutional layers of
AlexNet pre-trained on ImageNet as our second choice of a
ﬁlter bank.
Image Descriptor Let Ci (m, n) denote the response map
to the i-th convolutional kernel or Gabor ﬁlter in our ﬁlter
bank, and φi its histogram. To obtain more discriminative
histogram features, we ﬁrst obtain the upper bound hui and
lower bound hli of the i-th response map by scanning the
entire target domain D t . Then the interval hli , hui is divided
into a set of small bins. We adaptively set the width of every histogram bin so that each of them contains a roughly
equal percentage of pixels. In this manner, we can avoid a
large percentage of pixels falling into the same bin. We concatenate the histograms
maps to form a
 of all ﬁlter response

feature vector, φk = φ1 , φ2 , , φD , for image xk .
Nearest Neighbor Ranking Given the histogram-based
descriptor of a training image xti in the target domain,
we search for its nearest neighbors in the source domain
D s . Note that the number of kernels in different convolutional layers of AlexNet might be different. To ensure
equal weighting among different convolutional layers during nearest neighbor search, each histogram of kernel responses is normalized by the total number of kernels in the
corresponding layer. Thus the distance between the descriptor of a source image xsj and that of a target image xti is
computed as follows.

CNN Architecture : Almost any existing deep convolutional neural network, such as AlexNet [22], VGGNet [18],
and ResidualNet [12], can be used in our selective joint ﬁnetuning. We use the 152-layer residual network with identity
mappings [13] as the CNN architecture in our experiments.
The entire residual network is shared by the source and target learning tasks. An extra output layer is added on top of
the residual network for each of the two learning tasks. This
output layer is not shared because the two learning tasks
may not share the same label space. The residual network
is pre-trained either on ImageNet or Places.
Source-Target Joint Fine-tuning : Each task uses its
own cost function during selective joint ﬁne-tuning, and every training image only contributes to the cost function corresponding to the domain it comes from. The source domain images selected by the aforementioned searches are
used as training images for the source learning task only
while the entire target domain is used as the training set for
the target learning task only. Since the residual network
(with all its convolutional layers) is shared by these two
learning tasks, it is ﬁne-tuned by both training sets. And the
output layers on top of the residual network are ﬁne-tuned
by its corresponding training set only. Thus we conduct
end-to-end joint ﬁne-tuning to minimize the original loss
functions of the source learning task and the target learning
task simultaneously.

3.2. Similar Image Search
There is a unique step in our pipeline. For each image
from the target domain, we search a certain number of images with similar low-level characteristics from the source
domain. Only images returned from these searches are used
as training images for the source learning task in selective
joint ﬁne-tuning. We elaborate this image search step below.

D
 

j,s
j,s
i,t
wh [κ(φi,t
H xti , xsj =
h , φh ) + κ(φh , φh )], (1)
h=1

13

where wh = 1/Nh , Nh is the number of convolutional kerj,s
nels in the corresponding layer, φi,t
h and φh are the hth histogram for images xti and xsj , and κ(·, ·) is the KLdivergence.
Hard Samples in the Target Domain The labels of training samples in the target domain have varying degrees of
difﬁculty to satisfy. Intuitively, we would like to seek extra
help for those hard training samples in the target domain by
searching for more and more nearest neighbors in the source
domain. We propose an iterative scheme for this purpose.
We calculate the information entropy to measure the classiﬁcation uncertainty of training samples in the target domain
after the m-th iteration as follows.
Him = −

C


m
pm
i,c log(pi,c ),

Filter Bank
Conv1-Conv2 in AlexNet
Conv1-Conv5 in AlexNet
Conv4-Conv5 in AlexNet
Gabor Filters
Fine-tuning w/o source domain

over all Accuracy(%)
89.59
88.82
88.48
88.90
88.12

Table 1. A comparison of classiﬁcation performance on Oxford
Flowers 102 using various choices for the ﬁlter bank in selective
joint ﬁne-tuning.

any source datasets as our baseline. Note that the network
architecture we use is different from those used in most
published methods for the datasets we run experiments on,
and many existing methods adopt sophisticated parts models and feature encodings. The performance of such methods are still included in this paper to indicate that our simple
holistic method without incorporating parts models and feature encodings is capable of achieving state-of-the-art performance.
We use the pre-trained model released in [12] to initialize the residual network. During selective joint ﬁne-turning,
source and target samples are mixed together in each minibatch. Once the data has passed the average pooling layer
in the residual network, we split the source and target samples, and send them to their corresponding softmax classiﬁer layer respectively. Both the source and target classiﬁers
are initialized randomly.
We run all our experiments on a TITAN X GPU with
12GB memory. All training data is augmented as in [31]
ﬁrst, and we follow the training and testing settings in [12].
Every mini-batch can include 20 224×224 images using a
modiﬁed implementation of the residual network. We include randomly chosen samples from the target domain in a
mini-batch. Then for each of the chosen target sample, we
further include one of its retrieved nearest neighbors from
the source domain in the same mini-batch. We set the iter
size to 10 for each iteration in Caffe [17]. The momentum parameter is set to 0.9 and the weight decay is 0.0001
in SGD. During selective joint ﬁne-tuning, the learning rate
starts from 0.01 and is divided by 10 after every 2400−5000
iterations in all the experiments. Most of the experiments
can ﬁnish in 16000 iterations.

(2)

c=1

where C is the number of classes, pm
i,c is the probability
that the i-th training sample belongs to the c-th class after a
softmax layer in the m-th iteration.
Training samples that have high classiﬁcation uncertainty are considered hard training samples. In the next
iteration, we increase the number of nearest neighbors of
the hard training samples as in Eq. (3.2), and continue ﬁnetuning the model trained in the current iteration. For a training sample xti in the target domain, the number of its nearest
neighbors in the next iteration is deﬁned as follows.
⎧ m
yit = yit
⎨ Ki + σ 0 ,
m+1
m
t
t
K + σ1 , yi = yi and Him ≥ δ (3)
=
Ki
⎩ im
yit = yit and Him < δ
Ki ,
where σ0 , σ1 and δ are constants, yit is predicted label of
xti , and Kim is the number of nearest neighbors in the mth iteration. By changing the number of nearest neighbors
for samples in the target domain, the subset of the source
domain used as training data evolves over iterations, which
in turn gradually changes the feature representation learned
in the deep network. In the above equation, we typically set
δ = 0.1, σ0 = 4K0 and σ1 = 2K0 , where K0 is the initial
number of nearest neighbors for all samples in the target
domain. In our experiments, we stop after ﬁve iterations.
In Table 1, we compare the effectiveness of Gabor ﬁlters
and various combinations of kernels from AlexNet in our
selective joint ﬁne-tuning. In this experiment, we use the
50-layer residual network [12] with half the number of kernels in each convolutional layer of the original architecture.

4.2. Source Image Retrieval
We use the ImageNet ILSVRC 2012 training set [36] as
the source domain for Stanford Dogs [19], Oxford Flowers [29], and Caltech 256 [11], and the combination of the
ImageNet and Places 205 [50] training sets as the source
domain for MIT Indoor 67 [33]. Fig. 2 shows the retrieved
1-st, 10-th, 20-th, 30-th, and 40-th nearest neighbors from
ImageNet [36] or Places [50]. It can be observed that corresponding source and target images share similar colors,

4. Experiments
4.1. Implementation
In all experiments, we use the 152-layer residual network
with identity mappings [13] as the deep convolutional architecture, and conventional ﬁne-tuning performed on a pretrained network with the same architecture without using

14

a.1 Chihuahua

a.2 Chihuahua

a.3 Beagle

a.4 Tench

a.5 Bib

a.5 Diaper

b.1 Pink Primrose

b.2 Bee

b.3 Capuchin

b.4 Measuring Cup

b.4 Butterfly

b.5 Bee

c.1 AK-47

c.2 Horse Cart

c.3 Horn

c.4 Hard Disk

c.5 Snowmobile

c.6 Jeep

d.1 Airport Inside

d.2 Restaurant

d.3 Restaurant

d.4 Butcher Shop

d.5 Coffee Mug

d.6 Grocery Store

e.1 Airport Inside

e.2 Game Room

e.3 Restaurant

e.4 Supermarket

e.5 Lobby

e.5 Museum

Figure 2. Images in the source domain that have similar low-level characteristics with the target images. The ﬁrst column shows target
images from Stanford Dogs 120 [19], Oxford Flowers 102 [29], Caltech 256 [11], and MIT Indoor 67 [33]. The following columns in rows
(a)-(d) are the corresponding 1st, 10-th, 20-th, 30-th and 40-th nearest images in ImageNet (source domain). The following columns in row
(e) are images retrieved from Places (source domain for MIT Indoor 67).

formation during selective joint ﬁne-tuning, and use the
commonly used mean class accuracy to evaluate the performance as in [11].

local patterns and global structures. Since low-level ﬁlter
bank responses do not encode strong semantic information,
the 50 nearest neighbors from a target domain include images from various and sometimes completely unrelated categories.
We determined experimentally that there should be at
least 200,000 retrieved images from the source domain. Too
few source images give rise to overﬁtting quickly. Therefore, the initial number of retrieved nearest neighbors (K0 )
for each target training sample is set to meet this requirement. On the other hand, a surprising result is that setting K0 too large would make the performance of the target
learning task drop signiﬁcantly. In our experiments, we set
K0 to different values for Stanford Dogs (K0 = 100), Oxford Flowers (K0 = 300), Caltech 256 (K0 = 50 − 100),
and MIT Indoor 67 (K0 = 100). Since there exists much
overlap among the nearest neighbors of different target samples, the retrieved images typically do not cover the entire
ImageNet or Places datasets.

4.3. Fine-grained Object Recognition

As shown in Table 2, the mean class accuracy achieved
by ﬁne-tuning the residual network using the training samples of this dataset only and without a source domain is
80.4%. It shows that the 152-layer residual network [12, 13]
pre-trained on the ImageNet dataset [36] has a strong generalization capability on this ﬁne-grained classiﬁcation task.
Using the entire ImageNet dataset during regular joint ﬁnetuning can improve the performance by 5.1%. When we
ﬁnally perform our proposed selective joint ﬁne-tuning using a subset of source domain images retrieved using histograms of low-level convolutional features, the performance is further improved to 90.2%, which is 9.8% higher
than the performance of conventional ﬁne-tuning without a
source domain and 4.3% higher than the result reported in
[21], which expands the original target training set using
Google image search. This comparison demonstrates that
selective joint ﬁne-tuning can signiﬁcantly outperform conventional ﬁne-tuning.

Stanford Dogs 120. Stanford Dogs 120 [19] contains 120
categories of dogs. There are 12000 images for training,
and 8580 images for testing. We do not use the parts in-

Oxford Flowers 102. Oxford Flowers 102 [29] consists of
102 ﬂower categories. 1020 images are used for training,
1020 for validation, and 6149 images are used for testing.

15

Method

that Simon et al. [38] used the validation set in this dataset
as additional training data. To verify the effectiveness of
our joint ﬁne-tuning strategy, we have also conducted experiments using this training setting and our result from a
single network outperforms that of [38] by 1.7%.

mean Acc(%)

HAR-CNN [44]
Local Alignment [9]
Multi scale metric learning [32]
MagNet [35]
Web Data + Original Data [21]

49.4
57.0
70.3
75.1
85.9

Training from scratch using target domain only
Selective joint training from scratch
Fine-tuning w/o source domain
Joint ﬁne-tuning with all source samples
Selective joint FT with random source samples
Selective joint FT w/o iterative NN retrieval
Selective joint FT with Gabor ﬁlter bank
Selective joint ﬁne-tuning
Selective joint FT with Model Fusion

53.8
83.4
80.4
85.6
85.5
88.3
87.5
90.2
90.3

4.4. General Object Recognition
Caltech 256. Caltech 256 [11] has 256 object categories
and 1 background cluster class. In every category, there
are at least 80 images used for training, validation and testing. Researchers typically report results with the number
of training samples per class falling between 5 and 60. We
follow the testing procedure in [42] to compare with stateof-the-art results.
We conduct four experiments with the number of training samples per class set to 15, 30, 45 and 60, respectively.
According to Table 4, in comparison to conventional ﬁnetuning without using a source domain, selective joint ﬁnetuning improves classiﬁcation accuracy in all four experiments, and the degree of improvement varies between 2.6%
and 4.1%. Performance improvement due to selective joint
ﬁne-tuning is more obvious when a smaller number of target
training image per class are used. This is because limited diversity in the target training data imposes a greater need to
seek help from the source domain. In most of these experiments, the classiﬁcation performance of our selective joint
ﬁne-tuning is also signiﬁcantly better than previous stateof-the-art results.

Table 2. Classiﬁcation results on Stanford Dogs 120.

There are only 10 training images in each category.
As shown in Table 3, the mean class accuracy achieved
by conventional ﬁne-tuning using the training samples of
this dataset only and without a source domain is 92.3%. Selective joint ﬁne-tuning further improves the performance to
94.7%, 3.3% higher than previous best result from a single
network [35]. To compare with previous state-of-the-art results obtained using an ensemble of different networks, we
also average the performance of multiple models obtained
during iterative source image retrieval for hard training samples in the target domain. Experiments show that the performance of our ensemble model is 95.8%, 1.3% higher than
previous best ensemble performance reported in [20]. Note

Method

Method

mean Acc(%)

MPP [47]
Multi-model Feature Concat [1]
MagNet [35]
VGG-19 + GoogleNet + AlexNet [20]

91.3
91.3
91.4
94.5

Training from scratch using target domain only
Selective joint training from scratch
Fine-tuning w/o source domain
Joint ﬁne-tuning with all source samples
Selective joint FT with random source samples
Selective joint FT w/o iterative NN retrieval
Selective joint FT with Gabor ﬁlter bank
Selective joint ﬁne-tuning
Selective joint FT with model fusion

58.2
80.6
92.3
93.4
93.2
94.2
93.8
94.7
95.8

VGG-19 + Part Constellation Model [38]
Selective joint FT with val set

95.3
97.0

mean Acc(%)

MetaObject-CNN [43]
MPP + DFSL [47]
VGG-19 + FV [4]
VGG-19 + GoogleNet [20]
Multi scale + multi model ensemble [14]

78.9
80.8
81.0
84.7
86.0

Fine-tuning w/o source domain
Selective joint FT with ImageNet(i)
Selective joint FT with Places(ii)
Selective joint FT with hybrid data(iii)
Average the output of (ii) and (iii)

81.7
82.8
85.8
85.5
86.9

Table 5. Classiﬁcation results on MIT Indoor 67.

4.5. Scene Classiﬁcation
MIT Indoor 67. MIT Indoor 67 [33] has 67 scene categories. In each category, there are 80 images for training
and 20 images for testing. Since MIT Indoor 67 is a scene
dataset, in addition to the ImageNet ILSVRC 2012 training
set [36], the Places-205 training set [50] is also a potential
source domain. We compare three settings during selective
joint ﬁne-tuning: ImageNet as the source domain, Places as

Table 3. Classiﬁcation results on Oxford Flowers 102. The last two
rows compare performance using the validation set as additional
training data.

16

mean Acc(%)
15/class

mean Acc(%)
30/class

mean Acc(%)
45/class

mean Acc(%)
60/class

40.5±0.4
65.7±0.2
-

48.0±0.2
70.6±0.2
-

51.9±0.2
72.7±0.4
-

55.2±0.3
74.2±0.3
85.1±0.3
86.1
86.2±0.3

Fine-tuning w/o source domain
76.4±0.1
81.2±0.2
83.5±0.2
Selective joint ﬁne-tuning
80.5±0.3
83.8±0.5
87.0±0.1
Table 4. Classiﬁcation results on Caltech 256.

86.4±0.3
89.1±0.2

Method
M-HMP [2]
Z. & F. Net [49]
VGG-19 [18]
VGG-19 + GoogleNet +AlexNet [20]
VGG-19 + VGG-16 [18]

the source domain, and the combination of both ImageNet
and Places as the source domain.
As shown in Table 5, the mean class accuracy of selective joint ﬁne-tuning with ImageNet as the source domain is 82.8%, 1.1% higher than that of conventional ﬁnetuning without using a source domain. Since ImageNet is
an object-centric dataset while MIT Indoor 67 is a scene
dataset, it is hard for training images in the target domain to
retrieve source domain images with similar low-level characteristics. But source images retrieved from ImageNet still
prevent the network from overﬁtting too heavily and help
achieve a performance gain. When the Places dataset serves
as the source domain, the mean class accuracy reaches
85.8%, which is 4.1% higher than the performance of ﬁnetuning without a source domain and 4.8% higher than previous best result from a single network [4]. And the hybrid
source domain based on both ImageNet and Places does not
further improve the performance. Once averaging the output from the networks jointly ﬁne-tuned with Places and
the hybrid source domain, we obtain a classiﬁcation accuracy 0.9% higher than previous best result from an ensemble
model [14].

more relevant data from the source domain is actually more
helpful. Third, instead of using a subset of retrieved training images, we use the same number of randomly chosen
training images from the source domain. Again, the performance drops by 4.7% and 1.5% respectively. Fourth, to validate the effectiveness of iteratively increasing the number
of retrieved images for hard training samples in the target
domain, we turn off this feature and only use the same number (K0 ) of retrieved images for all training samples in the
target domain. The performance drops by 1.9% and 0.5%
respectively. This indicates that our adaptive scheme for
hard samples is useful in improving the performance. Fifth,
we use convolutional kernels in the two bottom layers of a
pre-trained AlexNet as our ﬁlter bank. If we replace this ﬁlter bank with the Gabor ﬁlter bank, the overall performance
drops by 2.7% and 0.9% respectively, which indicates a ﬁlter bank learned from a diverse dataset could be more powerful than an analytically deﬁned one. Finally, if we perform conventional ﬁne-tuning without using a source domain, the performance drop becomes quite signiﬁcant and
reaches 9.8% and 2.4% respectively.

5. Conclusions
In this paper, we address deep learning tasks with insufﬁcient training data by introducing a new deep transfer learning scheme called selective joint ﬁne-tuning, which performs a target learning task with insufﬁcient training data
simultaneously with another source learning task with abundant training data. Different from previous work which directly adds extra training data to the target learning task,
our scheme borrows samples from a large-scale labeled
dataset for the source learning task, and do not require additional labeling effort beyond the existing datasets. Experiments show that our deep transfer learning scheme achieves
state-of-the-art performance on multiple visual classiﬁcation tasks with insufﬁcient training data for deep networks.
Nevertheless, how to ﬁnd the most suitable source domain
for a speciﬁc target learning task remains an open problem
for future investigation.
Acknowledgment This work was partially supported by Hong Kong Innovation and Technology Fund
(ITP/055/14LP).

4.6. Ablation Study
We perform an ablation study on both Stanford Dogs 120
[19] and Oxford Flowers 102 [29] by replacing or removing a single component from our pipeline. First, instead of
ﬁne-tuning, we perform training from scratch in two settings, one using the target domain only and the other using
selective joint training. Tables 2 and 3 show that while selective joint training obviously improves the performance, it
is still inferior than ﬁne-tuning pre-trained networks. This
is because we only subsample a relatively small percentage
(20-30%) of the source data, which is still insufﬁcient to
train deep networks from scratch. Second, instead of using a
subset of retrieved training images from the source domain,
we simply use all training images in the source domain.
Joint ﬁne-tuning with the entire source domain decrease the
performance by 4.6% and 1.3% respectively. This demonstrates that using more training data from the source domain is not always better. On the contrary, using less but

17

References

[16] J. Huang, A. Gretton, K. M. Borgwardt, B. Schölkopf, and
A. J. Smola. Correcting sample selection bias by unlabeled
data. In Advances in neural information processing systems,
pages 601–608, 2006. 3
[17] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia, pages 675–678. ACM, 2014. 5
[18] S. Karen and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2015. 1, 4, 8
[19] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC), volume 2, 2011. 1, 2, 5, 6, 8
[20] Y.-D. Kim, T. Jang, B. Han, and S. Choi. Learning to select pre-trained deep representations with bayesian evidence
framework. arXiv preprint arXiv:1506.02565, 2015. 7, 8
[21] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev,
T. Duerig, J. Philbin, and L. Fei-Fei. The unreasonable effectiveness of noisy data for ﬁne-grained recognition. In European Conference on Computer Vision, 2015. 3, 6, 7
[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks. Advances in neural information processing systems, 2012. 1,
4
[23] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740–755. Springer, 2014. 1, 3
[24] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear cnn models for ﬁne-grained visual recognition. In Proceedings of the
IEEE International Conference on Computer Vision, pages
1449–1457, 2015. 1
[25] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages 3431–3440, 2015. 1
[26] M. Long and J. Wang. Learning transferable features with
deep adaptation networks. CoRR, abs/1502.02791, 1:2,
2015. 3
[27] A. Mahendran and A. Vedaldi. Understanding deep image
representations by inverting them. In 2015 IEEE conference
on computer vision and pattern recognition (CVPR), pages
5188–5196. IEEE, 2015. 3
[28] B. S. Manjunath and W.-Y. Ma. Texture features for browsing and retrieval of image data. IEEE Transactions on
pattern analysis and machine intelligence, 18(8):837–842,
1996. 2, 4
[29] M.-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In Computer Vision,
Graphics & Image Processing, 2008. ICVGIP’08. Sixth Indian Conference on, pages 722–729. IEEE, 2008. 1, 2, 5, 6,
8
[30] S. J. Pan and Q. Yang. A survey on transfer learning.
IEEE Transactions on knowledge and data engineering,
22(10):1345–1359, 2010. 1, 3

[1] H. Azizpour, A. Sharif Razavian, J. Sullivan, A. Maki, and
S. Carlsson. From generic to speciﬁc deep representations
for visual recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 36–45, 2015. 3, 7
[2] L. Bo, X. Ren, and D. Fox. Multipath sparse coding using
hierarchical matching pursuit. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 660–667, 2013. 8
[3] R. Caruana. Multitask learning. In Learning to learn, pages
95–133. Springer, 1998. 2
[4] M. Cimpoi, S. Maji, I. Kokkinos, and A. Vedaldi. Deep ﬁlter banks for texture recognition, description, and segmentation. International Journal of Computer Vision, 118(1):65–
94, 2016. 7, 8
[5] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, pages
647–655, 2014. 2
[6] L. Duan, D. Xu, I. W.-H. Tsang, and J. Luo. Visual event
recognition in videos by learning from web data. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
34(9):1667–1680, 2012. 3
[7] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2650–2658, 2015. 2
[8] A. Evgeniou and M. Pontil. Multi-task feature learning.
Advances in neural information processing systems, 19:41,
2007. 2
[9] E. Gavves, B. Fernando, C. G. Snoek, A. W. Smeulders, and
T. Tuytelaars. Local alignments for ﬁne-grained categorization. International Journal of Computer Vision, 111(2):191–
212, 2015. 7
[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic
segmentation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 580–587,
2014. 1, 3
[11] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. 2007. 2, 5, 6, 7
[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385,
2015. 1, 4, 5, 6
[13] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. arXiv preprint arXiv:1603.05027,
2016. 4, 5, 6
[14] L. Herranz, S. Jiang, and X. Li. Scene recognition with
cnns: objects, scales and dataset bias. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages 571–579, 2016. 7, 8
[15] S. Hong, J. Oh, B. Han, and H. Lee. Learning transferrable
knowledge for semantic segmentation with deep convolutional neural network. In IEEE Conference on Computer
Vision and Pattern Recognition, 2016. 3

18

[45] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. Multitask learning for classiﬁcation with dirichlet process priors.
Journal of Machine Learning Research, 8(Jan):35–63, 2007.
1
[46] Z. Yan, H. Zhang, R. Piramuthu, V. Jagadeesh, D. DeCoste,
W. Di, and Y. Yu. Hd-cnn: Hierarchical deep convolutional
neural networks for large scale visual recognition. In International Conference on Computer Vision (ICCV), 2015. 1
[47] D. Yoo, S. Park, J.-Y. Lee, and I. So Kweon. Multi-scale
pyramid pooling for deep convolutional representation. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, pages 71–80, 2015. 7
[48] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances
in neural information processing systems, pages 3320–3328,
2014. 3, 4
[49] M. D. Zeiler and R. Fergus. Visualizing and understanding
convolutional networks. In European Conference on Computer Vision, pages 818–833. Springer, 2014. 8
[50] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. In Advances in neural information processing systems, pages 487–495, 2014. 1, 2, 3, 5, 7

[31] M. Paulin, J. Revaud, Z. Harchaoui, F. Perronnin, and
C. Schmid. Transformation pursuit for image classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3646–3653, 2014. 5
[32] Q. Qian, R. Jin, S. Zhu, and Y. Lin. Fine-grained visual
categorization via multi-stage metric learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3716–3724, 2015. 7
[33] A. Quattoni and A. Torralba. Recognizing indoor scenes.
In Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 413–420. IEEE, 2009. 2,
5, 6, 7
[34] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in neural information processing systems, pages
91–99, 2015. 2
[35] O. Rippel, M. Paluri, P. Dollar, and L. Bourdev. Metric learning with adaptive density discrimination. stat, 1050:2, 2016.
7
[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211–252,
2015. 1, 2, 3, 5, 6, 7
[37] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: an astounding baseline for
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 806–
813, 2014. 2
[38] M. Simon and E. Rodner. Neural activation constellations:
Unsupervised part model discovery with convolutional networks. In Proceedings of the IEEE International Conference
on Computer Vision, pages 1143–1151, 2015. 7
[39] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014. 1
[40] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In IEEE Conference on
Computer Vision and Pattern Recognition, 2015. 1
[41] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 4068–4076, 2015. 3
[42] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong.
Locality-constrained linear coding for image classiﬁcation.
In Computer Vision and Pattern Recognition (CVPR), 2010
IEEE Conference on, pages 3360–3367. IEEE, 2010. 7
[43] R. Wu, B. Wang, W. Wang, and Y. Yu. Harvesting discriminative meta objects with deep cnn features for scene classiﬁcation. In Proceedings of the IEEE International Conference
on Computer Vision, pages 1287–1295, 2015. 7
[44] S. Xie, T. Yang, X. Wang, and Y. Lin. Hyper-class augmented and regularized deep learning for ﬁne-grained image classiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 2645–
2654, 2015. 7

19

