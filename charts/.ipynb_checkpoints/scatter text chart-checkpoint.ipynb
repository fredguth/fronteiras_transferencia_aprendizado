{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scattertext as st\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O scattertext só funciona com categorias binárias. \n",
    "https://github.com/JasonKessler/scattertext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "@article{kessler2017scattertext,\n",
    "  author    = {Kessler, Jason S.},\n",
    "  title     = {Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ},\n",
    "  booktitle = {Proceedings of ACL-2017 System Demonstrations},\n",
    "  year      = {2017},\n",
    "  address   = {Vancouver, Canada},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('keywords.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['categoria']= np.where(data['Year']> 2017, 'Recente', 'Não Recente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words not common in english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = st.CorpusFromPandas(data, category_col='categoria', text_col='Keywords', nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(corpus.get_scaled_f_scores_vs_background().index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df = corpus.get_term_freq_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df['R Score'] = corpus.get_scaled_f_scores('Recente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(term_freq_df.sort_values(by='R Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df['NR Score'] = corpus.get_scaled_f_scores('Não Recente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(term_freq_df.sort_values(by='NR Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus,\n",
    "category='Não Recente',\n",
    "category_name='< 2018',\n",
    "not_category_name='> 2017',\n",
    "width_in_pixels=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"Keyword-Visualization.html\", 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5: expected 10 fields, saw 11\\nSkipping line 8: expected 10 fields, saw 13\\nSkipping line 10: expected 10 fields, saw 17\\nSkipping line 22: expected 10 fields, saw 13\\nSkipping line 24: expected 10 fields, saw 25\\nSkipping line 25: expected 10 fields, saw 20\\nSkipping line 26: expected 10 fields, saw 15\\nSkipping line 27: expected 10 fields, saw 11\\nSkipping line 30: expected 10 fields, saw 24\\nSkipping line 31: expected 10 fields, saw 14\\nSkipping line 34: expected 10 fields, saw 15\\nSkipping line 35: expected 10 fields, saw 13\\nSkipping line 41: expected 10 fields, saw 16\\nSkipping line 45: expected 10 fields, saw 12\\nSkipping line 46: expected 10 fields, saw 15\\nSkipping line 47: expected 10 fields, saw 17\\nSkipping line 50: expected 10 fields, saw 13\\nSkipping line 53: expected 10 fields, saw 16\\nSkipping line 58: expected 10 fields, saw 16\\nSkipping line 59: expected 10 fields, saw 23\\nSkipping line 62: expected 10 fields, saw 11\\nSkipping line 63: expected 10 fields, saw 11\\nSkipping line 64: expected 10 fields, saw 14\\nSkipping line 65: expected 10 fields, saw 13\\nSkipping line 66: expected 10 fields, saw 12\\nSkipping line 70: expected 10 fields, saw 16\\nSkipping line 72: expected 10 fields, saw 11\\nSkipping line 73: expected 10 fields, saw 14\\nSkipping line 77: expected 10 fields, saw 11\\nSkipping line 80: expected 10 fields, saw 17\\nSkipping line 81: expected 10 fields, saw 24\\nSkipping line 83: expected 10 fields, saw 11\\nSkipping line 84: expected 10 fields, saw 17\\nSkipping line 85: expected 10 fields, saw 11\\nSkipping line 86: expected 10 fields, saw 14\\nSkipping line 89: expected 10 fields, saw 11\\nSkipping line 92: expected 10 fields, saw 11\\nSkipping line 95: expected 10 fields, saw 12\\nSkipping line 96: expected 10 fields, saw 18\\nSkipping line 106: expected 10 fields, saw 11\\nSkipping line 108: expected 10 fields, saw 13\\nSkipping line 113: expected 10 fields, saw 18\\nSkipping line 114: expected 10 fields, saw 12\\nSkipping line 116: expected 10 fields, saw 15\\nSkipping line 117: expected 10 fields, saw 11\\nSkipping line 123: expected 10 fields, saw 17\\nSkipping line 128: expected 10 fields, saw 11\\nSkipping line 129: expected 10 fields, saw 11\\nSkipping line 130: expected 10 fields, saw 16\\nSkipping line 131: expected 10 fields, saw 14\\nSkipping line 133: expected 10 fields, saw 11\\nSkipping line 134: expected 10 fields, saw 21\\nSkipping line 136: expected 10 fields, saw 12\\nSkipping line 139: expected 10 fields, saw 13\\nSkipping line 140: expected 10 fields, saw 12\\nSkipping line 143: expected 10 fields, saw 14\\nSkipping line 153: expected 10 fields, saw 16\\nSkipping line 156: expected 10 fields, saw 26\\nSkipping line 159: expected 10 fields, saw 16\\nSkipping line 164: expected 10 fields, saw 16\\nSkipping line 165: expected 10 fields, saw 11\\nSkipping line 167: expected 10 fields, saw 12\\nSkipping line 171: expected 10 fields, saw 12\\nSkipping line 173: expected 10 fields, saw 12\\nSkipping line 177: expected 10 fields, saw 13\\nSkipping line 185: expected 10 fields, saw 11\\nSkipping line 186: expected 10 fields, saw 13\\nSkipping line 187: expected 10 fields, saw 12\\nSkipping line 191: expected 10 fields, saw 11\\nSkipping line 199: expected 10 fields, saw 17\\nSkipping line 200: expected 10 fields, saw 12\\nSkipping line 204: expected 10 fields, saw 12\\nSkipping line 208: expected 10 fields, saw 11\\nSkipping line 212: expected 10 fields, saw 18\\nSkipping line 219: expected 10 fields, saw 21\\nSkipping line 220: expected 10 fields, saw 12\\nSkipping line 221: expected 10 fields, saw 15\\nSkipping line 222: expected 10 fields, saw 20\\nSkipping line 226: expected 10 fields, saw 12\\nSkipping line 234: expected 10 fields, saw 11\\nSkipping line 236: expected 10 fields, saw 13\\nSkipping line 242: expected 10 fields, saw 12\\nSkipping line 243: expected 10 fields, saw 13\\nSkipping line 253: expected 10 fields, saw 15\\nSkipping line 257: expected 10 fields, saw 13\\nSkipping line 258: expected 10 fields, saw 14\\nSkipping line 263: expected 10 fields, saw 11\\nSkipping line 269: expected 10 fields, saw 17\\nSkipping line 272: expected 10 fields, saw 12\\nSkipping line 273: expected 10 fields, saw 16\\nSkipping line 281: expected 10 fields, saw 13\\nSkipping line 282: expected 10 fields, saw 11\\nSkipping line 297: expected 10 fields, saw 28\\nSkipping line 299: expected 10 fields, saw 14\\nSkipping line 301: expected 10 fields, saw 12\\nSkipping line 303: expected 10 fields, saw 17\\nSkipping line 304: expected 10 fields, saw 12\\nSkipping line 309: expected 10 fields, saw 14\\nSkipping line 310: expected 10 fields, saw 11\\nSkipping line 311: expected 10 fields, saw 12\\nSkipping line 320: expected 10 fields, saw 15\\nSkipping line 322: expected 10 fields, saw 15\\nSkipping line 330: expected 10 fields, saw 15\\nSkipping line 331: expected 10 fields, saw 12\\nSkipping line 332: expected 10 fields, saw 11\\nSkipping line 335: expected 10 fields, saw 20\\nSkipping line 342: expected 10 fields, saw 20\\nSkipping line 343: expected 10 fields, saw 11\\nSkipping line 344: expected 10 fields, saw 13\\nSkipping line 347: expected 10 fields, saw 16\\nSkipping line 349: expected 10 fields, saw 14\\nSkipping line 351: expected 10 fields, saw 16\\nSkipping line 353: expected 10 fields, saw 15\\nSkipping line 355: expected 10 fields, saw 13\\nSkipping line 357: expected 10 fields, saw 16\\nSkipping line 358: expected 10 fields, saw 11\\nSkipping line 359: expected 10 fields, saw 13\\nSkipping line 361: expected 10 fields, saw 11\\nSkipping line 362: expected 10 fields, saw 11\\nSkipping line 364: expected 10 fields, saw 14\\nSkipping line 365: expected 10 fields, saw 23\\nSkipping line 367: expected 10 fields, saw 14\\nSkipping line 370: expected 10 fields, saw 13\\nSkipping line 371: expected 10 fields, saw 14\\nSkipping line 376: expected 10 fields, saw 11\\nSkipping line 377: expected 10 fields, saw 15\\nSkipping line 378: expected 10 fields, saw 14\\nSkipping line 381: expected 10 fields, saw 11\\nSkipping line 383: expected 10 fields, saw 13\\nSkipping line 384: expected 10 fields, saw 17\\nSkipping line 385: expected 10 fields, saw 11\\nSkipping line 386: expected 10 fields, saw 11\\nSkipping line 390: expected 10 fields, saw 12\\nSkipping line 405: expected 10 fields, saw 11\\nSkipping line 428: expected 10 fields, saw 23\\nSkipping line 431: expected 10 fields, saw 11\\nSkipping line 434: expected 10 fields, saw 12\\nSkipping line 439: expected 10 fields, saw 11\\nSkipping line 442: expected 10 fields, saw 13\\nSkipping line 447: expected 10 fields, saw 12\\nSkipping line 453: expected 10 fields, saw 11\\nSkipping line 454: expected 10 fields, saw 13\\nSkipping line 455: expected 10 fields, saw 13\\nSkipping line 458: expected 10 fields, saw 15\\nSkipping line 459: expected 10 fields, saw 13\\nSkipping line 470: expected 10 fields, saw 15\\nSkipping line 473: expected 10 fields, saw 11\\nSkipping line 491: expected 10 fields, saw 14\\nSkipping line 496: expected 10 fields, saw 12\\nSkipping line 503: expected 10 fields, saw 12\\nSkipping line 525: expected 10 fields, saw 12\\nSkipping line 529: expected 10 fields, saw 13\\nSkipping line 530: expected 10 fields, saw 11\\nSkipping line 532: expected 10 fields, saw 16\\nSkipping line 535: expected 10 fields, saw 13\\nSkipping line 542: expected 10 fields, saw 14\\nSkipping line 549: expected 10 fields, saw 14\\nSkipping line 553: expected 10 fields, saw 11\\nSkipping line 561: expected 10 fields, saw 11\\nSkipping line 563: expected 10 fields, saw 12\\nSkipping line 570: expected 10 fields, saw 11\\nSkipping line 576: expected 10 fields, saw 14\\nSkipping line 578: expected 10 fields, saw 13\\nSkipping line 584: expected 10 fields, saw 12\\nSkipping line 590: expected 10 fields, saw 11\\nSkipping line 597: expected 10 fields, saw 12\\nSkipping line 598: expected 10 fields, saw 14\\nSkipping line 599: expected 10 fields, saw 14\\nSkipping line 600: expected 10 fields, saw 11\\nSkipping line 602: expected 10 fields, saw 11\\nSkipping line 603: expected 10 fields, saw 11\\nSkipping line 604: expected 10 fields, saw 12\\nSkipping line 605: expected 10 fields, saw 15\\nSkipping line 611: expected 10 fields, saw 11\\nSkipping line 612: expected 10 fields, saw 12\\nSkipping line 614: expected 10 fields, saw 17\\nSkipping line 615: expected 10 fields, saw 19\\nSkipping line 619: expected 10 fields, saw 12\\nSkipping line 624: expected 10 fields, saw 11\\nSkipping line 627: expected 10 fields, saw 14\\nSkipping line 630: expected 10 fields, saw 13\\nSkipping line 635: expected 10 fields, saw 11\\nSkipping line 646: expected 10 fields, saw 12\\nSkipping line 647: expected 10 fields, saw 12\\nSkipping line 650: expected 10 fields, saw 16\\nSkipping line 655: expected 10 fields, saw 25\\nSkipping line 684: expected 10 fields, saw 16\\nSkipping line 690: expected 10 fields, saw 13\\nSkipping line 693: expected 10 fields, saw 14\\nSkipping line 698: expected 10 fields, saw 12\\nSkipping line 699: expected 10 fields, saw 12\\nSkipping line 705: expected 10 fields, saw 13\\nSkipping line 707: expected 10 fields, saw 12\\nSkipping line 713: expected 10 fields, saw 11\\nSkipping line 728: expected 10 fields, saw 11\\nSkipping line 730: expected 10 fields, saw 13\\nSkipping line 734: expected 10 fields, saw 11\\nSkipping line 743: expected 10 fields, saw 12\\nSkipping line 753: expected 10 fields, saw 12\\nSkipping line 762: expected 10 fields, saw 12\\nSkipping line 764: expected 10 fields, saw 16\\nSkipping line 766: expected 10 fields, saw 15\\nSkipping line 773: expected 10 fields, saw 20\\nSkipping line 776: expected 10 fields, saw 16\\nSkipping line 778: expected 10 fields, saw 16\\nSkipping line 783: expected 10 fields, saw 14\\nSkipping line 784: expected 10 fields, saw 15\\nSkipping line 785: expected 10 fields, saw 12\\nSkipping line 786: expected 10 fields, saw 16\\nSkipping line 792: expected 10 fields, saw 15\\nSkipping line 794: expected 10 fields, saw 18\\nSkipping line 796: expected 10 fields, saw 14\\nSkipping line 802: expected 10 fields, saw 12\\nSkipping line 806: expected 10 fields, saw 19\\nSkipping line 808: expected 10 fields, saw 12\\nSkipping line 809: expected 10 fields, saw 11\\nSkipping line 811: expected 10 fields, saw 19\\nSkipping line 812: expected 10 fields, saw 12\\nSkipping line 820: expected 10 fields, saw 17\\nSkipping line 825: expected 10 fields, saw 14\\nSkipping line 828: expected 10 fields, saw 11\\nSkipping line 832: expected 10 fields, saw 12\\nSkipping line 837: expected 10 fields, saw 12\\nSkipping line 838: expected 10 fields, saw 13\\nSkipping line 845: expected 10 fields, saw 12\\nSkipping line 861: expected 10 fields, saw 11\\nSkipping line 863: expected 10 fields, saw 11\\nSkipping line 865: expected 10 fields, saw 12\\nSkipping line 874: expected 10 fields, saw 18\\nSkipping line 878: expected 10 fields, saw 13\\nSkipping line 884: expected 10 fields, saw 13\\nSkipping line 891: expected 10 fields, saw 14\\nSkipping line 892: expected 10 fields, saw 15\\nSkipping line 894: expected 10 fields, saw 14\\nSkipping line 896: expected 10 fields, saw 11\\nSkipping line 898: expected 10 fields, saw 11\\nSkipping line 899: expected 10 fields, saw 11\\nSkipping line 902: expected 10 fields, saw 13\\nSkipping line 906: expected 10 fields, saw 15\\nSkipping line 914: expected 10 fields, saw 17\\nSkipping line 922: expected 10 fields, saw 12\\nSkipping line 925: expected 10 fields, saw 13\\nSkipping line 927: expected 10 fields, saw 15\\nSkipping line 928: expected 10 fields, saw 22\\nSkipping line 929: expected 10 fields, saw 11\\nSkipping line 934: expected 10 fields, saw 17\\nSkipping line 935: expected 10 fields, saw 12\\nSkipping line 936: expected 10 fields, saw 11\\nSkipping line 938: expected 10 fields, saw 11\\nSkipping line 939: expected 10 fields, saw 14\\nSkipping line 941: expected 10 fields, saw 12\\nSkipping line 942: expected 10 fields, saw 21\\nSkipping line 944: expected 10 fields, saw 12\\nSkipping line 945: expected 10 fields, saw 13\\nSkipping line 950: expected 10 fields, saw 11\\nSkipping line 953: expected 10 fields, saw 11\\nSkipping line 955: expected 10 fields, saw 19\\nSkipping line 986: expected 10 fields, saw 11\\nSkipping line 996: expected 10 fields, saw 12\\nSkipping line 1010: expected 10 fields, saw 13\\nSkipping line 1014: expected 10 fields, saw 13\\nSkipping line 1016: expected 10 fields, saw 11\\nSkipping line 1018: expected 10 fields, saw 11\\nSkipping line 1020: expected 10 fields, saw 16\\nSkipping line 1023: expected 10 fields, saw 12\\nSkipping line 1025: expected 10 fields, saw 11\\nSkipping line 1026: expected 10 fields, saw 11\\nSkipping line 1028: expected 10 fields, saw 13\\nSkipping line 1029: expected 10 fields, saw 11\\nSkipping line 1035: expected 10 fields, saw 11\\nSkipping line 1037: expected 10 fields, saw 17\\nSkipping line 1038: expected 10 fields, saw 12\\nSkipping line 1040: expected 10 fields, saw 13\\nSkipping line 1058: expected 10 fields, saw 13\\nSkipping line 1067: expected 10 fields, saw 15\\nSkipping line 1071: expected 10 fields, saw 11\\nSkipping line 1087: expected 10 fields, saw 17\\nSkipping line 1093: expected 10 fields, saw 13\\nSkipping line 1094: expected 10 fields, saw 17\\nSkipping line 1095: expected 10 fields, saw 11\\nSkipping line 1096: expected 10 fields, saw 16\\nSkipping line 1098: expected 10 fields, saw 16\\nSkipping line 1100: expected 10 fields, saw 12\\nSkipping line 1103: expected 10 fields, saw 13\\nSkipping line 1107: expected 10 fields, saw 14\\nSkipping line 1134: expected 10 fields, saw 11\\nSkipping line 1135: expected 10 fields, saw 11\\nSkipping line 1148: expected 10 fields, saw 13\\nSkipping line 1159: expected 10 fields, saw 13\\nSkipping line 1166: expected 10 fields, saw 11\\nSkipping line 1186: expected 10 fields, saw 12\\nSkipping line 1187: expected 10 fields, saw 13\\nSkipping line 1190: expected 10 fields, saw 14\\nSkipping line 1195: expected 10 fields, saw 13\\nSkipping line 1198: expected 10 fields, saw 13\\nSkipping line 1203: expected 10 fields, saw 15\\nSkipping line 1212: expected 10 fields, saw 12\\nSkipping line 1213: expected 10 fields, saw 14\\nSkipping line 1223: expected 10 fields, saw 11\\nSkipping line 1227: expected 10 fields, saw 11\\nSkipping line 1231: expected 10 fields, saw 14\\nSkipping line 1233: expected 10 fields, saw 11\\nSkipping line 1236: expected 10 fields, saw 11\\nSkipping line 1237: expected 10 fields, saw 12\\nSkipping line 1238: expected 10 fields, saw 25\\nSkipping line 1266: expected 10 fields, saw 15\\n'\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('abstracts2.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Year; Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019;Deep learning  Dictionary learning  Feature learning  Land-use classification  Sparse representation Land-use classification in very high spatial resolution images is critical in the remote sensing field. Consequently</th>\n",
       "      <th>remarkable efforts have been conducted towards developing increasingly accurate approaches for this task. In recent years</th>\n",
       "      <th>deep learning has emerged as a dominant paradigm for machine learning</th>\n",
       "      <th>and methodologies based on deep convolutional neural networks have received particular attention from the remote sensing community. These methods typically utilize transfer learning and/or data augmentation to accommodate a small number of labeled images in the publicly available datasets in this field. However</th>\n",
       "      <th>they typically require powerful computers and/or a long time for training. In this work</th>\n",
       "      <th>we propose a simple and novel method for land-use classification in very high spatial resolution images</th>\n",
       "      <th>which efficiently combines transfer learning with a sparse representation. Specifically</th>\n",
       "      <th>the proposed method performs the classification of land-use scenes using a modified version of the well-known sparse representation-based classification method. While this method directly uses the training images to form dictionaries</th>\n",
       "      <th>which are employed to classify test images</th>\n",
       "      <td>our method utilizes a pre-trained deep convol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019;MRI classification  Abnormal brain images  Deep transfer learning  CNN Magnetic resonance imaging (MRI) is the most common imaging technique used to detect abnormal brain tumors. Traditionally</th>\n",
       "      <th>MRI images are analyzed manually by radiologists to detect the abnormal conditions in the brain. Manual interpretation of huge volume of images is time consuming and difficult. Hence</th>\n",
       "      <th>computer-based detection helps in accurate and fast diagnosis. In this study</th>\n",
       "      <th>we proposed an approach that uses deep transfer learning to automatically classify normal and abnormal brain MR images. Convolutional neural network (CNN) based ResNet34 model is used as a deep learning model. We have used current deep learning techniques such as data augmentation</th>\n",
       "      <th>optimal learning rate finder and fine-tuning to train the model. The proposed model achieved 5-fold classification accuracy of 100% on 613 MR images. Our developed system is ready to test on huge database and can assist the radiologists in their daily screening of MR images. (C) 2018 Elsevier B.V. All rights reserved.</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019;Deep learning  Kernel methods  Deep kernel  Deep embedding kernel  Supervised learning In this paper</th>\n",
       "      <th>we propose a novel supervised learning method that is called Deep Embedding Kernel (DEK). DEK combines the advantages of deep learning and kernel methods in a unified framework. More specifically</th>\n",
       "      <th>DEK is a learnable kernel represented by a newly designed deep architecture. Compared with predefined kernels</th>\n",
       "      <th>this kernel can be explicitly trained to map data to an optimized high-level feature space where data may have favorable features toward the application. Compared with typical deep learning using SoftMax or logistic regression as the top layer</th>\n",
       "      <th>DEK is expected to be more generalizable to new data. Experimental results show that DEK has superior performance than typical machine learning methods in identity detection and classification</th>\n",
       "      <th>and transfer learning</th>\n",
       "      <th>on different types of data including images</th>\n",
       "      <th>sequences</th>\n",
       "      <th>and regularly structured data. (C) 2019 Elsevier B.V. All rights reserved.</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019;Transfer learning  Fault diagnosis  Dictionary learning  Sucker rod pumping systems Sucker rod pumping wells are systems that their operation states varies slowly. For a relatively new well</th>\n",
       "      <th>it is hard to collect all kinds of fault samples for training. Moreover</th>\n",
       "      <th>samples from different wells are not always have similar distributions</th>\n",
       "      <th>so directly using samples from other wells as the training data may hardly get good results. In this paper</th>\n",
       "      <th>a novel framework is proposed to solve the aforementioned problems. For the source data from one well and the target data from another well</th>\n",
       "      <th>a transform matrix is calculated to transfer these data into a common low dimensional subspace. In this subspace</th>\n",
       "      <th>the source data that contain all kinds of fault samples and the target data that lack some kinds of fault samples can be represented by a shared dictionary matrix. By introducing two idea regularization terms</th>\n",
       "      <th>the structure information of source data and target data are included into the dictionary learning process. So the obtained dictionary has discriminative ability. Extensive experiments are conducted to evaluate the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019;Web page categorization  Metric learning  Transfer learning  Deep learning The growing amounts of online multimedia content challenge the current search</th>\n",
       "      <th>recommendation and information retrieval systems. Information in the form of visual elements is highly valuable in a range of web mining tasks. However</th>\n",
       "      <th>the mining of these resources is a difficult task due to the complexity and variability of images</th>\n",
       "      <th>and the cost of collecting big enough datasets to successfully train accurate deep learning models. This paper proposes a novel framework for the categorization of web pages on the basis of their visual content. This is achieved by exploring the joint application of a transfer learning strategy and metric learning techniques to build a Deep Convolutional Neural Network (DCNN) for feature extraction</th>\n",
       "      <th>even when training data is scarce. The obtained experimental results evidence that the proposed approach outperforms the state-of-the-art handcrafted image descriptors and achieves a high categorization accuracy. In addition</th>\n",
       "      <th>we address the problem of over-time learning</th>\n",
       "      <th>so the proposed framework can learn to identify new web page categories as new labeled images are provided at test time. As a result</th>\n",
       "      <th>prior knowledge of the complete set of possible web categories is not necessary in the initial training phase. (C) 2019 Elsevier B.V. All rights reserved.</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019;Texture classification  Texture scaling  Transfer learning  Partial least-square regression  Coupled dictionary learning  Local binary patterns Classification of texture patterns with large scale variations poses a great challenge for expert and intelligent systems. A pure learning approach addresses this issue by including texture patterns at all scales in the training dataset. This approach makes the construction of an expert system quite costly and unrealistic given the large variations in real-world texture scales and patterns. We propose a transfer learning approach where the full range of texture scales is available only for a small subset of the texture classes. Such a subset is used to learn the scaling map through partial least-square regression or coupled dictionary learning. Experimental results on classifiers equipped with the learned maps show promising reduction in training data scale variability with improved classification accuracy compared to the data-intensive pure learning approach. The proposed approach can be followed to build image-based expert systems of reasonable accuracy and limited data requirements. (C) 2018 Elsevier Ltd. All rights reserved.</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019;Transfer learning  multi-domain adaptation  constrained clustering  utility function Domain adaptation has been a primal approach to addressing the issues by lack of labels in many data mining tasks. Although considerable efforts have been devoted to domain adaptation with promising results</th>\n",
       "      <th>most existing work learns a classifier on a source domain and then predicts the labels for target data</th>\n",
       "      <th>where only the instances near the boundary determine the hyperplane and the whole structure information is ignored. Moreover</th>\n",
       "      <th>little work has been done regarding to multi-source domain adaptation. To that end</th>\n",
       "      <th>we develop a novel unsupervised domain adaptation framework</th>\n",
       "      <th>which ensures the whole structure of source domains is preserved to guide the target structure learning in a semi-supervised clustering fashion. To our knowledge</th>\n",
       "      <th>this is the first time when the domain adaptation problem is re-formulated as a semi-supervised clustering problem with target labels as missing values. Furthermore</th>\n",
       "      <th>by introducing an augmented matrix</th>\n",
       "      <th>a non-trivial solution is designed</th>\n",
       "      <td>which can be exactly mapped into a K-means-li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019;Transfer learning  distance metric learning  heterogeneous domains  knowledge fragments  nonlinear The goal of transfer learning is to improve the performance of target learning task by leveraging information (or transferring knowledge) from other related tasks. In this paper</th>\n",
       "      <th>we examine the problem of transfer distance metric learning (DML)</th>\n",
       "      <th>which usually aims to mitigate the label information deficiency issue in the target DML. Most of the current Transfer DML (TDML) methods are not applicable to the scenario where data are drawn from heterogeneous domains. Some existing heterogeneous transfer learning (HTL) approaches can learn target distance metric by usually transforming the samples of source and target domain into a common subspace. However</th>\n",
       "      <th>these approaches lack flexibility in real-world applications</th>\n",
       "      <th>and the learned transformations are often restricted to be linear. This motivates us to develop a general flexible heterogeneous TDML (HTDML) framework. In particular</th>\n",
       "      <th>any (linear/nonlinear) DML algorithms can be employed to learn the source metric beforehand. Then the pre-learned source metric is represented as a set of knowledge fragments to help target metric learning. We show how generalization error in the target domain could be reduced using the proposed transfer strategy</th>\n",
       "      <th>and develop novel algorithm to learn either linear or nonlinear target metric. Extensive experiments on various applications demonstrate the effectiveness of the proposed method.</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019; Rising crimes are likely to promote the need for effective security systems for baggage screening at airports. Therefore</th>\n",
       "      <th>technologies enabling public safety are of paramount importance. Out of all the technologies available</th>\n",
       "      <th>X-ray based baggage-screening plays a major role in threat detection. Originally the screening is done manually where a person scrutinizes the X-ray images on a screen to identify potential threat objects. Within this context</th>\n",
       "      <th>with limited dataset availability</th>\n",
       "      <th>we employ an imaging model for a generation of new X-ray images. In this article</th>\n",
       "      <th>an effort is made to perform threat object detection by using deep neural networks based framework. The framework is built upon Convolutional Neural Network (CNN) based techniques such as You Only Look Once (YOLO) and Faster Region based CNN (FRCNN) to perform threat object detection. Apart from this</th>\n",
       "      <th>to improve the model performance with limited original training data the transfer-learning paradigm is also tested out. The performance is studied on 4 classes of threat objects: 1) Gun  2) Shuriken  3) Razor-blade  4) Knife. As compared to traditional Machine Learning (ML) techniques</th>\n",
       "      <th>FRCNN uses region proposal in its first stage to produce better results. On using Faster RCNN with RESNET which was pre-trained on ImageNet dataset</th>\n",
       "      <th>98.4% accuracy is achieved for 4class threat recognition requiring 0.16 sec per image. Comparative performance of these threat detection techniques for cluttered X-ray baggage imagery is also presented. We firmly believe that it is possible to fully automate this screening process by using these computer vision techniques (C) 2019 Elsevier B.V. All rights reserved.</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019;Multi-view learning  Transfer learning  Learning using privileged information  Support vector machine In this paper</th>\n",
       "      <th>we present a multi-view transfer learning model named Multi-view Transfer Discriminative Model (MTDM) for both image and text classification tasks. Transfer learning</th>\n",
       "      <th>which aims to learn a robust classifier for the target domain using data from a different distribution</th>\n",
       "      <th>has been proved to be effective in many real-world applications. However</th>\n",
       "      <th>most of the existing transfer learning methods map across domain data into a high-dimension space which the distance between domains is closed. This strategy always fails in the multi-view scenario. On the contrary</th>\n",
       "      <th>the multi-view learning methods are also difficult to extend in the transfer learning settings. One of our goals in this paper is to develop a model which can perform better in both multi-view and transfer learning settings. On the one hand</th>\n",
       "      <th>the problem of multi-view is implemented by the paradigm of learning using privileged information (LUPI)</th>\n",
       "      <th>which could guarantee the principle of complementary and consensus. On the other hand</th>\n",
       "      <th>the model adequately utilizes the source domain data to build a robust classifier for the target domain. We evaluate our model on both image and text classification tasks and show the effectiveness compared with other baseline approaches. (C) 2019 Elsevier B.V. All rights reserved.</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Year; Abstract\n",
       "2019;Deep learning  Dictionary learning  Featur...  remarkable efforts have been conducted towards...  deep learning has emerged as a dominant paradi...  and methodologies based on deep convolutional ...  they typically require powerful computers and/...  we propose a simple and novel method for land-...  which efficiently combines transfer learning w...  the proposed method performs the classificatio...  which are employed to classify test images          our method utilizes a pre-trained deep convol...\n",
       "2019;MRI classification  Abnormal brain images ...  MRI images are analyzed manually by radiologis...  computer-based detection helps in accurate and...  we proposed an approach that uses deep transfe...  optimal learning rate finder and fine-tuning t... NaN                                                NaN                                                NaN                                                NaN                                                                                               NaN\n",
       "2019;Deep learning  Kernel methods  Deep kernel...  we propose a novel supervised learning method ...  DEK is a learnable kernel represented by a new...  this kernel can be explicitly trained to map d...  DEK is expected to be more generalizable to ne...  and transfer learning                              on different types of data including images        sequences                                          and regularly structured data. (C) 2019 Elsevi...                                                NaN\n",
       "2019;Transfer learning  Fault diagnosis  Dictio...  it is hard to collect all kinds of fault sampl...  samples from different wells are not always ha...  so directly using samples from other wells as ...  a novel framework is proposed to solve the afo...  a transform matrix is calculated to transfer t...  the source data that contain all kinds of faul...  the structure information of source data and t... NaN                                                                                               NaN\n",
       "2019;Web page categorization  Metric learning  ...  recommendation and information retrieval syste...  the mining of these resources is a difficult t...  and the cost of collecting big enough datasets...  even when training data is scarce. The obtaine...  we address the problem of over-time learning       so the proposed framework can learn to identif...  prior knowledge of the complete set of possibl... NaN                                                                                               NaN\n",
       "2019;Texture classification  Texture scaling  T... NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                                                               NaN\n",
       "2019;Transfer learning  multi-domain adaptation...  most existing work learns a classifier on a so...  where only the instances near the boundary det...  little work has been done regarding to multi-s...  we develop a novel unsupervised domain adaptat...  which ensures the whole structure of source do...  this is the first time when the domain adaptat...  by introducing an augmented matrix                 a non-trivial solution is designed                  which can be exactly mapped into a K-means-li...\n",
       "2019;Transfer learning  distance metric learnin...  we examine the problem of transfer distance me...  which usually aims to mitigate the label infor...  these approaches lack flexibility in real-worl...  and the learned transformations are often rest...  any (linear/nonlinear) DML algorithms can be e...  and develop novel algorithm to learn either li... NaN                                                NaN                                                                                               NaN\n",
       "2019; Rising crimes are likely to promote the n...  technologies enabling public safety are of par...  X-ray based baggage-screening plays a major ro...  with limited dataset availability                  we employ an imaging model for a generation of...  an effort is made to perform threat object det...  to improve the model performance with limited ...  FRCNN uses region proposal in its first stage ...  98.4% accuracy is achieved for 4class threat r...                                                NaN\n",
       "2019;Multi-view learning  Transfer learning  Le...  we present a multi-view transfer learning mode...  which aims to learn a robust classifier for th...  has been proved to be effective in many real-w...  most of the existing transfer learning methods...  the multi-view learning methods are also diffi...  the problem of multi-view is implemented by th...  which could guarantee the principle of complem...  the model adequately utilizes the source domai...                                                NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
