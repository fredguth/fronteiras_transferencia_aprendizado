<!-- some code adapted from www.degeneratestate.org/static/metal_lyrics/metal_line.html -->
<!-- <!DOCTYPE html>
<meta content="utf-8"> -->
<style> /* set the CSS */

body {
  font: 12px Arial;
}

svg {
  font: 12px Helvetica;
}

path {
  stroke: steelblue;
  stroke-width: 2;
  fill: none;
}

.grid line {
  stroke: lightgrey;
  stroke-opacity: 0.4;
  shape-rendering: crispEdges;
}

.grid path {
  stroke-width: 0;
}

.axis path,
.axis lineper {
  fill: none;
  stroke: grey;
  stroke-width: 1;
  shape-rendering: crispEdges;
}

div.tooltip {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 28px;
  padding: 2px;
  font: 12px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

div.tooltipscore {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 50px;
  padding: 2px;
  font: 10px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

.category_header {
  font: 12px sans-serif;
  font-weight: bolder;
  text-decoration: underline;
}

div.label {
  color: rgb(252, 251, 253);
  color: rgb(63, 0, 125);
  color: rgb(158, 155, 201);

  position: absolute;
  text-align: left;
  padding: 1px;
  border-spacing: 1px;
  font: 10px sans-serif;
  font-family: Sans-Serif;
  border: 0;
  pointer-events: none;
}

input {
  border: 1px dotted #ccc;
  background: white;
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

.alert {
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

ul.top_terms li {
  padding-right: 20px;
  font-size: 30pt;
  color: red;
}

input:focus {
  background-color: lightyellow;
  outline: none;
}

.snippet {
  padding-bottom: 10px;
  padding-left: 5px;
  padding-right: 5px;
  white-space: pre-wrap;
}

.snippet_header {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  font-weight: bolder;
  #text-decoration: underline;
  text-align: center;
  border-bottom-width: 10px;
  border-bottom-color: #888888;
  padding-bottom: 10px;
}

.topic_preview {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;
  font-weight: normal;
  text-decoration: none;
}


#d3-div-1-categoryinfo {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;    

}


#d3-div-1-title-div {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
}

.text_header {
  font: 18px sans-serif;
  font-size: 18px;
  font-family: Helvetica, Arial, Sans-Serif;

  font-weight: bolder;
  text-decoration: underline;
  text-align: center;
  color: darkblue;
  padding-bottom: 10px;
}

.text_subheader {
  font-size: 14px;
  font-family: Helvetica, Arial, Sans-Serif;

  text-align: center;
}

.snippet_meta {
  border-top: 3px solid #4588ba;
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  color: darkblue;
}

.contexts {
  width: 45%;
  float: left;
}

.neut_display {
  display: none;
  float: left
}

.scattertext {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.label {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.obscured {
  /*font-size: 14px;
  font-weight: normal;
  color: dimgrey;
  font-family: Helvetica;*/
  text-align: center;
}

.small_label {
  font-size: 10px;
}

#d3-div-1-corpus-stats {
  text-align: center;
}

#d3-div-1-cat {
}

#d3-div-1-notcat {
}

#d3-div-1-neut {
}

#d3-div-1-neutcol {
  display: none;
}

</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.6.0/d3.min.js" charset="utf-8"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js" charset="utf-8"></script>

<!-- INSERT SEMIOTIC SQUARE -->
<!--<a onclick="maxFreq = Math.log(data.map(d => d.cat + d.ncat).reduce((a,b) => Math.max(a,b))); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, false); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, true)">View Score Plot</a>-->
<span id="d3-div-1-title-div"></span>
<div class="scattertext" id="d3-div-1"></div>
<div id="d3-div-1-corpus-stats"></div>
<div id="d3-div-1-overlapped-terms"></div>
<form name="d3-div-1-termForm" onSubmit="plotInterface.handleSearch(); return false;">
  <input name="Submit" type="submit" value="Search for term">
  <input type="text" id="d3-div-1-searchTerm" placeholder="Type a word or two&hellip;">
  <span id="d3-div-1-alertMessage" class="alert"></span>
</form>
<a name="d3-div-1-snippets"></a>
<a name="d3-div-1-snippetsalt"></a>
<div id="d3-div-1-termstats"></div>
<div id="d3-div-1-overlapped-terms-clicked"></div>
<div id="d3-div-1-categoryinfo" style="display: hidden"></div>
<div id="d3-div-2">
  <div class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-cathead"></div>
    <div class="snippet" id="d3-div-1-cat"></div>
  </div>
  <div id="d3-div-1-notcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-notcathead"></div>
    <div class="snippet" id="d3-div-1-notcat"></div>
  </div>
  <div id="d3-div-1-neutcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-neuthead"></div>
    <div class="snippet" id="d3-div-1-neut"></div>
  </div>
</div>
<script charset="utf-8">
    // Created using Cozy: github.com/uwplse/cozy
function Rectangle(ax1, ay1, ax2, ay2) {
    this.ax1 = ax1;
    this.ay1 = ay1;
    this.ax2 = ax2;
    this.ay2 = ay2;
    this._left7 = undefined;
    this._right8 = undefined;
    this._parent9 = undefined;
    this._min_ax12 = undefined;
    this._min_ay13 = undefined;
    this._max_ay24 = undefined;
    this._height10 = undefined;
}
function RectangleHolder() {
    this.my_size = 0;
    (this)._root1 = null;
}
RectangleHolder.prototype.size = function () {
    return this.my_size;
};
RectangleHolder.prototype.add = function (x) {
    ++this.my_size;
    var _idx69 = (x).ax2;
    (x)._left7 = null;
    (x)._right8 = null;
    (x)._min_ax12 = (x).ax1;
    (x)._min_ay13 = (x).ay1;
    (x)._max_ay24 = (x).ay2;
    (x)._height10 = 0;
    var _previous70 = null;
    var _current71 = (this)._root1;
    var _is_left72 = false;
    while (!((_current71) == null)) {
        _previous70 = _current71;
        if ((_idx69) < ((_current71).ax2)) {
            _current71 = (_current71)._left7;
            _is_left72 = true;
        } else {
            _current71 = (_current71)._right8;
            _is_left72 = false;
        }
    }
    if ((_previous70) == null) {
        (this)._root1 = x;
    } else {
        (x)._parent9 = _previous70;
        if (_is_left72) {
            (_previous70)._left7 = x;
        } else {
            (_previous70)._right8 = x;
        }
    }
    var _cursor73 = (x)._parent9;
    var _changed74 = true;
    while ((_changed74) && (!((_cursor73) == (null)))) {
        var _old__min_ax1275 = (_cursor73)._min_ax12;
        var _old__min_ay1376 = (_cursor73)._min_ay13;
        var _old__max_ay2477 = (_cursor73)._max_ay24;
        var _old_height78 = (_cursor73)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval79 = (_cursor73).ax1;
        var _child80 = (_cursor73)._left7;
        if (!((_child80) == null)) {
            var _val81 = (_child80)._min_ax12;
            _augval79 = ((_augval79) < (_val81)) ? (_augval79) : (_val81);
        }
        var _child82 = (_cursor73)._right8;
        if (!((_child82) == null)) {
            var _val83 = (_child82)._min_ax12;
            _augval79 = ((_augval79) < (_val83)) ? (_augval79) : (_val83);
        }
        (_cursor73)._min_ax12 = _augval79;
        /* _min_ay13 is min of ay1 */
        var _augval84 = (_cursor73).ay1;
        var _child85 = (_cursor73)._left7;
        if (!((_child85) == null)) {
            var _val86 = (_child85)._min_ay13;
            _augval84 = ((_augval84) < (_val86)) ? (_augval84) : (_val86);
        }
        var _child87 = (_cursor73)._right8;
        if (!((_child87) == null)) {
            var _val88 = (_child87)._min_ay13;
            _augval84 = ((_augval84) < (_val88)) ? (_augval84) : (_val88);
        }
        (_cursor73)._min_ay13 = _augval84;
        /* _max_ay24 is max of ay2 */
        var _augval89 = (_cursor73).ay2;
        var _child90 = (_cursor73)._left7;
        if (!((_child90) == null)) {
            var _val91 = (_child90)._max_ay24;
            _augval89 = ((_augval89) < (_val91)) ? (_val91) : (_augval89);
        }
        var _child92 = (_cursor73)._right8;
        if (!((_child92) == null)) {
            var _val93 = (_child92)._max_ay24;
            _augval89 = ((_augval89) < (_val93)) ? (_val93) : (_augval89);
        }
        (_cursor73)._max_ay24 = _augval89;
        (_cursor73)._height10 = 1 + ((((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) > ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10))) ? ((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) : ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10)));
        _changed74 = false;
        _changed74 = (_changed74) || (!((_old__min_ax1275) == ((_cursor73)._min_ax12)));
        _changed74 = (_changed74) || (!((_old__min_ay1376) == ((_cursor73)._min_ay13)));
        _changed74 = (_changed74) || (!((_old__max_ay2477) == ((_cursor73)._max_ay24)));
        _changed74 = (_changed74) || (!((_old_height78) == ((_cursor73)._height10)));
        _cursor73 = (_cursor73)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor94 = x;
    var _imbalance95;
    while (!(((_cursor94)._parent9) == null)) {
        _cursor94 = (_cursor94)._parent9;
        (_cursor94)._height10 = 1 + ((((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) > ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10))) ? ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) : ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10)));
        _imbalance95 = ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) - ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10));
        if ((_imbalance95) > (1)) {
            if ((((((_cursor94)._left7)._left7) == null) ? (-1) : ((((_cursor94)._left7)._left7)._height10)) < (((((_cursor94)._left7)._right8) == null) ? (-1) : ((((_cursor94)._left7)._right8)._height10))) {
                /* rotate ((_cursor94)._left7)._right8 */
                var _a96 = (_cursor94)._left7;
                var _b97 = (_a96)._right8;
                var _c98 = (_b97)._left7;
                /* replace _a96 with _b97 in (_a96)._parent9 */
                if (!(((_a96)._parent9) == null)) {
                    if ((((_a96)._parent9)._left7) == (_a96)) {
                        ((_a96)._parent9)._left7 = _b97;
                    } else {
                        ((_a96)._parent9)._right8 = _b97;
                    }
                }
                if (!((_b97) == null)) {
                    (_b97)._parent9 = (_a96)._parent9;
                }
                /* replace _c98 with _a96 in _b97 */
                (_b97)._left7 = _a96;
                if (!((_a96) == null)) {
                    (_a96)._parent9 = _b97;
                }
                /* replace _b97 with _c98 in _a96 */
                (_a96)._right8 = _c98;
                if (!((_c98) == null)) {
                    (_c98)._parent9 = _a96;
                }
                /* _min_ax12 is min of ax1 */
                var _augval99 = (_a96).ax1;
                var _child100 = (_a96)._left7;
                if (!((_child100) == null)) {
                    var _val101 = (_child100)._min_ax12;
                    _augval99 = ((_augval99) < (_val101)) ? (_augval99) : (_val101);
                }
                var _child102 = (_a96)._right8;
                if (!((_child102) == null)) {
                    var _val103 = (_child102)._min_ax12;
                    _augval99 = ((_augval99) < (_val103)) ? (_augval99) : (_val103);
                }
                (_a96)._min_ax12 = _augval99;
                /* _min_ay13 is min of ay1 */
                var _augval104 = (_a96).ay1;
                var _child105 = (_a96)._left7;
                if (!((_child105) == null)) {
                    var _val106 = (_child105)._min_ay13;
                    _augval104 = ((_augval104) < (_val106)) ? (_augval104) : (_val106);
                }
                var _child107 = (_a96)._right8;
                if (!((_child107) == null)) {
                    var _val108 = (_child107)._min_ay13;
                    _augval104 = ((_augval104) < (_val108)) ? (_augval104) : (_val108);
                }
                (_a96)._min_ay13 = _augval104;
                /* _max_ay24 is max of ay2 */
                var _augval109 = (_a96).ay2;
                var _child110 = (_a96)._left7;
                if (!((_child110) == null)) {
                    var _val111 = (_child110)._max_ay24;
                    _augval109 = ((_augval109) < (_val111)) ? (_val111) : (_augval109);
                }
                var _child112 = (_a96)._right8;
                if (!((_child112) == null)) {
                    var _val113 = (_child112)._max_ay24;
                    _augval109 = ((_augval109) < (_val113)) ? (_val113) : (_augval109);
                }
                (_a96)._max_ay24 = _augval109;
                (_a96)._height10 = 1 + ((((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) > ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10))) ? ((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) : ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval114 = (_b97).ax1;
                var _child115 = (_b97)._left7;
                if (!((_child115) == null)) {
                    var _val116 = (_child115)._min_ax12;
                    _augval114 = ((_augval114) < (_val116)) ? (_augval114) : (_val116);
                }
                var _child117 = (_b97)._right8;
                if (!((_child117) == null)) {
                    var _val118 = (_child117)._min_ax12;
                    _augval114 = ((_augval114) < (_val118)) ? (_augval114) : (_val118);
                }
                (_b97)._min_ax12 = _augval114;
                /* _min_ay13 is min of ay1 */
                var _augval119 = (_b97).ay1;
                var _child120 = (_b97)._left7;
                if (!((_child120) == null)) {
                    var _val121 = (_child120)._min_ay13;
                    _augval119 = ((_augval119) < (_val121)) ? (_augval119) : (_val121);
                }
                var _child122 = (_b97)._right8;
                if (!((_child122) == null)) {
                    var _val123 = (_child122)._min_ay13;
                    _augval119 = ((_augval119) < (_val123)) ? (_augval119) : (_val123);
                }
                (_b97)._min_ay13 = _augval119;
                /* _max_ay24 is max of ay2 */
                var _augval124 = (_b97).ay2;
                var _child125 = (_b97)._left7;
                if (!((_child125) == null)) {
                    var _val126 = (_child125)._max_ay24;
                    _augval124 = ((_augval124) < (_val126)) ? (_val126) : (_augval124);
                }
                var _child127 = (_b97)._right8;
                if (!((_child127) == null)) {
                    var _val128 = (_child127)._max_ay24;
                    _augval124 = ((_augval124) < (_val128)) ? (_val128) : (_augval124);
                }
                (_b97)._max_ay24 = _augval124;
                (_b97)._height10 = 1 + ((((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) > ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10))) ? ((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) : ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10)));
                if (!(((_b97)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval129 = ((_b97)._parent9).ax1;
                    var _child130 = ((_b97)._parent9)._left7;
                    if (!((_child130) == null)) {
                        var _val131 = (_child130)._min_ax12;
                        _augval129 = ((_augval129) < (_val131)) ? (_augval129) : (_val131);
                    }
                    var _child132 = ((_b97)._parent9)._right8;
                    if (!((_child132) == null)) {
                        var _val133 = (_child132)._min_ax12;
                        _augval129 = ((_augval129) < (_val133)) ? (_augval129) : (_val133);
                    }
                    ((_b97)._parent9)._min_ax12 = _augval129;
                    /* _min_ay13 is min of ay1 */
                    var _augval134 = ((_b97)._parent9).ay1;
                    var _child135 = ((_b97)._parent9)._left7;
                    if (!((_child135) == null)) {
                        var _val136 = (_child135)._min_ay13;
                        _augval134 = ((_augval134) < (_val136)) ? (_augval134) : (_val136);
                    }
                    var _child137 = ((_b97)._parent9)._right8;
                    if (!((_child137) == null)) {
                        var _val138 = (_child137)._min_ay13;
                        _augval134 = ((_augval134) < (_val138)) ? (_augval134) : (_val138);
                    }
                    ((_b97)._parent9)._min_ay13 = _augval134;
                    /* _max_ay24 is max of ay2 */
                    var _augval139 = ((_b97)._parent9).ay2;
                    var _child140 = ((_b97)._parent9)._left7;
                    if (!((_child140) == null)) {
                        var _val141 = (_child140)._max_ay24;
                        _augval139 = ((_augval139) < (_val141)) ? (_val141) : (_augval139);
                    }
                    var _child142 = ((_b97)._parent9)._right8;
                    if (!((_child142) == null)) {
                        var _val143 = (_child142)._max_ay24;
                        _augval139 = ((_augval139) < (_val143)) ? (_val143) : (_augval139);
                    }
                    ((_b97)._parent9)._max_ay24 = _augval139;
                    ((_b97)._parent9)._height10 = 1 + (((((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) > (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10))) ? (((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) : (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b97;
                }
            }
            /* rotate (_cursor94)._left7 */
            var _a144 = _cursor94;
            var _b145 = (_a144)._left7;
            var _c146 = (_b145)._right8;
            /* replace _a144 with _b145 in (_a144)._parent9 */
            if (!(((_a144)._parent9) == null)) {
                if ((((_a144)._parent9)._left7) == (_a144)) {
                    ((_a144)._parent9)._left7 = _b145;
                } else {
                    ((_a144)._parent9)._right8 = _b145;
                }
            }
            if (!((_b145) == null)) {
                (_b145)._parent9 = (_a144)._parent9;
            }
            /* replace _c146 with _a144 in _b145 */
            (_b145)._right8 = _a144;
            if (!((_a144) == null)) {
                (_a144)._parent9 = _b145;
            }
            /* replace _b145 with _c146 in _a144 */
            (_a144)._left7 = _c146;
            if (!((_c146) == null)) {
                (_c146)._parent9 = _a144;
            }
            /* _min_ax12 is min of ax1 */
            var _augval147 = (_a144).ax1;
            var _child148 = (_a144)._left7;
            if (!((_child148) == null)) {
                var _val149 = (_child148)._min_ax12;
                _augval147 = ((_augval147) < (_val149)) ? (_augval147) : (_val149);
            }
            var _child150 = (_a144)._right8;
            if (!((_child150) == null)) {
                var _val151 = (_child150)._min_ax12;
                _augval147 = ((_augval147) < (_val151)) ? (_augval147) : (_val151);
            }
            (_a144)._min_ax12 = _augval147;
            /* _min_ay13 is min of ay1 */
            var _augval152 = (_a144).ay1;
            var _child153 = (_a144)._left7;
            if (!((_child153) == null)) {
                var _val154 = (_child153)._min_ay13;
                _augval152 = ((_augval152) < (_val154)) ? (_augval152) : (_val154);
            }
            var _child155 = (_a144)._right8;
            if (!((_child155) == null)) {
                var _val156 = (_child155)._min_ay13;
                _augval152 = ((_augval152) < (_val156)) ? (_augval152) : (_val156);
            }
            (_a144)._min_ay13 = _augval152;
            /* _max_ay24 is max of ay2 */
            var _augval157 = (_a144).ay2;
            var _child158 = (_a144)._left7;
            if (!((_child158) == null)) {
                var _val159 = (_child158)._max_ay24;
                _augval157 = ((_augval157) < (_val159)) ? (_val159) : (_augval157);
            }
            var _child160 = (_a144)._right8;
            if (!((_child160) == null)) {
                var _val161 = (_child160)._max_ay24;
                _augval157 = ((_augval157) < (_val161)) ? (_val161) : (_augval157);
            }
            (_a144)._max_ay24 = _augval157;
            (_a144)._height10 = 1 + ((((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) > ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10))) ? ((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) : ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval162 = (_b145).ax1;
            var _child163 = (_b145)._left7;
            if (!((_child163) == null)) {
                var _val164 = (_child163)._min_ax12;
                _augval162 = ((_augval162) < (_val164)) ? (_augval162) : (_val164);
            }
            var _child165 = (_b145)._right8;
            if (!((_child165) == null)) {
                var _val166 = (_child165)._min_ax12;
                _augval162 = ((_augval162) < (_val166)) ? (_augval162) : (_val166);
            }
            (_b145)._min_ax12 = _augval162;
            /* _min_ay13 is min of ay1 */
            var _augval167 = (_b145).ay1;
            var _child168 = (_b145)._left7;
            if (!((_child168) == null)) {
                var _val169 = (_child168)._min_ay13;
                _augval167 = ((_augval167) < (_val169)) ? (_augval167) : (_val169);
            }
            var _child170 = (_b145)._right8;
            if (!((_child170) == null)) {
                var _val171 = (_child170)._min_ay13;
                _augval167 = ((_augval167) < (_val171)) ? (_augval167) : (_val171);
            }
            (_b145)._min_ay13 = _augval167;
            /* _max_ay24 is max of ay2 */
            var _augval172 = (_b145).ay2;
            var _child173 = (_b145)._left7;
            if (!((_child173) == null)) {
                var _val174 = (_child173)._max_ay24;
                _augval172 = ((_augval172) < (_val174)) ? (_val174) : (_augval172);
            }
            var _child175 = (_b145)._right8;
            if (!((_child175) == null)) {
                var _val176 = (_child175)._max_ay24;
                _augval172 = ((_augval172) < (_val176)) ? (_val176) : (_augval172);
            }
            (_b145)._max_ay24 = _augval172;
            (_b145)._height10 = 1 + ((((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) > ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10))) ? ((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) : ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10)));
            if (!(((_b145)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval177 = ((_b145)._parent9).ax1;
                var _child178 = ((_b145)._parent9)._left7;
                if (!((_child178) == null)) {
                    var _val179 = (_child178)._min_ax12;
                    _augval177 = ((_augval177) < (_val179)) ? (_augval177) : (_val179);
                }
                var _child180 = ((_b145)._parent9)._right8;
                if (!((_child180) == null)) {
                    var _val181 = (_child180)._min_ax12;
                    _augval177 = ((_augval177) < (_val181)) ? (_augval177) : (_val181);
                }
                ((_b145)._parent9)._min_ax12 = _augval177;
                /* _min_ay13 is min of ay1 */
                var _augval182 = ((_b145)._parent9).ay1;
                var _child183 = ((_b145)._parent9)._left7;
                if (!((_child183) == null)) {
                    var _val184 = (_child183)._min_ay13;
                    _augval182 = ((_augval182) < (_val184)) ? (_augval182) : (_val184);
                }
                var _child185 = ((_b145)._parent9)._right8;
                if (!((_child185) == null)) {
                    var _val186 = (_child185)._min_ay13;
                    _augval182 = ((_augval182) < (_val186)) ? (_augval182) : (_val186);
                }
                ((_b145)._parent9)._min_ay13 = _augval182;
                /* _max_ay24 is max of ay2 */
                var _augval187 = ((_b145)._parent9).ay2;
                var _child188 = ((_b145)._parent9)._left7;
                if (!((_child188) == null)) {
                    var _val189 = (_child188)._max_ay24;
                    _augval187 = ((_augval187) < (_val189)) ? (_val189) : (_augval187);
                }
                var _child190 = ((_b145)._parent9)._right8;
                if (!((_child190) == null)) {
                    var _val191 = (_child190)._max_ay24;
                    _augval187 = ((_augval187) < (_val191)) ? (_val191) : (_augval187);
                }
                ((_b145)._parent9)._max_ay24 = _augval187;
                ((_b145)._parent9)._height10 = 1 + (((((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) > (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10))) ? (((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) : (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b145;
            }
            _cursor94 = (_cursor94)._parent9;
        } else if ((_imbalance95) < (-1)) {
            if ((((((_cursor94)._right8)._left7) == null) ? (-1) : ((((_cursor94)._right8)._left7)._height10)) > (((((_cursor94)._right8)._right8) == null) ? (-1) : ((((_cursor94)._right8)._right8)._height10))) {
                /* rotate ((_cursor94)._right8)._left7 */
                var _a192 = (_cursor94)._right8;
                var _b193 = (_a192)._left7;
                var _c194 = (_b193)._right8;
                /* replace _a192 with _b193 in (_a192)._parent9 */
                if (!(((_a192)._parent9) == null)) {
                    if ((((_a192)._parent9)._left7) == (_a192)) {
                        ((_a192)._parent9)._left7 = _b193;
                    } else {
                        ((_a192)._parent9)._right8 = _b193;
                    }
                }
                if (!((_b193) == null)) {
                    (_b193)._parent9 = (_a192)._parent9;
                }
                /* replace _c194 with _a192 in _b193 */
                (_b193)._right8 = _a192;
                if (!((_a192) == null)) {
                    (_a192)._parent9 = _b193;
                }
                /* replace _b193 with _c194 in _a192 */
                (_a192)._left7 = _c194;
                if (!((_c194) == null)) {
                    (_c194)._parent9 = _a192;
                }
                /* _min_ax12 is min of ax1 */
                var _augval195 = (_a192).ax1;
                var _child196 = (_a192)._left7;
                if (!((_child196) == null)) {
                    var _val197 = (_child196)._min_ax12;
                    _augval195 = ((_augval195) < (_val197)) ? (_augval195) : (_val197);
                }
                var _child198 = (_a192)._right8;
                if (!((_child198) == null)) {
                    var _val199 = (_child198)._min_ax12;
                    _augval195 = ((_augval195) < (_val199)) ? (_augval195) : (_val199);
                }
                (_a192)._min_ax12 = _augval195;
                /* _min_ay13 is min of ay1 */
                var _augval200 = (_a192).ay1;
                var _child201 = (_a192)._left7;
                if (!((_child201) == null)) {
                    var _val202 = (_child201)._min_ay13;
                    _augval200 = ((_augval200) < (_val202)) ? (_augval200) : (_val202);
                }
                var _child203 = (_a192)._right8;
                if (!((_child203) == null)) {
                    var _val204 = (_child203)._min_ay13;
                    _augval200 = ((_augval200) < (_val204)) ? (_augval200) : (_val204);
                }
                (_a192)._min_ay13 = _augval200;
                /* _max_ay24 is max of ay2 */
                var _augval205 = (_a192).ay2;
                var _child206 = (_a192)._left7;
                if (!((_child206) == null)) {
                    var _val207 = (_child206)._max_ay24;
                    _augval205 = ((_augval205) < (_val207)) ? (_val207) : (_augval205);
                }
                var _child208 = (_a192)._right8;
                if (!((_child208) == null)) {
                    var _val209 = (_child208)._max_ay24;
                    _augval205 = ((_augval205) < (_val209)) ? (_val209) : (_augval205);
                }
                (_a192)._max_ay24 = _augval205;
                (_a192)._height10 = 1 + ((((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) > ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10))) ? ((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) : ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval210 = (_b193).ax1;
                var _child211 = (_b193)._left7;
                if (!((_child211) == null)) {
                    var _val212 = (_child211)._min_ax12;
                    _augval210 = ((_augval210) < (_val212)) ? (_augval210) : (_val212);
                }
                var _child213 = (_b193)._right8;
                if (!((_child213) == null)) {
                    var _val214 = (_child213)._min_ax12;
                    _augval210 = ((_augval210) < (_val214)) ? (_augval210) : (_val214);
                }
                (_b193)._min_ax12 = _augval210;
                /* _min_ay13 is min of ay1 */
                var _augval215 = (_b193).ay1;
                var _child216 = (_b193)._left7;
                if (!((_child216) == null)) {
                    var _val217 = (_child216)._min_ay13;
                    _augval215 = ((_augval215) < (_val217)) ? (_augval215) : (_val217);
                }
                var _child218 = (_b193)._right8;
                if (!((_child218) == null)) {
                    var _val219 = (_child218)._min_ay13;
                    _augval215 = ((_augval215) < (_val219)) ? (_augval215) : (_val219);
                }
                (_b193)._min_ay13 = _augval215;
                /* _max_ay24 is max of ay2 */
                var _augval220 = (_b193).ay2;
                var _child221 = (_b193)._left7;
                if (!((_child221) == null)) {
                    var _val222 = (_child221)._max_ay24;
                    _augval220 = ((_augval220) < (_val222)) ? (_val222) : (_augval220);
                }
                var _child223 = (_b193)._right8;
                if (!((_child223) == null)) {
                    var _val224 = (_child223)._max_ay24;
                    _augval220 = ((_augval220) < (_val224)) ? (_val224) : (_augval220);
                }
                (_b193)._max_ay24 = _augval220;
                (_b193)._height10 = 1 + ((((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) > ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10))) ? ((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) : ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10)));
                if (!(((_b193)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval225 = ((_b193)._parent9).ax1;
                    var _child226 = ((_b193)._parent9)._left7;
                    if (!((_child226) == null)) {
                        var _val227 = (_child226)._min_ax12;
                        _augval225 = ((_augval225) < (_val227)) ? (_augval225) : (_val227);
                    }
                    var _child228 = ((_b193)._parent9)._right8;
                    if (!((_child228) == null)) {
                        var _val229 = (_child228)._min_ax12;
                        _augval225 = ((_augval225) < (_val229)) ? (_augval225) : (_val229);
                    }
                    ((_b193)._parent9)._min_ax12 = _augval225;
                    /* _min_ay13 is min of ay1 */
                    var _augval230 = ((_b193)._parent9).ay1;
                    var _child231 = ((_b193)._parent9)._left7;
                    if (!((_child231) == null)) {
                        var _val232 = (_child231)._min_ay13;
                        _augval230 = ((_augval230) < (_val232)) ? (_augval230) : (_val232);
                    }
                    var _child233 = ((_b193)._parent9)._right8;
                    if (!((_child233) == null)) {
                        var _val234 = (_child233)._min_ay13;
                        _augval230 = ((_augval230) < (_val234)) ? (_augval230) : (_val234);
                    }
                    ((_b193)._parent9)._min_ay13 = _augval230;
                    /* _max_ay24 is max of ay2 */
                    var _augval235 = ((_b193)._parent9).ay2;
                    var _child236 = ((_b193)._parent9)._left7;
                    if (!((_child236) == null)) {
                        var _val237 = (_child236)._max_ay24;
                        _augval235 = ((_augval235) < (_val237)) ? (_val237) : (_augval235);
                    }
                    var _child238 = ((_b193)._parent9)._right8;
                    if (!((_child238) == null)) {
                        var _val239 = (_child238)._max_ay24;
                        _augval235 = ((_augval235) < (_val239)) ? (_val239) : (_augval235);
                    }
                    ((_b193)._parent9)._max_ay24 = _augval235;
                    ((_b193)._parent9)._height10 = 1 + (((((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) > (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10))) ? (((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) : (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b193;
                }
            }
            /* rotate (_cursor94)._right8 */
            var _a240 = _cursor94;
            var _b241 = (_a240)._right8;
            var _c242 = (_b241)._left7;
            /* replace _a240 with _b241 in (_a240)._parent9 */
            if (!(((_a240)._parent9) == null)) {
                if ((((_a240)._parent9)._left7) == (_a240)) {
                    ((_a240)._parent9)._left7 = _b241;
                } else {
                    ((_a240)._parent9)._right8 = _b241;
                }
            }
            if (!((_b241) == null)) {
                (_b241)._parent9 = (_a240)._parent9;
            }
            /* replace _c242 with _a240 in _b241 */
            (_b241)._left7 = _a240;
            if (!((_a240) == null)) {
                (_a240)._parent9 = _b241;
            }
            /* replace _b241 with _c242 in _a240 */
            (_a240)._right8 = _c242;
            if (!((_c242) == null)) {
                (_c242)._parent9 = _a240;
            }
            /* _min_ax12 is min of ax1 */
            var _augval243 = (_a240).ax1;
            var _child244 = (_a240)._left7;
            if (!((_child244) == null)) {
                var _val245 = (_child244)._min_ax12;
                _augval243 = ((_augval243) < (_val245)) ? (_augval243) : (_val245);
            }
            var _child246 = (_a240)._right8;
            if (!((_child246) == null)) {
                var _val247 = (_child246)._min_ax12;
                _augval243 = ((_augval243) < (_val247)) ? (_augval243) : (_val247);
            }
            (_a240)._min_ax12 = _augval243;
            /* _min_ay13 is min of ay1 */
            var _augval248 = (_a240).ay1;
            var _child249 = (_a240)._left7;
            if (!((_child249) == null)) {
                var _val250 = (_child249)._min_ay13;
                _augval248 = ((_augval248) < (_val250)) ? (_augval248) : (_val250);
            }
            var _child251 = (_a240)._right8;
            if (!((_child251) == null)) {
                var _val252 = (_child251)._min_ay13;
                _augval248 = ((_augval248) < (_val252)) ? (_augval248) : (_val252);
            }
            (_a240)._min_ay13 = _augval248;
            /* _max_ay24 is max of ay2 */
            var _augval253 = (_a240).ay2;
            var _child254 = (_a240)._left7;
            if (!((_child254) == null)) {
                var _val255 = (_child254)._max_ay24;
                _augval253 = ((_augval253) < (_val255)) ? (_val255) : (_augval253);
            }
            var _child256 = (_a240)._right8;
            if (!((_child256) == null)) {
                var _val257 = (_child256)._max_ay24;
                _augval253 = ((_augval253) < (_val257)) ? (_val257) : (_augval253);
            }
            (_a240)._max_ay24 = _augval253;
            (_a240)._height10 = 1 + ((((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) > ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10))) ? ((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) : ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval258 = (_b241).ax1;
            var _child259 = (_b241)._left7;
            if (!((_child259) == null)) {
                var _val260 = (_child259)._min_ax12;
                _augval258 = ((_augval258) < (_val260)) ? (_augval258) : (_val260);
            }
            var _child261 = (_b241)._right8;
            if (!((_child261) == null)) {
                var _val262 = (_child261)._min_ax12;
                _augval258 = ((_augval258) < (_val262)) ? (_augval258) : (_val262);
            }
            (_b241)._min_ax12 = _augval258;
            /* _min_ay13 is min of ay1 */
            var _augval263 = (_b241).ay1;
            var _child264 = (_b241)._left7;
            if (!((_child264) == null)) {
                var _val265 = (_child264)._min_ay13;
                _augval263 = ((_augval263) < (_val265)) ? (_augval263) : (_val265);
            }
            var _child266 = (_b241)._right8;
            if (!((_child266) == null)) {
                var _val267 = (_child266)._min_ay13;
                _augval263 = ((_augval263) < (_val267)) ? (_augval263) : (_val267);
            }
            (_b241)._min_ay13 = _augval263;
            /* _max_ay24 is max of ay2 */
            var _augval268 = (_b241).ay2;
            var _child269 = (_b241)._left7;
            if (!((_child269) == null)) {
                var _val270 = (_child269)._max_ay24;
                _augval268 = ((_augval268) < (_val270)) ? (_val270) : (_augval268);
            }
            var _child271 = (_b241)._right8;
            if (!((_child271) == null)) {
                var _val272 = (_child271)._max_ay24;
                _augval268 = ((_augval268) < (_val272)) ? (_val272) : (_augval268);
            }
            (_b241)._max_ay24 = _augval268;
            (_b241)._height10 = 1 + ((((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) > ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10))) ? ((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) : ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10)));
            if (!(((_b241)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval273 = ((_b241)._parent9).ax1;
                var _child274 = ((_b241)._parent9)._left7;
                if (!((_child274) == null)) {
                    var _val275 = (_child274)._min_ax12;
                    _augval273 = ((_augval273) < (_val275)) ? (_augval273) : (_val275);
                }
                var _child276 = ((_b241)._parent9)._right8;
                if (!((_child276) == null)) {
                    var _val277 = (_child276)._min_ax12;
                    _augval273 = ((_augval273) < (_val277)) ? (_augval273) : (_val277);
                }
                ((_b241)._parent9)._min_ax12 = _augval273;
                /* _min_ay13 is min of ay1 */
                var _augval278 = ((_b241)._parent9).ay1;
                var _child279 = ((_b241)._parent9)._left7;
                if (!((_child279) == null)) {
                    var _val280 = (_child279)._min_ay13;
                    _augval278 = ((_augval278) < (_val280)) ? (_augval278) : (_val280);
                }
                var _child281 = ((_b241)._parent9)._right8;
                if (!((_child281) == null)) {
                    var _val282 = (_child281)._min_ay13;
                    _augval278 = ((_augval278) < (_val282)) ? (_augval278) : (_val282);
                }
                ((_b241)._parent9)._min_ay13 = _augval278;
                /* _max_ay24 is max of ay2 */
                var _augval283 = ((_b241)._parent9).ay2;
                var _child284 = ((_b241)._parent9)._left7;
                if (!((_child284) == null)) {
                    var _val285 = (_child284)._max_ay24;
                    _augval283 = ((_augval283) < (_val285)) ? (_val285) : (_augval283);
                }
                var _child286 = ((_b241)._parent9)._right8;
                if (!((_child286) == null)) {
                    var _val287 = (_child286)._max_ay24;
                    _augval283 = ((_augval283) < (_val287)) ? (_val287) : (_augval283);
                }
                ((_b241)._parent9)._max_ay24 = _augval283;
                ((_b241)._parent9)._height10 = 1 + (((((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) > (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10))) ? (((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) : (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b241;
            }
            _cursor94 = (_cursor94)._parent9;
        }
    }
};
RectangleHolder.prototype.remove = function (x) {
    --this.my_size;
    var _parent288 = (x)._parent9;
    var _left289 = (x)._left7;
    var _right290 = (x)._right8;
    var _new_x291;
    if (((_left289) == null) && ((_right290) == null)) {
        _new_x291 = null;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if ((!((_left289) == null)) && ((_right290) == null)) {
        _new_x291 = _left289;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if (((_left289) == null) && (!((_right290) == null))) {
        _new_x291 = _right290;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else {
        var _root292 = (x)._right8;
        var _x293 = _root292;
        var _descend294 = true;
        var _from_left295 = true;
        while (true) {
            if ((_x293) == null) {
                _x293 = null;
                break;
            }
            if (_descend294) {
                /* too small? */
                if (false) {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                } else if ((!(((_x293)._left7) == null)) && (true)) {
                    _x293 = (_x293)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x293) == (_root292)) {
                    _root292 = (_x293)._right8;
                    _x293 = (_x293)._right8;
                } else {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                }
            } else if (_from_left295) {
                if (false) {
                    _x293 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x293)._right8) == null)) && (true)) {
                    _descend294 = true;
                    if ((_x293) == (_root292)) {
                        _root292 = (_x293)._right8;
                    }
                    _x293 = (_x293)._right8;
                } else if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            } else {
                if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            }
        }
        _new_x291 = _x293;
        var _mp296 = (_x293)._parent9;
        var _mr297 = (_x293)._right8;
        /* replace _x293 with _mr297 in _mp296 */
        if (!((_mp296) == null)) {
            if (((_mp296)._left7) == (_x293)) {
                (_mp296)._left7 = _mr297;
            } else {
                (_mp296)._right8 = _mr297;
            }
        }
        if (!((_mr297) == null)) {
            (_mr297)._parent9 = _mp296;
        }
        /* replace x with _x293 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _x293;
            } else {
                (_parent288)._right8 = _x293;
            }
        }
        if (!((_x293) == null)) {
            (_x293)._parent9 = _parent288;
        }
        /* replace null with _left289 in _x293 */
        (_x293)._left7 = _left289;
        if (!((_left289) == null)) {
            (_left289)._parent9 = _x293;
        }
        /* replace _mr297 with (x)._right8 in _x293 */
        (_x293)._right8 = (x)._right8;
        if (!(((x)._right8) == null)) {
            ((x)._right8)._parent9 = _x293;
        }
        /* _min_ax12 is min of ax1 */
        var _augval298 = (_x293).ax1;
        var _child299 = (_x293)._left7;
        if (!((_child299) == null)) {
            var _val300 = (_child299)._min_ax12;
            _augval298 = ((_augval298) < (_val300)) ? (_augval298) : (_val300);
        }
        var _child301 = (_x293)._right8;
        if (!((_child301) == null)) {
            var _val302 = (_child301)._min_ax12;
            _augval298 = ((_augval298) < (_val302)) ? (_augval298) : (_val302);
        }
        (_x293)._min_ax12 = _augval298;
        /* _min_ay13 is min of ay1 */
        var _augval303 = (_x293).ay1;
        var _child304 = (_x293)._left7;
        if (!((_child304) == null)) {
            var _val305 = (_child304)._min_ay13;
            _augval303 = ((_augval303) < (_val305)) ? (_augval303) : (_val305);
        }
        var _child306 = (_x293)._right8;
        if (!((_child306) == null)) {
            var _val307 = (_child306)._min_ay13;
            _augval303 = ((_augval303) < (_val307)) ? (_augval303) : (_val307);
        }
        (_x293)._min_ay13 = _augval303;
        /* _max_ay24 is max of ay2 */
        var _augval308 = (_x293).ay2;
        var _child309 = (_x293)._left7;
        if (!((_child309) == null)) {
            var _val310 = (_child309)._max_ay24;
            _augval308 = ((_augval308) < (_val310)) ? (_val310) : (_augval308);
        }
        var _child311 = (_x293)._right8;
        if (!((_child311) == null)) {
            var _val312 = (_child311)._max_ay24;
            _augval308 = ((_augval308) < (_val312)) ? (_val312) : (_augval308);
        }
        (_x293)._max_ay24 = _augval308;
        (_x293)._height10 = 1 + ((((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) > ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10))) ? ((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) : ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10)));
        var _cursor313 = _mp296;
        var _changed314 = true;
        while ((_changed314) && (!((_cursor313) == (_parent288)))) {
            var _old__min_ax12315 = (_cursor313)._min_ax12;
            var _old__min_ay13316 = (_cursor313)._min_ay13;
            var _old__max_ay24317 = (_cursor313)._max_ay24;
            var _old_height318 = (_cursor313)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval319 = (_cursor313).ax1;
            var _child320 = (_cursor313)._left7;
            if (!((_child320) == null)) {
                var _val321 = (_child320)._min_ax12;
                _augval319 = ((_augval319) < (_val321)) ? (_augval319) : (_val321);
            }
            var _child322 = (_cursor313)._right8;
            if (!((_child322) == null)) {
                var _val323 = (_child322)._min_ax12;
                _augval319 = ((_augval319) < (_val323)) ? (_augval319) : (_val323);
            }
            (_cursor313)._min_ax12 = _augval319;
            /* _min_ay13 is min of ay1 */
            var _augval324 = (_cursor313).ay1;
            var _child325 = (_cursor313)._left7;
            if (!((_child325) == null)) {
                var _val326 = (_child325)._min_ay13;
                _augval324 = ((_augval324) < (_val326)) ? (_augval324) : (_val326);
            }
            var _child327 = (_cursor313)._right8;
            if (!((_child327) == null)) {
                var _val328 = (_child327)._min_ay13;
                _augval324 = ((_augval324) < (_val328)) ? (_augval324) : (_val328);
            }
            (_cursor313)._min_ay13 = _augval324;
            /* _max_ay24 is max of ay2 */
            var _augval329 = (_cursor313).ay2;
            var _child330 = (_cursor313)._left7;
            if (!((_child330) == null)) {
                var _val331 = (_child330)._max_ay24;
                _augval329 = ((_augval329) < (_val331)) ? (_val331) : (_augval329);
            }
            var _child332 = (_cursor313)._right8;
            if (!((_child332) == null)) {
                var _val333 = (_child332)._max_ay24;
                _augval329 = ((_augval329) < (_val333)) ? (_val333) : (_augval329);
            }
            (_cursor313)._max_ay24 = _augval329;
            (_cursor313)._height10 = 1 + ((((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) > ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10))) ? ((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) : ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10)));
            _changed314 = false;
            _changed314 = (_changed314) || (!((_old__min_ax12315) == ((_cursor313)._min_ax12)));
            _changed314 = (_changed314) || (!((_old__min_ay13316) == ((_cursor313)._min_ay13)));
            _changed314 = (_changed314) || (!((_old__max_ay24317) == ((_cursor313)._max_ay24)));
            _changed314 = (_changed314) || (!((_old_height318) == ((_cursor313)._height10)));
            _cursor313 = (_cursor313)._parent9;
        }
    }
    var _cursor334 = _parent288;
    var _changed335 = true;
    while ((_changed335) && (!((_cursor334) == (null)))) {
        var _old__min_ax12336 = (_cursor334)._min_ax12;
        var _old__min_ay13337 = (_cursor334)._min_ay13;
        var _old__max_ay24338 = (_cursor334)._max_ay24;
        var _old_height339 = (_cursor334)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval340 = (_cursor334).ax1;
        var _child341 = (_cursor334)._left7;
        if (!((_child341) == null)) {
            var _val342 = (_child341)._min_ax12;
            _augval340 = ((_augval340) < (_val342)) ? (_augval340) : (_val342);
        }
        var _child343 = (_cursor334)._right8;
        if (!((_child343) == null)) {
            var _val344 = (_child343)._min_ax12;
            _augval340 = ((_augval340) < (_val344)) ? (_augval340) : (_val344);
        }
        (_cursor334)._min_ax12 = _augval340;
        /* _min_ay13 is min of ay1 */
        var _augval345 = (_cursor334).ay1;
        var _child346 = (_cursor334)._left7;
        if (!((_child346) == null)) {
            var _val347 = (_child346)._min_ay13;
            _augval345 = ((_augval345) < (_val347)) ? (_augval345) : (_val347);
        }
        var _child348 = (_cursor334)._right8;
        if (!((_child348) == null)) {
            var _val349 = (_child348)._min_ay13;
            _augval345 = ((_augval345) < (_val349)) ? (_augval345) : (_val349);
        }
        (_cursor334)._min_ay13 = _augval345;
        /* _max_ay24 is max of ay2 */
        var _augval350 = (_cursor334).ay2;
        var _child351 = (_cursor334)._left7;
        if (!((_child351) == null)) {
            var _val352 = (_child351)._max_ay24;
            _augval350 = ((_augval350) < (_val352)) ? (_val352) : (_augval350);
        }
        var _child353 = (_cursor334)._right8;
        if (!((_child353) == null)) {
            var _val354 = (_child353)._max_ay24;
            _augval350 = ((_augval350) < (_val354)) ? (_val354) : (_augval350);
        }
        (_cursor334)._max_ay24 = _augval350;
        (_cursor334)._height10 = 1 + ((((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) > ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10))) ? ((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) : ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10)));
        _changed335 = false;
        _changed335 = (_changed335) || (!((_old__min_ax12336) == ((_cursor334)._min_ax12)));
        _changed335 = (_changed335) || (!((_old__min_ay13337) == ((_cursor334)._min_ay13)));
        _changed335 = (_changed335) || (!((_old__max_ay24338) == ((_cursor334)._max_ay24)));
        _changed335 = (_changed335) || (!((_old_height339) == ((_cursor334)._height10)));
        _cursor334 = (_cursor334)._parent9;
    }
    if (((this)._root1) == (x)) {
        (this)._root1 = _new_x291;
    }
};
RectangleHolder.prototype.updateAx1 = function (__x, new_val) {
    if ((__x).ax1 != new_val) {
        /* _min_ax12 is min of ax1 */
        var _augval355 = new_val;
        var _child356 = (__x)._left7;
        if (!((_child356) == null)) {
            var _val357 = (_child356)._min_ax12;
            _augval355 = ((_augval355) < (_val357)) ? (_augval355) : (_val357);
        }
        var _child358 = (__x)._right8;
        if (!((_child358) == null)) {
            var _val359 = (_child358)._min_ax12;
            _augval355 = ((_augval355) < (_val359)) ? (_augval355) : (_val359);
        }
        (__x)._min_ax12 = _augval355;
        var _cursor360 = (__x)._parent9;
        var _changed361 = true;
        while ((_changed361) && (!((_cursor360) == (null)))) {
            var _old__min_ax12362 = (_cursor360)._min_ax12;
            var _old_height363 = (_cursor360)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval364 = (_cursor360).ax1;
            var _child365 = (_cursor360)._left7;
            if (!((_child365) == null)) {
                var _val366 = (_child365)._min_ax12;
                _augval364 = ((_augval364) < (_val366)) ? (_augval364) : (_val366);
            }
            var _child367 = (_cursor360)._right8;
            if (!((_child367) == null)) {
                var _val368 = (_child367)._min_ax12;
                _augval364 = ((_augval364) < (_val368)) ? (_augval364) : (_val368);
            }
            (_cursor360)._min_ax12 = _augval364;
            (_cursor360)._height10 = 1 + ((((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) > ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10))) ? ((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) : ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10)));
            _changed361 = false;
            _changed361 = (_changed361) || (!((_old__min_ax12362) == ((_cursor360)._min_ax12)));
            _changed361 = (_changed361) || (!((_old_height363) == ((_cursor360)._height10)));
            _cursor360 = (_cursor360)._parent9;
        }
        (__x).ax1 = new_val;
    }
}
RectangleHolder.prototype.updateAy1 = function (__x, new_val) {
    if ((__x).ay1 != new_val) {
        /* _min_ay13 is min of ay1 */
        var _augval369 = new_val;
        var _child370 = (__x)._left7;
        if (!((_child370) == null)) {
            var _val371 = (_child370)._min_ay13;
            _augval369 = ((_augval369) < (_val371)) ? (_augval369) : (_val371);
        }
        var _child372 = (__x)._right8;
        if (!((_child372) == null)) {
            var _val373 = (_child372)._min_ay13;
            _augval369 = ((_augval369) < (_val373)) ? (_augval369) : (_val373);
        }
        (__x)._min_ay13 = _augval369;
        var _cursor374 = (__x)._parent9;
        var _changed375 = true;
        while ((_changed375) && (!((_cursor374) == (null)))) {
            var _old__min_ay13376 = (_cursor374)._min_ay13;
            var _old_height377 = (_cursor374)._height10;
            /* _min_ay13 is min of ay1 */
            var _augval378 = (_cursor374).ay1;
            var _child379 = (_cursor374)._left7;
            if (!((_child379) == null)) {
                var _val380 = (_child379)._min_ay13;
                _augval378 = ((_augval378) < (_val380)) ? (_augval378) : (_val380);
            }
            var _child381 = (_cursor374)._right8;
            if (!((_child381) == null)) {
                var _val382 = (_child381)._min_ay13;
                _augval378 = ((_augval378) < (_val382)) ? (_augval378) : (_val382);
            }
            (_cursor374)._min_ay13 = _augval378;
            (_cursor374)._height10 = 1 + ((((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) > ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10))) ? ((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) : ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10)));
            _changed375 = false;
            _changed375 = (_changed375) || (!((_old__min_ay13376) == ((_cursor374)._min_ay13)));
            _changed375 = (_changed375) || (!((_old_height377) == ((_cursor374)._height10)));
            _cursor374 = (_cursor374)._parent9;
        }
        (__x).ay1 = new_val;
    }
}
RectangleHolder.prototype.updateAx2 = function (__x, new_val) {
    if ((__x).ax2 != new_val) {
        var _parent383 = (__x)._parent9;
        var _left384 = (__x)._left7;
        var _right385 = (__x)._right8;
        var _new_x386;
        if (((_left384) == null) && ((_right385) == null)) {
            _new_x386 = null;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if ((!((_left384) == null)) && ((_right385) == null)) {
            _new_x386 = _left384;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if (((_left384) == null) && (!((_right385) == null))) {
            _new_x386 = _right385;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else {
            var _root387 = (__x)._right8;
            var _x388 = _root387;
            var _descend389 = true;
            var _from_left390 = true;
            while (true) {
                if ((_x388) == null) {
                    _x388 = null;
                    break;
                }
                if (_descend389) {
                    /* too small? */
                    if (false) {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    } else if ((!(((_x388)._left7) == null)) && (true)) {
                        _x388 = (_x388)._left7;
                        /* too large? */
                    } else if (false) {
                        if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                        /* node ok? */
                    } else if (true) {
                        break;
                    } else if ((_x388) == (_root387)) {
                        _root387 = (_x388)._right8;
                        _x388 = (_x388)._right8;
                    } else {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    }
                } else if (_from_left390) {
                    if (false) {
                        _x388 = null;
                        break;
                    } else if (true) {
                        break;
                    } else if ((!(((_x388)._right8) == null)) && (true)) {
                        _descend389 = true;
                        if ((_x388) == (_root387)) {
                            _root387 = (_x388)._right8;
                        }
                        _x388 = (_x388)._right8;
                    } else if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                } else {
                    if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                }
            }
            _new_x386 = _x388;
            var _mp391 = (_x388)._parent9;
            var _mr392 = (_x388)._right8;
            /* replace _x388 with _mr392 in _mp391 */
            if (!((_mp391) == null)) {
                if (((_mp391)._left7) == (_x388)) {
                    (_mp391)._left7 = _mr392;
                } else {
                    (_mp391)._right8 = _mr392;
                }
            }
            if (!((_mr392) == null)) {
                (_mr392)._parent9 = _mp391;
            }
            /* replace __x with _x388 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _x388;
                } else {
                    (_parent383)._right8 = _x388;
                }
            }
            if (!((_x388) == null)) {
                (_x388)._parent9 = _parent383;
            }
            /* replace null with _left384 in _x388 */
            (_x388)._left7 = _left384;
            if (!((_left384) == null)) {
                (_left384)._parent9 = _x388;
            }
            /* replace _mr392 with (__x)._right8 in _x388 */
            (_x388)._right8 = (__x)._right8;
            if (!(((__x)._right8) == null)) {
                ((__x)._right8)._parent9 = _x388;
            }
            /* _min_ax12 is min of ax1 */
            var _augval393 = (_x388).ax1;
            var _child394 = (_x388)._left7;
            if (!((_child394) == null)) {
                var _val395 = (_child394)._min_ax12;
                _augval393 = ((_augval393) < (_val395)) ? (_augval393) : (_val395);
            }
            var _child396 = (_x388)._right8;
            if (!((_child396) == null)) {
                var _val397 = (_child396)._min_ax12;
                _augval393 = ((_augval393) < (_val397)) ? (_augval393) : (_val397);
            }
            (_x388)._min_ax12 = _augval393;
            /* _min_ay13 is min of ay1 */
            var _augval398 = (_x388).ay1;
            var _child399 = (_x388)._left7;
            if (!((_child399) == null)) {
                var _val400 = (_child399)._min_ay13;
                _augval398 = ((_augval398) < (_val400)) ? (_augval398) : (_val400);
            }
            var _child401 = (_x388)._right8;
            if (!((_child401) == null)) {
                var _val402 = (_child401)._min_ay13;
                _augval398 = ((_augval398) < (_val402)) ? (_augval398) : (_val402);
            }
            (_x388)._min_ay13 = _augval398;
            /* _max_ay24 is max of ay2 */
            var _augval403 = (_x388).ay2;
            var _child404 = (_x388)._left7;
            if (!((_child404) == null)) {
                var _val405 = (_child404)._max_ay24;
                _augval403 = ((_augval403) < (_val405)) ? (_val405) : (_augval403);
            }
            var _child406 = (_x388)._right8;
            if (!((_child406) == null)) {
                var _val407 = (_child406)._max_ay24;
                _augval403 = ((_augval403) < (_val407)) ? (_val407) : (_augval403);
            }
            (_x388)._max_ay24 = _augval403;
            (_x388)._height10 = 1 + ((((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) > ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10))) ? ((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) : ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10)));
            var _cursor408 = _mp391;
            var _changed409 = true;
            while ((_changed409) && (!((_cursor408) == (_parent383)))) {
                var _old__min_ax12410 = (_cursor408)._min_ax12;
                var _old__min_ay13411 = (_cursor408)._min_ay13;
                var _old__max_ay24412 = (_cursor408)._max_ay24;
                var _old_height413 = (_cursor408)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval414 = (_cursor408).ax1;
                var _child415 = (_cursor408)._left7;
                if (!((_child415) == null)) {
                    var _val416 = (_child415)._min_ax12;
                    _augval414 = ((_augval414) < (_val416)) ? (_augval414) : (_val416);
                }
                var _child417 = (_cursor408)._right8;
                if (!((_child417) == null)) {
                    var _val418 = (_child417)._min_ax12;
                    _augval414 = ((_augval414) < (_val418)) ? (_augval414) : (_val418);
                }
                (_cursor408)._min_ax12 = _augval414;
                /* _min_ay13 is min of ay1 */
                var _augval419 = (_cursor408).ay1;
                var _child420 = (_cursor408)._left7;
                if (!((_child420) == null)) {
                    var _val421 = (_child420)._min_ay13;
                    _augval419 = ((_augval419) < (_val421)) ? (_augval419) : (_val421);
                }
                var _child422 = (_cursor408)._right8;
                if (!((_child422) == null)) {
                    var _val423 = (_child422)._min_ay13;
                    _augval419 = ((_augval419) < (_val423)) ? (_augval419) : (_val423);
                }
                (_cursor408)._min_ay13 = _augval419;
                /* _max_ay24 is max of ay2 */
                var _augval424 = (_cursor408).ay2;
                var _child425 = (_cursor408)._left7;
                if (!((_child425) == null)) {
                    var _val426 = (_child425)._max_ay24;
                    _augval424 = ((_augval424) < (_val426)) ? (_val426) : (_augval424);
                }
                var _child427 = (_cursor408)._right8;
                if (!((_child427) == null)) {
                    var _val428 = (_child427)._max_ay24;
                    _augval424 = ((_augval424) < (_val428)) ? (_val428) : (_augval424);
                }
                (_cursor408)._max_ay24 = _augval424;
                (_cursor408)._height10 = 1 + ((((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) > ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10))) ? ((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) : ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10)));
                _changed409 = false;
                _changed409 = (_changed409) || (!((_old__min_ax12410) == ((_cursor408)._min_ax12)));
                _changed409 = (_changed409) || (!((_old__min_ay13411) == ((_cursor408)._min_ay13)));
                _changed409 = (_changed409) || (!((_old__max_ay24412) == ((_cursor408)._max_ay24)));
                _changed409 = (_changed409) || (!((_old_height413) == ((_cursor408)._height10)));
                _cursor408 = (_cursor408)._parent9;
            }
        }
        var _cursor429 = _parent383;
        var _changed430 = true;
        while ((_changed430) && (!((_cursor429) == (null)))) {
            var _old__min_ax12431 = (_cursor429)._min_ax12;
            var _old__min_ay13432 = (_cursor429)._min_ay13;
            var _old__max_ay24433 = (_cursor429)._max_ay24;
            var _old_height434 = (_cursor429)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval435 = (_cursor429).ax1;
            var _child436 = (_cursor429)._left7;
            if (!((_child436) == null)) {
                var _val437 = (_child436)._min_ax12;
                _augval435 = ((_augval435) < (_val437)) ? (_augval435) : (_val437);
            }
            var _child438 = (_cursor429)._right8;
            if (!((_child438) == null)) {
                var _val439 = (_child438)._min_ax12;
                _augval435 = ((_augval435) < (_val439)) ? (_augval435) : (_val439);
            }
            (_cursor429)._min_ax12 = _augval435;
            /* _min_ay13 is min of ay1 */
            var _augval440 = (_cursor429).ay1;
            var _child441 = (_cursor429)._left7;
            if (!((_child441) == null)) {
                var _val442 = (_child441)._min_ay13;
                _augval440 = ((_augval440) < (_val442)) ? (_augval440) : (_val442);
            }
            var _child443 = (_cursor429)._right8;
            if (!((_child443) == null)) {
                var _val444 = (_child443)._min_ay13;
                _augval440 = ((_augval440) < (_val444)) ? (_augval440) : (_val444);
            }
            (_cursor429)._min_ay13 = _augval440;
            /* _max_ay24 is max of ay2 */
            var _augval445 = (_cursor429).ay2;
            var _child446 = (_cursor429)._left7;
            if (!((_child446) == null)) {
                var _val447 = (_child446)._max_ay24;
                _augval445 = ((_augval445) < (_val447)) ? (_val447) : (_augval445);
            }
            var _child448 = (_cursor429)._right8;
            if (!((_child448) == null)) {
                var _val449 = (_child448)._max_ay24;
                _augval445 = ((_augval445) < (_val449)) ? (_val449) : (_augval445);
            }
            (_cursor429)._max_ay24 = _augval445;
            (_cursor429)._height10 = 1 + ((((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) > ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10))) ? ((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) : ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10)));
            _changed430 = false;
            _changed430 = (_changed430) || (!((_old__min_ax12431) == ((_cursor429)._min_ax12)));
            _changed430 = (_changed430) || (!((_old__min_ay13432) == ((_cursor429)._min_ay13)));
            _changed430 = (_changed430) || (!((_old__max_ay24433) == ((_cursor429)._max_ay24)));
            _changed430 = (_changed430) || (!((_old_height434) == ((_cursor429)._height10)));
            _cursor429 = (_cursor429)._parent9;
        }
        if (((this)._root1) == (__x)) {
            (this)._root1 = _new_x386;
        }
        (__x)._left7 = null;
        (__x)._right8 = null;
        (__x)._min_ax12 = (__x).ax1;
        (__x)._min_ay13 = (__x).ay1;
        (__x)._max_ay24 = (__x).ay2;
        (__x)._height10 = 0;
        var _previous450 = null;
        var _current451 = (this)._root1;
        var _is_left452 = false;
        while (!((_current451) == null)) {
            _previous450 = _current451;
            if ((new_val) < ((_current451).ax2)) {
                _current451 = (_current451)._left7;
                _is_left452 = true;
            } else {
                _current451 = (_current451)._right8;
                _is_left452 = false;
            }
        }
        if ((_previous450) == null) {
            (this)._root1 = __x;
        } else {
            (__x)._parent9 = _previous450;
            if (_is_left452) {
                (_previous450)._left7 = __x;
            } else {
                (_previous450)._right8 = __x;
            }
        }
        var _cursor453 = (__x)._parent9;
        var _changed454 = true;
        while ((_changed454) && (!((_cursor453) == (null)))) {
            var _old__min_ax12455 = (_cursor453)._min_ax12;
            var _old__min_ay13456 = (_cursor453)._min_ay13;
            var _old__max_ay24457 = (_cursor453)._max_ay24;
            var _old_height458 = (_cursor453)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval459 = (_cursor453).ax1;
            var _child460 = (_cursor453)._left7;
            if (!((_child460) == null)) {
                var _val461 = (_child460)._min_ax12;
                _augval459 = ((_augval459) < (_val461)) ? (_augval459) : (_val461);
            }
            var _child462 = (_cursor453)._right8;
            if (!((_child462) == null)) {
                var _val463 = (_child462)._min_ax12;
                _augval459 = ((_augval459) < (_val463)) ? (_augval459) : (_val463);
            }
            (_cursor453)._min_ax12 = _augval459;
            /* _min_ay13 is min of ay1 */
            var _augval464 = (_cursor453).ay1;
            var _child465 = (_cursor453)._left7;
            if (!((_child465) == null)) {
                var _val466 = (_child465)._min_ay13;
                _augval464 = ((_augval464) < (_val466)) ? (_augval464) : (_val466);
            }
            var _child467 = (_cursor453)._right8;
            if (!((_child467) == null)) {
                var _val468 = (_child467)._min_ay13;
                _augval464 = ((_augval464) < (_val468)) ? (_augval464) : (_val468);
            }
            (_cursor453)._min_ay13 = _augval464;
            /* _max_ay24 is max of ay2 */
            var _augval469 = (_cursor453).ay2;
            var _child470 = (_cursor453)._left7;
            if (!((_child470) == null)) {
                var _val471 = (_child470)._max_ay24;
                _augval469 = ((_augval469) < (_val471)) ? (_val471) : (_augval469);
            }
            var _child472 = (_cursor453)._right8;
            if (!((_child472) == null)) {
                var _val473 = (_child472)._max_ay24;
                _augval469 = ((_augval469) < (_val473)) ? (_val473) : (_augval469);
            }
            (_cursor453)._max_ay24 = _augval469;
            (_cursor453)._height10 = 1 + ((((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) > ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10))) ? ((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) : ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10)));
            _changed454 = false;
            _changed454 = (_changed454) || (!((_old__min_ax12455) == ((_cursor453)._min_ax12)));
            _changed454 = (_changed454) || (!((_old__min_ay13456) == ((_cursor453)._min_ay13)));
            _changed454 = (_changed454) || (!((_old__max_ay24457) == ((_cursor453)._max_ay24)));
            _changed454 = (_changed454) || (!((_old_height458) == ((_cursor453)._height10)));
            _cursor453 = (_cursor453)._parent9;
        }
        /* rebalance AVL tree */
        var _cursor474 = __x;
        var _imbalance475;
        while (!(((_cursor474)._parent9) == null)) {
            _cursor474 = (_cursor474)._parent9;
            (_cursor474)._height10 = 1 + ((((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) > ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10))) ? ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) : ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10)));
            _imbalance475 = ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) - ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10));
            if ((_imbalance475) > (1)) {
                if ((((((_cursor474)._left7)._left7) == null) ? (-1) : ((((_cursor474)._left7)._left7)._height10)) < (((((_cursor474)._left7)._right8) == null) ? (-1) : ((((_cursor474)._left7)._right8)._height10))) {
                    /* rotate ((_cursor474)._left7)._right8 */
                    var _a476 = (_cursor474)._left7;
                    var _b477 = (_a476)._right8;
                    var _c478 = (_b477)._left7;
                    /* replace _a476 with _b477 in (_a476)._parent9 */
                    if (!(((_a476)._parent9) == null)) {
                        if ((((_a476)._parent9)._left7) == (_a476)) {
                            ((_a476)._parent9)._left7 = _b477;
                        } else {
                            ((_a476)._parent9)._right8 = _b477;
                        }
                    }
                    if (!((_b477) == null)) {
                        (_b477)._parent9 = (_a476)._parent9;
                    }
                    /* replace _c478 with _a476 in _b477 */
                    (_b477)._left7 = _a476;
                    if (!((_a476) == null)) {
                        (_a476)._parent9 = _b477;
                    }
                    /* replace _b477 with _c478 in _a476 */
                    (_a476)._right8 = _c478;
                    if (!((_c478) == null)) {
                        (_c478)._parent9 = _a476;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval479 = (_a476).ax1;
                    var _child480 = (_a476)._left7;
                    if (!((_child480) == null)) {
                        var _val481 = (_child480)._min_ax12;
                        _augval479 = ((_augval479) < (_val481)) ? (_augval479) : (_val481);
                    }
                    var _child482 = (_a476)._right8;
                    if (!((_child482) == null)) {
                        var _val483 = (_child482)._min_ax12;
                        _augval479 = ((_augval479) < (_val483)) ? (_augval479) : (_val483);
                    }
                    (_a476)._min_ax12 = _augval479;
                    /* _min_ay13 is min of ay1 */
                    var _augval484 = (_a476).ay1;
                    var _child485 = (_a476)._left7;
                    if (!((_child485) == null)) {
                        var _val486 = (_child485)._min_ay13;
                        _augval484 = ((_augval484) < (_val486)) ? (_augval484) : (_val486);
                    }
                    var _child487 = (_a476)._right8;
                    if (!((_child487) == null)) {
                        var _val488 = (_child487)._min_ay13;
                        _augval484 = ((_augval484) < (_val488)) ? (_augval484) : (_val488);
                    }
                    (_a476)._min_ay13 = _augval484;
                    /* _max_ay24 is max of ay2 */
                    var _augval489 = (_a476).ay2;
                    var _child490 = (_a476)._left7;
                    if (!((_child490) == null)) {
                        var _val491 = (_child490)._max_ay24;
                        _augval489 = ((_augval489) < (_val491)) ? (_val491) : (_augval489);
                    }
                    var _child492 = (_a476)._right8;
                    if (!((_child492) == null)) {
                        var _val493 = (_child492)._max_ay24;
                        _augval489 = ((_augval489) < (_val493)) ? (_val493) : (_augval489);
                    }
                    (_a476)._max_ay24 = _augval489;
                    (_a476)._height10 = 1 + ((((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) > ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10))) ? ((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) : ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval494 = (_b477).ax1;
                    var _child495 = (_b477)._left7;
                    if (!((_child495) == null)) {
                        var _val496 = (_child495)._min_ax12;
                        _augval494 = ((_augval494) < (_val496)) ? (_augval494) : (_val496);
                    }
                    var _child497 = (_b477)._right8;
                    if (!((_child497) == null)) {
                        var _val498 = (_child497)._min_ax12;
                        _augval494 = ((_augval494) < (_val498)) ? (_augval494) : (_val498);
                    }
                    (_b477)._min_ax12 = _augval494;
                    /* _min_ay13 is min of ay1 */
                    var _augval499 = (_b477).ay1;
                    var _child500 = (_b477)._left7;
                    if (!((_child500) == null)) {
                        var _val501 = (_child500)._min_ay13;
                        _augval499 = ((_augval499) < (_val501)) ? (_augval499) : (_val501);
                    }
                    var _child502 = (_b477)._right8;
                    if (!((_child502) == null)) {
                        var _val503 = (_child502)._min_ay13;
                        _augval499 = ((_augval499) < (_val503)) ? (_augval499) : (_val503);
                    }
                    (_b477)._min_ay13 = _augval499;
                    /* _max_ay24 is max of ay2 */
                    var _augval504 = (_b477).ay2;
                    var _child505 = (_b477)._left7;
                    if (!((_child505) == null)) {
                        var _val506 = (_child505)._max_ay24;
                        _augval504 = ((_augval504) < (_val506)) ? (_val506) : (_augval504);
                    }
                    var _child507 = (_b477)._right8;
                    if (!((_child507) == null)) {
                        var _val508 = (_child507)._max_ay24;
                        _augval504 = ((_augval504) < (_val508)) ? (_val508) : (_augval504);
                    }
                    (_b477)._max_ay24 = _augval504;
                    (_b477)._height10 = 1 + ((((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) > ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10))) ? ((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) : ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10)));
                    if (!(((_b477)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval509 = ((_b477)._parent9).ax1;
                        var _child510 = ((_b477)._parent9)._left7;
                        if (!((_child510) == null)) {
                            var _val511 = (_child510)._min_ax12;
                            _augval509 = ((_augval509) < (_val511)) ? (_augval509) : (_val511);
                        }
                        var _child512 = ((_b477)._parent9)._right8;
                        if (!((_child512) == null)) {
                            var _val513 = (_child512)._min_ax12;
                            _augval509 = ((_augval509) < (_val513)) ? (_augval509) : (_val513);
                        }
                        ((_b477)._parent9)._min_ax12 = _augval509;
                        /* _min_ay13 is min of ay1 */
                        var _augval514 = ((_b477)._parent9).ay1;
                        var _child515 = ((_b477)._parent9)._left7;
                        if (!((_child515) == null)) {
                            var _val516 = (_child515)._min_ay13;
                            _augval514 = ((_augval514) < (_val516)) ? (_augval514) : (_val516);
                        }
                        var _child517 = ((_b477)._parent9)._right8;
                        if (!((_child517) == null)) {
                            var _val518 = (_child517)._min_ay13;
                            _augval514 = ((_augval514) < (_val518)) ? (_augval514) : (_val518);
                        }
                        ((_b477)._parent9)._min_ay13 = _augval514;
                        /* _max_ay24 is max of ay2 */
                        var _augval519 = ((_b477)._parent9).ay2;
                        var _child520 = ((_b477)._parent9)._left7;
                        if (!((_child520) == null)) {
                            var _val521 = (_child520)._max_ay24;
                            _augval519 = ((_augval519) < (_val521)) ? (_val521) : (_augval519);
                        }
                        var _child522 = ((_b477)._parent9)._right8;
                        if (!((_child522) == null)) {
                            var _val523 = (_child522)._max_ay24;
                            _augval519 = ((_augval519) < (_val523)) ? (_val523) : (_augval519);
                        }
                        ((_b477)._parent9)._max_ay24 = _augval519;
                        ((_b477)._parent9)._height10 = 1 + (((((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) > (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10))) ? (((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) : (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b477;
                    }
                }
                /* rotate (_cursor474)._left7 */
                var _a524 = _cursor474;
                var _b525 = (_a524)._left7;
                var _c526 = (_b525)._right8;
                /* replace _a524 with _b525 in (_a524)._parent9 */
                if (!(((_a524)._parent9) == null)) {
                    if ((((_a524)._parent9)._left7) == (_a524)) {
                        ((_a524)._parent9)._left7 = _b525;
                    } else {
                        ((_a524)._parent9)._right8 = _b525;
                    }
                }
                if (!((_b525) == null)) {
                    (_b525)._parent9 = (_a524)._parent9;
                }
                /* replace _c526 with _a524 in _b525 */
                (_b525)._right8 = _a524;
                if (!((_a524) == null)) {
                    (_a524)._parent9 = _b525;
                }
                /* replace _b525 with _c526 in _a524 */
                (_a524)._left7 = _c526;
                if (!((_c526) == null)) {
                    (_c526)._parent9 = _a524;
                }
                /* _min_ax12 is min of ax1 */
                var _augval527 = (_a524).ax1;
                var _child528 = (_a524)._left7;
                if (!((_child528) == null)) {
                    var _val529 = (_child528)._min_ax12;
                    _augval527 = ((_augval527) < (_val529)) ? (_augval527) : (_val529);
                }
                var _child530 = (_a524)._right8;
                if (!((_child530) == null)) {
                    var _val531 = (_child530)._min_ax12;
                    _augval527 = ((_augval527) < (_val531)) ? (_augval527) : (_val531);
                }
                (_a524)._min_ax12 = _augval527;
                /* _min_ay13 is min of ay1 */
                var _augval532 = (_a524).ay1;
                var _child533 = (_a524)._left7;
                if (!((_child533) == null)) {
                    var _val534 = (_child533)._min_ay13;
                    _augval532 = ((_augval532) < (_val534)) ? (_augval532) : (_val534);
                }
                var _child535 = (_a524)._right8;
                if (!((_child535) == null)) {
                    var _val536 = (_child535)._min_ay13;
                    _augval532 = ((_augval532) < (_val536)) ? (_augval532) : (_val536);
                }
                (_a524)._min_ay13 = _augval532;
                /* _max_ay24 is max of ay2 */
                var _augval537 = (_a524).ay2;
                var _child538 = (_a524)._left7;
                if (!((_child538) == null)) {
                    var _val539 = (_child538)._max_ay24;
                    _augval537 = ((_augval537) < (_val539)) ? (_val539) : (_augval537);
                }
                var _child540 = (_a524)._right8;
                if (!((_child540) == null)) {
                    var _val541 = (_child540)._max_ay24;
                    _augval537 = ((_augval537) < (_val541)) ? (_val541) : (_augval537);
                }
                (_a524)._max_ay24 = _augval537;
                (_a524)._height10 = 1 + ((((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) > ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10))) ? ((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) : ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval542 = (_b525).ax1;
                var _child543 = (_b525)._left7;
                if (!((_child543) == null)) {
                    var _val544 = (_child543)._min_ax12;
                    _augval542 = ((_augval542) < (_val544)) ? (_augval542) : (_val544);
                }
                var _child545 = (_b525)._right8;
                if (!((_child545) == null)) {
                    var _val546 = (_child545)._min_ax12;
                    _augval542 = ((_augval542) < (_val546)) ? (_augval542) : (_val546);
                }
                (_b525)._min_ax12 = _augval542;
                /* _min_ay13 is min of ay1 */
                var _augval547 = (_b525).ay1;
                var _child548 = (_b525)._left7;
                if (!((_child548) == null)) {
                    var _val549 = (_child548)._min_ay13;
                    _augval547 = ((_augval547) < (_val549)) ? (_augval547) : (_val549);
                }
                var _child550 = (_b525)._right8;
                if (!((_child550) == null)) {
                    var _val551 = (_child550)._min_ay13;
                    _augval547 = ((_augval547) < (_val551)) ? (_augval547) : (_val551);
                }
                (_b525)._min_ay13 = _augval547;
                /* _max_ay24 is max of ay2 */
                var _augval552 = (_b525).ay2;
                var _child553 = (_b525)._left7;
                if (!((_child553) == null)) {
                    var _val554 = (_child553)._max_ay24;
                    _augval552 = ((_augval552) < (_val554)) ? (_val554) : (_augval552);
                }
                var _child555 = (_b525)._right8;
                if (!((_child555) == null)) {
                    var _val556 = (_child555)._max_ay24;
                    _augval552 = ((_augval552) < (_val556)) ? (_val556) : (_augval552);
                }
                (_b525)._max_ay24 = _augval552;
                (_b525)._height10 = 1 + ((((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) > ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10))) ? ((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) : ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10)));
                if (!(((_b525)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval557 = ((_b525)._parent9).ax1;
                    var _child558 = ((_b525)._parent9)._left7;
                    if (!((_child558) == null)) {
                        var _val559 = (_child558)._min_ax12;
                        _augval557 = ((_augval557) < (_val559)) ? (_augval557) : (_val559);
                    }
                    var _child560 = ((_b525)._parent9)._right8;
                    if (!((_child560) == null)) {
                        var _val561 = (_child560)._min_ax12;
                        _augval557 = ((_augval557) < (_val561)) ? (_augval557) : (_val561);
                    }
                    ((_b525)._parent9)._min_ax12 = _augval557;
                    /* _min_ay13 is min of ay1 */
                    var _augval562 = ((_b525)._parent9).ay1;
                    var _child563 = ((_b525)._parent9)._left7;
                    if (!((_child563) == null)) {
                        var _val564 = (_child563)._min_ay13;
                        _augval562 = ((_augval562) < (_val564)) ? (_augval562) : (_val564);
                    }
                    var _child565 = ((_b525)._parent9)._right8;
                    if (!((_child565) == null)) {
                        var _val566 = (_child565)._min_ay13;
                        _augval562 = ((_augval562) < (_val566)) ? (_augval562) : (_val566);
                    }
                    ((_b525)._parent9)._min_ay13 = _augval562;
                    /* _max_ay24 is max of ay2 */
                    var _augval567 = ((_b525)._parent9).ay2;
                    var _child568 = ((_b525)._parent9)._left7;
                    if (!((_child568) == null)) {
                        var _val569 = (_child568)._max_ay24;
                        _augval567 = ((_augval567) < (_val569)) ? (_val569) : (_augval567);
                    }
                    var _child570 = ((_b525)._parent9)._right8;
                    if (!((_child570) == null)) {
                        var _val571 = (_child570)._max_ay24;
                        _augval567 = ((_augval567) < (_val571)) ? (_val571) : (_augval567);
                    }
                    ((_b525)._parent9)._max_ay24 = _augval567;
                    ((_b525)._parent9)._height10 = 1 + (((((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) > (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10))) ? (((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) : (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b525;
                }
                _cursor474 = (_cursor474)._parent9;
            } else if ((_imbalance475) < (-1)) {
                if ((((((_cursor474)._right8)._left7) == null) ? (-1) : ((((_cursor474)._right8)._left7)._height10)) > (((((_cursor474)._right8)._right8) == null) ? (-1) : ((((_cursor474)._right8)._right8)._height10))) {
                    /* rotate ((_cursor474)._right8)._left7 */
                    var _a572 = (_cursor474)._right8;
                    var _b573 = (_a572)._left7;
                    var _c574 = (_b573)._right8;
                    /* replace _a572 with _b573 in (_a572)._parent9 */
                    if (!(((_a572)._parent9) == null)) {
                        if ((((_a572)._parent9)._left7) == (_a572)) {
                            ((_a572)._parent9)._left7 = _b573;
                        } else {
                            ((_a572)._parent9)._right8 = _b573;
                        }
                    }
                    if (!((_b573) == null)) {
                        (_b573)._parent9 = (_a572)._parent9;
                    }
                    /* replace _c574 with _a572 in _b573 */
                    (_b573)._right8 = _a572;
                    if (!((_a572) == null)) {
                        (_a572)._parent9 = _b573;
                    }
                    /* replace _b573 with _c574 in _a572 */
                    (_a572)._left7 = _c574;
                    if (!((_c574) == null)) {
                        (_c574)._parent9 = _a572;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval575 = (_a572).ax1;
                    var _child576 = (_a572)._left7;
                    if (!((_child576) == null)) {
                        var _val577 = (_child576)._min_ax12;
                        _augval575 = ((_augval575) < (_val577)) ? (_augval575) : (_val577);
                    }
                    var _child578 = (_a572)._right8;
                    if (!((_child578) == null)) {
                        var _val579 = (_child578)._min_ax12;
                        _augval575 = ((_augval575) < (_val579)) ? (_augval575) : (_val579);
                    }
                    (_a572)._min_ax12 = _augval575;
                    /* _min_ay13 is min of ay1 */
                    var _augval580 = (_a572).ay1;
                    var _child581 = (_a572)._left7;
                    if (!((_child581) == null)) {
                        var _val582 = (_child581)._min_ay13;
                        _augval580 = ((_augval580) < (_val582)) ? (_augval580) : (_val582);
                    }
                    var _child583 = (_a572)._right8;
                    if (!((_child583) == null)) {
                        var _val584 = (_child583)._min_ay13;
                        _augval580 = ((_augval580) < (_val584)) ? (_augval580) : (_val584);
                    }
                    (_a572)._min_ay13 = _augval580;
                    /* _max_ay24 is max of ay2 */
                    var _augval585 = (_a572).ay2;
                    var _child586 = (_a572)._left7;
                    if (!((_child586) == null)) {
                        var _val587 = (_child586)._max_ay24;
                        _augval585 = ((_augval585) < (_val587)) ? (_val587) : (_augval585);
                    }
                    var _child588 = (_a572)._right8;
                    if (!((_child588) == null)) {
                        var _val589 = (_child588)._max_ay24;
                        _augval585 = ((_augval585) < (_val589)) ? (_val589) : (_augval585);
                    }
                    (_a572)._max_ay24 = _augval585;
                    (_a572)._height10 = 1 + ((((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) > ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10))) ? ((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) : ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval590 = (_b573).ax1;
                    var _child591 = (_b573)._left7;
                    if (!((_child591) == null)) {
                        var _val592 = (_child591)._min_ax12;
                        _augval590 = ((_augval590) < (_val592)) ? (_augval590) : (_val592);
                    }
                    var _child593 = (_b573)._right8;
                    if (!((_child593) == null)) {
                        var _val594 = (_child593)._min_ax12;
                        _augval590 = ((_augval590) < (_val594)) ? (_augval590) : (_val594);
                    }
                    (_b573)._min_ax12 = _augval590;
                    /* _min_ay13 is min of ay1 */
                    var _augval595 = (_b573).ay1;
                    var _child596 = (_b573)._left7;
                    if (!((_child596) == null)) {
                        var _val597 = (_child596)._min_ay13;
                        _augval595 = ((_augval595) < (_val597)) ? (_augval595) : (_val597);
                    }
                    var _child598 = (_b573)._right8;
                    if (!((_child598) == null)) {
                        var _val599 = (_child598)._min_ay13;
                        _augval595 = ((_augval595) < (_val599)) ? (_augval595) : (_val599);
                    }
                    (_b573)._min_ay13 = _augval595;
                    /* _max_ay24 is max of ay2 */
                    var _augval600 = (_b573).ay2;
                    var _child601 = (_b573)._left7;
                    if (!((_child601) == null)) {
                        var _val602 = (_child601)._max_ay24;
                        _augval600 = ((_augval600) < (_val602)) ? (_val602) : (_augval600);
                    }
                    var _child603 = (_b573)._right8;
                    if (!((_child603) == null)) {
                        var _val604 = (_child603)._max_ay24;
                        _augval600 = ((_augval600) < (_val604)) ? (_val604) : (_augval600);
                    }
                    (_b573)._max_ay24 = _augval600;
                    (_b573)._height10 = 1 + ((((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) > ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10))) ? ((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) : ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10)));
                    if (!(((_b573)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval605 = ((_b573)._parent9).ax1;
                        var _child606 = ((_b573)._parent9)._left7;
                        if (!((_child606) == null)) {
                            var _val607 = (_child606)._min_ax12;
                            _augval605 = ((_augval605) < (_val607)) ? (_augval605) : (_val607);
                        }
                        var _child608 = ((_b573)._parent9)._right8;
                        if (!((_child608) == null)) {
                            var _val609 = (_child608)._min_ax12;
                            _augval605 = ((_augval605) < (_val609)) ? (_augval605) : (_val609);
                        }
                        ((_b573)._parent9)._min_ax12 = _augval605;
                        /* _min_ay13 is min of ay1 */
                        var _augval610 = ((_b573)._parent9).ay1;
                        var _child611 = ((_b573)._parent9)._left7;
                        if (!((_child611) == null)) {
                            var _val612 = (_child611)._min_ay13;
                            _augval610 = ((_augval610) < (_val612)) ? (_augval610) : (_val612);
                        }
                        var _child613 = ((_b573)._parent9)._right8;
                        if (!((_child613) == null)) {
                            var _val614 = (_child613)._min_ay13;
                            _augval610 = ((_augval610) < (_val614)) ? (_augval610) : (_val614);
                        }
                        ((_b573)._parent9)._min_ay13 = _augval610;
                        /* _max_ay24 is max of ay2 */
                        var _augval615 = ((_b573)._parent9).ay2;
                        var _child616 = ((_b573)._parent9)._left7;
                        if (!((_child616) == null)) {
                            var _val617 = (_child616)._max_ay24;
                            _augval615 = ((_augval615) < (_val617)) ? (_val617) : (_augval615);
                        }
                        var _child618 = ((_b573)._parent9)._right8;
                        if (!((_child618) == null)) {
                            var _val619 = (_child618)._max_ay24;
                            _augval615 = ((_augval615) < (_val619)) ? (_val619) : (_augval615);
                        }
                        ((_b573)._parent9)._max_ay24 = _augval615;
                        ((_b573)._parent9)._height10 = 1 + (((((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) > (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10))) ? (((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) : (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b573;
                    }
                }
                /* rotate (_cursor474)._right8 */
                var _a620 = _cursor474;
                var _b621 = (_a620)._right8;
                var _c622 = (_b621)._left7;
                /* replace _a620 with _b621 in (_a620)._parent9 */
                if (!(((_a620)._parent9) == null)) {
                    if ((((_a620)._parent9)._left7) == (_a620)) {
                        ((_a620)._parent9)._left7 = _b621;
                    } else {
                        ((_a620)._parent9)._right8 = _b621;
                    }
                }
                if (!((_b621) == null)) {
                    (_b621)._parent9 = (_a620)._parent9;
                }
                /* replace _c622 with _a620 in _b621 */
                (_b621)._left7 = _a620;
                if (!((_a620) == null)) {
                    (_a620)._parent9 = _b621;
                }
                /* replace _b621 with _c622 in _a620 */
                (_a620)._right8 = _c622;
                if (!((_c622) == null)) {
                    (_c622)._parent9 = _a620;
                }
                /* _min_ax12 is min of ax1 */
                var _augval623 = (_a620).ax1;
                var _child624 = (_a620)._left7;
                if (!((_child624) == null)) {
                    var _val625 = (_child624)._min_ax12;
                    _augval623 = ((_augval623) < (_val625)) ? (_augval623) : (_val625);
                }
                var _child626 = (_a620)._right8;
                if (!((_child626) == null)) {
                    var _val627 = (_child626)._min_ax12;
                    _augval623 = ((_augval623) < (_val627)) ? (_augval623) : (_val627);
                }
                (_a620)._min_ax12 = _augval623;
                /* _min_ay13 is min of ay1 */
                var _augval628 = (_a620).ay1;
                var _child629 = (_a620)._left7;
                if (!((_child629) == null)) {
                    var _val630 = (_child629)._min_ay13;
                    _augval628 = ((_augval628) < (_val630)) ? (_augval628) : (_val630);
                }
                var _child631 = (_a620)._right8;
                if (!((_child631) == null)) {
                    var _val632 = (_child631)._min_ay13;
                    _augval628 = ((_augval628) < (_val632)) ? (_augval628) : (_val632);
                }
                (_a620)._min_ay13 = _augval628;
                /* _max_ay24 is max of ay2 */
                var _augval633 = (_a620).ay2;
                var _child634 = (_a620)._left7;
                if (!((_child634) == null)) {
                    var _val635 = (_child634)._max_ay24;
                    _augval633 = ((_augval633) < (_val635)) ? (_val635) : (_augval633);
                }
                var _child636 = (_a620)._right8;
                if (!((_child636) == null)) {
                    var _val637 = (_child636)._max_ay24;
                    _augval633 = ((_augval633) < (_val637)) ? (_val637) : (_augval633);
                }
                (_a620)._max_ay24 = _augval633;
                (_a620)._height10 = 1 + ((((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) > ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10))) ? ((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) : ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval638 = (_b621).ax1;
                var _child639 = (_b621)._left7;
                if (!((_child639) == null)) {
                    var _val640 = (_child639)._min_ax12;
                    _augval638 = ((_augval638) < (_val640)) ? (_augval638) : (_val640);
                }
                var _child641 = (_b621)._right8;
                if (!((_child641) == null)) {
                    var _val642 = (_child641)._min_ax12;
                    _augval638 = ((_augval638) < (_val642)) ? (_augval638) : (_val642);
                }
                (_b621)._min_ax12 = _augval638;
                /* _min_ay13 is min of ay1 */
                var _augval643 = (_b621).ay1;
                var _child644 = (_b621)._left7;
                if (!((_child644) == null)) {
                    var _val645 = (_child644)._min_ay13;
                    _augval643 = ((_augval643) < (_val645)) ? (_augval643) : (_val645);
                }
                var _child646 = (_b621)._right8;
                if (!((_child646) == null)) {
                    var _val647 = (_child646)._min_ay13;
                    _augval643 = ((_augval643) < (_val647)) ? (_augval643) : (_val647);
                }
                (_b621)._min_ay13 = _augval643;
                /* _max_ay24 is max of ay2 */
                var _augval648 = (_b621).ay2;
                var _child649 = (_b621)._left7;
                if (!((_child649) == null)) {
                    var _val650 = (_child649)._max_ay24;
                    _augval648 = ((_augval648) < (_val650)) ? (_val650) : (_augval648);
                }
                var _child651 = (_b621)._right8;
                if (!((_child651) == null)) {
                    var _val652 = (_child651)._max_ay24;
                    _augval648 = ((_augval648) < (_val652)) ? (_val652) : (_augval648);
                }
                (_b621)._max_ay24 = _augval648;
                (_b621)._height10 = 1 + ((((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) > ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10))) ? ((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) : ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10)));
                if (!(((_b621)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval653 = ((_b621)._parent9).ax1;
                    var _child654 = ((_b621)._parent9)._left7;
                    if (!((_child654) == null)) {
                        var _val655 = (_child654)._min_ax12;
                        _augval653 = ((_augval653) < (_val655)) ? (_augval653) : (_val655);
                    }
                    var _child656 = ((_b621)._parent9)._right8;
                    if (!((_child656) == null)) {
                        var _val657 = (_child656)._min_ax12;
                        _augval653 = ((_augval653) < (_val657)) ? (_augval653) : (_val657);
                    }
                    ((_b621)._parent9)._min_ax12 = _augval653;
                    /* _min_ay13 is min of ay1 */
                    var _augval658 = ((_b621)._parent9).ay1;
                    var _child659 = ((_b621)._parent9)._left7;
                    if (!((_child659) == null)) {
                        var _val660 = (_child659)._min_ay13;
                        _augval658 = ((_augval658) < (_val660)) ? (_augval658) : (_val660);
                    }
                    var _child661 = ((_b621)._parent9)._right8;
                    if (!((_child661) == null)) {
                        var _val662 = (_child661)._min_ay13;
                        _augval658 = ((_augval658) < (_val662)) ? (_augval658) : (_val662);
                    }
                    ((_b621)._parent9)._min_ay13 = _augval658;
                    /* _max_ay24 is max of ay2 */
                    var _augval663 = ((_b621)._parent9).ay2;
                    var _child664 = ((_b621)._parent9)._left7;
                    if (!((_child664) == null)) {
                        var _val665 = (_child664)._max_ay24;
                        _augval663 = ((_augval663) < (_val665)) ? (_val665) : (_augval663);
                    }
                    var _child666 = ((_b621)._parent9)._right8;
                    if (!((_child666) == null)) {
                        var _val667 = (_child666)._max_ay24;
                        _augval663 = ((_augval663) < (_val667)) ? (_val667) : (_augval663);
                    }
                    ((_b621)._parent9)._max_ay24 = _augval663;
                    ((_b621)._parent9)._height10 = 1 + (((((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) > (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10))) ? (((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) : (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b621;
                }
                _cursor474 = (_cursor474)._parent9;
            }
        }
        (__x).ax2 = new_val;
    }
}
RectangleHolder.prototype.updateAy2 = function (__x, new_val) {
    if ((__x).ay2 != new_val) {
        /* _max_ay24 is max of ay2 */
        var _augval668 = new_val;
        var _child669 = (__x)._left7;
        if (!((_child669) == null)) {
            var _val670 = (_child669)._max_ay24;
            _augval668 = ((_augval668) < (_val670)) ? (_val670) : (_augval668);
        }
        var _child671 = (__x)._right8;
        if (!((_child671) == null)) {
            var _val672 = (_child671)._max_ay24;
            _augval668 = ((_augval668) < (_val672)) ? (_val672) : (_augval668);
        }
        (__x)._max_ay24 = _augval668;
        var _cursor673 = (__x)._parent9;
        var _changed674 = true;
        while ((_changed674) && (!((_cursor673) == (null)))) {
            var _old__max_ay24675 = (_cursor673)._max_ay24;
            var _old_height676 = (_cursor673)._height10;
            /* _max_ay24 is max of ay2 */
            var _augval677 = (_cursor673).ay2;
            var _child678 = (_cursor673)._left7;
            if (!((_child678) == null)) {
                var _val679 = (_child678)._max_ay24;
                _augval677 = ((_augval677) < (_val679)) ? (_val679) : (_augval677);
            }
            var _child680 = (_cursor673)._right8;
            if (!((_child680) == null)) {
                var _val681 = (_child680)._max_ay24;
                _augval677 = ((_augval677) < (_val681)) ? (_val681) : (_augval677);
            }
            (_cursor673)._max_ay24 = _augval677;
            (_cursor673)._height10 = 1 + ((((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) > ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10))) ? ((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) : ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10)));
            _changed674 = false;
            _changed674 = (_changed674) || (!((_old__max_ay24675) == ((_cursor673)._max_ay24)));
            _changed674 = (_changed674) || (!((_old_height676) == ((_cursor673)._height10)));
            _cursor673 = (_cursor673)._parent9;
        }
        (__x).ay2 = new_val;
    }
}
RectangleHolder.prototype.update = function (__x, ax1, ay1, ax2, ay2) {
    var _parent682 = (__x)._parent9;
    var _left683 = (__x)._left7;
    var _right684 = (__x)._right8;
    var _new_x685;
    if (((_left683) == null) && ((_right684) == null)) {
        _new_x685 = null;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if ((!((_left683) == null)) && ((_right684) == null)) {
        _new_x685 = _left683;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if (((_left683) == null) && (!((_right684) == null))) {
        _new_x685 = _right684;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else {
        var _root686 = (__x)._right8;
        var _x687 = _root686;
        var _descend688 = true;
        var _from_left689 = true;
        while (true) {
            if ((_x687) == null) {
                _x687 = null;
                break;
            }
            if (_descend688) {
                /* too small? */
                if (false) {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                } else if ((!(((_x687)._left7) == null)) && (true)) {
                    _x687 = (_x687)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x687) == (_root686)) {
                    _root686 = (_x687)._right8;
                    _x687 = (_x687)._right8;
                } else {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                }
            } else if (_from_left689) {
                if (false) {
                    _x687 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x687)._right8) == null)) && (true)) {
                    _descend688 = true;
                    if ((_x687) == (_root686)) {
                        _root686 = (_x687)._right8;
                    }
                    _x687 = (_x687)._right8;
                } else if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            } else {
                if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            }
        }
        _new_x685 = _x687;
        var _mp690 = (_x687)._parent9;
        var _mr691 = (_x687)._right8;
        /* replace _x687 with _mr691 in _mp690 */
        if (!((_mp690) == null)) {
            if (((_mp690)._left7) == (_x687)) {
                (_mp690)._left7 = _mr691;
            } else {
                (_mp690)._right8 = _mr691;
            }
        }
        if (!((_mr691) == null)) {
            (_mr691)._parent9 = _mp690;
        }
        /* replace __x with _x687 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _x687;
            } else {
                (_parent682)._right8 = _x687;
            }
        }
        if (!((_x687) == null)) {
            (_x687)._parent9 = _parent682;
        }
        /* replace null with _left683 in _x687 */
        (_x687)._left7 = _left683;
        if (!((_left683) == null)) {
            (_left683)._parent9 = _x687;
        }
        /* replace _mr691 with (__x)._right8 in _x687 */
        (_x687)._right8 = (__x)._right8;
        if (!(((__x)._right8) == null)) {
            ((__x)._right8)._parent9 = _x687;
        }
        /* _min_ax12 is min of ax1 */
        var _augval692 = (_x687).ax1;
        var _child693 = (_x687)._left7;
        if (!((_child693) == null)) {
            var _val694 = (_child693)._min_ax12;
            _augval692 = ((_augval692) < (_val694)) ? (_augval692) : (_val694);
        }
        var _child695 = (_x687)._right8;
        if (!((_child695) == null)) {
            var _val696 = (_child695)._min_ax12;
            _augval692 = ((_augval692) < (_val696)) ? (_augval692) : (_val696);
        }
        (_x687)._min_ax12 = _augval692;
        /* _min_ay13 is min of ay1 */
        var _augval697 = (_x687).ay1;
        var _child698 = (_x687)._left7;
        if (!((_child698) == null)) {
            var _val699 = (_child698)._min_ay13;
            _augval697 = ((_augval697) < (_val699)) ? (_augval697) : (_val699);
        }
        var _child700 = (_x687)._right8;
        if (!((_child700) == null)) {
            var _val701 = (_child700)._min_ay13;
            _augval697 = ((_augval697) < (_val701)) ? (_augval697) : (_val701);
        }
        (_x687)._min_ay13 = _augval697;
        /* _max_ay24 is max of ay2 */
        var _augval702 = (_x687).ay2;
        var _child703 = (_x687)._left7;
        if (!((_child703) == null)) {
            var _val704 = (_child703)._max_ay24;
            _augval702 = ((_augval702) < (_val704)) ? (_val704) : (_augval702);
        }
        var _child705 = (_x687)._right8;
        if (!((_child705) == null)) {
            var _val706 = (_child705)._max_ay24;
            _augval702 = ((_augval702) < (_val706)) ? (_val706) : (_augval702);
        }
        (_x687)._max_ay24 = _augval702;
        (_x687)._height10 = 1 + ((((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) > ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10))) ? ((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) : ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10)));
        var _cursor707 = _mp690;
        var _changed708 = true;
        while ((_changed708) && (!((_cursor707) == (_parent682)))) {
            var _old__min_ax12709 = (_cursor707)._min_ax12;
            var _old__min_ay13710 = (_cursor707)._min_ay13;
            var _old__max_ay24711 = (_cursor707)._max_ay24;
            var _old_height712 = (_cursor707)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval713 = (_cursor707).ax1;
            var _child714 = (_cursor707)._left7;
            if (!((_child714) == null)) {
                var _val715 = (_child714)._min_ax12;
                _augval713 = ((_augval713) < (_val715)) ? (_augval713) : (_val715);
            }
            var _child716 = (_cursor707)._right8;
            if (!((_child716) == null)) {
                var _val717 = (_child716)._min_ax12;
                _augval713 = ((_augval713) < (_val717)) ? (_augval713) : (_val717);
            }
            (_cursor707)._min_ax12 = _augval713;
            /* _min_ay13 is min of ay1 */
            var _augval718 = (_cursor707).ay1;
            var _child719 = (_cursor707)._left7;
            if (!((_child719) == null)) {
                var _val720 = (_child719)._min_ay13;
                _augval718 = ((_augval718) < (_val720)) ? (_augval718) : (_val720);
            }
            var _child721 = (_cursor707)._right8;
            if (!((_child721) == null)) {
                var _val722 = (_child721)._min_ay13;
                _augval718 = ((_augval718) < (_val722)) ? (_augval718) : (_val722);
            }
            (_cursor707)._min_ay13 = _augval718;
            /* _max_ay24 is max of ay2 */
            var _augval723 = (_cursor707).ay2;
            var _child724 = (_cursor707)._left7;
            if (!((_child724) == null)) {
                var _val725 = (_child724)._max_ay24;
                _augval723 = ((_augval723) < (_val725)) ? (_val725) : (_augval723);
            }
            var _child726 = (_cursor707)._right8;
            if (!((_child726) == null)) {
                var _val727 = (_child726)._max_ay24;
                _augval723 = ((_augval723) < (_val727)) ? (_val727) : (_augval723);
            }
            (_cursor707)._max_ay24 = _augval723;
            (_cursor707)._height10 = 1 + ((((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) > ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10))) ? ((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) : ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10)));
            _changed708 = false;
            _changed708 = (_changed708) || (!((_old__min_ax12709) == ((_cursor707)._min_ax12)));
            _changed708 = (_changed708) || (!((_old__min_ay13710) == ((_cursor707)._min_ay13)));
            _changed708 = (_changed708) || (!((_old__max_ay24711) == ((_cursor707)._max_ay24)));
            _changed708 = (_changed708) || (!((_old_height712) == ((_cursor707)._height10)));
            _cursor707 = (_cursor707)._parent9;
        }
    }
    var _cursor728 = _parent682;
    var _changed729 = true;
    while ((_changed729) && (!((_cursor728) == (null)))) {
        var _old__min_ax12730 = (_cursor728)._min_ax12;
        var _old__min_ay13731 = (_cursor728)._min_ay13;
        var _old__max_ay24732 = (_cursor728)._max_ay24;
        var _old_height733 = (_cursor728)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval734 = (_cursor728).ax1;
        var _child735 = (_cursor728)._left7;
        if (!((_child735) == null)) {
            var _val736 = (_child735)._min_ax12;
            _augval734 = ((_augval734) < (_val736)) ? (_augval734) : (_val736);
        }
        var _child737 = (_cursor728)._right8;
        if (!((_child737) == null)) {
            var _val738 = (_child737)._min_ax12;
            _augval734 = ((_augval734) < (_val738)) ? (_augval734) : (_val738);
        }
        (_cursor728)._min_ax12 = _augval734;
        /* _min_ay13 is min of ay1 */
        var _augval739 = (_cursor728).ay1;
        var _child740 = (_cursor728)._left7;
        if (!((_child740) == null)) {
            var _val741 = (_child740)._min_ay13;
            _augval739 = ((_augval739) < (_val741)) ? (_augval739) : (_val741);
        }
        var _child742 = (_cursor728)._right8;
        if (!((_child742) == null)) {
            var _val743 = (_child742)._min_ay13;
            _augval739 = ((_augval739) < (_val743)) ? (_augval739) : (_val743);
        }
        (_cursor728)._min_ay13 = _augval739;
        /* _max_ay24 is max of ay2 */
        var _augval744 = (_cursor728).ay2;
        var _child745 = (_cursor728)._left7;
        if (!((_child745) == null)) {
            var _val746 = (_child745)._max_ay24;
            _augval744 = ((_augval744) < (_val746)) ? (_val746) : (_augval744);
        }
        var _child747 = (_cursor728)._right8;
        if (!((_child747) == null)) {
            var _val748 = (_child747)._max_ay24;
            _augval744 = ((_augval744) < (_val748)) ? (_val748) : (_augval744);
        }
        (_cursor728)._max_ay24 = _augval744;
        (_cursor728)._height10 = 1 + ((((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) > ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10))) ? ((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) : ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10)));
        _changed729 = false;
        _changed729 = (_changed729) || (!((_old__min_ax12730) == ((_cursor728)._min_ax12)));
        _changed729 = (_changed729) || (!((_old__min_ay13731) == ((_cursor728)._min_ay13)));
        _changed729 = (_changed729) || (!((_old__max_ay24732) == ((_cursor728)._max_ay24)));
        _changed729 = (_changed729) || (!((_old_height733) == ((_cursor728)._height10)));
        _cursor728 = (_cursor728)._parent9;
    }
    if (((this)._root1) == (__x)) {
        (this)._root1 = _new_x685;
    }
    (__x)._left7 = null;
    (__x)._right8 = null;
    (__x)._min_ax12 = (__x).ax1;
    (__x)._min_ay13 = (__x).ay1;
    (__x)._max_ay24 = (__x).ay2;
    (__x)._height10 = 0;
    var _previous749 = null;
    var _current750 = (this)._root1;
    var _is_left751 = false;
    while (!((_current750) == null)) {
        _previous749 = _current750;
        if ((ax2) < ((_current750).ax2)) {
            _current750 = (_current750)._left7;
            _is_left751 = true;
        } else {
            _current750 = (_current750)._right8;
            _is_left751 = false;
        }
    }
    if ((_previous749) == null) {
        (this)._root1 = __x;
    } else {
        (__x)._parent9 = _previous749;
        if (_is_left751) {
            (_previous749)._left7 = __x;
        } else {
            (_previous749)._right8 = __x;
        }
    }
    var _cursor752 = (__x)._parent9;
    var _changed753 = true;
    while ((_changed753) && (!((_cursor752) == (null)))) {
        var _old__min_ax12754 = (_cursor752)._min_ax12;
        var _old__min_ay13755 = (_cursor752)._min_ay13;
        var _old__max_ay24756 = (_cursor752)._max_ay24;
        var _old_height757 = (_cursor752)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval758 = (_cursor752).ax1;
        var _child759 = (_cursor752)._left7;
        if (!((_child759) == null)) {
            var _val760 = (_child759)._min_ax12;
            _augval758 = ((_augval758) < (_val760)) ? (_augval758) : (_val760);
        }
        var _child761 = (_cursor752)._right8;
        if (!((_child761) == null)) {
            var _val762 = (_child761)._min_ax12;
            _augval758 = ((_augval758) < (_val762)) ? (_augval758) : (_val762);
        }
        (_cursor752)._min_ax12 = _augval758;
        /* _min_ay13 is min of ay1 */
        var _augval763 = (_cursor752).ay1;
        var _child764 = (_cursor752)._left7;
        if (!((_child764) == null)) {
            var _val765 = (_child764)._min_ay13;
            _augval763 = ((_augval763) < (_val765)) ? (_augval763) : (_val765);
        }
        var _child766 = (_cursor752)._right8;
        if (!((_child766) == null)) {
            var _val767 = (_child766)._min_ay13;
            _augval763 = ((_augval763) < (_val767)) ? (_augval763) : (_val767);
        }
        (_cursor752)._min_ay13 = _augval763;
        /* _max_ay24 is max of ay2 */
        var _augval768 = (_cursor752).ay2;
        var _child769 = (_cursor752)._left7;
        if (!((_child769) == null)) {
            var _val770 = (_child769)._max_ay24;
            _augval768 = ((_augval768) < (_val770)) ? (_val770) : (_augval768);
        }
        var _child771 = (_cursor752)._right8;
        if (!((_child771) == null)) {
            var _val772 = (_child771)._max_ay24;
            _augval768 = ((_augval768) < (_val772)) ? (_val772) : (_augval768);
        }
        (_cursor752)._max_ay24 = _augval768;
        (_cursor752)._height10 = 1 + ((((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) > ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10))) ? ((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) : ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10)));
        _changed753 = false;
        _changed753 = (_changed753) || (!((_old__min_ax12754) == ((_cursor752)._min_ax12)));
        _changed753 = (_changed753) || (!((_old__min_ay13755) == ((_cursor752)._min_ay13)));
        _changed753 = (_changed753) || (!((_old__max_ay24756) == ((_cursor752)._max_ay24)));
        _changed753 = (_changed753) || (!((_old_height757) == ((_cursor752)._height10)));
        _cursor752 = (_cursor752)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor773 = __x;
    var _imbalance774;
    while (!(((_cursor773)._parent9) == null)) {
        _cursor773 = (_cursor773)._parent9;
        (_cursor773)._height10 = 1 + ((((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) > ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10))) ? ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) : ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10)));
        _imbalance774 = ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) - ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10));
        if ((_imbalance774) > (1)) {
            if ((((((_cursor773)._left7)._left7) == null) ? (-1) : ((((_cursor773)._left7)._left7)._height10)) < (((((_cursor773)._left7)._right8) == null) ? (-1) : ((((_cursor773)._left7)._right8)._height10))) {
                /* rotate ((_cursor773)._left7)._right8 */
                var _a775 = (_cursor773)._left7;
                var _b776 = (_a775)._right8;
                var _c777 = (_b776)._left7;
                /* replace _a775 with _b776 in (_a775)._parent9 */
                if (!(((_a775)._parent9) == null)) {
                    if ((((_a775)._parent9)._left7) == (_a775)) {
                        ((_a775)._parent9)._left7 = _b776;
                    } else {
                        ((_a775)._parent9)._right8 = _b776;
                    }
                }
                if (!((_b776) == null)) {
                    (_b776)._parent9 = (_a775)._parent9;
                }
                /* replace _c777 with _a775 in _b776 */
                (_b776)._left7 = _a775;
                if (!((_a775) == null)) {
                    (_a775)._parent9 = _b776;
                }
                /* replace _b776 with _c777 in _a775 */
                (_a775)._right8 = _c777;
                if (!((_c777) == null)) {
                    (_c777)._parent9 = _a775;
                }
                /* _min_ax12 is min of ax1 */
                var _augval778 = (_a775).ax1;
                var _child779 = (_a775)._left7;
                if (!((_child779) == null)) {
                    var _val780 = (_child779)._min_ax12;
                    _augval778 = ((_augval778) < (_val780)) ? (_augval778) : (_val780);
                }
                var _child781 = (_a775)._right8;
                if (!((_child781) == null)) {
                    var _val782 = (_child781)._min_ax12;
                    _augval778 = ((_augval778) < (_val782)) ? (_augval778) : (_val782);
                }
                (_a775)._min_ax12 = _augval778;
                /* _min_ay13 is min of ay1 */
                var _augval783 = (_a775).ay1;
                var _child784 = (_a775)._left7;
                if (!((_child784) == null)) {
                    var _val785 = (_child784)._min_ay13;
                    _augval783 = ((_augval783) < (_val785)) ? (_augval783) : (_val785);
                }
                var _child786 = (_a775)._right8;
                if (!((_child786) == null)) {
                    var _val787 = (_child786)._min_ay13;
                    _augval783 = ((_augval783) < (_val787)) ? (_augval783) : (_val787);
                }
                (_a775)._min_ay13 = _augval783;
                /* _max_ay24 is max of ay2 */
                var _augval788 = (_a775).ay2;
                var _child789 = (_a775)._left7;
                if (!((_child789) == null)) {
                    var _val790 = (_child789)._max_ay24;
                    _augval788 = ((_augval788) < (_val790)) ? (_val790) : (_augval788);
                }
                var _child791 = (_a775)._right8;
                if (!((_child791) == null)) {
                    var _val792 = (_child791)._max_ay24;
                    _augval788 = ((_augval788) < (_val792)) ? (_val792) : (_augval788);
                }
                (_a775)._max_ay24 = _augval788;
                (_a775)._height10 = 1 + ((((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) > ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10))) ? ((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) : ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval793 = (_b776).ax1;
                var _child794 = (_b776)._left7;
                if (!((_child794) == null)) {
                    var _val795 = (_child794)._min_ax12;
                    _augval793 = ((_augval793) < (_val795)) ? (_augval793) : (_val795);
                }
                var _child796 = (_b776)._right8;
                if (!((_child796) == null)) {
                    var _val797 = (_child796)._min_ax12;
                    _augval793 = ((_augval793) < (_val797)) ? (_augval793) : (_val797);
                }
                (_b776)._min_ax12 = _augval793;
                /* _min_ay13 is min of ay1 */
                var _augval798 = (_b776).ay1;
                var _child799 = (_b776)._left7;
                if (!((_child799) == null)) {
                    var _val800 = (_child799)._min_ay13;
                    _augval798 = ((_augval798) < (_val800)) ? (_augval798) : (_val800);
                }
                var _child801 = (_b776)._right8;
                if (!((_child801) == null)) {
                    var _val802 = (_child801)._min_ay13;
                    _augval798 = ((_augval798) < (_val802)) ? (_augval798) : (_val802);
                }
                (_b776)._min_ay13 = _augval798;
                /* _max_ay24 is max of ay2 */
                var _augval803 = (_b776).ay2;
                var _child804 = (_b776)._left7;
                if (!((_child804) == null)) {
                    var _val805 = (_child804)._max_ay24;
                    _augval803 = ((_augval803) < (_val805)) ? (_val805) : (_augval803);
                }
                var _child806 = (_b776)._right8;
                if (!((_child806) == null)) {
                    var _val807 = (_child806)._max_ay24;
                    _augval803 = ((_augval803) < (_val807)) ? (_val807) : (_augval803);
                }
                (_b776)._max_ay24 = _augval803;
                (_b776)._height10 = 1 + ((((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) > ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10))) ? ((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) : ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10)));
                if (!(((_b776)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval808 = ((_b776)._parent9).ax1;
                    var _child809 = ((_b776)._parent9)._left7;
                    if (!((_child809) == null)) {
                        var _val810 = (_child809)._min_ax12;
                        _augval808 = ((_augval808) < (_val810)) ? (_augval808) : (_val810);
                    }
                    var _child811 = ((_b776)._parent9)._right8;
                    if (!((_child811) == null)) {
                        var _val812 = (_child811)._min_ax12;
                        _augval808 = ((_augval808) < (_val812)) ? (_augval808) : (_val812);
                    }
                    ((_b776)._parent9)._min_ax12 = _augval808;
                    /* _min_ay13 is min of ay1 */
                    var _augval813 = ((_b776)._parent9).ay1;
                    var _child814 = ((_b776)._parent9)._left7;
                    if (!((_child814) == null)) {
                        var _val815 = (_child814)._min_ay13;
                        _augval813 = ((_augval813) < (_val815)) ? (_augval813) : (_val815);
                    }
                    var _child816 = ((_b776)._parent9)._right8;
                    if (!((_child816) == null)) {
                        var _val817 = (_child816)._min_ay13;
                        _augval813 = ((_augval813) < (_val817)) ? (_augval813) : (_val817);
                    }
                    ((_b776)._parent9)._min_ay13 = _augval813;
                    /* _max_ay24 is max of ay2 */
                    var _augval818 = ((_b776)._parent9).ay2;
                    var _child819 = ((_b776)._parent9)._left7;
                    if (!((_child819) == null)) {
                        var _val820 = (_child819)._max_ay24;
                        _augval818 = ((_augval818) < (_val820)) ? (_val820) : (_augval818);
                    }
                    var _child821 = ((_b776)._parent9)._right8;
                    if (!((_child821) == null)) {
                        var _val822 = (_child821)._max_ay24;
                        _augval818 = ((_augval818) < (_val822)) ? (_val822) : (_augval818);
                    }
                    ((_b776)._parent9)._max_ay24 = _augval818;
                    ((_b776)._parent9)._height10 = 1 + (((((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) > (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10))) ? (((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) : (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b776;
                }
            }
            /* rotate (_cursor773)._left7 */
            var _a823 = _cursor773;
            var _b824 = (_a823)._left7;
            var _c825 = (_b824)._right8;
            /* replace _a823 with _b824 in (_a823)._parent9 */
            if (!(((_a823)._parent9) == null)) {
                if ((((_a823)._parent9)._left7) == (_a823)) {
                    ((_a823)._parent9)._left7 = _b824;
                } else {
                    ((_a823)._parent9)._right8 = _b824;
                }
            }
            if (!((_b824) == null)) {
                (_b824)._parent9 = (_a823)._parent9;
            }
            /* replace _c825 with _a823 in _b824 */
            (_b824)._right8 = _a823;
            if (!((_a823) == null)) {
                (_a823)._parent9 = _b824;
            }
            /* replace _b824 with _c825 in _a823 */
            (_a823)._left7 = _c825;
            if (!((_c825) == null)) {
                (_c825)._parent9 = _a823;
            }
            /* _min_ax12 is min of ax1 */
            var _augval826 = (_a823).ax1;
            var _child827 = (_a823)._left7;
            if (!((_child827) == null)) {
                var _val828 = (_child827)._min_ax12;
                _augval826 = ((_augval826) < (_val828)) ? (_augval826) : (_val828);
            }
            var _child829 = (_a823)._right8;
            if (!((_child829) == null)) {
                var _val830 = (_child829)._min_ax12;
                _augval826 = ((_augval826) < (_val830)) ? (_augval826) : (_val830);
            }
            (_a823)._min_ax12 = _augval826;
            /* _min_ay13 is min of ay1 */
            var _augval831 = (_a823).ay1;
            var _child832 = (_a823)._left7;
            if (!((_child832) == null)) {
                var _val833 = (_child832)._min_ay13;
                _augval831 = ((_augval831) < (_val833)) ? (_augval831) : (_val833);
            }
            var _child834 = (_a823)._right8;
            if (!((_child834) == null)) {
                var _val835 = (_child834)._min_ay13;
                _augval831 = ((_augval831) < (_val835)) ? (_augval831) : (_val835);
            }
            (_a823)._min_ay13 = _augval831;
            /* _max_ay24 is max of ay2 */
            var _augval836 = (_a823).ay2;
            var _child837 = (_a823)._left7;
            if (!((_child837) == null)) {
                var _val838 = (_child837)._max_ay24;
                _augval836 = ((_augval836) < (_val838)) ? (_val838) : (_augval836);
            }
            var _child839 = (_a823)._right8;
            if (!((_child839) == null)) {
                var _val840 = (_child839)._max_ay24;
                _augval836 = ((_augval836) < (_val840)) ? (_val840) : (_augval836);
            }
            (_a823)._max_ay24 = _augval836;
            (_a823)._height10 = 1 + ((((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) > ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10))) ? ((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) : ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval841 = (_b824).ax1;
            var _child842 = (_b824)._left7;
            if (!((_child842) == null)) {
                var _val843 = (_child842)._min_ax12;
                _augval841 = ((_augval841) < (_val843)) ? (_augval841) : (_val843);
            }
            var _child844 = (_b824)._right8;
            if (!((_child844) == null)) {
                var _val845 = (_child844)._min_ax12;
                _augval841 = ((_augval841) < (_val845)) ? (_augval841) : (_val845);
            }
            (_b824)._min_ax12 = _augval841;
            /* _min_ay13 is min of ay1 */
            var _augval846 = (_b824).ay1;
            var _child847 = (_b824)._left7;
            if (!((_child847) == null)) {
                var _val848 = (_child847)._min_ay13;
                _augval846 = ((_augval846) < (_val848)) ? (_augval846) : (_val848);
            }
            var _child849 = (_b824)._right8;
            if (!((_child849) == null)) {
                var _val850 = (_child849)._min_ay13;
                _augval846 = ((_augval846) < (_val850)) ? (_augval846) : (_val850);
            }
            (_b824)._min_ay13 = _augval846;
            /* _max_ay24 is max of ay2 */
            var _augval851 = (_b824).ay2;
            var _child852 = (_b824)._left7;
            if (!((_child852) == null)) {
                var _val853 = (_child852)._max_ay24;
                _augval851 = ((_augval851) < (_val853)) ? (_val853) : (_augval851);
            }
            var _child854 = (_b824)._right8;
            if (!((_child854) == null)) {
                var _val855 = (_child854)._max_ay24;
                _augval851 = ((_augval851) < (_val855)) ? (_val855) : (_augval851);
            }
            (_b824)._max_ay24 = _augval851;
            (_b824)._height10 = 1 + ((((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) > ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10))) ? ((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) : ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10)));
            if (!(((_b824)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval856 = ((_b824)._parent9).ax1;
                var _child857 = ((_b824)._parent9)._left7;
                if (!((_child857) == null)) {
                    var _val858 = (_child857)._min_ax12;
                    _augval856 = ((_augval856) < (_val858)) ? (_augval856) : (_val858);
                }
                var _child859 = ((_b824)._parent9)._right8;
                if (!((_child859) == null)) {
                    var _val860 = (_child859)._min_ax12;
                    _augval856 = ((_augval856) < (_val860)) ? (_augval856) : (_val860);
                }
                ((_b824)._parent9)._min_ax12 = _augval856;
                /* _min_ay13 is min of ay1 */
                var _augval861 = ((_b824)._parent9).ay1;
                var _child862 = ((_b824)._parent9)._left7;
                if (!((_child862) == null)) {
                    var _val863 = (_child862)._min_ay13;
                    _augval861 = ((_augval861) < (_val863)) ? (_augval861) : (_val863);
                }
                var _child864 = ((_b824)._parent9)._right8;
                if (!((_child864) == null)) {
                    var _val865 = (_child864)._min_ay13;
                    _augval861 = ((_augval861) < (_val865)) ? (_augval861) : (_val865);
                }
                ((_b824)._parent9)._min_ay13 = _augval861;
                /* _max_ay24 is max of ay2 */
                var _augval866 = ((_b824)._parent9).ay2;
                var _child867 = ((_b824)._parent9)._left7;
                if (!((_child867) == null)) {
                    var _val868 = (_child867)._max_ay24;
                    _augval866 = ((_augval866) < (_val868)) ? (_val868) : (_augval866);
                }
                var _child869 = ((_b824)._parent9)._right8;
                if (!((_child869) == null)) {
                    var _val870 = (_child869)._max_ay24;
                    _augval866 = ((_augval866) < (_val870)) ? (_val870) : (_augval866);
                }
                ((_b824)._parent9)._max_ay24 = _augval866;
                ((_b824)._parent9)._height10 = 1 + (((((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) > (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10))) ? (((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) : (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b824;
            }
            _cursor773 = (_cursor773)._parent9;
        } else if ((_imbalance774) < (-1)) {
            if ((((((_cursor773)._right8)._left7) == null) ? (-1) : ((((_cursor773)._right8)._left7)._height10)) > (((((_cursor773)._right8)._right8) == null) ? (-1) : ((((_cursor773)._right8)._right8)._height10))) {
                /* rotate ((_cursor773)._right8)._left7 */
                var _a871 = (_cursor773)._right8;
                var _b872 = (_a871)._left7;
                var _c873 = (_b872)._right8;
                /* replace _a871 with _b872 in (_a871)._parent9 */
                if (!(((_a871)._parent9) == null)) {
                    if ((((_a871)._parent9)._left7) == (_a871)) {
                        ((_a871)._parent9)._left7 = _b872;
                    } else {
                        ((_a871)._parent9)._right8 = _b872;
                    }
                }
                if (!((_b872) == null)) {
                    (_b872)._parent9 = (_a871)._parent9;
                }
                /* replace _c873 with _a871 in _b872 */
                (_b872)._right8 = _a871;
                if (!((_a871) == null)) {
                    (_a871)._parent9 = _b872;
                }
                /* replace _b872 with _c873 in _a871 */
                (_a871)._left7 = _c873;
                if (!((_c873) == null)) {
                    (_c873)._parent9 = _a871;
                }
                /* _min_ax12 is min of ax1 */
                var _augval874 = (_a871).ax1;
                var _child875 = (_a871)._left7;
                if (!((_child875) == null)) {
                    var _val876 = (_child875)._min_ax12;
                    _augval874 = ((_augval874) < (_val876)) ? (_augval874) : (_val876);
                }
                var _child877 = (_a871)._right8;
                if (!((_child877) == null)) {
                    var _val878 = (_child877)._min_ax12;
                    _augval874 = ((_augval874) < (_val878)) ? (_augval874) : (_val878);
                }
                (_a871)._min_ax12 = _augval874;
                /* _min_ay13 is min of ay1 */
                var _augval879 = (_a871).ay1;
                var _child880 = (_a871)._left7;
                if (!((_child880) == null)) {
                    var _val881 = (_child880)._min_ay13;
                    _augval879 = ((_augval879) < (_val881)) ? (_augval879) : (_val881);
                }
                var _child882 = (_a871)._right8;
                if (!((_child882) == null)) {
                    var _val883 = (_child882)._min_ay13;
                    _augval879 = ((_augval879) < (_val883)) ? (_augval879) : (_val883);
                }
                (_a871)._min_ay13 = _augval879;
                /* _max_ay24 is max of ay2 */
                var _augval884 = (_a871).ay2;
                var _child885 = (_a871)._left7;
                if (!((_child885) == null)) {
                    var _val886 = (_child885)._max_ay24;
                    _augval884 = ((_augval884) < (_val886)) ? (_val886) : (_augval884);
                }
                var _child887 = (_a871)._right8;
                if (!((_child887) == null)) {
                    var _val888 = (_child887)._max_ay24;
                    _augval884 = ((_augval884) < (_val888)) ? (_val888) : (_augval884);
                }
                (_a871)._max_ay24 = _augval884;
                (_a871)._height10 = 1 + ((((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) > ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10))) ? ((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) : ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval889 = (_b872).ax1;
                var _child890 = (_b872)._left7;
                if (!((_child890) == null)) {
                    var _val891 = (_child890)._min_ax12;
                    _augval889 = ((_augval889) < (_val891)) ? (_augval889) : (_val891);
                }
                var _child892 = (_b872)._right8;
                if (!((_child892) == null)) {
                    var _val893 = (_child892)._min_ax12;
                    _augval889 = ((_augval889) < (_val893)) ? (_augval889) : (_val893);
                }
                (_b872)._min_ax12 = _augval889;
                /* _min_ay13 is min of ay1 */
                var _augval894 = (_b872).ay1;
                var _child895 = (_b872)._left7;
                if (!((_child895) == null)) {
                    var _val896 = (_child895)._min_ay13;
                    _augval894 = ((_augval894) < (_val896)) ? (_augval894) : (_val896);
                }
                var _child897 = (_b872)._right8;
                if (!((_child897) == null)) {
                    var _val898 = (_child897)._min_ay13;
                    _augval894 = ((_augval894) < (_val898)) ? (_augval894) : (_val898);
                }
                (_b872)._min_ay13 = _augval894;
                /* _max_ay24 is max of ay2 */
                var _augval899 = (_b872).ay2;
                var _child900 = (_b872)._left7;
                if (!((_child900) == null)) {
                    var _val901 = (_child900)._max_ay24;
                    _augval899 = ((_augval899) < (_val901)) ? (_val901) : (_augval899);
                }
                var _child902 = (_b872)._right8;
                if (!((_child902) == null)) {
                    var _val903 = (_child902)._max_ay24;
                    _augval899 = ((_augval899) < (_val903)) ? (_val903) : (_augval899);
                }
                (_b872)._max_ay24 = _augval899;
                (_b872)._height10 = 1 + ((((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) > ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10))) ? ((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) : ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10)));
                if (!(((_b872)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval904 = ((_b872)._parent9).ax1;
                    var _child905 = ((_b872)._parent9)._left7;
                    if (!((_child905) == null)) {
                        var _val906 = (_child905)._min_ax12;
                        _augval904 = ((_augval904) < (_val906)) ? (_augval904) : (_val906);
                    }
                    var _child907 = ((_b872)._parent9)._right8;
                    if (!((_child907) == null)) {
                        var _val908 = (_child907)._min_ax12;
                        _augval904 = ((_augval904) < (_val908)) ? (_augval904) : (_val908);
                    }
                    ((_b872)._parent9)._min_ax12 = _augval904;
                    /* _min_ay13 is min of ay1 */
                    var _augval909 = ((_b872)._parent9).ay1;
                    var _child910 = ((_b872)._parent9)._left7;
                    if (!((_child910) == null)) {
                        var _val911 = (_child910)._min_ay13;
                        _augval909 = ((_augval909) < (_val911)) ? (_augval909) : (_val911);
                    }
                    var _child912 = ((_b872)._parent9)._right8;
                    if (!((_child912) == null)) {
                        var _val913 = (_child912)._min_ay13;
                        _augval909 = ((_augval909) < (_val913)) ? (_augval909) : (_val913);
                    }
                    ((_b872)._parent9)._min_ay13 = _augval909;
                    /* _max_ay24 is max of ay2 */
                    var _augval914 = ((_b872)._parent9).ay2;
                    var _child915 = ((_b872)._parent9)._left7;
                    if (!((_child915) == null)) {
                        var _val916 = (_child915)._max_ay24;
                        _augval914 = ((_augval914) < (_val916)) ? (_val916) : (_augval914);
                    }
                    var _child917 = ((_b872)._parent9)._right8;
                    if (!((_child917) == null)) {
                        var _val918 = (_child917)._max_ay24;
                        _augval914 = ((_augval914) < (_val918)) ? (_val918) : (_augval914);
                    }
                    ((_b872)._parent9)._max_ay24 = _augval914;
                    ((_b872)._parent9)._height10 = 1 + (((((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) > (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10))) ? (((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) : (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b872;
                }
            }
            /* rotate (_cursor773)._right8 */
            var _a919 = _cursor773;
            var _b920 = (_a919)._right8;
            var _c921 = (_b920)._left7;
            /* replace _a919 with _b920 in (_a919)._parent9 */
            if (!(((_a919)._parent9) == null)) {
                if ((((_a919)._parent9)._left7) == (_a919)) {
                    ((_a919)._parent9)._left7 = _b920;
                } else {
                    ((_a919)._parent9)._right8 = _b920;
                }
            }
            if (!((_b920) == null)) {
                (_b920)._parent9 = (_a919)._parent9;
            }
            /* replace _c921 with _a919 in _b920 */
            (_b920)._left7 = _a919;
            if (!((_a919) == null)) {
                (_a919)._parent9 = _b920;
            }
            /* replace _b920 with _c921 in _a919 */
            (_a919)._right8 = _c921;
            if (!((_c921) == null)) {
                (_c921)._parent9 = _a919;
            }
            /* _min_ax12 is min of ax1 */
            var _augval922 = (_a919).ax1;
            var _child923 = (_a919)._left7;
            if (!((_child923) == null)) {
                var _val924 = (_child923)._min_ax12;
                _augval922 = ((_augval922) < (_val924)) ? (_augval922) : (_val924);
            }
            var _child925 = (_a919)._right8;
            if (!((_child925) == null)) {
                var _val926 = (_child925)._min_ax12;
                _augval922 = ((_augval922) < (_val926)) ? (_augval922) : (_val926);
            }
            (_a919)._min_ax12 = _augval922;
            /* _min_ay13 is min of ay1 */
            var _augval927 = (_a919).ay1;
            var _child928 = (_a919)._left7;
            if (!((_child928) == null)) {
                var _val929 = (_child928)._min_ay13;
                _augval927 = ((_augval927) < (_val929)) ? (_augval927) : (_val929);
            }
            var _child930 = (_a919)._right8;
            if (!((_child930) == null)) {
                var _val931 = (_child930)._min_ay13;
                _augval927 = ((_augval927) < (_val931)) ? (_augval927) : (_val931);
            }
            (_a919)._min_ay13 = _augval927;
            /* _max_ay24 is max of ay2 */
            var _augval932 = (_a919).ay2;
            var _child933 = (_a919)._left7;
            if (!((_child933) == null)) {
                var _val934 = (_child933)._max_ay24;
                _augval932 = ((_augval932) < (_val934)) ? (_val934) : (_augval932);
            }
            var _child935 = (_a919)._right8;
            if (!((_child935) == null)) {
                var _val936 = (_child935)._max_ay24;
                _augval932 = ((_augval932) < (_val936)) ? (_val936) : (_augval932);
            }
            (_a919)._max_ay24 = _augval932;
            (_a919)._height10 = 1 + ((((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) > ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10))) ? ((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) : ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval937 = (_b920).ax1;
            var _child938 = (_b920)._left7;
            if (!((_child938) == null)) {
                var _val939 = (_child938)._min_ax12;
                _augval937 = ((_augval937) < (_val939)) ? (_augval937) : (_val939);
            }
            var _child940 = (_b920)._right8;
            if (!((_child940) == null)) {
                var _val941 = (_child940)._min_ax12;
                _augval937 = ((_augval937) < (_val941)) ? (_augval937) : (_val941);
            }
            (_b920)._min_ax12 = _augval937;
            /* _min_ay13 is min of ay1 */
            var _augval942 = (_b920).ay1;
            var _child943 = (_b920)._left7;
            if (!((_child943) == null)) {
                var _val944 = (_child943)._min_ay13;
                _augval942 = ((_augval942) < (_val944)) ? (_augval942) : (_val944);
            }
            var _child945 = (_b920)._right8;
            if (!((_child945) == null)) {
                var _val946 = (_child945)._min_ay13;
                _augval942 = ((_augval942) < (_val946)) ? (_augval942) : (_val946);
            }
            (_b920)._min_ay13 = _augval942;
            /* _max_ay24 is max of ay2 */
            var _augval947 = (_b920).ay2;
            var _child948 = (_b920)._left7;
            if (!((_child948) == null)) {
                var _val949 = (_child948)._max_ay24;
                _augval947 = ((_augval947) < (_val949)) ? (_val949) : (_augval947);
            }
            var _child950 = (_b920)._right8;
            if (!((_child950) == null)) {
                var _val951 = (_child950)._max_ay24;
                _augval947 = ((_augval947) < (_val951)) ? (_val951) : (_augval947);
            }
            (_b920)._max_ay24 = _augval947;
            (_b920)._height10 = 1 + ((((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) > ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10))) ? ((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) : ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10)));
            if (!(((_b920)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval952 = ((_b920)._parent9).ax1;
                var _child953 = ((_b920)._parent9)._left7;
                if (!((_child953) == null)) {
                    var _val954 = (_child953)._min_ax12;
                    _augval952 = ((_augval952) < (_val954)) ? (_augval952) : (_val954);
                }
                var _child955 = ((_b920)._parent9)._right8;
                if (!((_child955) == null)) {
                    var _val956 = (_child955)._min_ax12;
                    _augval952 = ((_augval952) < (_val956)) ? (_augval952) : (_val956);
                }
                ((_b920)._parent9)._min_ax12 = _augval952;
                /* _min_ay13 is min of ay1 */
                var _augval957 = ((_b920)._parent9).ay1;
                var _child958 = ((_b920)._parent9)._left7;
                if (!((_child958) == null)) {
                    var _val959 = (_child958)._min_ay13;
                    _augval957 = ((_augval957) < (_val959)) ? (_augval957) : (_val959);
                }
                var _child960 = ((_b920)._parent9)._right8;
                if (!((_child960) == null)) {
                    var _val961 = (_child960)._min_ay13;
                    _augval957 = ((_augval957) < (_val961)) ? (_augval957) : (_val961);
                }
                ((_b920)._parent9)._min_ay13 = _augval957;
                /* _max_ay24 is max of ay2 */
                var _augval962 = ((_b920)._parent9).ay2;
                var _child963 = ((_b920)._parent9)._left7;
                if (!((_child963) == null)) {
                    var _val964 = (_child963)._max_ay24;
                    _augval962 = ((_augval962) < (_val964)) ? (_val964) : (_augval962);
                }
                var _child965 = ((_b920)._parent9)._right8;
                if (!((_child965) == null)) {
                    var _val966 = (_child965)._max_ay24;
                    _augval962 = ((_augval962) < (_val966)) ? (_val966) : (_augval962);
                }
                ((_b920)._parent9)._max_ay24 = _augval962;
                ((_b920)._parent9)._height10 = 1 + (((((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) > (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10))) ? (((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) : (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b920;
            }
            _cursor773 = (_cursor773)._parent9;
        }
    }
    (__x).ax1 = ax1;
    (__x).ay1 = ay1;
    (__x).ax2 = ax2;
    (__x).ay2 = ay2;
}
RectangleHolder.prototype.findMatchingRectangles = function (bx1, by1, bx2, by2, __callback) {
    var _root967 = (this)._root1;
    var _x968 = _root967;
    var _descend969 = true;
    var _from_left970 = true;
    while (true) {
        if ((_x968) == null) {
            _x968 = null;
            break;
        }
        if (_descend969) {
            /* too small? */
            if ((false) || (((_x968).ax2) <= (bx1))) {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            } else if ((!(((_x968)._left7) == null)) && ((((true) && ((((_x968)._left7)._min_ax12) < (bx2))) && ((((_x968)._left7)._min_ay13) < (by2))) && ((((_x968)._left7)._max_ay24) > (by1)))) {
                _x968 = (_x968)._left7;
                /* too large? */
            } else if (false) {
                if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
                /* node ok? */
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((_x968) == (_root967)) {
                _root967 = (_x968)._right8;
                _x968 = (_x968)._right8;
            } else {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            }
        } else if (_from_left970) {
            if (false) {
                _x968 = null;
                break;
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                _descend969 = true;
                if ((_x968) == (_root967)) {
                    _root967 = (_x968)._right8;
                }
                _x968 = (_x968)._right8;
            } else if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        } else {
            if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        }
    }
    var _prev_cursor5 = null;
    var _cursor6 = _x968;
    for (; ;) {
        if (!(!((_cursor6) == null))) break;
        var _name971 = _cursor6;
        /* ADVANCE */
        _prev_cursor5 = _cursor6;
        do {
            var _right_min972 = null;
            if ((!(((_cursor6)._right8) == null)) && ((((true) && ((((_cursor6)._right8)._min_ax12) < (bx2))) && ((((_cursor6)._right8)._min_ay13) < (by2))) && ((((_cursor6)._right8)._max_ay24) > (by1)))) {
                var _root973 = (_cursor6)._right8;
                var _x974 = _root973;
                var _descend975 = true;
                var _from_left976 = true;
                while (true) {
                    if ((_x974) == null) {
                        _x974 = null;
                        break;
                    }
                    if (_descend975) {
                        /* too small? */
                        if ((false) || (((_x974).ax2) <= (bx1))) {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        } else if ((!(((_x974)._left7) == null)) && ((((true) && ((((_x974)._left7)._min_ax12) < (bx2))) && ((((_x974)._left7)._min_ay13) < (by2))) && ((((_x974)._left7)._max_ay24) > (by1)))) {
                            _x974 = (_x974)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                            /* node ok? */
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((_x974) == (_root973)) {
                            _root973 = (_x974)._right8;
                            _x974 = (_x974)._right8;
                        } else {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        }
                    } else if (_from_left976) {
                        if (false) {
                            _x974 = null;
                            break;
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                            _descend975 = true;
                            if ((_x974) == (_root973)) {
                                _root973 = (_x974)._right8;
                            }
                            _x974 = (_x974)._right8;
                        } else if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    } else {
                        if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    }
                }
                _right_min972 = _x974;
            }
            if (!((_right_min972) == null)) {
                _cursor6 = _right_min972;
                break;
            } else {
                while ((!(((_cursor6)._parent9) == null)) && ((_cursor6) == (((_cursor6)._parent9)._right8))) {
                    _cursor6 = (_cursor6)._parent9;
                }
                _cursor6 = (_cursor6)._parent9;
                if ((!((_cursor6) == null)) && (false)) {
                    _cursor6 = null;
                }
            }
        } while ((!((_cursor6) == null)) && (!((((true) && (((_cursor6).ax1) < (bx2))) && (((_cursor6).ay1) < (by2))) && (((_cursor6).ay2) > (by1)))));
        if (__callback(_name971)) {
            var _to_remove977 = _prev_cursor5;
            var _parent978 = (_to_remove977)._parent9;
            var _left979 = (_to_remove977)._left7;
            var _right980 = (_to_remove977)._right8;
            var _new_x981;
            if (((_left979) == null) && ((_right980) == null)) {
                _new_x981 = null;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if ((!((_left979) == null)) && ((_right980) == null)) {
                _new_x981 = _left979;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if (((_left979) == null) && (!((_right980) == null))) {
                _new_x981 = _right980;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else {
                var _root982 = (_to_remove977)._right8;
                var _x983 = _root982;
                var _descend984 = true;
                var _from_left985 = true;
                while (true) {
                    if ((_x983) == null) {
                        _x983 = null;
                        break;
                    }
                    if (_descend984) {
                        /* too small? */
                        if (false) {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        } else if ((!(((_x983)._left7) == null)) && (true)) {
                            _x983 = (_x983)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                            /* node ok? */
                        } else if (true) {
                            break;
                        } else if ((_x983) == (_root982)) {
                            _root982 = (_x983)._right8;
                            _x983 = (_x983)._right8;
                        } else {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        }
                    } else if (_from_left985) {
                        if (false) {
                            _x983 = null;
                            break;
                        } else if (true) {
                            break;
                        } else if ((!(((_x983)._right8) == null)) && (true)) {
                            _descend984 = true;
                            if ((_x983) == (_root982)) {
                                _root982 = (_x983)._right8;
                            }
                            _x983 = (_x983)._right8;
                        } else if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    } else {
                        if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    }
                }
                _new_x981 = _x983;
                var _mp986 = (_x983)._parent9;
                var _mr987 = (_x983)._right8;
                /* replace _x983 with _mr987 in _mp986 */
                if (!((_mp986) == null)) {
                    if (((_mp986)._left7) == (_x983)) {
                        (_mp986)._left7 = _mr987;
                    } else {
                        (_mp986)._right8 = _mr987;
                    }
                }
                if (!((_mr987) == null)) {
                    (_mr987)._parent9 = _mp986;
                }
                /* replace _to_remove977 with _x983 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _x983;
                    } else {
                        (_parent978)._right8 = _x983;
                    }
                }
                if (!((_x983) == null)) {
                    (_x983)._parent9 = _parent978;
                }
                /* replace null with _left979 in _x983 */
                (_x983)._left7 = _left979;
                if (!((_left979) == null)) {
                    (_left979)._parent9 = _x983;
                }
                /* replace _mr987 with (_to_remove977)._right8 in _x983 */
                (_x983)._right8 = (_to_remove977)._right8;
                if (!(((_to_remove977)._right8) == null)) {
                    ((_to_remove977)._right8)._parent9 = _x983;
                }
                /* _min_ax12 is min of ax1 */
                var _augval988 = (_x983).ax1;
                var _child989 = (_x983)._left7;
                if (!((_child989) == null)) {
                    var _val990 = (_child989)._min_ax12;
                    _augval988 = ((_augval988) < (_val990)) ? (_augval988) : (_val990);
                }
                var _child991 = (_x983)._right8;
                if (!((_child991) == null)) {
                    var _val992 = (_child991)._min_ax12;
                    _augval988 = ((_augval988) < (_val992)) ? (_augval988) : (_val992);
                }
                (_x983)._min_ax12 = _augval988;
                /* _min_ay13 is min of ay1 */
                var _augval993 = (_x983).ay1;
                var _child994 = (_x983)._left7;
                if (!((_child994) == null)) {
                    var _val995 = (_child994)._min_ay13;
                    _augval993 = ((_augval993) < (_val995)) ? (_augval993) : (_val995);
                }
                var _child996 = (_x983)._right8;
                if (!((_child996) == null)) {
                    var _val997 = (_child996)._min_ay13;
                    _augval993 = ((_augval993) < (_val997)) ? (_augval993) : (_val997);
                }
                (_x983)._min_ay13 = _augval993;
                /* _max_ay24 is max of ay2 */
                var _augval998 = (_x983).ay2;
                var _child999 = (_x983)._left7;
                if (!((_child999) == null)) {
                    var _val1000 = (_child999)._max_ay24;
                    _augval998 = ((_augval998) < (_val1000)) ? (_val1000) : (_augval998);
                }
                var _child1001 = (_x983)._right8;
                if (!((_child1001) == null)) {
                    var _val1002 = (_child1001)._max_ay24;
                    _augval998 = ((_augval998) < (_val1002)) ? (_val1002) : (_augval998);
                }
                (_x983)._max_ay24 = _augval998;
                (_x983)._height10 = 1 + ((((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) > ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10))) ? ((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) : ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10)));
                var _cursor1003 = _mp986;
                var _changed1004 = true;
                while ((_changed1004) && (!((_cursor1003) == (_parent978)))) {
                    var _old__min_ax121005 = (_cursor1003)._min_ax12;
                    var _old__min_ay131006 = (_cursor1003)._min_ay13;
                    var _old__max_ay241007 = (_cursor1003)._max_ay24;
                    var _old_height1008 = (_cursor1003)._height10;
                    /* _min_ax12 is min of ax1 */
                    var _augval1009 = (_cursor1003).ax1;
                    var _child1010 = (_cursor1003)._left7;
                    if (!((_child1010) == null)) {
                        var _val1011 = (_child1010)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1011)) ? (_augval1009) : (_val1011);
                    }
                    var _child1012 = (_cursor1003)._right8;
                    if (!((_child1012) == null)) {
                        var _val1013 = (_child1012)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1013)) ? (_augval1009) : (_val1013);
                    }
                    (_cursor1003)._min_ax12 = _augval1009;
                    /* _min_ay13 is min of ay1 */
                    var _augval1014 = (_cursor1003).ay1;
                    var _child1015 = (_cursor1003)._left7;
                    if (!((_child1015) == null)) {
                        var _val1016 = (_child1015)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1016)) ? (_augval1014) : (_val1016);
                    }
                    var _child1017 = (_cursor1003)._right8;
                    if (!((_child1017) == null)) {
                        var _val1018 = (_child1017)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1018)) ? (_augval1014) : (_val1018);
                    }
                    (_cursor1003)._min_ay13 = _augval1014;
                    /* _max_ay24 is max of ay2 */
                    var _augval1019 = (_cursor1003).ay2;
                    var _child1020 = (_cursor1003)._left7;
                    if (!((_child1020) == null)) {
                        var _val1021 = (_child1020)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1021)) ? (_val1021) : (_augval1019);
                    }
                    var _child1022 = (_cursor1003)._right8;
                    if (!((_child1022) == null)) {
                        var _val1023 = (_child1022)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1023)) ? (_val1023) : (_augval1019);
                    }
                    (_cursor1003)._max_ay24 = _augval1019;
                    (_cursor1003)._height10 = 1 + ((((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) > ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10))) ? ((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) : ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10)));
                    _changed1004 = false;
                    _changed1004 = (_changed1004) || (!((_old__min_ax121005) == ((_cursor1003)._min_ax12)));
                    _changed1004 = (_changed1004) || (!((_old__min_ay131006) == ((_cursor1003)._min_ay13)));
                    _changed1004 = (_changed1004) || (!((_old__max_ay241007) == ((_cursor1003)._max_ay24)));
                    _changed1004 = (_changed1004) || (!((_old_height1008) == ((_cursor1003)._height10)));
                    _cursor1003 = (_cursor1003)._parent9;
                }
            }
            var _cursor1024 = _parent978;
            var _changed1025 = true;
            while ((_changed1025) && (!((_cursor1024) == (null)))) {
                var _old__min_ax121026 = (_cursor1024)._min_ax12;
                var _old__min_ay131027 = (_cursor1024)._min_ay13;
                var _old__max_ay241028 = (_cursor1024)._max_ay24;
                var _old_height1029 = (_cursor1024)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval1030 = (_cursor1024).ax1;
                var _child1031 = (_cursor1024)._left7;
                if (!((_child1031) == null)) {
                    var _val1032 = (_child1031)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1032)) ? (_augval1030) : (_val1032);
                }
                var _child1033 = (_cursor1024)._right8;
                if (!((_child1033) == null)) {
                    var _val1034 = (_child1033)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1034)) ? (_augval1030) : (_val1034);
                }
                (_cursor1024)._min_ax12 = _augval1030;
                /* _min_ay13 is min of ay1 */
                var _augval1035 = (_cursor1024).ay1;
                var _child1036 = (_cursor1024)._left7;
                if (!((_child1036) == null)) {
                    var _val1037 = (_child1036)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1037)) ? (_augval1035) : (_val1037);
                }
                var _child1038 = (_cursor1024)._right8;
                if (!((_child1038) == null)) {
                    var _val1039 = (_child1038)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1039)) ? (_augval1035) : (_val1039);
                }
                (_cursor1024)._min_ay13 = _augval1035;
                /* _max_ay24 is max of ay2 */
                var _augval1040 = (_cursor1024).ay2;
                var _child1041 = (_cursor1024)._left7;
                if (!((_child1041) == null)) {
                    var _val1042 = (_child1041)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1042)) ? (_val1042) : (_augval1040);
                }
                var _child1043 = (_cursor1024)._right8;
                if (!((_child1043) == null)) {
                    var _val1044 = (_child1043)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1044)) ? (_val1044) : (_augval1040);
                }
                (_cursor1024)._max_ay24 = _augval1040;
                (_cursor1024)._height10 = 1 + ((((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) > ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10))) ? ((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) : ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10)));
                _changed1025 = false;
                _changed1025 = (_changed1025) || (!((_old__min_ax121026) == ((_cursor1024)._min_ax12)));
                _changed1025 = (_changed1025) || (!((_old__min_ay131027) == ((_cursor1024)._min_ay13)));
                _changed1025 = (_changed1025) || (!((_old__max_ay241028) == ((_cursor1024)._max_ay24)));
                _changed1025 = (_changed1025) || (!((_old_height1029) == ((_cursor1024)._height10)));
                _cursor1024 = (_cursor1024)._parent9;
            }
            if (((this)._root1) == (_to_remove977)) {
                (this)._root1 = _new_x981;
            }
            _prev_cursor5 = null;
        }
    };
}
; 
 
 buildViz = function (d3) {
    return function (widthInPixels = 1000,
                     heightInPixels = 600,
                     max_snippets = null,
                     color = null,
                     sortByDist = true,
                     useFullDoc = false,
                     greyZeroScores = false,
                     asianMode = false,
                     nonTextFeaturesMode = false,
                     showCharacteristic = true,
                     wordVecMaxPValue = false,
                     saveSvgButton = false,
                     reverseSortScoresForNotCategory = false,
                     minPVal = 0.1,
                     pValueColors = false,
                     xLabelText = null,
                     yLabelText = null,
                     fullData = null,
                     showTopTerms = true,
                     showNeutral = false,
                     getTooltipContent = null,
                     xAxisValues = null,
                     yAxisValues = null,
                     colorFunc = null,
                     showAxes = true,
                     showExtra = false,
                     doCensorPoints = true,
                     centerLabelsOverPoints = false,
                     xAxisLabels = null,
                     yAxisLabels = null,
                     topic_model_preview_size=10,
                     verticalLines = null,
                     horizontal_line_y_position = null,
                     vertical_line_x_position = null,
                     unifiedContexts = false,
                     showCategoryHeadings = true,
                     showCrossAxes = true,
                     divName = 'd3-div-1',
                     alternativeTermFunc = null) {
        //var divName = 'd3-div-1';
        // Set the dimensions of the canvas / graph
        var padding = {top: 30, right: 20, bottom: 30, left: 50};
        if (!showAxes) {
            padding = {top: 30, right: 20, bottom: 30, left: 50};
        }
        var margin = padding,
            width = widthInPixels - margin.left - margin.right,
            height = heightInPixels - margin.top - margin.bottom;
        fullData.data.forEach(function (x, i) {x.i = i});
        
        // Set the ranges
        var x = d3.scaleLinear().range([0, width]);
        var y = d3.scaleLinear().range([height, 0]);

        if (unifiedContexts) {
            document.querySelectorAll('#'+divName+'-'+'notcol')
                .forEach(function (x) {x.style.display = 'none'});
            document.querySelectorAll('.'+divName+'-'+'contexts')
                .forEach(function (x) {x.style.width = '90%'});
        } 
        else if (showNeutral) {
            if (showExtra) {
                document.querySelectorAll('.'+divName+'-'+'contexts')
                .forEach(function (x) {
                    x.style.width = '25%'
                    x.style.float = 'left'
                });

                ['notcol','neutcol','extracol'].forEach(function (columnName) { 
                    document.querySelectorAll('#'+divName+'-'+columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '25%'
                        });
                })

            } else {
                document.querySelectorAll('.'+divName+'-'+'contexts')
                .forEach(function (x) {
                    x.style.width = '33%'
                    x.style.float = 'left'
                });

                ['notcol','neutcol'].forEach(function (columnName) { 
                    document.querySelectorAll('#'+divName+'-'+columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '33%'
                        });
                })


            }
        } else {
            document.querySelectorAll('.'+divName+'-'+'contexts')
                .forEach(function (x) {
                    x.style.width = '45%'
                    //x.style.display = 'inline'
                    x.style.float = 'left'
                });

            ['notcol'].forEach(function (columnName) { 
                document.querySelectorAll('#'+divName+'-'+columnName)
                    .forEach(function (x) {
                        //x.style.display = 'inline'
                        x.style.float = 'left'
                        x.style.width = '45%'
                    });
            })
        }

        var yAxis = null;
        var xAxis = null;

        function axisLabelerFactory(axis) {
            if ((axis == "x" && xLabelText == null)
                || (axis == "y" && yLabelText == null))
                return function (d, i) {
                    return ["Infrequent", "Average", "Frequent"][i];
                };

            return function (d, i) {
                return ["Low", "Medium", "High"][i];
            }
        }


        function bs(ar, x) {
            function bsa(s, e) {
                var mid = Math.floor((s + e) / 2);
                var midval = ar[mid];
                if (s == e) {
                    return s;
                }
                if (midval == x) {
                    return mid;
                } else if (midval < x) {
                    return bsa(mid + 1, e);
                } else {
                    return bsa(s, mid);
                }
            }

            return bsa(0, ar.length);
        }
        

        console.log("fullData");
        console.log(fullData);

        
        var sortedX = fullData.data.map(x=>x).sort(function (a, b) {
            return a.x < b.x ? -1 : (a.x == b.x ? 0 : 1);
        }).map(function (x) {
            return x.x
        });

        var sortedOx = fullData.data.map(x=>x).sort(function (a, b) {
            return a.ox < b.ox ? -1 : (a.ox == b.ox ? 0 : 1);
        }).map(function (x) {
            return x.ox
        });

        var sortedY = fullData.data.map(x=>x).sort(function (a, b) {
            return a.y < b.y ? -1 : (a.y == b.y ? 0 : 1);
        }).map(function (x) {
            return x.y
        });

        var sortedOy = fullData.data.map(x=>x).sort(function (a, b) {
            return a.oy < b.oy ? -1 : (a.oy == b.oy ? 0 : 1);
        }).map(function (x) {
            return x.oy
        });
        console.log("444");
        console.log(fullData.data[0])


        function labelWithZScore(axis, axisName, tickPoints) {
            var myVals = axisName === 'x' ? sortedOx : sortedOy;
            var myPlotedVals = axisName === 'x' ? sortedX : sortedY;
            var ticks = tickPoints.map(function (x) {
                return myPlotedVals[bs(myVals, x)]
            });
            return axis.tickValues(ticks).tickFormat(
                function (d, i) {
                    return tickPoints[i];
                })
        }

        if (xAxisValues) {
            xAxis = labelWithZScore(d3.axisBottom(x), 'x', xAxisValues);
        } else if (xAxisLabels) {
            xAxis = d3.axisBottom(x)
                .ticks(xAxisLabels.length)
                .tickFormat(function (d, i) {
                    return xAxisLabels[i];
                });
        } else {
            xAxis = d3.axisBottom(x).ticks(3).tickFormat(axisLabelerFactory('x'));
        }
        if (yAxisValues) {
            yAxis = labelWithZScore(d3.axisLeft(y), 'y', yAxisValues);
        } else if (yAxisLabels) {
            yAxis = d3.axisLeft(y)
                .ticks(yAxisLabels.length)
                .tickFormat(function (d, i) {
                    return yAxisLabels[i];
                });
        } else {
            yAxis = d3.axisLeft(y).ticks(3).tickFormat(axisLabelerFactory('y'));
        }

        // var label = d3.select("body").append("div")
        var label = d3.select('#' + divName).append("div")
            .attr("class", "label");

        var interpolateLightGreys = d3.interpolate(d3.rgb(230, 230, 230),
            d3.rgb(130, 130, 130));
        // setup fill color
        if (color == null) {
            color = d3.interpolateRdYlBu;
            //color = d3.interpolateWarm;
        }

        var pixelsToAddToWidth = 200;
        if (!showTopTerms && !showCharacteristic) {
            pixelsToAddToWidth = 0;
        }

        // Adds the svg canvas
        // var svg = d3.select("body")
        svg = d3.select('#' + divName)
            .append("svg")
            .attr("width", width + margin.left + margin.right + pixelsToAddToWidth)
            .attr("height", height + margin.top + margin.bottom)
            .append("g")
            .attr("transform",
                "translate(" + margin.left + "," + margin.top + ")");


        origSVGLeft = svg.node().getBoundingClientRect().left;
        origSVGTop = svg.node().getBoundingClientRect().top;
        var lastCircleSelected = null;

        function getCorpusWordCounts() {
            var binaryLabels = fullData.docs.labels.map(function (label) {
                return 1 * (fullData.docs.categories[label] != fullData.info.category_internal_name);
            });
            var wordCounts = {}; // word -> [cat counts, not-cat-counts]
            var wordCountSums = [0, 0];
            fullData.docs.texts.forEach(function (text, i) {
                text.toLowerCase().trim().split(/\W+/).forEach(function (word) {
                    if (word.trim() !== '') {
                        if (!(word in wordCounts))
                            wordCounts[word] = [0, 0];
                        wordCounts[word][binaryLabels[i]]++;
                        wordCountSums[binaryLabels[i]]++;
                    }
                })
            });
            return {
                avgDocLen: (wordCountSums[0] + wordCountSums[1]) / fullData.docs.texts.length,
                counts: wordCounts,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)]
                })
            };
        }

        function getContextWordCounts(query) {
            var wordCounts = {};
            var wordCountSums = [0, 0];
            var priorCountSums = [0, 0];
            gatherTermContexts(termDict[query])
                .contexts
                .forEach(function (contextSet, categoryIdx) {
                    contextSet.forEach(function (context) {
                        context.snippets.forEach(function (snippet) {
                            var tokens = snippet.toLowerCase().trim().replace('<b>', '').replace('</b>', '').split(/\W+/);
                            var matchIndices = [];
                            tokens.forEach(function (word, i) {
                                if (word === query) matchIndices.push(i)
                            });
                            tokens.forEach(function (word, i) {
                                if (word.trim() !== '') {
                                    var isValid = false;
                                    for (var matchI in matchIndices) {
                                        if (Math.abs(i - matchI) < 3) {
                                            isValid = true;
                                            break
                                        }
                                    }
                                    if (isValid) {
                                        //console.log([word, i, matchI, isValid]);
                                        if (!(word in wordCounts)) {
                                            var priorCounts = corpusWordCounts.counts[word]
                                            wordCounts[word] = [0, 0].concat(priorCounts);
                                            priorCountSums[0] += priorCounts[0];
                                            priorCountSums[1] += priorCounts[1];
                                        }
                                        wordCounts[word][categoryIdx]++;
                                        wordCountSums[categoryIdx]++;
                                    }
                                }
                            })
                        })
                    })
                });
            return {
                counts: wordCounts,
                priorSums: priorCountSums,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)];
                })
            }

        }
        
        function denseRank(ar) {
            var markedAr = ar.map((x,i) => [x,i]).sort((a,b) => a[0] - b[0]);
            var curRank = 1
            var rankedAr = markedAr.map(
                function(x, i) {
                    if(i > 0 && x[0] != markedAr[i-1][0]) {
                        curRank++;
                    }
                    return [curRank, x[0], x[1]];
                }
            )
            return rankedAr.map(x=>x).sort((a,b) => (a[2] - b[2])).map(x => x[0]);    
        }
        
        
        function getDenseRanks(fullData, categoryNum) {
            var fgFreqs = Array(fullData.data.length).fill(0);
            var bgFreqs = Array(fullData.data.length).fill(0);
            var categoryTermCounts = fullData.termCounts[categoryNum];
            
            
            Object.keys(categoryTermCounts).forEach(
                key => fgFreqs[key] = categoryTermCounts[key][0]
            )
            fullData.termCounts.forEach( 
                function (categoryTermCounts, otherCategoryNum) {
                    if(otherCategoryNum != categoryNum) {
                        Object.keys(categoryTermCounts).forEach(
                           key => bgFreqs[key] += categoryTermCounts[key][0]
                        )                        
                    }
                }
            )
            var fgDenseRanks = denseRank(fgFreqs);
            var bgDenseRanks = denseRank(bgFreqs);
            
            var maxfgDenseRanks = Math.max(...fgDenseRanks);
            var minfgDenseRanks = Math.min(...fgDenseRanks);
            var scalefgDenseRanks = fgDenseRanks.map(
                x => (x - minfgDenseRanks)/(maxfgDenseRanks - minfgDenseRanks)
            )

            var maxbgDenseRanks = Math.max(...bgDenseRanks);
            var minbgDenseRanks = Math.min(...bgDenseRanks);
            var scalebgDenseRanks = bgDenseRanks.map(
                x => (x - minbgDenseRanks)/(maxbgDenseRanks - minbgDenseRanks)
            )

            return {'fg': scalefgDenseRanks, 'bg': scalebgDenseRanks, 'bgFreqs': bgFreqs, 'fgFreqs': fgFreqs}
        }

        function getCategoryDenseRankScores(fullData, categoryNum) {
            var denseRanks = getDenseRanks(fullData, categoryNum)
            return denseRanks.fg.map((x,i) => x - denseRanks.bg[i]);
        }
        
        function getTermCounts(fullData) {
            var counts = Array(fullData.data.length).fill(0);  
            fullData.termCounts.forEach( 
                function (categoryTermCounts) {
                    Object.keys(categoryTermCounts).forEach(
                       key => counts[key] = categoryTermCounts[key][0]
                    )                        
                }
            )
            return counts;
        }
        
        function getContextWordLORIPs(query) {
            var contextWordCounts = getContextWordCounts(query);
            var ni_k = contextWordCounts.sums[0];
            var nj_k = contextWordCounts.sums[1];
            var n = ni_k + nj_k;
            //var ai_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            //var aj_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            var a0 = 0.00001 //corpusWordCounts.avgDocLen;
            var a_k0 = Object.keys(contextWordCounts.counts)
                .map(function (x) {
                    var counts = contextWordCounts.counts[x];
                    return a0 * (counts[2] + counts[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                })
                .reduce(function (a, b) {
                    return a + b
                });
            var ai_k0 = a_k0 / ni_k;
            var aj_k0 = a_k0 / nj_k;
            var scores = Object.keys(contextWordCounts.counts).map(
                function (word) {
                    var countData = contextWordCounts.counts[word];
                    var yi = countData[0];
                    var yj = countData[1];
                    //var ai = countData[2];
                    //var aj = countData[3];
                    //var ai = countData[2] + countData[3];
                    //var aj = ai;
                    //var ai = (countData[2] + countData[3]) * a0/ni_k;
                    //var aj = (countData[2] + countData[3]) * a0/nj_k;
                    var ai = a0 * (countData[2] + countData[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                    var aj = ai;
                    var deltahat_i_j =
                        +Math.log((yi + ai) * 1. / (ni_k + ai_k0 - yi - ai))
                        - Math.log((yj + aj) * 1. / (nj_k + aj_k0 - yj - aj));
                    var var_deltahat_i_j = 1. / (yi + ai) + 1. / (ni_k + ai_k0 - yi - ai)
                        + 1. / (yj + aj) + 1. / (nj_k + aj_k0 - yj - aj);
                    var zeta_ij = deltahat_i_j / Math.sqrt(var_deltahat_i_j);
                    return [word, yi, yj, ai, aj, ai_k0, zeta_ij];
                }
            ).sort(function (a, b) {
                return b[5] - a[5];
            });
            return scores;
        }

        function getContextWordSFS(query) {
            // from https://stackoverflow.com/questions/14846767/std-normal-cdf-normal-cdf-or-error-function
            function cdf(x, mean, variance) {
                return 0.5 * (1 + erf((x - mean) / (Math.sqrt(2 * variance))));
            }

            function erf(x) {
                // save the sign of x
                var sign = (x >= 0) ? 1 : -1;
                x = Math.abs(x);

                // constants
                var a1 = 0.254829592;
                var a2 = -0.284496736;
                var a3 = 1.421413741;
                var a4 = -1.453152027;
                var a5 = 1.061405429;
                var p = 0.3275911;

                // A&S formula 7.1.26
                var t = 1.0 / (1.0 + p * x);
                var y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * Math.exp(-x * x);
                return sign * y; // erf(-x) = -erf(x);
            }

            function scale(a) {
                return Math.log(a + 0.0000001);
            }

            var contextWordCounts = getContextWordCounts(query);
            var wordList = Object.keys(contextWordCounts.counts).map(function (word) {
                return contextWordCounts.counts[word].concat([word]);
            });
            var cat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - cat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - cat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            var ncat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - ncat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - ncat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            function scaledFScore(cnt, other, freq_xbar, freq_var, prec_xbar, prec_var) {
                var beta = 1.5;
                var normFreq = cdf(scale(cnt), freq_xbar, freq_var);
                var normPrec = cdf(scale(cnt / (cnt + other)), prec_xbar, prec_var);
                return (1 + Math.pow(beta, 2)) * normFreq * normPrec / (Math.pow(beta, 2) * normFreq + normPrec);
            }

            var sfs = wordList.map(function (x) {
                cat_sfs = scaledFScore(x[0], x[1], cat_freq_xbar,
                    cat_freq_var, cat_prec_xbar, cat_prec_var);
                ncat_sfs = scaledFScore(x[1], x[0], ncat_freq_xbar,
                    ncat_freq_var, ncat_prec_xbar, ncat_prec_var);
                return [cat_sfs > ncat_sfs ? cat_sfs : -ncat_sfs].concat(x);

            }).sort(function (a, b) {
                return b[0] - a[0];
            });
            return sfs;
        }

        function deselectLastCircle() {
            if (lastCircleSelected) {
                lastCircleSelected.style["stroke"] = null;
                lastCircleSelected = null;
            }
        }

        function getSentenceBoundaries(text) {
            // !!! need to use spacy's sentence splitter
            if (asianMode) {
                var sentenceRe = /\n/gmi;
            } else {
                var sentenceRe = /\(?[^\.\?\!\n\b]+[\n\.!\?]\)?/g;
            }
            var offsets = [];
            var match;
            while ((match = sentenceRe.exec(text)) != null) {
                offsets.push(match.index);
            }
            offsets.push(text.length);
            return offsets;
        }

        function getMatchingSnippet(text, boundaries, start, end) {
            var sentenceStart = null;
            var sentenceEnd = null;
            for (var i in boundaries) {
                var position = boundaries[i];
                if (position <= start && (sentenceStart == null || position > sentenceStart)) {
                    sentenceStart = position;
                }
                if (position >= end) {
                    sentenceEnd = position;
                    break;
                }
            }
            var snippet = (text.slice(sentenceStart, start) + "<b>" + text.slice(start, end)
                + "</b>" + text.slice(end, sentenceEnd)).trim();
            if (sentenceStart == null) {
                sentenceStart = 0;
            }
            return {'snippet': snippet, 'sentenceStart': sentenceStart};
        }

        function gatherTermContexts(d) {
            var category_name = fullData['info']['category_name'];
            var not_category_name = fullData['info']['not_category_name'];
            var matches = [[], [], [], []];
            console.log("searching")

            if (fullData.docs === undefined) return matches;
            if (!nonTextFeaturesMode) {
                return searchInText(d);
            } else {
                return searchInExtraFeatures(d);
            }
        }

        function searchInExtraFeatures(d) {
            var matches = [[], [], [], []];
            var term = d.term;
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });

            var pattern = null;
            if ('metalists' in fullData && term in fullData.metalists) {
                // from https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
                function escapeRegExp(str) {
                    return str.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
                }

                console.log('term');
                console.log(term);
                pattern = new RegExp(
                    '\\b(' + fullData.metalists[term].map(escapeRegExp).join('|') + ')\\b',
                    'gim'
                );
            }

            for (var i in fullData.docs.extra) {
                if (term in fullData.docs.extra[i]) {
                    var strength = fullData.docs.extra[i][term] /
                        Object.values(fullData.docs.extra[i]).reduce(
                            function (a, b) {
                                return a + b
                            });

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }
                    var text = fullData.docs.texts[i];
                    if (!useFullDoc)
                        text = text.slice(0, 300);
                    if (pattern !== null) {
                        text = text.replace(pattern, '<b>$&</b>');
                    }
                    var curMatch = {
                        'id': i,
                        'snippets': [text],
                        'strength': strength,
                        'docLabel': docLabel,
                        'meta': fullData.docs.meta ? fullData.docs.meta[i] : ""
                    }

                    matches[numericLabel].push(curMatch);
                }
            }
            for (var i in [0, 1]) {
                matches[i] = matches[i].sort(function (a, b) {
                    return a.strength < b.strength ? 1 : -1
                })
            }
            return {'contexts': matches, 'info': d};
        }

        // from https://mathiasbynens.be/notes/es-unicode-property-escapes#emoji
        var emojiRE = (/(?:[\u261D\u26F9\u270A-\u270D]|\uD83C[\uDF85\uDFC2-\uDFC4\uDFC7\uDFCA-\uDFCC]|\uD83D[\uDC42\uDC43\uDC46-\uDC50\uDC66-\uDC69\uDC6E\uDC70-\uDC78\uDC7C\uDC81-\uDC83\uDC85-\uDC87\uDCAA\uDD74\uDD75\uDD7A\uDD90\uDD95\uDD96\uDE45-\uDE47\uDE4B-\uDE4F\uDEA3\uDEB4-\uDEB6\uDEC0\uDECC]|\uD83E[\uDD18-\uDD1C\uDD1E\uDD1F\uDD26\uDD30-\uDD39\uDD3D\uDD3E\uDDD1-\uDDDD])(?:\uD83C[\uDFFB-\uDFFF])?|(?:[\u231A\u231B\u23E9-\u23EC\u23F0\u23F3\u25FD\u25FE\u2614\u2615\u2648-\u2653\u267F\u2693\u26A1\u26AA\u26AB\u26BD\u26BE\u26C4\u26C5\u26CE\u26D4\u26EA\u26F2\u26F3\u26F5\u26FA\u26FD\u2705\u270A\u270B\u2728\u274C\u274E\u2753-\u2755\u2757\u2795-\u2797\u27B0\u27BF\u2B1B\u2B1C\u2B50\u2B55]|\uD83C[\uDC04\uDCCF\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE1A\uDE2F\uDE32-\uDE36\uDE38-\uDE3A\uDE50\uDE51\uDF00-\uDF20\uDF2D-\uDF35\uDF37-\uDF7C\uDF7E-\uDF93\uDFA0-\uDFCA\uDFCF-\uDFD3\uDFE0-\uDFF0\uDFF4\uDFF8-\uDFFF]|\uD83D[\uDC00-\uDC3E\uDC40\uDC42-\uDCFC\uDCFF-\uDD3D\uDD4B-\uDD4E\uDD50-\uDD67\uDD7A\uDD95\uDD96\uDDA4\uDDFB-\uDE4F\uDE80-\uDEC5\uDECC\uDED0-\uDED2\uDEEB\uDEEC\uDEF4-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])|(?:[#\*0-9\xA9\xAE\u203C\u2049\u2122\u2139\u2194-\u2199\u21A9\u21AA\u231A\u231B\u2328\u23CF\u23E9-\u23F3\u23F8-\u23FA\u24C2\u25AA\u25AB\u25B6\u25C0\u25FB-\u25FE\u2600-\u2604\u260E\u2611\u2614\u2615\u2618\u261D\u2620\u2622\u2623\u2626\u262A\u262E\u262F\u2638-\u263A\u2640\u2642\u2648-\u2653\u2660\u2663\u2665\u2666\u2668\u267B\u267F\u2692-\u2697\u2699\u269B\u269C\u26A0\u26A1\u26AA\u26AB\u26B0\u26B1\u26BD\u26BE\u26C4\u26C5\u26C8\u26CE\u26CF\u26D1\u26D3\u26D4\u26E9\u26EA\u26F0-\u26F5\u26F7-\u26FA\u26FD\u2702\u2705\u2708-\u270D\u270F\u2712\u2714\u2716\u271D\u2721\u2728\u2733\u2734\u2744\u2747\u274C\u274E\u2753-\u2755\u2757\u2763\u2764\u2795-\u2797\u27A1\u27B0\u27BF\u2934\u2935\u2B05-\u2B07\u2B1B\u2B1C\u2B50\u2B55\u3030\u303D\u3297\u3299]|\uD83C[\uDC04\uDCCF\uDD70\uDD71\uDD7E\uDD7F\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE02\uDE1A\uDE2F\uDE32-\uDE3A\uDE50\uDE51\uDF00-\uDF21\uDF24-\uDF93\uDF96\uDF97\uDF99-\uDF9B\uDF9E-\uDFF0\uDFF3-\uDFF5\uDFF7-\uDFFF]|\uD83D[\uDC00-\uDCFD\uDCFF-\uDD3D\uDD49-\uDD4E\uDD50-\uDD67\uDD6F\uDD70\uDD73-\uDD7A\uDD87\uDD8A-\uDD8D\uDD90\uDD95\uDD96\uDDA4\uDDA5\uDDA8\uDDB1\uDDB2\uDDBC\uDDC2-\uDDC4\uDDD1-\uDDD3\uDDDC-\uDDDE\uDDE1\uDDE3\uDDE8\uDDEF\uDDF3\uDDFA-\uDE4F\uDE80-\uDEC5\uDECB-\uDED2\uDEE0-\uDEE5\uDEE9\uDEEB\uDEEC\uDEF0\uDEF3-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])\uFE0F/g);

        function isEmoji(str) {
            if (str.match(emojiRE)) return true;
            return false;
        }

        function displayObscuredTerms(obscuredTerms, data, term, termInfo, div='#'+divName+'-'+'overlapped-terms') {
            d3.select('#'+divName+'-'+'overlapped-terms')
                .selectAll('div')
                .remove();
            d3.select(div)
                .selectAll('div')
                .remove();
            if (obscuredTerms.length > 1) {
                var obscuredDiv = d3.select(div)
                    .append('div')
                    .attr("class", "obscured")
                    .style('align', 'center')
                    .style('text-align', 'center')
                    .html("<b>\"" + term + "\" obstructs</b>: ");
                obscuredTerms.map(
                    function (term, i) {
                        makeWordInteractive(
                            data,
                            svg,
                            obscuredDiv.append("text").text(term),
                            term,
                            data.filter(t=>t.term == term)[0],//termInfo
                            false
                        );
                        if (i < obscuredTerms.length - 1) {
                            obscuredDiv.append("text").text(", ");
                        }
                    }
                )
            }
        }

        function displayTermContexts(data, termInfo, jump=true) {
            var contexts = termInfo.contexts;
            var info = termInfo.info;
            if (contexts[0].length + contexts[1].length + contexts[2].length + contexts[3].length == 0) {
                return null;
            }
            //!!! Future feature: context words
            //var contextWords = getContextWordSFS(info.term);
            //var contextWords = getContextWordLORIPs(info.term);
            //var categoryNames = [fullData.info.category_name,
            //    fullData.info.not_category_name];
            var catInternalName = fullData.info.category_internal_name;


            function addSnippets(contexts, divId) {
                var meta = contexts.meta ? contexts.meta : '&nbsp;';
                d3.select(divId)
                    .append("div")
                    .attr('class', 'snippet_meta docLabel' + contexts.docLabel)
                    .html(meta);
                contexts.snippets.forEach(function (snippet) {
                    d3.select(divId)
                        .append("div")
                        .attr('class', 'snippet docLabel' + contexts.docLabel)
                        .html(snippet);
                })
            }
            if (unifiedContexts) {
               divId = '#'+divName+'-'+'cat';
                var docLabelCounts = fullData.docs.labels.reduce(
                    function(map, label) {map[label] = (map[label]||0)+1; return map;},
                    Object.create(null)
                );
               var numMatches = Object.create(null);
               var temp = d3.select(divId).selectAll("div").remove();
               var allContexts = contexts[0].concat(contexts[1]).concat(contexts[2]).concat(contexts[3]);
                allContexts.forEach(function (singleDoc) {
                    numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel]||0) + 1;
                });

               /*contexts.forEach(function(context) {
                    context.forEach(function (singleDoc) {
                        numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel]||0) + 1;
                        addSnippets(singleDoc, divId);
                    });
                });*/
                var docLabelCountsSorted = Object.keys(docLabelCounts).map(key => (
                    {"label": fullData.docs.categories[key],
                     "labelNum": key,
                     "matches": numMatches[key]||0,
                     "overall": docLabelCounts[key],
                     'percent': (numMatches[key]||0)*100./docLabelCounts[key]}))
                    .sort(function(a,b) {return b.percent-a.percent});
                console.log("docLabelCountsSorted")
                console.log(docLabelCountsSorted);
                console.log(numMatches)
                d3.select('#'+divName+'-'+'categoryinfo').selectAll("div").remove();
                if(showCategoryHeadings) {
                    d3.select('#'+divName+'-'+'categoryinfo').attr('display', 'inline')
                }
                function getCategoryStatsHTML(counts) {
                    return counts.matches + " document"
                        + (counts.matches == 1 ? "" : "s") + " out of " + counts.overall +': '
                        +counts['percent'].toFixed(2) + '%';
                }
                
                function getCategoryInlineHeadingHTML(counts) {
                    return '<a name="'+divName+'-category'
                        + counts.labelNum + '"></a>' 
                        + counts.label + ": <span class=topic_preview>"
                        + getCategoryStatsHTML(counts)
                        + "</span>";
                }

                docLabelCountsSorted.forEach(function(counts) {
                    var htmlToAdd = "<b>"+counts.label + "</b>: " +  getCategoryStatsHTML(counts);
                    console.log(htmlToAdd);
                    if(showCategoryHeadings) {
                        d3.select('#'+divName+'-'+'categoryinfo')
                            .attr('display', 'inline')
                            .append('div')
                            .html(htmlToAdd)
                            .on("click", function() {window.location.hash = '#'+divName+'-'+'category' + counts.labelNum});
                    }
                    if(counts.matches > 0) {
                        d3.select(divId)
                            .append("div")
                            .attr('class', 'text_header')
                            .html(getCategoryInlineHeadingHTML(counts));
                        allContexts
                            .filter(singleDoc => singleDoc.docLabel == counts.labelNum)
                            .forEach(function (singleDoc) {
                                addSnippets(singleDoc, divId);
                            });
                    }
                })

            
            } else {
                var contextColumns = [
                    fullData.info.category_internal_name,
                    fullData.info.not_category_name
                ];
                if (showNeutral) {
                    if ('neutral_category_name' in fullData.info) {
                        contextColumns.push(fullData.info.neutral_category_name)
                    } else {
                        contextColumns.push("Neutral")
                    }
                    if (showExtra) {
                        if ('extra_category_name' in fullData.info) {
                            contextColumns.push(fullData.info.extra_category_name)
                        } else {
                            contextColumns.push("Extra")
                        }
                    }

                }
                contextColumns.map(
                    function (catName, catIndex) {
                        if (max_snippets != null) {
                            var contextsToDisplay = contexts[catIndex].slice(0, max_snippets);
                        }
                        console.log("CATCAT")
                        console.log(catName, catIndex)
                        //var divId = catName == catInternalName ? '#cat' : '#notcat';
                        var divId = null
                        if (catName == fullData.info.category_internal_name) {
                            divId = '#'+divName+'-'+'cat'
                        } else if (fullData.info.not_category_name == catName) {
                            divId = '#'+divName+'-'+'notcat'
                        } else if (fullData.info.neutral_category_name == catName) {
                            divId = '#'+divName+'-'+'neut';
                        } else if (fullData.info.extra_category_name == catName) {
                            divId = '#'+divName+'-'+'extra'
                        } else {
                            return;
                        }
                        console.log('divid');
                        console.log(divId)

                        var temp = d3.select(divId).selectAll("div").remove();
                        contexts[catIndex].forEach(function (context) {
                            addSnippets(context, divId);
                        });
                    }
                );
            }

            var obscuredTerms = getObscuredTerms(data, termInfo.info);
            displayObscuredTerms(obscuredTerms, data, info.term, info, '#'+divName+'-'+'overlapped-terms-clicked');

            d3.select('#'+divName+'-'+'termstats')
                .selectAll("div")
                .remove();
            var termHtml = 'Term: <b>' + info.term + '</b>';
            if ('metalists' in fullData && info.term in fullData.metalists) {
                termHtml = 'Topic: <b>' + info.term + '</b>';
            }
            d3.select('#'+divName+'-'+'termstats')
                .append('div')
                .attr("class", "snippet_header")
                .html(termHtml);
            if ('metalists' in fullData && info.term in fullData.metalists) {
                d3.select('#'+divName+'-'+'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Topic preview</b>: "
                        + fullData.metalists[info.term]
                            .slice(0, topic_model_preview_size)
                            .reduce(function (x, y) {
                                return x + ', ' + y
                            }));
            }
            if ('metadescriptions' in fullData && info.term in fullData.metadescriptions) {
                d3.select('#'+divName+'-'+'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Description</b>: " + fullData.metadescriptions[info.term]);
            }
            var message = '';
            var cat_name = fullData.info.category_name;
            var ncat_name = fullData.info.not_category_name;


            var numCatDocs = fullData.docs.labels
                .map(function (x) {
                    return (x == fullData.docs.categories.indexOf(
                        fullData.info.category_internal_name)) + 0
                })
                .reduce(function (a, b) {
                    return a + b;
                })

            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });


            var numNCatDocs = fullData.docs.labels
                .map(function (x) {
                    return notCategoryNumList.indexOf(x) > -1
                })
                .reduce(function (a, b) {
                    return a + b;
                });

            function getFrequencyDescription(name, count25k, count, ndocs) {
                var desc = name + ' frequency: <div class=text_subhead>' + count25k
                    + ' per 25,000 terms</div><div class=text_subhead>' + Math.round(ndocs)
                    + ' per 1,000 docs</div>';
                if (count == 0) {
                    desc += '<u>Not found in any ' + name + ' documents.</u>';
                } else {
                    desc += '<u>Some of the ' + count + ' mentions:</u>';
                }
                /*
                desc += '<br><b>Discriminative:</b> ';

                desc += contextWords
                    .slice(cat_name === name ? 0 : contextWords.length - 3,
                        cat_name === name ? 3 : contextWords.length)
                    .filter(function (x) {
                        //return Math.abs(x[5]) > 1.96;
                        return true;
                    })
                    .map(function (x) {return x.join(', ')}).join('<br>');
                */
                return desc;
            }

            if (!unifiedContexts) {
                console.log("NOT UNIFIED CONTEXTS")
                d3.select('#'+divName+'-'+'cathead')
                    .style('fill', color(1))
                    .html(
                        getFrequencyDescription(cat_name,
                            info.cat25k,
                            info.cat,
                            termInfo.contexts[0].length * 1000 / numCatDocs
                        )
                    );
                d3.select('#'+divName+'-'+'notcathead')
                    .style('fill', color(0))
                    .html(
                        getFrequencyDescription(ncat_name,
                            info.ncat25k,
                            info.ncat,
                            termInfo.contexts[1].length * 1000 / numNCatDocs)
                    );
                console.log("TermINfo")
                console.log(termInfo);
                console.log(info)
                if (showNeutral) {
                    console.log("NEUTRAL")

                    var numList = fullData.docs.categories.map(function (x, i) {
                        if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                            return i;
                        } else {
                            return -1;
                        }
                    }).filter(function (x) {
                        return x > -1
                    });

                    var numDocs = fullData.docs.labels
                        .map(function (x) {
                            return numList.indexOf(x) > -1
                        })
                        .reduce(function (a, b) {
                            return a + b;
                        });

                    d3.select("#" + divName + "-neuthead")
                        .style('fill', color(0))
                        .html(
                            getFrequencyDescription(fullData.info.neutral_category_name,
                                info.neut25k,
                                info.neut,
                                termInfo.contexts[2].length * 1000 / numDocs)
                        );

                    if (showExtra) {
                        console.log("EXTRA")
                        var numList = fullData.docs.categories.map(function (x, i) {
                            if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                                return i;
                            } else {
                                return -1;
                            }
                        }).filter(function (x) {
                            return x > -1
                        });

                        var numDocs = fullData.docs.labels
                            .map(function (x) {
                                return numList.indexOf(x) > -1
                            })
                            .reduce(function (a, b) {
                                return a + b;
                            });

                        d3.select("#" + divName + "-extrahead")
                            .style('fill', color(0))
                            .html(
                                getFrequencyDescription(fullData.info.extra_category_name,
                                    info.extra25k,
                                    info.extra,
                                    termInfo.contexts[3].length * 1000 / numDocs)
                            );

                    }
                }
            } else {
                // extra unified context code goes here
            }
            if (jump) {
                if (window.location.hash == '#'+divName+'-'+'snippets') {
                    window.location.hash = '#'+divName+'-'+'snippetsalt';
                } else {
                    window.location.hash = '#'+divName+'-'+'snippets';
                }
            }
        }

        function searchInText(d) {
            function stripNonWordChars(term) {
                //d.term.replace(" ", "[^\\w]+")
            }

            function removeUnderScoreJoin(term) {
                /*
                '_ _asjdklf_jaksdlf_jaksdfl skld_Jjskld asdfjkl_sjkdlf'
                  ->
                "_ _asjdklf jaksdlf jaksdfl skld Jjskld asdfjkl_sjkdlf"
                 */
                return term.replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3");
            }

            function buildMatcher(term) {

                var boundary = '\\b';
                var wordSep = "[^\\w]+";
                if (asianMode) {
                    boundary = '( |$|^)';
                    wordSep = ' ';
                }
                if (isEmoji(term)) {
                    boundary = '';
                    wordSep = '';
                }
                var termToRegex = term;
                ['[', ']', '(', ')', '{', '}', '^', '$', '.', '|', '?', "'", '"',
                    '*', '+', '-', '=', '~', '`', '{', '#'].forEach(function (a) {
                    termToRegex = termToRegex.replace(a, '\\\\' + a)
                });
                var regexp = new RegExp(boundary + '('
                    + removeUnderScoreJoin(
                        termToRegex.replace(' ', wordSep, 'gim')
                    )
                    + ')' + boundary, 'gim');
                try {
                    regexp.exec('X');
                } catch (err) {
                    console.log("Can't search " + term);
                    console.log(err);
                    return null;
                }
                return regexp;
            }

            var matches = [[], [], [], []];
            var pattern = buildMatcher(d.term);
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            console.log('extraCategoryNumList')
            console.log(extraCategoryNumList);
            console.log("categoryNum");
            console.log(categoryNum);
            console.log("categoryNum");
            if (pattern !== null) {
                for (var i in fullData.docs.texts) {
                    //var numericLabel = 1 * (fullData.docs.categories[fullData.docs.labels[i]] != fullData.info.category_internal_name);

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }

                    var text = removeUnderScoreJoin(fullData.docs.texts[i]);
                    //var pattern = new RegExp("\\b(" + stripNonWordChars(d.term) + ")\\b", "gim");
                    var match;
                    var sentenceOffsets = null;
                    var lastSentenceStart = null;
                    var matchFound = false;
                    var curMatch = {'id': i, 'snippets': [], 'docLabel': docLabel};
                    if (fullData.docs.meta) {
                        curMatch['meta'] = fullData.docs.meta[i];
                    }
                    while ((match = pattern.exec(text)) != null) {
                        if (sentenceOffsets == null) {
                            sentenceOffsets = getSentenceBoundaries(text);
                        }
                        var foundSnippet = getMatchingSnippet(text, sentenceOffsets,
                            match.index, pattern.lastIndex);
                        if (foundSnippet.sentenceStart == lastSentenceStart) continue; // ensure we don't duplicate sentences
                        lastSentenceStart = foundSnippet.sentenceStart;
                        curMatch.snippets.push(foundSnippet.snippet);
                        matchFound = true;
                    }
                    if (matchFound) {
                        if (useFullDoc) {
                            curMatch.snippets = [
                                text
                                    .replace(/\n$/g, '\n\n')
                                    .replace(
                                        //new RegExp("\\b(" + d.term.replace(" ", "[^\\w]+") + ")\\b",
                                        //    'gim'),
                                        pattern,
                                        '<b>$&</b>')
                            ];
                        }
                        matches[numericLabel].push(curMatch);
                    }
                }
            }
            var toRet = {'contexts': matches, 'info': d, 'docLabel': docLabel};
            return toRet;
        }

        function getDefaultTooltipContent(d) {
            var message = d.term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            message += '<br/>score: ' + d.os.toFixed(5);
            return message;
        }

        function getDefaultTooltipContentWithoutScore(d) {
            var message = d.term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            return message;
        }

        function getObscuredTerms(data, d) {
            //data = fullData['data']
            var matches = (data.filter(function (term) {
                    return term.x === d.x && term.y === d.y && (term.display === undefined || term.display === true);
                }).map(function (term) {
                    return term.term
                }).sort()
            );
            return matches;
        }

        function showTooltip(data, d, pageX, pageY, showObscured=true) {
            deselectLastCircle();

            var obscuredTerms = getObscuredTerms(data, d);
            var message = '';
            console.log("!!!!! " + obscuredTerms.length)
            console.log(showObscured)
            if (obscuredTerms.length > 1 && showObscured)
                displayObscuredTerms(obscuredTerms, data, d.term, d);
            if (getTooltipContent !== null) {
                message += getTooltipContent(d);
            } else {
                if (sortByDist) {
                    message += getDefaultTooltipContentWithoutScore(d);
                } else {
                    message += getDefaultTooltipContent(d);
                }
            }
            pageX -= (svg.node().getBoundingClientRect().left) - origSVGLeft;
            pageY -= (svg.node().getBoundingClientRect().top) - origSVGTop;
            tooltip.transition()
                .duration(0)
                .style("opacity", 1)
                .style("z-index", 10000000);
            tooltip.html(message)
                .style("left", (pageX - 40) + "px")
                .style("top", (pageY - 85 > 0 ? pageY - 85 : 0) + "px");
            tooltip.on('click', function () {
                tooltip.transition()
                    .style('opacity', 0)
            }).on('mouseout', function () {
                tooltip.transition().style('opacity', 0)
            });
        }

        handleSearch = function (event) {
            deselectLastCircle();
            var searchTerm = document
                .getElementById(this.divName + "-searchTerm")
                .value
                .toLowerCase()
                .replace("'", " '")
                .trim();
            if(this.termDict[searchTerm] !== undefined) {
                showToolTipForTerm(this.data, this.svg, searchTerm, this.termDict[searchTerm], true);
            }
            if (this.termDict[searchTerm] != null) {
                var runDisplayTermContexts = true;
                if(alternativeTermFunc != null) {
                    runDisplayTermContexts = this.alternativeTermFunc(this.termDict[searchTerm]);
                }
                if(runDisplayTermContexts) {
                    displayTermContexts(this.data, this.gatherTermContexts(this.termDict[searchTerm]), false);
                }
            }
            return false;
        };

        function showToolTipForTerm(data, mysvg, searchTerm, searchTermInfo, showObscured=true) {
            //var searchTermInfo = termDict[searchTerm];
            console.log("showing tool tip")
            console.log(searchTerm)
            console.log(searchTermInfo)
            if (searchTermInfo === undefined) {
                console.log("can't show")
                d3.select("#" + divName + "-alertMessage")
                    .text(searchTerm + " didn't make it into the visualization.");
            } else {
                d3.select("#" + divName + "-alertMessage").text("");
                var circle = mysvg; 
                console.log("mysvg"); console.log(mysvg)
                if(circle.tagName !== "circle") { // need to clean this thing up
                    circle = mysvg._groups[0][searchTermInfo.ci];
                    if(circle === undefined || circle.tagName != 'circle') {
                        console.log("circle0")
                        if(mysvg._groups[0].children !== undefined) {
                            circle = mysvg._groups[0].children[searchTermInfo.ci];
                        }
                    }
                    if(circle === undefined || circle.tagName != 'circle') {
                        console.log("circle1"); 
                        if(mysvg._groups[0][0].children !== undefined) {
                            circle = Array.prototype.filter.call(
                                mysvg._groups[0][0].children, 
                                x=> (x.tagName == "circle" && x.__data__['term'] == searchTermInfo.term)
                            )[0];
                        }
                        console.log(circle)
                    }
                    if((circle === undefined || circle.tagName != 'circle') && mysvg._groups[0][0].children !== undefined) {
                        console.log("circle2"); 
                        console.log(mysvg._groups[0][0])
                        console.log(mysvg._groups[0][0].children)
                        console.log(searchTermInfo.ci);
                        circle = mysvg._groups[0][0].children[searchTermInfo.ci];
                        console.log(circle)
                    }
                }
                if(circle) {
                    var mySVGMatrix = circle.getScreenCTM().translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;
                    circle.style["stroke"] = "black";
                    //var circlePos = circle.position();
                    //var el = circle.node()
                    //showTooltip(searchTermInfo, pageX, pageY, circle.cx.baseVal.value, circle.cx.baseVal.value);
                    showTooltip(
                        data,
                        searchTermInfo,
                        pageX,
                        pageY,
                        showObscured
                    );

                    lastCircleSelected = circle;
                }

            }
        };


        function makeWordInteractive(data, svg, domObj, term, termInfo, showObscured=true) {
            return domObj
                .on("mouseover", function (d) {
                    console.log("mouseover" )
                    console.log(term)
                    console.log(termInfo)
                    showToolTipForTerm(data, svg, term, termInfo, showObscured);
                    d3.select(this).style("stroke", "black");
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    if (showObscured) {
                        d3.select('#'+divName+'-'+'overlapped-terms')
                            .selectAll('div')
                            .remove();
                    }
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if(alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(termInfo);
                    }
                    if(runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(termInfo));
                    }
                });
        }

        function processData(fullData) {
            
            modelInfo = fullData['info'];
            /*
             categoryTermList.data(modelInfo['category_terms'])
             .enter()
             .append("li")
             .text(function(d) {return d;});
             */
            var data = fullData['data'];
            termDict = Object();
            data.forEach(function (x, i) {
                termDict[x.term] = x;
                //!!!
                //termDict[x.term].i = i;
            });

            var padding = 0;
            if (showAxes) {
                padding = 0.1;
            }

            // Scale the range of the data.  Add some space on either end.
            x.domain([-1 * padding, d3.max(data, function (d) {
                return d.x;
            }) + padding]);
            y.domain([-1 * padding, d3.max(data, function (d) {
                return d.y;
            }) + padding]);

            /*
             data.sort(function (a, b) {
             return Math.abs(b.os) - Math.abs(a.os)
             });
             */


            //var rangeTree = null; // keep boxes of all points and labels here
            var rectHolder = new RectangleHolder();
            // Add the scatterplot
            data.forEach(function(d,i) {d.ci = i});
            //console.log('XXXXX'); console.log(data)
            var mysvg = svg
                .selectAll("dot")
                .data(data.filter(d=>d.display === undefined || d.display === true))
                //.filter(function (d) {return d.display === undefined || d.display === true})
                .enter()
                .append("circle")
                .attr("r", function (d) {
                    if (pValueColors && d.p) {
                        return (d.p >= 1 - minPVal || d.p <= minPVal) ? 2 : 1.75;
                    }
                    return 2;
                })
                .attr("cx", function (d) {
                    return x(d.x);
                })
                .attr("cy", function (d) {
                    return y(d.y);
                })
                .style("fill", function (d) {
                    //.attr("fill", function (d) {
                    if (colorFunc) {
                        return colorFunc(d);
                    } else if (greyZeroScores && d.os == 0) {
                        return d3.rgb(230, 230, 230);
                    } else if (pValueColors && d.p) {
                        if (d.p >= 1 - minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else if (d.p <= minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else {
                            return interpolateLightGreys(d.s);
                        }
                    } else {
                        if(d.term == "psychological") {
                            console.log("COLS " + d.s + " " + color(d.s) + " " + d.term)
                            console.log(d)
                            console.log(color)
                        }
                        return color(d.s);
                    }
                })
                .on("mouseover", function (d) {
                    /*var mySVGMatrix = circle.getScreenCTM()n
                        .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;*/

                    /*showTooltip(
                        d,
                        d3.event.pageX,
                        d3.event.pageY
                    );*/
                    console.log("point MOUSOEVER")
                    console.log(d)
                    showToolTipForTerm(data, this, d.term, d, true);
                    d3.select(this).style("stroke", "black");
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if(alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(d);
                    }
                    if(runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(d));
                    }
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    d3.select('#'+divName+'-'+'overlapped-terms')
                        .selectAll('div')
                        .remove();
                })
            
            
            coords = Object();

            var pointStore = [];
            var pointRects = [];

            function censorPoints(datum, getX, getY) {
                var term = datum.term;
                var curLabel = svg.append("text")
                    .attr("x", x(getX(datum)))
                    .attr("y", y(getY(datum)) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, '~~' + term);
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }
            
            function censorCircle(xCoord, yCoord) {
                var curLabel = svg.append("text")
                    .attr("x", x(xCoord))
                    .attr("y", y(yCoord) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }

            function labelPointsIfPossible(datum, myX, myY) {
                var term = datum.term;
                if(term == "the") 
                    console.log("TERM " + term + " " + myX + " " + myY)
                //console.log('xxx'); console.log(term); console.log(term.display !== undefined && term.display === false)
                //if(term.display !== undefined && term.display === false) {
                //    return false;
                //}
                var configs = [
                    {'anchor': 'end', 'xoff': -5, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'end', 'xoff': -5, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                    
                    {'anchor': 'end', 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'end', 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'end', 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'end', 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                    
                    {'anchor': 'start', 'xoff': 3, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 3, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 5, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 5, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                    {'anchor': 'start', 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                ];
                if (centerLabelsOverPoints) {
                    configs = [{'anchor': 'middle', 'xoff': 0, 'yoff': 0, 'alignment-baseline': 'middle'}];
                }
                var matchedElement = null;
                for (var configI in configs) {
                    var config = configs[configI];
                    var curLabel = svg.append("text")
                    //.attr("x", x(data[i].x) + config['xoff'])
                    //.attr("y", y(data[i].y) + config['yoff'])
                        .attr("x", x(myX) + config['xoff'])
                        .attr("y", y(myY) + config['yoff'])
                        .attr('class', 'label')
                        .attr('class', 'pointlabel')
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("text-anchor", config['anchor'])
                        .attr("alignment-baseline", config['alignment'])
                        .text(term);
                    var bbox = curLabel.node().getBBox();
                    var borderToRemove = .25;
                    if (doCensorPoints) {
                        var borderToRemove = .5;
                    }

                    var x1 = bbox.x + borderToRemove,
                        y1 = bbox.y + borderToRemove,
                        x2 = bbox.x + bbox.width - borderToRemove,
                        y2 = bbox.y + bbox.height - borderToRemove;
                    //matchedElement = searchRangeTree(rangeTree, x1, y1, x2, y2);
                    var matchedElement = false;
                    rectHolder.findMatchingRectangles(x1, y1, x2, y2, function (elem) {
                        matchedElement = true;
                        return false;
                    });
                    if (matchedElement) {
                        curLabel.remove();
                    } else {
                        curLabel = makeWordInteractive(data, svg, curLabel, term, datum);
                        break;
                    }
                }

                if (!matchedElement) {
                    coords[term] = [x1, y1, x2, y2];
                    //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, term);
                    var labelRect = new Rectangle(x1, y1, x2, y2)
                    rectHolder.add(labelRect);
                    pointStore.push([x1, y1]);
                    pointStore.push([x2, y1]);
                    pointStore.push([x1, y2]);
                    pointStore.push([x2, y2]);
                    return {label: curLabel, rect: labelRect};
                } else {
                    //curLabel.remove();
                    return false;
                }

            }

            var radius = 2;

            function euclideanDistanceSort(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (Math.min(aCatDist, aNotCatDist) > Math.min(bCatDist, bNotCatDist)) * 2 - 1;
            }

            function euclideanDistanceSortForCategory(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                return (aCatDist > bCatDist) * 2 - 1;
            }

            function euclideanDistanceSortForNotCategory(a, b) {
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (aNotCatDist > bNotCatDist) * 2 - 1;
            }

            function scoreSort(a, b) {
                return a.s - b.s;
            }

            function scoreSortReverse(a, b) {
                return b.s - a.s;
            }

            function backgroundScoreSort(a, b) {
                if (b.bg === a.bg)
                    return (b.cat + b.ncat) - (a.cat + a.ncat);
                return b.bg - a.bg;
            }

            function arePointsPredictiveOfDifferentCategories(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                var aGood = aCatDist < aNotCatDist;
                var bGood = bCatDist < bNotCatDist;
                return {aGood: aGood, bGood: bGood};
            }

            function scoreSortForCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return -1;
                    if (!aGood && bGood) return 1;
                }
                return b.s - a.s;
            }

            function scoreSortForNotCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return 1;
                    if (!aGood && bGood) return -1;
                }
                if (reverseSortScoresForNotCategory)
                    return a.s - b.s;
                else
                    return b.s - a.s;
            }

            var sortedData = data.map(x=>x).sort(sortByDist ? euclideanDistanceSort : scoreSort);
            if (doCensorPoints) {
                for (var i in data) {
                    var d = sortedData[i];
                    censorPoints(
                        d,
                        function (d) {
                            return d.x
                        },
                        function (d) {
                            return d.y
                        }
                    );
                }
            }


            function registerFigureBBox(curLabel) {
                var bbox = curLabel.node().getBBox();
                var borderToRemove = 1.5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                rectHolder.add(new Rectangle(x1, y1, x2, y2));
                //return insertRangeTree(rangeTree, x1, y1, x2, y2, '~~_other_');
            }
            function drawXLabel(svg, labelText) {
                    return svg.append("text")
                    .attr("class", "x label")
                    .attr("text-anchor", "end")
                    .attr("x", width)
                    .attr("y", height - 6)
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '10px')
                    .text(labelText);
            }
            function drawYLabel(svg, labelText) {
                    return svg.append("text")
                        .attr("class", "y label")
                        .attr("text-anchor", "end")
                        .attr("y", 6)
                        .attr("dy", ".75em")
                        .attr("transform", "rotate(-90)")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .text(labelText);
                        registerFigureBBox(yLabel);
                }

            d3.selection.prototype.moveToBack = function () {
                return this.each(function () {
                    var firstChild = this.parentNode.firstChild;
                    if (firstChild) {
                        this.parentNode.insertBefore(this, firstChild);
                    }
                });
            };

            if (verticalLines) {
                for (i in verticalLines) {
                    svg.append("g")
                        .attr("transform", "translate(" + x(verticalLines) + ", 1)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#dddddd")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            if (showAxes) {

                var myXAxis = svg.append("g")
                    .attr("class", "x axis")
                    .attr("transform", "translate(0," + height + ")")
                    .call(xAxis);

                //rangeTree = registerFigureBBox(myXAxis);
                

                var xLabel = drawXLabel(svg, getLabelText('x'));

                //console.log('xLabel');
                //console.log(xLabel);

                //rangeTree = registerFigureBBox(xLabel);
                // Add the Y Axis

                if (!yAxisValues) {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr("dx", "30px")
                        .attr("dy", "-13px")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("transform", "rotate(-90)");
                } else {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px');
                }
                registerFigureBBox(myYAxis);

                function getLabelText(axis) {
                    if (axis == 'y') {
                        if (yLabelText == null)
                            return modelInfo['category_name'] + " Frequency";
                        else
                            return yLabelText;
                    } else {
                        if (xLabelText == null)
                            return modelInfo['not_category_name'] + " Frequency";
                        else
                            return xLabelText;
                    }
                }
                var yLabel = drawYLabel(svg, getLabelText('y'))
                
            
            } else {
                horizontal_line_y_position_translated = 0.5;
                if(horizontal_line_y_position !== null) {
                   var loOy = null, hiOy = null, loY = null, hiY = null;
                   for(i in fullData.data) {
                        var curOy = fullData.data[i].oy;
                        if(curOy < horizontal_line_y_position && (curOy > loOy || loOy === null)) {
                              loOy = curOy;
                              loY = fullData.data[i].y
                        }
                        if(curOy > horizontal_line_y_position && (curOy < hiOy || hiOy === null)) {
                              hiOy = curOy;
                              hiY = fullData.data[i].y
                        }
                   }
                   horizontal_line_y_position_translated = loY + (hiY - loY)/2.
                   if(loY === null) {
                        horizontal_line_y_position_translated = 0;
                   }
                }
                if(vertical_line_x_position === null) {
                    vertical_line_x_position_translated = 0.5;
                } else {
                    if(vertical_line_x_position !== null) {
                       var loOx = null, hiOx = null, loX = null, hiX = null;
                       for(i in fullData.data) {
                            var curOx = fullData.data[i].ox;
                            if(curOx < vertical_line_x_position && (curOx > loOx || loOx === null)) {
                                  loOx = curOx;
                                  loX = fullData.data[i].x;
                            }
                            if(curOx > vertical_line_x_position && (curOx < hiOx || hiOx === null)) {
                                  hiOx = curOx;
                                  hiX = fullData.data[i].x
                            }
                       }
                       vertical_line_x_position_translated = loX + (hiX - loX)/2.
                       if(loX === null) {
                            vertical_line_x_position_translated = 0;
                       }
                    }
                }
                if(showCrossAxes) {
                    var x_line = svg.append("g")
                        .attr("transform", "translate(0, " + y(horizontal_line_y_position_translated) + ")")
                        .append("line")
                        .attr("x2", width)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                    var y_line = svg.append("g")
                        .attr("transform", "translate(" + x(vertical_line_x_position_translated) + ", 0)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            function showWordList(word, termDataList) {
                var maxWidth = word.node().getBBox().width;
                var wordObjList = [];
                for (var i in termDataList) {
                    var curTerm = termDataList[i].term;
                    word = (function (word, curTerm) {
                        var curWordPrinted = svg.append("text")
                                .attr("text-anchor", "start")
                                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                                .attr('font-size', '12px')
                                .attr("x", word.node().getBBox().x)
                                .attr("y", word.node().getBBox().y
                                    + 2 * word.node().getBBox().height)
                                .text(curTerm);
                        wordObjList.push(curWordPrinted)
                        return makeWordInteractive(
                            termDataList, //data,
                            svg, 
                            curWordPrinted,
                            curTerm,
                            termDataList[i]);
                    })(word, curTerm);
                    if (word.node().getBBox().width > maxWidth)
                        maxWidth = word.node().getBBox().width;
                    registerFigureBBox(word);
                }
                return {
                    'word': word,
                    'maxWidth': maxWidth,
                    'wordObjList':wordObjList
                };
            }

            function pickEuclideanDistanceSortAlgo(category) {
                if (category == true) return euclideanDistanceSortForCategory;
                return euclideanDistanceSortForNotCategory;
            }

            function pickScoreSortAlgo(category) {
                console.log("PICK SCORE ALGO")
                console.log(category)
                if (category == true) {
                    return scoreSortForCategory;
                } else {
                    return scoreSortForNotCategory;
                }
            }

            function pickTermSortingAlgorithm(category) {
                if (sortByDist) return pickEuclideanDistanceSortAlgo(category);
                return pickScoreSortAlgo(category);
            }

            function showAssociatedWordList(data, word, header, isAssociatedToCategory,  length=14) {
                var sortedData = null;
                var sortingAlgo = pickTermSortingAlgorithm(isAssociatedToCategory);
                sortedData = data.filter(term => (term.display === undefined || term.display === true)).sort(sortingAlgo);
                if (wordVecMaxPValue) {
                    function signifTest(x) {
                        if (isAssociatedToCategory)
                            return x.p >= 1 - minPVal;
                        return x.p <= minPVal;
                    }

                    sortedData = sortedData.filter(signifTest)
                }
                return showWordList(word, sortedData.slice(0, length));

            }
            var characteristicXOffset = width;
            function showCatHeader(startingOffset, catName, registerFigureBBox) {
                var catHeader =  svg.append("text")
                .attr("text-anchor", "start")
                .attr("x", startingOffset //width
                     )
                .attr("dy", "6px")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '12px')
                .attr('font-weight', 'bolder')
                .attr('font-decoration', 'underline')
                .text(catName
                      //"Top " + fullData['info']['category_name']
                     );
                registerFigureBBox(catHeader);
                return catHeader;
            }

            function showNotCatHeader(startingOffset, word, notCatName) {
                console.log("showNotCatHeader")
                console.log(word)
                console.log(word.node().getBBox().y - word.node().getBBox().height)
                console.log(word.node().getBBox().y + word.node().getBBox().height)
                return svg.append("text")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '12px')
                .attr('font-weight', 'bolder')
                .attr('font-decoration', 'underline')
                .attr("text-anchor", "start")
                .attr("x", startingOffset)
                .attr("y", word.node().getBBox().y + 3 * word.node().getBBox().height)
                .text(notCatName);
            }

            function showTopTermsPane(data,
                                       registerFigureBBox, 
                                       showAssociatedWordList,
                                       catName,
                                       notCatName,
                                       startingOffset) {
                data = data.filter(term => (term.display === undefined || term.display === true));
                //var catHeader = showCatHeader(startingOffset, catName, registerFigureBBox);
                var catHeader = svg.append("text")
                .attr("text-anchor", "start")
                .attr("x", startingOffset)
                .attr("dy", "6px")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '12px')
                .attr('font-weight', 'bolder')
                .attr('font-decoration', 'underline')
                .text(catName
                      //"Top " + fullData['info']['category_name']
                     );
                registerFigureBBox(catHeader);
                var word = catHeader;
                var wordListData = showAssociatedWordList(data, word, catHeader, true);
                word = wordListData.word;
                var maxWidth = wordListData.maxWidth;

                var notCatHeader = showNotCatHeader(startingOffset, word, notCatName);
                word = notCatHeader;
                characteristicXOffset = catHeader.node().getBBox().x + maxWidth + 10;

                var notWordListData = showAssociatedWordList(data, word, notCatHeader, false);
                word = wordListData.word;
                if (wordListData.maxWidth > maxWidth) {
                    maxWidth = wordListData.maxWidth;
                }
                return {wordListData, notWordListData,
                        word, maxWidth, characteristicXOffset, startingOffset,
                        catHeader, notCatHeader, registerFigureBBox};
            }

            var payload = Object();
            if (showTopTerms) {
                payload.topTermsPane = showTopTermsPane(
                    data,
                    registerFigureBBox, 
                    showAssociatedWordList,
                    "Top " + fullData['info']['category_name'],
                    "Top " + fullData['info']['not_category_name'],
                    width
                );
                payload.showTopTermsPane = showTopTermsPane;
                payload.showAssociatedWordList = showAssociatedWordList;
                payload.showWordList = showWordList;
                /*var wordListData = topTermsPane.wordListData;
                var word = topTermsPane.word;
                var maxWidth = topTermsPane.maxWidth;
                var catHeader = topTermsPane.catHeader;
                var notCatHeader = topTermsPane.notCatHeader;
                var startingOffset = topTermsPane.startingOffset;*/
                characteristicXOffset = payload.topTermsPane.characteristicXOffset;
            }


            if (!nonTextFeaturesMode && !asianMode && showCharacteristic) {
                var sortMethod = backgroundScoreSort;
                var title = 'Characteristic';
                if (wordVecMaxPValue) {
                    title = 'Most similar';
                    sortMethod = scoreSortReverse;
                } else if (data.reduce(function (a, b) {
                        return a + b.bg
                    }, 0) === 0) {
                    title = 'Most frequent';
                }
                word = svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr("text-anchor", "start")
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("x", characteristicXOffset)
                    .attr("dy", "6px")
                    .text(title);

                var wordListData = showWordList(word, data.filter(term => (term.display === undefined || term.display === true)).sort(sortMethod).slice(0, 30));

                word = wordListData.word;
                maxWidth = wordListData.maxWidth;
                console.log(maxWidth);
                console.log(word.node().getBBox().x + maxWidth);

                svg.attr('width', word.node().getBBox().x + 3 * maxWidth + 10);
            }

            function performPartialLabeling(data, existingLabels, getX, getY) {
                for (i in existingLabels) {
                    rectHolder.remove(existingLabels[i].rect);
                    existingLabels[i].label.remove();
                }
                console.log('labeling 1')
                

                var labeledPoints = [];
                //var filteredData = data.filter(d=>d.display === undefined || d.display === true);
                //for (var i = 0; i < filteredData.length; i++) {
                data.forEach(function(datum, i) {
                    //console.log(datum.i, datum.ci, i)
                    //var label = labelPointsIfPossible(i, getX(filteredData[i]), getY(filteredData[i]));
                    if(datum.display === undefined || datum.display === true) {
                        if(datum.term == "the" || i == 1) {
                            console.log("trying to label datum # " + i + ": " + datum.term)
                            console.log(datum)
                            console.log([getX(datum), getY(datum)])
                        }
                        var label = labelPointsIfPossible(datum, getX(datum), getY(datum));
                        if (label !== false) {
                            //console.log("labeled")
                            labeledPoints.push(label)
                        }
                    }
                    //if (labelPointsIfPossible(i), true) numPointsLabeled++;
                })
                return labeledPoints;
            }

            //var labeledPoints = performPartialLabeling();
            var labeledPoints = [];
            labeledPoints = performPartialLabeling(data,
                                                   labeledPoints,
                                                   function (d) {return d.x},
                                                   function (d) {return d.y});



            /*
            // pointset has to be sorted by X
            function convex(pointset) {
                function _cross(o, a, b) {
                    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0]);
                }

                function _upperTangent(pointset) {
                    var lower = [];
                    for (var l = 0; l < pointset.length; l++) {
                        while (lower.length >= 2 && (_cross(lower[lower.length - 2], lower[lower.length - 1], pointset[l]) <= 0)) {
                            lower.pop();
                        }
                        lower.push(pointset[l]);
                    }
                    lower.pop();
                    return lower;
                }

                function _lowerTangent(pointset) {
                    var reversed = pointset.reverse(),
                        upper = [];
                    for (var u = 0; u < reversed.length; u++) {
                        while (upper.length >= 2 && (_cross(upper[upper.length - 2], upper[upper.length - 1], reversed[u]) <= 0)) {
                            upper.pop();
                        }
                        upper.push(reversed[u]);
                    }
                    upper.pop();
                    return upper;
                }

                var convex,
                    upper = _upperTangent(pointset),
                    lower = _lowerTangent(pointset);
                convex = lower.concat(upper);
                convex.push(pointset[0]);
                return convex;
            }

            console.log("POINTSTORE")
            console.log(pointStore);
            pointStore.sort();
            var convexHull = convex(pointStore);
            var minX = convexHull.sort(function (a,b) {
                return a[0] < b[0] ? -1 : 1;
            })[0][0];
            var minY = convexHull.sort(function (a,b) {
                return a[1] < b[1] ? -1 : 1;
            })[0][0];
            //svg.append("text").text("BLAH BLAH").attr("text-anchor", "middle").attr("cx", x(0)).attr("y", minY);
            console.log("POINTSTORE")
            console.log(pointStore);
            console.log(convexHull);
            for (i in convexHull) {
                var i = parseInt(i);
                if (i + 1 == convexHull.length) {
                    var nextI = 0;
                } else {
                    var nextI = i + 1;
                }
                console.log(i, ',', nextI);
                svg.append("line")
                    .attr("x2", width)
                    .style("stroke", "#cc0000")
                    .style("stroke-width", "1px")
                    .attr("x1", convexHull[i][0])     // x position of the first end of the line
                    .attr("y1", convexHull[i][1])      // y position of the first end of the line
                    .attr("x2", convexHull[nextI][0])     // x position of the second end of the line
                    .attr("y2", convexHull[nextI][1]);    // y position of the second end of the line
            }*/

            function populateCorpusStats() {
                var wordCounts = {};
                var docCounts = {}
                fullData.docs.labels.forEach(function (x, i) {
                    var cnt = (
                        fullData.docs.texts[i]
                            .trim()
                            .replace(/['";:,.?\-!]+/g, '')
                            .match(/\S+/g) || []
                    ).length;
                    var name = null;
                    if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                        name = fullData.info.category_name;
                    } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                        name = fullData.info.not_category_name;
                    } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                        name = fullData.info.neutral_category_name;
                    } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                        name = fullData.info.extra_category_name;
                    }
                    if (name) {
                        wordCounts[name] = wordCounts[name] ? wordCounts[name] + cnt : cnt
                    }
                    //!!!

                });
                fullData.docs.labels.forEach(function (x) {
                    var name = null;
                    if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                        name = fullData.info.category_name;
                    } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                        name = fullData.info.not_category_name;
                    } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                        name = fullData.info.neutral_category_name;
                    } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                        name = fullData.info.extra_category_name;
                    }
                    if (name) {
                        docCounts[name] = docCounts[name] ? docCounts[name] + 1 : 1
                    }
                });
                console.log("docCounts");
                console.log(docCounts)
                var messages = [];
                [fullData.info.category_name,
                 fullData.info.not_category_name,
                 fullData.info.neutral_category_name,
                 fullData.info.extra_category_name].forEach(function (x, i) {
                    if (docCounts[x] > 0) {
                        messages.push('<b>' + x + '</b> document count: '
                            + Number(docCounts[x]).toLocaleString('en')
                            + '; word count: '
                            + Number(wordCounts[x]).toLocaleString('en'));
                    }
                });

                d3.select('#'+divName+'-'+'corpus-stats')
                    .style('width', width + margin.left + margin.right + 200)
                    .append('div')
                    .html(messages.join('<br />'));
            }


            if (fullData.docs) {
                populateCorpusStats();
            }

            if (saveSvgButton) {
                // from https://stackoverflow.com/questions/23218174/how-do-i-save-export-an-svg-file-after-creating-an-svg-with-d3-js-ie-safari-an
                var svgElement = document.getElementById(divName);

                var serializer = new XMLSerializer();
                var source = serializer.serializeToString(svgElement);

                if (!source.match(/^<svg[^>]+xmlns="http\:\/\/www\.w3\.org\/2000\/svg"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns="https://www.w3.org/2000/svg"');
                }
                if (!source.match(/^<svg[^>]+"http\:\/\/www\.w3\.org\/1999\/xlink"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns:xlink="https://www.w3.org/1999/xlink"');
                }

                source = '<?xml version="1.0" standalone="no"?>\r\n' + source;

                var url = "data:image/svg+xml;charset=utf-8," + encodeURIComponent(source);

                var downloadLink = document.createElement("a");
                downloadLink.href = url;
                downloadLink.download = fullData['info']['category_name'] + ".svg";
                downloadLink.innerText = 'Download SVG';
                document.body.appendChild(downloadLink);

            }
            function rerender(xCoords, yCoords, color) {
                labeledPoints.forEach(function (p) {
                    p.label.remove();
                    rectHolder.remove(p.rect);
                });
                pointRects.forEach(function (rect) {
                    rectHolder.remove(rect);
                });
                pointRects = []
                /*
                var circles = d3.select('#' + divName).selectAll('circle')
                    .attr("cy", function (d) {return y(yCoords[d.i])})
                    .transition(0)
                    .attr("cx", function (d) {return x(xCoords[d.i])})
                    .transition(0);    
                */
                d3.select('#' + divName).selectAll("dot").remove();
                d3.select('#' + divName).selectAll("circle").remove();
                console.log(fullData)
                console.log(this)
                var circles = this.svg//.select('#' + divName)
                .selectAll("dot")
                .data(this.fullData.data.filter(d=>d.display === undefined || d.display === true))
                //.filter(function (d) {return d.display === undefined || d.display === true})
                .enter()
                .append("circle")
                .attr("cy", d=>d.y)
                .attr("cx", d=>d.x)
                .attr("r", d=>2)
                .on("mouseover", function (d) {
                    /*var mySVGMatrix = circle.getScreenCTM()n
                        .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;*/

                    /*showTooltip(
                        d,
                        d3.event.pageX,
                        d3.event.pageY
                    );*/
                    console.log("point MOUSOEVER")
                    console.log(d)
                    showToolTipForTerm(data, this, d.term, d, true);
                    d3.select(this).style("stroke", "black");
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if(alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(d);
                    }
                    if(runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(d));
                    }
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    d3.select('#'+divName+'-'+'overlapped-terms')
                        .selectAll('div')
                        .remove();
                })
                
                if(color !== null) {
                     circles.style("fill", d => color(d));
                }
                xCoords.forEach((xCoord,i) => censorCircle(xCoord, yCoords[i]))
                labeledPoints = [];
                labeledPoints = performPartialLabeling(
                    this.fullData.data,
                    labeledPoints, 
                    (d=>d.ox), //function (d) {return xCoords[d.ci]},
                    (d=>d.oy) //function (d) {return yCoords[d.ci]}
                );
            };
            //return [performPartialLabeling, labeledPoints];
            return {...payload, 
                    ...{'rerender': rerender, 
                        'performPartialLabeling': performPartialLabeling,
                        'showToolTipForTerm': showToolTipForTerm,
                        'svg': svg,
                        'data': data,
                        'xLabel': xLabel,
                        'yLabel': yLabel,
                        'drawXLabel': drawXLabel,
                        'drawYLabel': drawYLabel,
                        'populateCorpusStats': populateCorpusStats}};
        };

        
        
        //fullData = getDataAndInfo();
        if (fullData.docs) {
            var corpusWordCounts = getCorpusWordCounts();
        }
        var payload = processData(fullData);
        
        // The tool tip is down here in order to make sure it has the highest z-index
        var tooltip = d3.select('#' + divName)
            .append("div")
            //.attr("class", getTooltipContent == null && sortByDist ? "tooltip" : "tooltipscore")
            .attr("class", "tooltipscore")
            .style("opacity", 0);
        
        plotInterface = {}
        if(payload.topTermsPane) {
            plotInterface.topTermsPane = payload.topTermsPane;
            plotInterface.showTopTermsPane = payload.showTopTermsPane;
            plotInterface.showAssociatedWordList = payload.showAssociatedWordList;
        }
        plotInterface.divName = divName; 
        plotInterface.displayTermContexts = displayTermContexts;
        plotInterface.gatherTermContexts = gatherTermContexts;
        plotInterface.xLabel = payload.xLabel;
        plotInterface.yLabel = payload.yLabel;
        plotInterface.drawXLabel = payload.drawXLabel;
        plotInterface.drawYLabel = payload.drawYLabel;
        plotInterface.svg = payload.svg;
        plotInterface.termDict = termDict;
        plotInterface.showToolTipForTerm = payload.showToolTipForTerm;
        plotInterface.fullData = fullData;
        plotInterface.data = payload.data;
        plotInterface.rerender = payload.rerender;
        plotInterface.populateCorpusStats = payload.populateCorpusStats;
        plotInterface.handleSearch = handleSearch;
        plotInterface.y = y;
        plotInterface.x = x;
        plotInterface.tooltip = tooltip;
        plotInterface.alternativeTermFunc = alternativeTermFunc;
        plotInterface.drawCategoryAssociation = function (categoryNum) {
            var rawLogTermCounts = getTermCounts(this.fullData).map(Math.log);
            var maxRawLogTermCounts = Math.max(...rawLogTermCounts);
            var minRawLogTermCounts = Math.min(...rawLogTermCounts);
            var logTermCounts = rawLogTermCounts.map(
                x => (x - minRawLogTermCounts)/maxRawLogTermCounts
            )
            
            var rawScores = getCategoryDenseRankScores(this.fullData, categoryNum);
            var maxRawScores = Math.max(...rawScores);
            var minRawScores = Math.min(...rawScores);
            var scores = rawScores.map(
                function(rawScore) {
                    if(rawScore == 0) {
                        return 0.5;
                    } else if(rawScore > 0) {
                        return rawScore/(2.*maxRawScores) + 0.5;
                    } else if(rawScore < 0) {
                        return 0.5 - rawScore/(2.*minRawScores);
                    }
                }
            )
            
            var denseRanks = getDenseRanks(this.fullData, categoryNum)
            console.log("denseRanks")
            console.log(denseRanks);
            var fgFreqSum = denseRanks.fgFreqs.reduce((a,b) => a + b, 0)
            var bgFreqSum = denseRanks.bgFreqs.reduce((a,b) => a + b, 0)
            var ox = denseRanks.bg;
            var oy = denseRanks.fg; 
            //var ox = logTermCounts
            //var oy = scores;
            var xf = this.x;
            var yf = this.y;
            
            this.fullData.data = this.fullData.data.map(function(term, i) { 
                //term.ci = i;
                term.s = scores[i];
                term.os = rawScores[i];
                term.cat = denseRanks.fgFreqs[i];
                term.ncat = denseRanks.bgFreqs[i];
                term.cat25k = parseInt(denseRanks.fgFreqs[i] * 25000/fgFreqSum);
                term.ncat25k = parseInt(25000 *denseRanks.bgFreqs[i]/bgFreqSum);
                term.x = xf(ox[i]) // logTermCounts[term.i];
                term.y = yf(oy[i]) // scores[term.i];
                term.ox = ox[i];
                term.oy = oy[i];
                term.display = false;
                return term;
             })
            
            // Feature selection
            var targetTermsToShow = 1500;
            
            var sortedBg = denseRanks.bg.map((x,i)=>[x,i]).sort((a,b)=>b[0]-a[0]).map(x=>x[1]).slice(0,parseInt(targetTermsToShow/2));
            var sortedFg = denseRanks.fg.map((x,i)=>[x,i]).sort((a,b)=>b[0]-a[0]).map(x=>x[1]).slice(0,parseInt(targetTermsToShow/2));
            var sortedScores = denseRanks.fg.map((x,i)=>[x,i]).sort((a,b)=>b[0]-a[0]).map(x=>x[1]);
            var myFullData = this.fullData
            
            sortedBg.concat(sortedFg)//.concat(sortedScores.slice(0, parseInt(targetTermsToShow/2))).concat(sortedScores.slice(-parseInt(targetTermsToShow/4)))
                .forEach(function(i) {
                myFullData.data[i].display = true;
            })
            
            console.log('newly filtered')
            console.log(myFullData)
            
            // begin rescaling to ignore hidden terms
            /*
            function scaleDenseRanks(ranks) { 
                var max = Math.max(...ranks); 
                return ranks.map(x=>x/max) 
            }
            var filteredData = myFullData.data.filter(d=>d.display);
            var catRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.cat)))
            var ncatRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.ncat)))
            var rawScores = catRanks.map((x,i) => x - ncatRanks[i]);
            function stretch_0_1(scores) {
                var max = 1.*Math.max(...rawScores);
                var min = -1.*Math.min(...rawScores);
                return scores.map(function(x, i) {
                    if(x == 0) return 0.5;
                    if(x > 0) return (x/max + 1)/2;
                    return (x/min + 1)/2;
                })
            }
            var scores = stretch_0_1(rawScores);
            console.log(scores)
            filteredData.forEach(function(d, i) {
                d.x = xf(catRanks[i]);
                d.y = yf(ncatRanks[i]);
                d.ox = catRanks[i];
                d.oy = ncatRanks[i];
                d.s = scores[i];
                d.os = rawScores[i];
            });
            console.log("rescaled");
            */
            // end rescaling
            
            
            this.rerender(//denseRanks.bg, 
                          fullData.data.map(x=>x.ox), //ox 
                          //denseRanks.fg, 
                          fullData.data.map(x=>x.oy), //oy,          
                          d => d3.interpolateRdYlBu(d.s));
            this.yLabel.remove()
            this.xLabel.remove()
            this.yLabel = this.drawYLabel(this.svg, this.fullData.info.categories[categoryNum] + ' Frequncy Rank')
            this.xLabel = this.drawXLabel(this.svg,
                               "Not " + this.fullData.info.categories[categoryNum] + ' Frequency Rank')
            console.log(this.topTermsPane)
            this.topTermsPane.catHeader.remove()
            this.topTermsPane.notCatHeader.remove()
            this.topTermsPane.wordListData.wordObjList.map(x => x.remove())
            this.topTermsPane.notWordListData.wordObjList.map(x => x.remove())
            this.showWordList = payload.showWordList;
            this.showAssociatedWordList = function(data, word, header, isAssociatedToCategory, length=14) {
                var sortedData = null;
                if(!isAssociatedToCategory) {
                    sortedData = data.map(x=>x).sort((a, b) => scores[a.i] - scores[b.i])
                } else {
                    sortedData = data.map(x=>x).sort((a, b) => scores[b.i] - scores[a.i])
                }
                console.log('sortedData'); 
                console.log(isAssociatedToCategory); 
                console.log(sortedData.slice(0, length))
                console.log(payload)
                console.log(word)
                return payload.showWordList(word, sortedData.slice(0, length));
            }
            this.topTermsPane = payload.showTopTermsPane(
                this.data,
                this.topTermsPane.registerFigureBBox,
                this.showAssociatedWordList,
                "Top " + this.fullData.info.categories[categoryNum],
                "Top Not " + this.fullData.info.categories[categoryNum],
                this.topTermsPane.startingOffset
            )
            
            fullData.info.category_name = this.fullData.info.categories[categoryNum];
            fullData.info.not_category_name = "Not " + this.fullData.info.categories[categoryNum];
            fullData.info.category_internal_name = this.fullData.info.categories[categoryNum];
            fullData.info.not_category_internal_names = this.fullData.info.categories.filter(x => x!==this.fullData.info.categories[categoryNum]);
            ['snippets', 
             'snippetsalt', 'termstats', 
             'overlapped-terms-clicked', 'categoryinfo', 
             'cathead', 'cat', 'corpus-stats',
             'notcathead', 'notcat', 'neuthead', 'nuet'].forEach(function(divSubName) {
                var mydiv = '#'+divName+'-'+divSubName;
                d3.select(mydiv).selectAll("*").remove();
                d3.select(mydiv).html("");

            })
            this.populateCorpusStats();
            console.log(fullData)
        }
        
        return plotInterface
    };
}(d3);
 
function getDataAndInfo() { return{"info": {"category_name": "< 2018", "not_category_name": "> 2017", "category_terms": ["- trained", "micro", "resnet", "fusion", "scale", "convolutional neural", "spatial", "- expression", "micro -", "demand"], "not_category_terms": ["- trained", "micro", "resnet", "fusion", "scale", "convolutional neural", "spatial", "- expression", "micro -", "demand"], "category_internal_name": "NORMAL", "not_category_internal_names": ["FRONTIER"], "categories": ["FRONTIER", "NORMAL"], "neutral_category_internal_names": [], "extra_category_internal_names": [], "neutral_category_name": "Neutral", "extra_category_name": "Extra"}, "data": [{"x": 0.9933417882054534, "y": 0.9831959416613824, "ox": 0.9933417882054534, "oy": 0.9831959416613824, "term": "deep", "cat25k": 62, "ncat25k": 132, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 452, "ncat": 336, "s": 0.012365250475586557, "os": -0.05918518780255111, "bg": 2.9359668608329354e-05}, {"x": 0.9980976537729866, "y": 0.9980976537729866, "ox": 0.9980976537729866, "oy": 0.9980976537729866, "term": "learning", "cat25k": 504, "ncat25k": 471, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3659, "ncat": 1201, "s": 0.39315155358275206, "os": 0.013674428465425215, "bg": 8.243674398371003e-05}, {"x": 0.8031071655041218, "y": 0.7311350665821179, "ox": 0.8031071655041218, "oy": 0.7311350665821179, "term": "dictionary", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 16, "s": 0.3335447051363348, "os": 0.009487297268647749, "bg": 2.2888182293366164e-06}, {"x": 0.9844641724793912, "y": 0.9835129993658845, "ox": 0.9844641724793912, "oy": 0.9835129993658845, "term": "feature", "cat25k": 63, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 459, "ncat": 178, "s": 0.9001268230818009, "os": 0.10109183421080525, "bg": 1.9745682735886165e-05}, {"x": 0.4099556119213697, "y": 0.02060875079264426, "ox": 0.4099556119213697, "oy": 0.02060875079264426, "term": "land", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 1.7414216409021397e-07}, {"x": 0.9673430564362714, "y": 0.9733671528218135, "ox": 0.9673430564362714, "oy": 0.9733671528218135, "term": "use", "cat25k": 43, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 310, "ncat": 97, "s": 0.9838300570703868, "os": 0.22347266881028938, "bg": 1.1302234801995246e-06}, {"x": 0.9889029803424223, "y": 0.9879518072289156, "ox": 0.9889029803424223, "oy": 0.9879518072289156, "term": "classification", "cat25k": 87, "ncat25k": 94, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 632, "ncat": 240, "s": 0.8544705136334813, "os": 0.07467933995265186, "bg": 8.649769494058417e-05}, {"x": 0.8671528218135701, "y": 0.9289790741915028, "ox": 0.8671528218135701, "oy": 0.9289790741915028, "term": "sparse", "cat25k": 16, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 117, "ncat": 24, "s": 0.9882688649334179, "os": 0.23884315041871312, "bg": 0.00014269225884375753}, {"x": 0.9514901712111604, "y": 0.9606848446417248, "ox": 0.9514901712111604, "oy": 0.9606848446417248, "term": "representation", "cat25k": 30, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 220, "ncat": 65, "s": 0.9895370957514268, "os": 0.2473587505741846, "bg": 3.0188536946452795e-05}, {"x": 0.998414711477489, "y": 0.998414711477489, "ox": 0.998414711477489, "oy": 0.998414711477489, "term": "in", "cat25k": 506, "ncat25k": 491, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3671, "ncat": 1251, "s": 0.3592263792010146, "os": 0.011395357054521105, "bg": 1.1622688920947772e-06}, {"x": 0.9251743817374762, "y": 0.932149651236525, "ox": 0.9251743817374762, "oy": 0.932149651236525, "term": "very", "cat25k": 17, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 122, "ncat": 38, "s": 0.9648065948002537, "os": 0.17799724391364266, "bg": 9.547837965193413e-07}, {"x": 0.9518072289156626, "y": 0.9416613823715916, "ox": 0.9518072289156626, "oy": 0.9416613823715916, "term": "high", "cat25k": 20, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 144, "ncat": 65, "s": 0.8880786303107165, "os": 0.09301791456132291, "bg": 1.2093359065532606e-06}, {"x": 0.8896639188332276, "y": 0.44673430564362715, "ox": 0.8896639188332276, "oy": 0.44673430564362715, "term": "spatial", "cat25k": 2, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 28, "s": 0.0019023462270133164, "os": -0.11110914808664005, "bg": 8.59894519605595e-06}, {"x": 0.7622067216233355, "y": 0.8126188966391883, "ox": 0.7622067216233355, "oy": 0.8126188966391883, "term": "resolution", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 13, "s": 0.8452758402029169, "os": 0.07098689092258223, "bg": 2.08406923285178e-06}, {"x": 0.984147114774889, "y": 0.972415979708307, "ox": 0.984147114774889, "oy": 0.972415979708307, "term": "images", "cat25k": 41, "ncat25k": 69, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 297, "ncat": 177, "s": 0.14521242866201647, "os": -0.005953853220734273, "bg": 8.314372383724424e-06}, {"x": 0.9977805960684845, "y": 0.9974635383639823, "ox": 0.9977805960684845, "oy": 0.9974635383639823, "term": "is", "cat25k": 310, "ncat25k": 314, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2248, "ncat": 801, "s": 0.38205453392517436, "os": 0.012738065792728182, "bg": 1.2957987325561404e-06}, {"x": 0.6477488902980343, "y": 0.5913126188966392, "ox": 0.6477488902980343, "oy": 0.5913126188966392, "term": "critical", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 9, "s": 0.3804692454026633, "os": 0.01257906081057207, "bg": 1.2508903697465866e-06}, {"x": 1.0, "y": 1.0, "ox": 1.0, "oy": 1.0, "term": "the", "cat25k": 1383, "ncat25k": 1375, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10040, "ncat": 3506, "s": 0.1959416613823716, "os": 0.0, "bg": 1.1709840968892713e-06}, {"x": 0.7133798351299937, "y": 0.6277742549143944, "ox": 0.7133798351299937, "oy": 0.6277742549143944, "term": "remote", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 11, "s": 0.30247305009511727, "os": 0.008020917988763643, "bg": 1.2962948377827442e-06}, {"x": 0.8034242232086239, "y": 0.5916296766011414, "ox": 0.8034242232086239, "oy": 0.5916296766011414, "term": "sensing", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 16, "s": 0.052314521242866195, "os": -0.025882477650966393, "bg": 1.484398048758765e-05}, {"x": 0.8478123018389346, "y": 0.790424857324033, "ox": 0.8478123018389346, "oy": 0.790424857324033, "term": "field", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 21, "s": 0.3532022828154724, "os": 0.01095367654853184, "bg": 9.002355761544213e-07}, {"x": 0.320862396956246, "y": 0.04819277108433735, "ox": 0.320862396956246, "oy": 0.04819277108433735, "term": "sparse representation", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.21908687381103362, "y": 0.13696892834495877, "ox": 0.21908687381103362, "oy": 0.13696892834495877, "term": "very high", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.5526315789473685, "y": 0.5481927710843374, "ox": 0.5526315789473685, "oy": 0.5481927710843374, "term": "remote sensing", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.42897907419150283, "os": 0.017137203632380478, "bg": 0.0}, {"x": 0.4102726696258719, "y": 0.08528852251109702, "ox": 0.4102726696258719, "oy": 0.08528852251109702, "term": "consequently", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 3.3759913282036088e-06}, {"x": 0.4889029803424223, "y": 0.011414077362079899, "ox": 0.4889029803424223, "oy": 0.011414077362079899, "term": "remarkable", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 2.4749299904178964e-06}, {"x": 0.48922003804692454, "y": 0.5710209258084972, "ox": 0.48922003804692454, "oy": 0.5710209258084972, "term": "efforts", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 6, "s": 0.5440710209258085, "os": 0.025847143210487256, "bg": 1.3002731093638907e-06}, {"x": 0.976854787571338, "y": 0.9781230183893469, "ox": 0.976854787571338, "oy": 0.9781230183893469, "term": "have", "cat25k": 48, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 349, "ncat": 126, "s": 0.9499048826886494, "os": 0.15538320200699618, "bg": 6.072483307218e-07}, {"x": 0.972415979708307, "y": 0.9651236525047558, "ox": 0.972415979708307, "oy": 0.9651236525047558, "term": "been", "cat25k": 33, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 240, "ncat": 107, "s": 0.8871274571972099, "os": 0.09269990459701072, "bg": 1.2064307046863787e-06}, {"x": 0.8544705136334813, "y": 0.7526949904882688, "ox": 0.8544705136334813, "oy": 0.7526949904882688, "term": "conducted", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 22, "s": 0.09892200380469246, "os": -0.013833433447581353, "bg": 4.491493011978772e-06}, {"x": 0.6480659480025365, "y": 0.6436271401395054, "ox": 0.6480659480025365, "oy": 0.6436271401395054, "term": "towards", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 9, "s": 0.5069752694990488, "os": 0.02222536306137593, "bg": 1.7283481722826099e-06}, {"x": 0.5529486366518707, "y": 0.5919467343056436, "ox": 0.5529486366518707, "oy": 0.5919467343056436, "term": "developing", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 7, "s": 0.5180722891566265, "os": 0.023568071799583055, "bg": 1.2558086966147584e-06}, {"x": 0.4895370957514268, "y": 0.6106531388712746, "ox": 0.4895370957514268, "oy": 0.6106531388712746, "term": "increasingly", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.6236525047558656, "os": 0.03227801137768983, "bg": 4.260887514463345e-06}, {"x": 0.8719086873811034, "y": 0.8671528218135701, "ox": 0.8719086873811034, "oy": 0.8671528218135701, "term": "accurate", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 25, "s": 0.8335447051363348, "os": 0.06614607257694075, "bg": 7.0146544801888816e-06}, {"x": 0.9527584020291693, "y": 0.9594166138237159, "ox": 0.9527584020291693, "oy": 0.9594166138237159, "term": "approaches", "cat25k": 29, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 210, "ncat": 67, "s": 0.984147114774889, "os": 0.22350800325076847, "bg": 3.2022556827755345e-05}, {"x": 0.99714648065948, "y": 0.9968294229549778, "ox": 0.99714648065948, "oy": 0.9968294229549778, "term": "for", "cat25k": 263, "ncat25k": 265, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1909, "ncat": 676, "s": 0.4308814204185161, "os": 0.017296208614536623, "bg": 8.713157367966473e-07}, {"x": 0.9958782498414711, "y": 0.9955611921369689, "ox": 0.9958782498414711, "oy": 0.9955611921369689, "term": "this", "cat25k": 215, "ncat25k": 213, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1558, "ncat": 543, "s": 0.5526315789473684, "os": 0.026412494258153396, "bg": 1.301451486915045e-06}, {"x": 0.9822447685478757, "y": 0.9882688649334179, "ox": 0.9822447685478757, "oy": 0.9882688649334179, "term": "task", "cat25k": 91, "ncat25k": 60, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 663, "ncat": 152, "s": 0.9705136334812936, "os": 0.1910003180099643, "bg": 4.068447967969983e-05}, {"x": 0.9334178820545339, "y": 0.896005072923272, "ox": 0.9334178820545339, "oy": 0.896005072923272, "term": "have been", "cat25k": 11, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 45, "s": 0.2951807228915662, "os": 0.007702908024451449, "bg": 0.0}, {"x": 0.9077362079898541, "y": 0.9080532656943564, "ox": 0.9077362079898541, "oy": 0.9080532656943564, "term": "recent", "cat25k": 12, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 32, "s": 0.9115409004438808, "os": 0.11128582028903575, "bg": 1.7183493932920274e-06}, {"x": 0.780279010779962, "y": 0.8211794546607483, "ox": 0.780279010779962, "oy": 0.8211794546607483, "term": "years", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 14, "s": 0.8471781864299303, "os": 0.07192325359527933, "bg": 3.6086960487961017e-07}, {"x": 0.9819277108433735, "y": 0.9800253646163601, "ox": 0.9819277108433735, "oy": 0.9800253646163601, "term": "has", "cat25k": 52, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 379, "ncat": 150, "s": 0.9023462270133165, "os": 0.10324723508003253, "bg": 1.0109393624189857e-06}, {"x": 0.6046290424857323, "y": 0.6280913126188966, "ox": 0.6046290424857323, "oy": 0.6280913126188966, "term": "emerged", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 8, "s": 0.5320228281547241, "os": 0.02450443447228013, "bg": 1.1920170617375437e-05}, {"x": 0.9930247305009512, "y": 0.9930247305009512, "ox": 0.9930247305009512, "oy": 0.9930247305009512, "term": "as", "cat25k": 123, "ncat25k": 129, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 893, "ncat": 330, "s": 0.7840837032339887, "os": 0.05241864245079675, "bg": 1.088241022091095e-06}, {"x": 0.9987317691819911, "y": 0.9990488268864933, "ox": 0.9987317691819911, "oy": 0.9990488268864933, "term": "a", "cat25k": 577, "ncat25k": 527, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4186, "ncat": 1345, "s": 0.3662016487000634, "os": 0.012331719727218138, "bg": 1.2180926361323936e-06}, {"x": 0.7352568167406468, "y": 0.7029169308814204, "ox": 0.7352568167406468, "oy": 0.7029169308814204, "term": "paradigm", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 12, "s": 0.5003170577045022, "os": 0.021819016995865873, "bg": 1.8557760449468958e-05}, {"x": 0.9733671528218135, "y": 0.9743183259353202, "ox": 0.9733671528218135, "oy": 0.9743183259353202, "term": "machine", "cat25k": 43, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 312, "ncat": 111, "s": 0.9600507292327204, "os": 0.16946397653793155, "bg": 1.2103478214876597e-05}, {"x": 0.9990488268864933, "y": 0.9987317691819911, "ox": 0.9990488268864933, "oy": 0.9987317691819911, "term": "and", "cat25k": 545, "ncat25k": 584, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3955, "ncat": 1489, "s": 0.24603677869372226, "os": 0.0036217801491114088, "bg": 8.376754348748704e-07}, {"x": 0.12492073557387444, "y": 0.08560558021559923, "ox": 0.12492073557387444, "oy": 0.08560558021559923, "term": "methodologies", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 5.43970988213962e-06}, {"x": 0.9920735573874445, "y": 0.989854153455929, "ox": 0.9920735573874445, "oy": 0.989854153455929, "term": "based", "cat25k": 96, "ncat25k": 119, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 698, "ncat": 303, "s": 0.6715282181357007, "os": 0.03674781809830041, "bg": 7.924410559721186e-06}, {"x": 0.9965123652504756, "y": 0.9958782498414711, "ox": 0.9965123652504756, "oy": 0.9958782498414711, "term": "on", "cat25k": 226, "ncat25k": 238, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1638, "ncat": 606, "s": 0.455294863665187, "os": 0.0186389173527437, "bg": 1.196590572717963e-06}, {"x": 0.9828788839568802, "y": 0.9622701331642359, "ox": 0.9828788839568802, "oy": 0.9622701331642359, "term": "convolutional", "cat25k": 31, "ncat25k": 60, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 225, "ncat": 154, "s": 0.0057070386810399495, "os": -0.08365428783435214, "bg": 0.0023892678375550037}, {"x": 0.9882688649334179, "y": 0.9803424223208624, "ox": 0.9882688649334179, "oy": 0.9803424223208624, "term": "neural", "cat25k": 52, "ncat25k": 86, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 381, "ncat": 219, "s": 0.22954977805960686, "os": 0.002067064768029425, "bg": 0.00021722779321000228}, {"x": 0.9831959416613824, "y": 0.9727330374128091, "ox": 0.9831959416613824, "oy": 0.9727330374128091, "term": "networks", "cat25k": 41, "ncat25k": 61, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 299, "ncat": 156, "s": 0.3934686112872543, "os": 0.013745097346383517, "bg": 1.484389967513881e-05}, {"x": 0.12523779327837667, "y": 0.4784400760938491, "ox": 0.12523779327837667, "oy": 0.4784400760938491, "term": "received", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6540900443880786, "os": 0.0349634288541041, "bg": 3.7665457708869585e-07}, {"x": 0.8722257450856056, "y": 0.8766645529486367, "ox": 0.8722257450856056, "oy": 0.8766645529486367, "term": "particular", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 25, "s": 0.8642993024730501, "os": 0.0790078089113459, "bg": 2.573044906066223e-06}, {"x": 0.9036144578313253, "y": 0.8189600507292327, "ox": 0.9036144578313253, "oy": 0.8189600507292327, "term": "attention", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 31, "s": 0.05738744451490171, "os": -0.0246987738949154, "bg": 3.423480465409276e-06}, {"x": 0.9946100190234622, "y": 0.9949270767279645, "ox": 0.9946100190234622, "oy": 0.9949270767279645, "term": "from", "cat25k": 194, "ncat25k": 165, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1410, "ncat": 420, "s": 0.7539632213062778, "os": 0.04745415356347826, "bg": 1.6082058444847336e-06}, {"x": 0.5532656943563729, "y": 0.525998731769182, "ox": 0.5532656943563729, "oy": 0.525998731769182, "term": "community", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 7, "s": 0.3937856689917565, "os": 0.01392176954877919, "bg": 1.8315715567361932e-07}, {"x": 0.6483830057070387, "y": 0.7530120481927711, "ox": 0.6483830057070387, "oy": 0.7530120481927711, "term": "recent years", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 9, "s": 0.8056436271401396, "os": 0.05759513798099007, "bg": 0.0}, {"x": 0.32117945466074826, "y": 0.26506024096385544, "ox": 0.32117945466074826, "oy": 0.26506024096385544, "term": "has emerged", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.41058972733037413, "y": 0.5025364616360177, "ox": 0.41058972733037413, "oy": 0.5025364616360177, "term": "emerged as", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 0.0}, {"x": 0.9787571337983513, "y": 0.9749524413443247, "ox": 0.9787571337983513, "oy": 0.9749524413443247, "term": "based on", "cat25k": 43, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 314, "ncat": 132, "s": 0.9004438807863031, "os": 0.1012508391929614, "bg": 0.0}, {"x": 0.9197844007609385, "y": 0.8325935320228282, "ox": 0.9197844007609385, "oy": 0.8325935320228282, "term": "deep convolutional", "cat25k": 7, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 36, "s": 0.027584020291693087, "os": -0.0393095650330377, "bg": 0.0}, {"x": 0.9781230183893469, "y": 0.9502219403931516, "ox": 0.9781230183893469, "oy": 0.9502219403931516, "term": "convolutional neural", "cat25k": 24, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 174, "ncat": 130, "s": 0.0015852885225110971, "os": -0.11190417299742061, "bg": 0.0}, {"x": 0.9689283449587826, "y": 0.9511731135066582, "ox": 0.9689283449587826, "oy": 0.9511731135066582, "term": "neural networks", "cat25k": 25, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 178, "ncat": 98, "s": 0.4549778059606848, "os": 0.018621250132504152, "bg": 0.0}, {"x": 0.9692454026632847, "y": 0.9676601141407736, "ox": 0.9692454026632847, "oy": 0.9676601141407736, "term": "these", "cat25k": 36, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 263, "ncat": 99, "s": 0.9524413443246671, "os": 0.15782127840005655, "bg": 1.3376808552368688e-06}, {"x": 0.9847812301838935, "y": 0.9860494610019024, "ox": 0.9847812301838935, "oy": 0.9860494610019024, "term": "methods", "cat25k": 74, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 536, "ncat": 179, "s": 0.9210526315789473, "os": 0.12132080138511014, "bg": 1.6226161371898114e-05}, {"x": 0.8167406467977172, "y": 0.6943563728598605, "ox": 0.8167406467977172, "oy": 0.6943563728598605, "term": "typically", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 17, "s": 0.12301838934686113, "os": -0.008868944560262881, "bg": 4.657600224411647e-06}, {"x": 0.8037412809131262, "y": 0.7907419150285352, "ox": 0.8037412809131262, "oy": 0.7907419150285352, "term": "utilize", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 16, "s": 0.6870640456563095, "os": 0.03842620402105931, "bg": 1.788597086726677e-05}, {"x": 0.9974635383639823, "y": 0.9977805960684845, "ox": 0.9974635383639823, "oy": 0.9977805960684845, "term": "transfer", "cat25k": 347, "ncat25k": 297, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2517, "ncat": 757, "s": 0.49651236525047554, "os": 0.02144800537083491, "bg": 0.00010691375740040605}, {"x": 0.553582752060875, "y": 0.50285351934052, "ox": 0.553582752060875, "oy": 0.50285351934052, "term": "and/or", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 0.0}, {"x": 0.9955611921369689, "y": 0.9961953075459734, "ox": 0.9955611921369689, "oy": 0.9961953075459734, "term": "data", "cat25k": 251, "ncat25k": 207, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1819, "ncat": 527, "s": 0.6867469879518072, "os": 0.03833786791986149, "bg": 1.152424735538418e-05}, {"x": 0.8547875713379836, "y": 0.5922637920101459, "ox": 0.8547875713379836, "oy": 0.5922637920101459, "term": "augmentation", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 22, "s": 0.013316423589093214, "os": -0.058849510617999354, "bg": 2.7132948540748036e-05}, {"x": 0.9993658845909955, "y": 0.9993658845909955, "ox": 0.9993658845909955, "oy": 0.9993658845909955, "term": "to", "cat25k": 622, "ncat25k": 591, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4515, "ncat": 1507, "s": 0.26125554850982874, "os": 0.004558142821808331, "bg": 9.923196762365217e-07}, {"x": 0.9422954977805961, "y": 0.9340519974635384, "ox": 0.9422954977805961, "oy": 0.9340519974635384, "term": "small", "cat25k": 17, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 124, "ncat": 54, "s": 0.8937856689917565, "os": 0.09651602416875732, "bg": 1.7065914958494017e-06}, {"x": 0.9483195941661382, "y": 0.9422954977805961, "ox": 0.9483195941661382, "oy": 0.9422954977805961, "term": "number", "cat25k": 20, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 146, "ncat": 62, "s": 0.9159797083069119, "os": 0.11593229921204196, "bg": 1.0832800297495777e-06}, {"x": 0.9996829422954978, "y": 0.9996829422954978, "ox": 0.9996829422954978, "oy": 0.9996829422954978, "term": "of", "cat25k": 705, "ncat25k": 733, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5115, "ncat": 1870, "s": 0.2305009511731135, "os": 0.002279071410904221, "bg": 1.0621812746645378e-06}, {"x": 0.9625871908687381, "y": 0.966708941027267, "ox": 0.9625871908687381, "oy": 0.966708941027267, "term": "labeled", "cat25k": 35, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 253, "ncat": 81, "s": 0.9806594800253646, "os": 0.21317267941062157, "bg": 0.0001113920740870686}, {"x": 0.7805960684844642, "y": 0.6439441978440076, "ox": 0.7805960684844642, "oy": 0.6439441978440076, "term": "publicly", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 14, "s": 0.14806594800253645, "os": -0.005247164411151542, "bg": 8.803982540988784e-06}, {"x": 0.950856055802156, "y": 0.9584654407102092, "ox": 0.950856055802156, "oy": 0.9584654407102092, "term": "available", "cat25k": 28, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 206, "ncat": 64, "s": 0.9866835764109069, "os": 0.23034521748348108, "bg": 1.4215180328420628e-06}, {"x": 0.9857324032974001, "y": 0.9765377298668357, "ox": 0.9857324032974001, "oy": 0.9765377298668357, "term": "datasets", "cat25k": 45, "ncat25k": 72, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 328, "ncat": 184, "s": 0.2929613189600507, "os": 0.0074378997208578435, "bg": 0.0004912008373439274}, {"x": 0.9961953075459734, "y": 0.9965123652504756, "ox": 0.9961953075459734, "oy": 0.9965123652504756, "term": "transfer learning", "cat25k": 255, "ncat25k": 229, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1853, "ncat": 583, "s": 0.6081166772352569, "os": 0.030564291014451794, "bg": 0.0}, {"x": 0.8256182625237793, "y": 0.26537729866835763, "ox": 0.8256182625237793, "oy": 0.26537729866835763, "term": "data augmentation", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 18, "s": 0.009828788839568801, "os": -0.06902582947599026, "bg": 0.0}, {"x": 0.489854153455929, "y": 0.5925808497146481, "ox": 0.489854153455929, "oy": 0.5925808497146481, "term": "small number", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 6, "s": 0.5900443880786304, "os": 0.02906257729408855, "bg": 0.0}, {"x": 0.7625237793278377, "y": 0.6109701965757768, "ox": 0.7625237793278377, "oy": 0.6109701965757768, "term": "publicly available", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 13, "s": 0.1436271401395054, "os": -0.006183527083848631, "bg": 0.0}, {"x": 0.9676601141407736, "y": 0.971147748890298, "ox": 0.9676601141407736, "oy": 0.971147748890298, "term": "however", "cat25k": 40, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 289, "ncat": 97, "s": 0.9755865567533292, "os": 0.20418006430868163, "bg": 4.701914344227539e-06}, {"x": 0.9254914394419784, "y": 0.9115409004438808, "ox": 0.9254914394419784, "oy": 0.9115409004438808, "term": "they", "cat25k": 13, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 38, "s": 0.8804692454026632, "os": 0.0879650895728066, "bg": 2.965629836406724e-07}, {"x": 0.8348129359543437, "y": 0.7533291058972733, "ox": 0.8348129359543437, "oy": 0.7533291058972733, "term": "require", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 19, "s": 0.2403297400126823, "os": 0.0026500830359351274, "bg": 1.972965239069968e-06}, {"x": 0.6861128725428028, "y": 0.5263157894736842, "ox": 0.6861128725428028, "oy": 0.5263157894736842, "term": "powerful", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 10, "s": 0.173430564362714, "os": -0.002561746934737291, "bg": 1.5990599422721591e-06}, {"x": 0.8481293595434369, "y": 0.8192771084337349, "ox": 0.8481293595434369, "oy": 0.8192771084337349, "term": "long", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 21, "s": 0.6046290424857325, "os": 0.03024628105013956, "bg": 5.301670239379156e-07}, {"x": 0.9635383639822448, "y": 0.9603677869372226, "ox": 0.9635383639822448, "oy": 0.9603677869372226, "term": "time", "cat25k": 29, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 214, "ncat": 86, "s": 0.9426125554850983, "os": 0.14524221758948447, "bg": 6.601119370294754e-07}, {"x": 0.9885859226379201, "y": 0.992707672796449, "ox": 0.9885859226379201, "oy": 0.992707672796449, "term": "training", "cat25k": 106, "ncat25k": 94, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 773, "ncat": 239, "s": 0.932149651236525, "os": 0.13162079078477795, "bg": 1.1476457091590884e-05}, {"x": 0.9594166138237159, "y": 0.9533925174381738, "ox": 0.9594166138237159, "oy": 0.9533925174381738, "term": "work", "cat25k": 25, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 185, "ncat": 76, "s": 0.9283449587824985, "os": 0.12810501395710405, "bg": 1.2437008132261795e-06}, {"x": 0.9968294229549778, "y": 0.99714648065948, "ox": 0.9968294229549778, "oy": 0.99714648065948, "term": "we", "cat25k": 308, "ncat25k": 257, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2236, "ncat": 655, "s": 0.5456563094483196, "os": 0.026006148192643352, "bg": 4.157033530519581e-06}, {"x": 0.9774889029803424, "y": 0.9838300570703868, "ox": 0.9774889029803424, "oy": 0.9838300570703868, "term": "propose", "cat25k": 64, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 466, "ncat": 127, "s": 0.9816106531388713, "os": 0.21419737818451645, "bg": 0.00018179457611436936}, {"x": 0.7628408370323398, "y": 0.8604946100190235, "ox": 0.7628408370323398, "oy": 0.8604946100190235, "term": "simple", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 13, "s": 0.9251743817374762, "os": 0.1256492703438041, "bg": 1.7472563619613483e-06}, {"x": 0.9657577679137603, "y": 0.9714648065948003, "ox": 0.9657577679137603, "oy": 0.9714648065948003, "term": "novel", "cat25k": 40, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 291, "ncat": 92, "s": 0.9847812301838935, "os": 0.22387901487579948, "bg": 3.4735363238939306e-05}, {"x": 0.9892200380469245, "y": 0.99143944197844, "ox": 0.9892200380469245, "oy": 0.99143944197844, "term": "method", "cat25k": 103, "ncat25k": 97, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 747, "ncat": 247, "s": 0.9067850348763475, "os": 0.10777004346136176, "bg": 2.349010052959542e-05}, {"x": 0.989854153455929, "y": 0.9904882688649335, "ox": 0.989854153455929, "oy": 0.9904882688649335, "term": "which", "cat25k": 99, "ncat25k": 99, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 718, "ncat": 253, "s": 0.8706404565630945, "os": 0.08164022472704147, "bg": 2.395324982806143e-06}, {"x": 0.8040583386176284, "y": 0.7682308180088776, "ox": 0.8040583386176284, "oy": 0.7682308180088776, "term": "efficiently", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 16, "s": 0.5428027901077996, "os": 0.025564467686654185, "bg": 1.5839887975438427e-05}, {"x": 0.735573874445149, "y": 0.6715282181357007, "ox": 0.735573874445149, "oy": 0.6715282181357007, "term": "combines", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 12, "s": 0.36525047558655677, "os": 0.012172714745062013, "bg": 1.0823069577565356e-05}, {"x": 0.9949270767279645, "y": 0.9942929613189601, "ox": 0.9949270767279645, "oy": 0.9942929613189601, "term": "with", "cat25k": 166, "ncat25k": 181, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1206, "ncat": 462, "s": 0.6255548509828789, "os": 0.032313345818169026, "bg": 1.047954858841839e-06}, {"x": 0.9372225745085606, "y": 0.9013950538998098, "ox": 0.9372225745085606, "oy": 0.9013950538998098, "term": "this work", "cat25k": 12, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 85, "ncat": 49, "s": 0.22828154724159797, "os": 0.001802056464435875, "bg": 0.0}, {"x": 0.9705136334812936, "y": 0.9816106531388713, "ox": 0.9705136334812936, "oy": 0.9816106531388713, "term": "we propose", "cat25k": 56, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 405, "ncat": 105, "s": 0.9933417882054534, "os": 0.26446061976608604, "bg": 0.0}, {"x": 0.8620798985415346, "y": 0.8871274571972099, "ox": 0.8620798985415346, "oy": 0.8871274571972099, "term": "specifically", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 23, "s": 0.9090044388078631, "os": 0.1092894244019646, "bg": 7.363814413353374e-06}, {"x": 0.9923906150919467, "y": 0.9892200380469245, "ox": 0.9923906150919467, "oy": 0.9892200380469245, "term": "proposed", "cat25k": 94, "ncat25k": 125, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 679, "ncat": 320, "s": 0.5323398858592263, "os": 0.024822444436592317, "bg": 3.7175789161251675e-05}, {"x": 0.7809131261889664, "y": 0.7970830691185796, "ox": 0.7809131261889664, "oy": 0.7970830691185796, "term": "performs", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 14, "s": 0.7850348763474954, "os": 0.052630649093671614, "bg": 1.730444832868132e-05}, {"x": 0.7358909321496513, "y": 0.7723525681674065, "ox": 0.7358909321496513, "oy": 0.7723525681674065, "term": "scenes", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 12, "s": 0.7714013950538999, "os": 0.05075792374827745, "bg": 7.0389611527634846e-06}, {"x": 0.9863665187064046, "y": 0.9854153455928979, "ox": 0.9863665187064046, "oy": 0.9854153455928979, "term": "using", "cat25k": 67, "ncat25k": 79, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 489, "ncat": 201, "s": 0.8795180722891566, "os": 0.08741740574538004, "bg": 5.117167335510102e-06}, {"x": 0.4109067850348763, "y": 0.37159162967660114, "ox": 0.4109067850348763, "oy": 0.37159162967660114, "term": "modified", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 6.44076516365918e-07}, {"x": 0.4901712111604312, "y": 0.2006975269499049, "ox": 0.4901712111604312, "oy": 0.2006975269499049, "term": "version", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 1.2893418490064657e-07}, {"x": 0.9578313253012049, "y": 0.9562460367786937, "ox": 0.9578313253012049, "oy": 0.9562460367786937, "term": "well", "cat25k": 27, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 195, "ncat": 74, "s": 0.9559289790741915, "os": 0.161602063531324, "bg": 1.4849002235095e-06}, {"x": 0.8674698795180723, "y": 0.8991756499682942, "ox": 0.8674698795180723, "oy": 0.8991756499682942, "term": "known", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 83, "ncat": 24, "s": 0.9337349397590362, "os": 0.13273382565987069, "bg": 2.470374410061027e-06}, {"x": 0.9530754597336716, "y": 0.9375396322130628, "ox": 0.9530754597336716, "oy": 0.9375396322130628, "term": "proposed method", "cat25k": 18, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 130, "ncat": 67, "s": 0.7488902980342423, "os": 0.04665912865269778, "bg": 0.0}, {"x": 0.5538998097653773, "y": 0.5928979074191503, "ox": 0.5538998097653773, "oy": 0.5928979074191503, "term": "well known", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 7, "s": 0.5180722891566265, "os": 0.023568071799583055, "bg": 0.0}, {"x": 0.946417247939125, "y": 0.9403931515535827, "ox": 0.946417247939125, "oy": 0.9403931515535827, "term": "while", "cat25k": 19, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 136, "ncat": 59, "s": 0.904565630944832, "os": 0.10441327161584396, "bg": 1.7224165196385427e-06}, {"x": 0.8043753963221306, "y": 0.8253012048192772, "ox": 0.8043753963221306, "oy": 0.8253012048192772, "term": "directly", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 16, "s": 0.8275206087507927, "os": 0.06414967668986962, "bg": 2.1927107340196825e-06}, {"x": 0.8677869372225745, "y": 0.87856689917565, "ox": 0.8677869372225745, "oy": 0.87856689917565, "term": "uses", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 69, "ncat": 24, "s": 0.8798351299936589, "os": 0.08771774848945266, "bg": 3.3170862094120394e-06}, {"x": 0.8551046290424857, "y": 0.8050095117311351, "ox": 0.8551046290424857, "oy": 0.8050095117311351, "term": "form", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 22, "s": 0.40266328471781865, "os": 0.015105473304830225, "bg": 6.447576720198961e-07}, {"x": 0.9939759036144579, "y": 0.9939759036144579, "ox": 0.9939759036144579, "oy": 0.9939759036144579, "term": "are", "cat25k": 161, "ncat25k": 153, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1165, "ncat": 391, "s": 0.7400126823081802, "os": 0.0455814282180842, "bg": 1.2999993732265233e-06}, {"x": 0.8046924540266328, "y": 0.6718452758402029, "ox": 0.8046924540266328, "oy": 0.6718452758402029, "term": "employed", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 16, "s": 0.1192136968928345, "os": -0.00980530723295997, "bg": 4.166441831238578e-06}, {"x": 0.8484464172479391, "y": 0.8053265694356373, "ox": 0.8484464172479391, "oy": 0.8053265694356373, "term": "classify", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 21, "s": 0.47653772986683574, "os": 0.020599978799335714, "bg": 6.55272434634015e-05}, {"x": 0.8950538998097654, "y": 0.9518072289156626, "ox": 0.8950538998097654, "oy": 0.9518072289156626, "term": "test", "cat25k": 25, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 181, "ncat": 29, "s": 0.9996829422954978, "os": 0.36571145895904744, "bg": 2.7056507302915297e-06}, {"x": 0.9908053265694357, "y": 0.9923906150919467, "ox": 0.9908053265694357, "oy": 0.9923906150919467, "term": "our", "cat25k": 105, "ncat25k": 105, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 761, "ncat": 268, "s": 0.8760304375396322, "os": 0.08444931274513268, "bg": 2.0600809021398176e-06}, {"x": 0.7362079898541535, "y": 0.6566265060240963, "ox": 0.7362079898541535, "oy": 0.6566265060240963, "term": "utilizes", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 12, "s": 0.31008243500317056, "os": 0.008957280661460731, "bg": 3.0154819871021134e-05}, {"x": 0.9597336715282181, "y": 0.9156626506024096, "ox": 0.9597336715282181, "oy": 0.9156626506024096, "term": "pre", "cat25k": 14, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 98, "ncat": 76, "s": 0.0031705770450221942, "os": -0.09375993781138475, "bg": 4.897050351781301e-06}, {"x": 0.9936588459099556, "y": 0.9920735573874445, "ox": 0.9936588459099556, "oy": 0.9920735573874445, "term": "-", "cat25k": 105, "ncat25k": 135, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 760, "ncat": 345, "s": 0.6220672162333545, "os": 0.031783329210981925, "bg": 0.0}, {"x": 0.9793912492073558, "y": 0.9679771718452759, "ox": 0.9793912492073558, "oy": 0.9679771718452759, "term": "trained", "cat25k": 37, "ncat25k": 53, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 266, "ncat": 134, "s": 0.5913126188966392, "os": 0.029168580615525852, "bg": 5.780604107812892e-05}, {"x": 0.9895370957514268, "y": 0.9793912492073558, "ox": 0.9895370957514268, "oy": 0.9793912492073558, "term": "network", "cat25k": 51, "ncat25k": 99, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 368, "ncat": 252, "s": 0.0326569435637286, "os": -0.035051764955301934, "bg": 5.500098862059264e-06}, {"x": 0.8259353202282815, "y": 0.7910589727330374, "ox": 0.8259353202282815, "oy": 0.7910589727330374, "term": "gaussian", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 18, "s": 0.5719720989220038, "os": 0.02743719303204832, "bg": 5.30219542886762e-05}, {"x": 0.41122384273937856, "y": 0.5031705770450222, "ox": 0.41122384273937856, "oy": 0.5031705770450222, "term": "mixture", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 4.551886301683418e-06}, {"x": 0.9917564996829423, "y": 0.9901712111604312, "ox": 0.9917564996829423, "oy": 0.9901712111604312, "term": "model", "cat25k": 99, "ncat25k": 114, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 717, "ncat": 290, "s": 0.736842105263158, "os": 0.04545775767640725, "bg": 1.2475406213999834e-05}, {"x": 0.8050095117311351, "y": 0.7222574508560557, "ox": 0.8050095117311351, "oy": 0.7222574508560557, "term": "generate", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 16, "s": 0.27774254914394414, "os": 0.006271863185046467, "bg": 5.008596275564256e-06}, {"x": 0.963855421686747, "y": 0.965440710209258, "ox": 0.963855421686747, "oy": 0.965440710209258, "term": "more", "cat25k": 34, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 244, "ncat": 86, "s": 0.9676601141407737, "os": 0.18382742659269996, "bg": 4.2718366833340896e-07}, {"x": 0.9080532656943564, "y": 0.8963221306277742, "ox": 0.9080532656943564, "oy": 0.8963221306277742, "term": "robust", "cat25k": 11, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 32, "s": 0.8652504755865568, "os": 0.07913147945302287, "bg": 3.0298951913263844e-05}, {"x": 0.05802155992390615, "y": 0.3268864933417882, "ox": 0.05802155992390615, "oy": 0.3268864933417882, "term": "compact", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 9.393994215961194e-07}, {"x": 0.9803424223208624, "y": 0.982561826252378, "ox": 0.9803424223208624, "oy": 0.982561826252378, "term": "features", "cat25k": 58, "ncat25k": 55, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 418, "ncat": 141, "s": 0.9445149017121116, "os": 0.14773329564326354, "bg": 6.9358053188680976e-06}, {"x": 0.9039315155358275, "y": 0.9318325935320229, "ox": 0.9039315155358275, "oy": 0.9318325935320229, "term": "our method", "cat25k": 17, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 121, "ncat": 31, "s": 0.9809765377298668, "os": 0.2132433482915798, "bg": 0.0}, {"x": 0.9587824984147115, "y": 0.9159797083069119, "ox": 0.9587824984147115, "oy": 0.9159797083069119, "term": "pre -", "cat25k": 14, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 98, "ncat": 75, "s": 0.004755865567533291, "os": -0.08826543231687928, "bg": 0.0}, {"x": 0.9467343056436271, "y": 0.8446417247939125, "ox": 0.9467343056436271, "oy": 0.8446417247939125, "term": "- trained", "cat25k": 8, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 60, "s": 0.0, "os": -0.1551005264831631, "bg": 0.0}, {"x": 0.9708306911857958, "y": 0.950856055802156, "ox": 0.9708306911857958, "oy": 0.950856055802156, "term": "neural network", "cat25k": 24, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 176, "ncat": 105, "s": 0.0944831959416614, "os": -0.015282145507225842, "bg": 0.0}, {"x": 0.4904882688649334, "y": 0.2010145846544071, "ox": 0.4904882688649334, "oy": 0.2010145846544071, "term": "more robust", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 0.0}, {"x": 0.9470513633481293, "y": 0.9461001902346227, "ox": 0.9470513633481293, "oy": 0.9461001902346227, "term": "effectiveness", "cat25k": 22, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 157, "ncat": 60, "s": 0.9441978440076094, "os": 0.1471502773753578, "bg": 2.879930902892844e-05}, {"x": 0.9156626506024096, "y": 0.8801521876981611, "ox": 0.9156626506024096, "oy": 0.8801521876981611, "term": "was", "cat25k": 10, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 34, "s": 0.6632847178186431, "os": 0.035988127627998995, "bg": 1.4019387670028347e-07}, {"x": 0.8351299936588459, "y": 0.8129359543436906, "ox": 0.8351299936588459, "oy": 0.8129359543436906, "term": "evaluated", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 19, "s": 0.681991122384274, "os": 0.03801985795554927, "bg": 1.216803717107205e-05}, {"x": 0.9784400760938491, "y": 0.9819277108433735, "ox": 0.9784400760938491, "oy": 0.9819277108433735, "term": "two", "cat25k": 56, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 409, "ncat": 130, "s": 0.9629042485732403, "os": 0.17426946044309388, "bg": 2.4409581362999324e-06}, {"x": 0.4115409004438808, "y": 0.5713379835129994, "ox": 0.4115409004438808, "oy": 0.5713379835129994, "term": "evaluated on", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.61572606214331, "os": 0.03134164870499275, "bg": 0.0}, {"x": 0.9606848446417248, "y": 0.9587824984147115, "ox": 0.9606848446417248, "oy": 0.9587824984147115, "term": "experimental", "cat25k": 29, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 208, "ncat": 77, "s": 0.9584654407102092, "os": 0.16762658563301652, "bg": 2.908758521960795e-05}, {"x": 0.9873176918199112, "y": 0.9847812301838935, "ox": 0.9873176918199112, "oy": 0.9847812301838935, "term": "results", "cat25k": 67, "ncat25k": 80, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 483, "ncat": 204, "s": 0.857006975269499, "os": 0.07549203208367206, "bg": 5.1189883126567195e-06}, {"x": 0.32149651236525045, "y": 0.3719086873811034, "ox": 0.32149651236525045, "oy": 0.3719086873811034, "term": "suggest", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 8.65865665163145e-07}, {"x": 0.9952441344324667, "y": 0.9952441344324667, "ox": 0.9952441344324667, "oy": 0.9952441344324667, "term": "that", "cat25k": 210, "ncat25k": 186, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1526, "ncat": 474, "s": 0.6984781230183894, "os": 0.03968057665806868, "bg": 1.1763792510362986e-06}, {"x": 0.9911223842739378, "y": 0.9917564996829423, "ox": 0.9911223842739378, "oy": 0.9917564996829423, "term": "can", "cat25k": 103, "ncat25k": 106, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 750, "ncat": 271, "s": 0.849714648065948, "os": 0.07252393908342458, "bg": 1.6433874873613079e-06}, {"x": 0.12555485098287889, "y": 0.6284083703233989, "ox": 0.12555485098287889, "oy": 0.6284083703233989, "term": "potentially", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 2, "s": 0.8046924540266329, "os": 0.0574714674393131, "bg": 4.455713273365621e-06}, {"x": 0.7136968928344959, "y": 0.8344958782498415, "ox": 0.7136968928344959, "oy": 0.8344958782498415, "term": "outperform", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 11, "s": 0.9007609384908053, "os": 0.10126850641320094, "bg": 0.00014138562473062617}, {"x": 0.9743183259353202, "y": 0.9752694990488269, "ox": 0.9743183259353202, "oy": 0.9752694990488269, "term": "state", "cat25k": 44, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 316, "ncat": 114, "s": 0.9575142675967027, "os": 0.1649058337161231, "bg": 1.8970499869031858e-06}, {"x": 0.971147748890298, "y": 0.9660748256182625, "ox": 0.971147748890298, "oy": 0.9660748256182625, "term": "art", "cat25k": 34, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 247, "ncat": 106, "s": 0.9074191502853519, "os": 0.10784071234232007, "bg": 3.2847303202210544e-06}, {"x": 0.9600507292327204, "y": 0.9438807863031071, "ox": 0.9600507292327204, "oy": 0.9438807863031071, "term": "techniques", "cat25k": 21, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 152, "ncat": 76, "s": 0.790424857324033, "os": 0.05415003003427443, "bg": 1.1256657442708118e-05}, {"x": 0.9511731135066582, "y": 0.9470513633481293, "ox": 0.9511731135066582, "oy": 0.9470513633481293, "term": "experimental results", "cat25k": 22, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 163, "ncat": 64, "s": 0.9375396322130628, "os": 0.13709762905904382, "bg": 0.0}, {"x": 0.21940393151553583, "y": 0.20133164235890932, "ox": 0.21940393151553583, "oy": 0.20133164235890932, "term": "suggest that", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.411857958148383, "y": 0.5716550412175015, "ox": 0.411857958148383, "oy": 0.5716550412175015, "term": "mri", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.61572606214331, "os": 0.03134164870499275, "bg": 1.4536772127919959e-05}, {"x": 0.49080532656943565, "y": 0.03170577045022194, "ox": 0.49080532656943565, "oy": 0.03170577045022194, "term": "abnormal", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 5.245308786559801e-06}, {"x": 0.7140139505389981, "y": 0.8195941661382372, "ox": 0.7140139505389981, "oy": 0.8195941661382372, "term": "brain", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 11, "s": 0.8779327837666455, "os": 0.0851913359951945, "bg": 3.3684237399059196e-06}, {"x": 0.9771718452758402, "y": 0.9556119213696893, "ox": 0.9771718452758402, "oy": 0.9556119213696893, "term": "cnn", "cat25k": 27, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 194, "ncat": 126, "s": 0.018706404565630944, "os": -0.05040457934348619, "bg": 6.786466174662008e-05}, {"x": 0.1258719086873811, "y": 0.137285986049461, "ox": 0.1258719086873811, "oy": 0.137285986049461, "term": "magnetic", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 1.0127684785938722e-06}, {"x": 0.12618896639188332, "y": 0.08592263792010146, "ox": 0.12618896639188332, "oy": 0.08592263792010146, "term": "resonance", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 4.161049250178925e-06}, {"x": 0.686429930247305, "y": 0.7790107799619531, "ox": 0.686429930247305, "oy": 0.7790107799619531, "term": "imaging", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 10, "s": 0.829422954977806, "os": 0.06496236882088971, "bg": 5.5841695772696384e-06}, {"x": 0.9619530754597336, "y": 0.9540266328471781, "ox": 0.9619530754597336, "oy": 0.9540266328471781, "term": "most", "cat25k": 26, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 189, "ncat": 80, "s": 0.9134432466708942, "os": 0.11255786014628455, "bg": 1.2918979494174504e-06}, {"x": 0.9270767279644896, "y": 0.9419784400760939, "ox": 0.9270767279644896, "oy": 0.9419784400760939, "term": "common", "cat25k": 20, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 145, "ncat": 39, "s": 0.9873176918199112, "os": 0.2335959860075616, "bg": 3.4051895885106905e-06}, {"x": 0.9362714013950539, "y": 0.901712111604312, "ox": 0.9362714013950539, "oy": 0.901712111604312, "term": "technique", "cat25k": 12, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 85, "ncat": 48, "s": 0.2926442612555485, "os": 0.0072965619589414055, "bg": 1.3954683267773427e-05}, {"x": 0.9800253646163601, "y": 0.9774889029803424, "ox": 0.9800253646163601, "oy": 0.9774889029803424, "term": "used", "cat25k": 46, "ncat25k": 55, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 333, "ncat": 140, "s": 0.901712111604312, "os": 0.1017808558001484, "bg": 2.2434632181383272e-06}, {"x": 0.6867469879518072, "y": 0.6442612555485099, "ox": 0.6867469879518072, "oy": 0.6442612555485099, "term": "detect", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 10, "s": 0.42802790107799615, "os": 0.016730857566870434, "bg": 7.619067848607275e-06}, {"x": 0.12650602409638553, "y": 0.08623969562460368, "ox": 0.12650602409638553, "oy": 0.08623969562460368, "term": "magnetic resonance", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.7143310082435003, "y": 0.3722257450856056, "ox": 0.7143310082435003, "oy": 0.3722257450856056, "term": "analyzed", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 11, "s": 0.05960684844641725, "os": -0.024133422847249217, "bg": 7.220716423786358e-06}, {"x": 0.6049461001902346, "y": 0.44705136334812934, "ox": 0.6049461001902346, "oy": 0.44705136334812934, "term": "manually", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 7.261458209730387e-06}, {"x": 0.992707672796449, "y": 0.9936588459099556, "ox": 0.992707672796449, "oy": 0.9936588459099556, "term": "by", "cat25k": 134, "ncat25k": 129, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 975, "ncat": 329, "s": 0.8281547241597972, "os": 0.06434401611250484, "bg": 7.78442153248876e-07}, {"x": 0.8170577045022194, "y": 0.8547875713379836, "ox": 0.8170577045022194, "oy": 0.8547875713379836, "term": "conditions", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 17, "s": 0.8899809765377299, "os": 0.09402494611497826, "bg": 8.984090617699022e-07}, {"x": 0.41217501585288524, "y": 0.02092580849714648, "ox": 0.41217501585288524, "oy": 0.02092580849714648, "term": "are analyzed", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 0.0}, {"x": 0.6870640456563094, "y": 0.611287254280279, "ox": 0.6870640456563094, "oy": 0.611287254280279, "term": "manual", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 10, "s": 0.3487634749524413, "os": 0.010299989399667857, "bg": 1.2417568624441232e-06}, {"x": 0.21972098922003805, "y": 0.04850982878883957, "ox": 0.21972098922003805, "oy": 0.04850982878883957, "term": "interpretation", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.1018605159669997e-06}, {"x": 0.6487000634115409, "y": 0.2656943563728599, "ox": 0.6487000634115409, "oy": 0.2656943563728599, "term": "huge", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 9, "s": 0.0735573874445149, "os": -0.019575280025440796, "bg": 5.361897307926821e-07}, {"x": 0.7365250475586557, "y": 0.7032339885859227, "ox": 0.7365250475586557, "oy": 0.7032339885859227, "term": "consuming", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 12, "s": 0.5003170577045022, "os": 0.021819016995865873, "bg": 1.5225821775665785e-05}, {"x": 0.8487634749524413, "y": 0.8592263792010146, "ox": 0.8487634749524413, "oy": 0.8592263792010146, "term": "difficult", "cat25k": 8, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 21, "s": 0.862714013950539, "os": 0.07847779230415887, "bg": 3.843695197314569e-06}, {"x": 0.7368421052631579, "y": 0.6721623335447051, "ox": 0.7368421052631579, "oy": 0.6721623335447051, "term": "time consuming", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 12, "s": 0.36525047558655677, "os": 0.012172714745062013, "bg": 0.0}, {"x": 0.3218135700697527, "y": 0.6724793912492073, "ox": 0.3218135700697527, "oy": 0.6724793912492073, "term": "hence", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 4, "s": 0.798985415345593, "os": 0.056128758701105966, "bg": 3.809962987523356e-06}, {"x": 0.9346861128725428, "y": 0.9197844007609385, "ox": 0.9346861128725428, "oy": 0.9197844007609385, "term": "computer", "cat25k": 14, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 104, "ncat": 46, "s": 0.865567533291059, "os": 0.07937882053637679, "bg": 1.3368502655378998e-06}, {"x": 0.9790741915028536, "y": 0.9644895370957515, "ox": 0.9790741915028536, "oy": 0.9644895370957515, "term": "detection", "cat25k": 32, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 233, "ncat": 133, "s": 0.1376030437539632, "os": -0.007137556976785198, "bg": 4.114456999034227e-05}, {"x": 0.32213062777425494, "y": 0.41058972733037413, "ox": 0.32213062777425494, "oy": 0.41058972733037413, "term": "helps", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 1.1394524970967422e-06}, {"x": 0.8053265694356373, "y": 0.6810399492707673, "ox": 0.8053265694356373, "oy": 0.6810399492707673, "term": "fast", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 16, "s": 0.13982244768547875, "os": -0.006589873149358674, "bg": 8.919811278880491e-07}, {"x": 0.87856689917565, "y": 0.8132530120481928, "ox": 0.87856689917565, "oy": 0.8132530120481928, "term": "diagnosis", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 26, "s": 0.19467343056436273, "os": -0.00044168050598919484, "bg": 8.417457522367615e-06}, {"x": 0.940710209258085, "y": 0.9365884590995561, "ox": 0.940710209258085, "oy": 0.9365884590995561, "term": "study", "cat25k": 18, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 128, "ncat": 52, "s": 0.9166138237159163, "os": 0.11715133740857214, "bg": 2.3497250325658835e-06}, {"x": 0.9901712111604312, "y": 0.9911223842739378, "ox": 0.9901712111604312, "oy": 0.9911223842739378, "term": "an", "cat25k": 102, "ncat25k": 100, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 742, "ncat": 254, "s": 0.8728598604946101, "os": 0.08257658739973861, "bg": 1.31182211814304e-06}, {"x": 0.9809765377298668, "y": 0.9863665187064046, "ox": 0.9809765377298668, "oy": 0.9863665187064046, "term": "approach", "cat25k": 75, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 544, "ncat": 144, "s": 0.9679771718452759, "os": 0.18497579590827185, "bg": 2.1369805461932806e-05}, {"x": 0.7371591629676602, "y": 0.8595434369055168, "ox": 0.7371591629676602, "oy": 0.8595434369055168, "term": "automatically", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 12, "s": 0.9280279010779963, "os": 0.12792834175470832, "bg": 4.11767619113373e-06}, {"x": 0.41249207355738743, "y": 0.20164870006341154, "ox": 0.41249207355738743, "oy": 0.20164870006341154, "term": "normal", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 4.940566920878833e-07}, {"x": 0.5542168674698795, "y": 0.011731135066582118, "ox": 0.5542168674698795, "oy": 0.011731135066582118, "term": "mr", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 3.576338245207412e-07}, {"x": 0.8554216867469879, "y": 0.7121116043119848, "ox": 0.8554216867469879, "oy": 0.7121116043119848, "term": "network cnn", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 22, "s": 0.043436905516804056, "os": -0.02991060386558779, "bg": 0.0}, {"x": 0.9083703233988586, "y": 0.8855421686746988, "ox": 0.9083703233988586, "oy": 0.8855421686746988, "term": "current", "cat25k": 10, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 32, "s": 0.8008877615726063, "os": 0.05662344086781387, "bg": 9.667701577240396e-07}, {"x": 0.9752694990488269, "y": 0.976854787571338, "ox": 0.9752694990488269, "oy": 0.976854787571338, "term": "such", "cat25k": 45, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 328, "ncat": 118, "s": 0.9556119213696893, "os": 0.1612840535670117, "bg": 2.3414702574740546e-06}, {"x": 0.8354470513633482, "y": 0.8348129359543437, "ox": 0.8354470513633482, "oy": 0.8348129359543437, "term": "optimal", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 19, "s": 0.803424223208624, "os": 0.05731246245715699, "bg": 1.3254455437858093e-05}, {"x": 0.8557387444514901, "y": 0.7850348763474952, "ox": 0.8557387444514901, "oy": 0.7850348763474952, "term": "rate", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 22, "s": 0.22986683576410907, "os": 0.00224373697042507, "bg": 5.869190503253355e-07}, {"x": 0.9454660748256183, "y": 0.9172479391249208, "ox": 0.9454660748256183, "oy": 0.9172479391249208, "term": "fine", "cat25k": 14, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 99, "ncat": 57, "s": 0.24096385542168675, "os": 0.002862089678809965, "bg": 4.321122381623164e-06}, {"x": 0.9121750158528852, "y": 0.8744451490171211, "ox": 0.9121750158528852, "oy": 0.8744451490171211, "term": "tuning", "cat25k": 9, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 33, "s": 0.6223842739378567, "os": 0.03183633087170065, "bg": 2.5934887870512293e-05}, {"x": 0.9375396322130628, "y": 0.9242232086239696, "ox": 0.9375396322130628, "oy": 0.9242232086239696, "term": "train", "cat25k": 15, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 109, "ncat": 49, "s": 0.8639822447685479, "os": 0.07897247447086675, "bg": 9.86020630110243e-06}, {"x": 0.9426125554850983, "y": 0.940710209258085, "ox": 0.9426125554850983, "oy": 0.940710209258085, "term": "such as", "cat25k": 19, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 138, "ncat": 54, "s": 0.9359543436905517, "os": 0.13510123317197276, "bg": 0.0}, {"x": 0.9042485732403297, "y": 0.8370323398858592, "ox": 0.9042485732403297, "oy": 0.8370323398858592, "term": "fine tuning", "cat25k": 7, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 31, "s": 0.14648065948002537, "os": -0.00540616939330768, "bg": 0.0}, {"x": 0.9201014584654407, "y": 0.8728598604946101, "ox": 0.9201014584654407, "oy": 0.8728598604946101, "term": "achieved", "cat25k": 9, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 36, "s": 0.3649334178820545, "os": 0.01213738030458289, "bg": 1.1780117863543965e-05}, {"x": 0.9679771718452759, "y": 0.9610019023462271, "ox": 0.9679771718452759, "oy": 0.9610019023462271, "term": "accuracy", "cat25k": 30, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 220, "ncat": 97, "s": 0.9048826886493342, "os": 0.10450160771704176, "bg": 2.1298596956605182e-05}, {"x": 0.9714648065948003, "y": 0.9473684210526315, "ox": 0.9714648065948003, "oy": 0.9473684210526315, "term": "%", "cat25k": 23, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 164, "ncat": 106, "s": 0.02092580849714648, "os": -0.04650012367054163, "bg": 0.0}, {"x": 0.8173747622067217, "y": 0.8056436271401395, "ox": 0.8173747622067217, "oy": 0.8056436271401395, "term": "classification accuracy", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 17, "s": 0.7175015852885226, "os": 0.0425780007773577, "bg": 0.0}, {"x": 0.9124920735573875, "y": 0.7793278376664553, "ox": 0.9124920735573875, "oy": 0.7793278376664553, "term": "developed", "cat25k": 5, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 33, "s": 0.011731135066582118, "os": -0.06141125755273666, "bg": 2.2368773041608405e-06}, {"x": 0.965440710209258, "y": 0.9629042485732403, "ox": 0.965440710209258, "oy": 0.9629042485732403, "term": "system", "cat25k": 31, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 227, "ncat": 91, "s": 0.9388078630310717, "os": 0.13934136602946895, "bg": 1.6011833933576535e-06}, {"x": 0.937856689917565, "y": 0.8484464172479391, "ox": 0.937856689917565, "oy": 0.8484464172479391, "term": "database", "cat25k": 8, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 49, "s": 0.004121750158528852, "os": -0.09144553196000144, "bg": 2.0199566134480883e-06}, {"x": 0.6873811033608117, "y": 0.4109067850348763, "ox": 0.6873811033608117, "oy": 0.4109067850348763, "term": "assist", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 10, "s": 0.09353202282815473, "os": -0.015423483269142432, "bg": 1.5234468236084051e-06}, {"x": 0.9549778059606848, "y": 0.9410272669625872, "ox": 0.9549778059606848, "oy": 0.9410272669625872, "term": "their", "cat25k": 19, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 139, "ncat": 69, "s": 0.817691819911224, "os": 0.06139359033249708, "bg": 5.31235293777287e-07}, {"x": 0.49112238427393784, "y": 0.0865567533291059, "ox": 0.49112238427393784, "oy": 0.0865567533291059, "term": "screening", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 6, "s": 0.10589727330374128, "os": -0.012738065792728175, "bg": 1.7956142122864902e-06}, {"x": 0.8788839568801522, "y": 0.9245402663284717, "ox": 0.8788839568801522, "oy": 0.9245402663284717, "term": "kernel", "cat25k": 15, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 109, "ncat": 26, "s": 0.9765377298668358, "os": 0.20534610084449315, "bg": 1.243742191775004e-05}, {"x": 0.839568801521877, "y": 0.8256182625237793, "ox": 0.839568801521877, "oy": 0.8256182625237793, "term": "embedding", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 20, "s": 0.7143310082435004, "os": 0.04217165471184764, "bg": 7.674757328123527e-05}, {"x": 0.9590995561192137, "y": 0.9568801521876982, "ox": 0.9590995561192137, "oy": 0.9568801521876982, "term": "supervised", "cat25k": 27, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 197, "ncat": 75, "s": 0.953709575142676, "os": 0.15932299212041978, "bg": 0.00016129156593356093}, {"x": 0.9866835764109068, "y": 0.9876347495244134, "ox": 0.9866835764109068, "oy": 0.9876347495244134, "term": "paper", "cat25k": 84, "ncat25k": 79, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 610, "ncat": 201, "s": 0.9099556119213696, "os": 0.10992544433058904, "bg": 1.1999604738050589e-05}, {"x": 0.920418516169943, "y": 0.9191502853519341, "ox": 0.920418516169943, "oy": 0.9191502853519341, "term": "called", "cat25k": 14, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 103, "ncat": 36, "s": 0.9315155358275207, "os": 0.13110844139783046, "bg": 2.5930617446897334e-06}, {"x": 0.9860494610019024, "y": 0.9873176918199112, "ox": 0.9860494610019024, "oy": 0.9873176918199112, "term": "this paper", "cat25k": 82, "ncat25k": 75, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 592, "ncat": 191, "s": 0.9169308814204186, "os": 0.11769902123599874, "bg": 0.0}, {"x": 0.9644895370957515, "y": 0.9806594800253646, "ox": 0.9644895370957515, "oy": 0.9806594800253646, "term": "paper we", "cat25k": 52, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 381, "ncat": 88, "s": 0.9977805960684845, "os": 0.3207483834493481, "bg": 0.0}, {"x": 0.9610019023462271, "y": 0.9625871908687381, "ox": 0.9610019023462271, "oy": 0.9625871908687381, "term": "a novel", "cat25k": 31, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 225, "ncat": 77, "s": 0.9730500951173113, "os": 0.19656549238542803, "bg": 0.0}, {"x": 0.7146480659480026, "y": 0.722574508560558, "ox": 0.7146480659480026, "oy": 0.722574508560558, "term": "advantages", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 11, "s": 0.6410906785034877, "os": 0.03374439065757394, "bg": 6.40618643861893e-06}, {"x": 0.6052631578947368, "y": 0.6569435637285986, "ox": 0.6052631578947368, "oy": 0.6569435637285986, "term": "unified", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 8, "s": 0.6144578313253012, "os": 0.030935302639482708, "bg": 8.050624338496552e-06}, {"x": 0.9749524413443247, "y": 0.979708306911858, "ox": 0.9749524413443247, "oy": 0.979708306911858, "term": "framework", "cat25k": 51, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 372, "ncat": 116, "s": 0.9736842105263158, "os": 0.19893289989753005, "bg": 2.9889862898935114e-05}, {"x": 0.5545339251743817, "y": 0.5932149651236525, "ox": 0.5545339251743817, "oy": 0.5932149651236525, "term": "a unified", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 7, "s": 0.5180722891566265, "os": 0.023568071799583055, "bg": 0.0}, {"x": 0.605580215599239, "y": 0.7314521242866202, "ox": 0.605580215599239, "oy": 0.7314521242866202, "term": "represented", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 8, "s": 0.7891566265060241, "os": 0.05344334122469171, "bg": 4.6010589868149575e-06}, {"x": 0.7374762206721623, "y": 0.6287254280279011, "ox": 0.7374762206721623, "oy": 0.6287254280279011, "term": "newly", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 12, "s": 0.23906150919467342, "os": 0.002526412494258154, "bg": 3.1875130488815435e-06}, {"x": 0.8560558021559924, "y": 0.7974001268230818, "ox": 0.8560558021559924, "oy": 0.7974001268230818, "term": "designed", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 22, "s": 0.30500951173113505, "os": 0.008674605137627661, "bg": 1.503751663480523e-06}, {"x": 0.9327837666455295, "y": 0.839568801521877, "ox": 0.9327837666455295, "oy": 0.839568801521877, "term": "architecture", "cat25k": 7, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 44, "s": 0.008560558021559923, "os": -0.0736193067382778, "bg": 4.651675021418686e-06}, {"x": 0.22003804692454026, "y": 0.5935320228281548, "ox": 0.22003804692454026, "oy": 0.5935320228281548, "term": "represented by", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 3, "s": 0.7371591629676602, "os": 0.04554609377760503, "bg": 0.0}, {"x": 0.9486366518706405, "y": 0.9413443246670894, "ox": 0.9486366518706405, "oy": 0.9413443246670894, "term": "compared", "cat25k": 19, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 139, "ncat": 62, "s": 0.8982244768547876, "os": 0.09985512879403552, "bg": 1.152876995939808e-05}, {"x": 0.22035510462904248, "y": 0.41122384273937856, "ox": 0.22035510462904248, "oy": 0.41122384273937856, "term": "kernels", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 1.7080686494141057e-05}, {"x": 0.9879518072289156, "y": 0.9885859226379201, "ox": 0.9879518072289156, "oy": 0.9885859226379201, "term": "be", "cat25k": 92, "ncat25k": 85, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 667, "ncat": 218, "s": 0.8994927076727964, "os": 0.10080915868697216, "bg": 7.378209739508725e-07}, {"x": 0.32244768547875713, "y": 0.5485098287888396, "ox": 0.32244768547875713, "oy": 0.5485098287888396, "term": "explicitly", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 4, "s": 0.6385542168674699, "os": 0.033620720115896965, "bg": 6.641615699813461e-06}, {"x": 0.8623969562460367, "y": 0.8259353202282815, "ox": 0.8623969562460367, "oy": 0.8259353202282815, "term": "map", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 23, "s": 0.543436905516804, "os": 0.02568813822833116, "bg": 4.582011871966946e-07}, {"x": 0.4128091312618897, "y": 0.7035510462904249, "ox": 0.4128091312618897, "oy": 0.7035510462904249, "term": "optimized", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 5, "s": 0.8132530120481929, "os": 0.06028055545740433, "bg": 1.2867282944533433e-05}, {"x": 0.9571972098922004, "y": 0.9381737476220672, "ox": 0.9571972098922004, "oy": 0.9381737476220672, "term": "level", "cat25k": 18, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 132, "ncat": 72, "s": 0.5431198478123018, "os": 0.025617469347372912, "bg": 2.0118866206423274e-06}, {"x": 0.9492707672796449, "y": 0.9682942295497781, "ox": 0.9492707672796449, "oy": 0.9682942295497781, "term": "space", "cat25k": 37, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 267, "ncat": 63, "s": 0.998414711477489, "os": 0.329087311402424, "bg": 5.421546561708154e-06}, {"x": 0.9629042485732403, "y": 0.9686112872542803, "ox": 0.9629042485732403, "oy": 0.9686112872542803, "term": "where", "cat25k": 37, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 268, "ncat": 81, "s": 0.9885859226379201, "os": 0.23889615207943182, "bg": 1.9351284505274017e-06}, {"x": 0.9337349397590361, "y": 0.9397590361445783, "ox": 0.9337349397590361, "oy": 0.9397590361445783, "term": "may", "cat25k": 19, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 135, "ncat": 45, "s": 0.9651236525047558, "os": 0.17812091445531963, "bg": 4.3475473083613645e-07}, {"x": 0.8855421686746988, "y": 0.8522511097019657, "ox": 0.8855421686746988, "oy": 0.8522511097019657, "term": "application", "cat25k": 8, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 27, "s": 0.6607482561826252, "os": 0.03586445708632202, "bg": 1.1110561973401026e-06}, {"x": 0.8953709575142677, "y": 0.8373493975903614, "ox": 0.8953709575142677, "oy": 0.8373493975903614, "term": "compared with", "cat25k": 7, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 29, "s": 0.2720355104629042, "os": 0.005582841595703325, "bg": 0.0}, {"x": 0.9762206721623335, "y": 0.9736842105263158, "ox": 0.9762206721623335, "oy": 0.9736842105263158, "term": "can be", "cat25k": 43, "ncat25k": 47, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 310, "ncat": 120, "s": 0.9235890932149652, "os": 0.1245715699091905, "bg": 0.0}, {"x": 0.7149651236525048, "y": 0.6813570069752695, "ox": 0.7149651236525048, "oy": 0.6813570069752695, "term": "high level", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 11, "s": 0.48065948002536457, "os": 0.020882654323168798, "bg": 0.0}, {"x": 0.8262523779327837, "y": 0.8985415345592898, "ox": 0.8262523779327837, "oy": 0.8985415345592898, "term": "feature space", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 82, "ncat": 18, "s": 0.9562460367786938, "os": 0.16248542454330234, "bg": 0.0}, {"x": 0.12682308180088775, "y": 0.4115409004438808, "ox": 0.12682308180088775, "oy": 0.4115409004438808, "term": "may have", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 0.0}, {"x": 0.5548509828788839, "y": 0.5488268864933418, "ox": 0.5548509828788839, "oy": 0.5488268864933418, "term": "typical", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.42897907419150283, "os": 0.017137203632380478, "bg": 2.389610166166322e-06}, {"x": 0.9765377298668357, "y": 0.9755865567533291, "ox": 0.9765377298668357, "oy": 0.9755865567533291, "term": "or", "cat25k": 44, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 320, "ncat": 125, "s": 0.9384908053265695, "os": 0.13836966891629277, "bg": 3.435005330514604e-07}, {"x": 0.4914394419784401, "y": 0.4473684210526316, "ox": 0.4914394419784401, "oy": 0.4473684210526316, "term": "logistic", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 2.5622039053111923e-05}, {"x": 0.9273937856689918, "y": 0.9350031705770451, "ox": 0.9273937856689918, "oy": 0.9350031705770451, "term": "regression", "cat25k": 17, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 125, "ncat": 39, "s": 0.966708941027267, "os": 0.18214904066994098, "bg": 5.011194213415151e-05}, {"x": 0.6058972733037413, "y": 0.549143944197844, "ox": 0.6058972733037413, "oy": 0.549143944197844, "term": "top", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 8, "s": 0.35954343690551677, "os": 0.01164269813787499, "bg": 1.0733939757030545e-07}, {"x": 0.8994927076727964, "y": 0.9083703233988586, "ox": 0.8994927076727964, "oy": 0.9083703233988586, "term": "layer", "cat25k": 12, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 30, "s": 0.922003804692454, "os": 0.12227483127804673, "bg": 8.841970120625104e-06}, {"x": 0.5551680405833862, "y": 0.6116043119847813, "ox": 0.5551680405833862, "oy": 0.6116043119847813, "term": "expected", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 7, "s": 0.5672162333544705, "os": 0.026783505883184337, "bg": 1.026061334941709e-06}, {"x": 0.9778059606848446, "y": 0.984147114774889, "ox": 0.9778059606848446, "oy": 0.984147114774889, "term": "new", "cat25k": 66, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 478, "ncat": 129, "s": 0.979708306911858, "os": 0.21191830677361223, "bg": 7.824735555585216e-07}, {"x": 0.41312618896639186, "y": 0.411857958148383, "ox": 0.41312618896639186, "oy": 0.411857958148383, "term": "logistic regression", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 0.0}, {"x": 0.9755865567533291, "y": 0.981293595434369, "ox": 0.9755865567533291, "oy": 0.981293595434369, "term": "show", "cat25k": 55, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 399, "ncat": 118, "s": 0.9771718452758402, "os": 0.2063001307374297, "bg": 4.173168155247991e-06}, {"x": 0.7377932783766645, "y": 0.6727964489537096, "ox": 0.7377932783766645, "oy": 0.6727964489537096, "term": "superior", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 12, "s": 0.36525047558655677, "os": 0.012172714745062013, "bg": 3.722536608883844e-06}, {"x": 0.9870006341154091, "y": 0.9870006341154091, "ox": 0.9870006341154091, "oy": 0.9870006341154091, "term": "performance", "cat25k": 80, "ncat25k": 79, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 579, "ncat": 201, "s": 0.9032974001268231, "os": 0.10349457616338653, "bg": 1.1147454512347384e-05}, {"x": 0.9457831325301205, "y": 0.9492707672796449, "ox": 0.9457831325301205, "oy": 0.9492707672796449, "term": "than", "cat25k": 23, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 168, "ncat": 57, "s": 0.9686112872542804, "os": 0.18614183244408322, "bg": 8.949161923116987e-07}, {"x": 0.6490171211160431, "y": 0.3272035510462904, "ox": 0.6490171211160431, "oy": 0.3272035510462904, "term": "identity", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.0897273303741281, "os": -0.016359845941839507, "bg": 1.4922915307904135e-06}, {"x": 0.9850982878883957, "y": 0.9889029803424223, "ox": 0.9850982878883957, "oy": 0.9889029803424223, "term": "different", "cat25k": 93, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 678, "ncat": 179, "s": 0.948002536461636, "os": 0.15347514222112302, "bg": 9.520848945549037e-06}, {"x": 0.8681039949270767, "y": 0.852568167406468, "ox": 0.8681039949270767, "oy": 0.852568167406468, "term": "types", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 24, "s": 0.7821813570069753, "os": 0.0523479735698385, "bg": 2.121557372137618e-06}, {"x": 0.8998097653772986, "y": 0.8449587824984147, "ox": 0.8998097653772986, "oy": 0.8449587824984147, "term": "including", "cat25k": 8, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 30, "s": 0.33449587824984145, "os": 0.009734638352001695, "bg": 7.928127365496707e-07}, {"x": 0.4134432466708941, "y": 0.6731135066582118, "ox": 0.4134432466708941, "oy": 0.6731135066582118, "term": "sequences", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 5, "s": 0.7707672796448954, "os": 0.05063425320660047, "bg": 5.4437401252822354e-06}, {"x": 0.41376030437539635, "y": 0.5034876347495244, "ox": 0.41376030437539635, "oy": 0.5034876347495244, "term": "structured", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 4.783407861485265e-06}, {"x": 0.9324667089410272, "y": 0.9058338617628409, "ox": 0.9324667089410272, "oy": 0.9058338617628409, "term": "results show", "cat25k": 12, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 43, "s": 0.7314521242866202, "os": 0.04441539168227274, "bg": 0.0}, {"x": 0.9651236525047558, "y": 0.9692454026632847, "ox": 0.9651236525047558, "oy": 0.9692454026632847, "term": "show that", "cat25k": 37, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 271, "ncat": 89, "s": 0.9800253646163603, "os": 0.21235998727960148, "bg": 0.0}, {"x": 0.41407736207989854, "y": 0.26601141407736206, "ox": 0.41407736207989854, "oy": 0.26601141407736206, "term": "superior performance", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 0.0}, {"x": 0.4143944197844008, "y": 0.4787571337983513, "ox": 0.4143944197844008, "oy": 0.4787571337983513, "term": "different types", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 5, "s": 0.45244134432466704, "os": 0.01847991237058761, "bg": 0.0}, {"x": 0.8056436271401395, "y": 0.8931515535827521, "ox": 0.8056436271401395, "oy": 0.8931515535827521, "term": "c", "cat25k": 11, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 16, "s": 0.9518072289156627, "os": 0.1573972651143069, "bg": 3.1163388920966884e-07}, {"x": 0.7631578947368421, "y": 0.8528852251109702, "ox": 0.7631578947368421, "oy": 0.8528852251109702, "term": "elsevier", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 13, "s": 0.9137603043753963, "os": 0.11278753400939895, "bg": 3.649057309729929e-05}, {"x": 0.6493341788205453, "y": 0.6816740646797718, "ox": 0.6493341788205453, "oy": 0.6816740646797718, "term": "b.v.", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 9, "s": 0.622701331642359, "os": 0.03187166531217979, "bg": 0.0}, {"x": 0.6062143310082435, "y": 0.681991122384274, "ox": 0.6062143310082435, "oy": 0.681991122384274, "term": "elsevier b.v.", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.6769181991122385, "os": 0.037366170806685285, "bg": 0.0}, {"x": 0.9495878249841471, "y": 0.9571972098922004, "ox": 0.9495878249841471, "oy": 0.9571972098922004, "term": "all", "cat25k": 27, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 197, "ncat": 63, "s": 0.9831959416613824, "os": 0.21976255255998023, "bg": 2.57083263123647e-07}, {"x": 0.7381103360811667, "y": 0.8291058972733037, "ox": 0.7381103360811667, "oy": 0.8291058972733037, "term": "rights", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 12, "s": 0.8830057070386811, "os": 0.08934313275149289, "bg": 3.4631318460989315e-07}, {"x": 0.738427393785669, "y": 0.8294229549778059, "ox": 0.738427393785669, "oy": 0.8294229549778059, "term": "reserved", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 12, "s": 0.8830057070386811, "os": 0.08934313275149289, "bg": 4.753110635269327e-07}, {"x": 0.7387444514901712, "y": 0.8297400126823081, "ox": 0.7387444514901712, "oy": 0.8297400126823081, "term": "all rights", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 12, "s": 0.8830057070386811, "os": 0.08934313275149289, "bg": 0.0}, {"x": 0.7390615091946734, "y": 0.8300570703868104, "ox": 0.7390615091946734, "oy": 0.8300570703868104, "term": "rights reserved", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 12, "s": 0.8830057070386811, "os": 0.08934313275149289, "bg": 0.0}, {"x": 0.885859226379201, "y": 0.9201014584654407, "ox": 0.885859226379201, "oy": 0.9201014584654407, "term": "scene", "cat25k": 14, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 104, "ncat": 27, "s": 0.9673430564362714, "os": 0.1837744249319812, "bg": 8.850326985802252e-06}, {"x": 0.8725428027901078, "y": 0.004755865567533291, "ox": 0.8725428027901078, "oy": 0.004755865567533291, "term": "resnet", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 25, "s": 0.0006341154090044388, "os": -0.13642627468994029, "bg": 0.0001243225021338045}, {"x": 0.862714013950539, "y": 0.9036144578313253, "ox": 0.862714013950539, "oy": 0.9036144578313253, "term": "significant", "cat25k": 12, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 23, "s": 0.9470513633481293, "os": 0.15109006748878132, "bg": 3.756213417255792e-06}, {"x": 0.7393785668991757, "y": 0.08687381103360811, "ox": 0.7393785668991757, "oy": 0.08687381103360811, "term": "aspect", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.02155992390615092, "os": -0.045705098759761136, "bg": 2.4386922397347417e-06}, {"x": 0.9359543436905516, "y": 0.8874445149017122, "ox": 0.9359543436905516, "oy": 0.8874445149017122, "term": "vision", "cat25k": 10, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 47, "s": 0.06404565630944832, "os": -0.022578707466167275, "bg": 5.543266073587589e-06}, {"x": 0.904565630944832, "y": 0.8329105897273303, "ox": 0.904565630944832, "oy": 0.8329105897273303, "term": "computer vision", "cat25k": 7, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 31, "s": 0.10970196575776792, "os": -0.011837037560510244, "bg": 0.0}, {"x": 0.9365884590995561, "y": 0.8551046290424857, "ox": 0.9365884590995561, "oy": 0.8551046290424857, "term": "cnns", "cat25k": 8, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 48, "s": 0.006658211794546607, "os": -0.07630472421469206, "bg": 0.0009264510431232657}, {"x": 0.7396956246036779, "y": 0.7390615091946734, "ox": 0.7396956246036779, "oy": 0.7390615091946734, "term": "development", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 12, "s": 0.653138871274572, "os": 0.034680753330271014, "bg": 3.071313773348149e-07}, {"x": 0.3227647431832594, "y": 0.6823081800887761, "ox": 0.3227647431832594, "oy": 0.6823081800887761, "term": "tool", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 4, "s": 0.8100824350031707, "os": 0.05934419278470726, "bg": 9.196721129802474e-07}, {"x": 0.9876347495244134, "y": 0.9822447685478757, "ox": 0.9876347495244134, "oy": 0.9822447685478757, "term": "image", "cat25k": 57, "ncat25k": 82, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 416, "ncat": 210, "s": 0.6528218135700697, "os": 0.03462775166955223, "bg": 6.319855637132788e-06}, {"x": 0.8059606848446417, "y": 0.6572606214331008, "ox": 0.8059606848446417, "oy": 0.6572606214331008, "term": "networks cnns", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 16, "s": 0.10494610019023462, "os": -0.013020741316561252, "bg": 0.0}, {"x": 0.927710843373494, "y": 0.8877615726062144, "ox": 0.927710843373494, "oy": 0.8877615726062144, "term": "image classification", "cat25k": 10, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 39, "s": 0.49619530754597335, "os": 0.02137733648987669, "bg": 0.0}, {"x": 0.9473684210526315, "y": 0.9546607482561826, "ox": 0.9473684210526315, "oy": 0.9546607482561826, "term": "but", "cat25k": 26, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 191, "ncat": 60, "s": 0.9828788839568802, "os": 0.2178898272145861, "bg": 5.019343535117072e-07}, {"x": 0.8861762840837032, "y": 0.8214965123652505, "ox": 0.8861762840837032, "oy": 0.8214965123652505, "term": "requires", "cat25k": 6, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 27, "s": 0.19942929613189603, "os": 0.0004946821667078938, "bg": 3.026591861040496e-06}, {"x": 0.9698795180722891, "y": 0.9670259987317692, "ox": 0.9698795180722891, "oy": 0.9670259987317692, "term": "large", "cat25k": 35, "ncat25k": 40, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 253, "ncat": 101, "s": 0.9308814204185162, "os": 0.1307550969930391, "bg": 4.262620814971076e-06}, {"x": 0.9489537095751427, "y": 0.8804692454026632, "ox": 0.9489537095751427, "oy": 0.8804692454026632, "term": "scale", "cat25k": 10, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 62, "s": 0.0012682308180088776, "os": -0.11236352072364936, "bg": 5.672717708806508e-06}, {"x": 0.9086873811033608, "y": 0.8218135700697526, "ox": 0.9086873811033608, "oy": 0.8218135700697526, "term": "large scale", "cat25k": 6, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 32, "s": 0.049143944197844014, "os": -0.026977845305819564, "bg": 0.0}, {"x": 0.49175649968294227, "y": 0.03202282815472416, "ox": 0.49175649968294227, "oy": 0.03202282815472416, "term": "scale datasets", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 0.0}, {"x": 0.7152821813570069, "y": 0.657577679137603, "ox": 0.7152821813570069, "oy": 0.657577679137603, "term": "addresses", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 11, "s": 0.3998097653772987, "os": 0.01445178615596622, "bg": 2.3649147198238235e-06}, {"x": 0.9835129993658845, "y": 0.9866835764109068, "ox": 0.9835129993658845, "oy": 0.9866835764109068, "term": "problem", "cat25k": 76, "ncat25k": 68, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 549, "ncat": 173, "s": 0.9454660748256183, "os": 0.14972969153033455, "bg": 1.0223470045671726e-05}, {"x": 0.32308180088776156, "y": 0.08719086873811034, "ox": 0.32308180088776156, "oy": 0.08719086873811034, "term": "produces", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 2.165950819920683e-06}, {"x": 0.9048826886493342, "y": 0.911857958148383, "ox": 0.9048826886493342, "oy": 0.911857958148383, "term": "solution", "cat25k": 13, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 31, "s": 0.9264426125554851, "os": 0.12642662803434504, "bg": 3.340895477188655e-06}, {"x": 0.8728598604946101, "y": 0.8912492073557388, "ox": 0.8728598604946101, "oy": 0.8912492073557388, "term": "because", "cat25k": 10, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 25, "s": 0.9051997463538364, "os": 0.10473128158015618, "bg": 7.438641271926052e-07}, {"x": 0.8563728598604946, "y": 0.9096385542168675, "ox": 0.8563728598604946, "oy": 0.9096385542168675, "term": "complex", "cat25k": 13, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 91, "ncat": 22, "s": 0.9597336715282182, "os": 0.16944630931769195, "bg": 4.798609779048651e-06}, {"x": 0.3233988585922638, "y": 0.5038046924540266, "ox": 0.3233988585922638, "oy": 0.5038046924540266, "term": "more complex", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 0.0}, {"x": 0.7812301838934687, "y": 0.5719720989220038, "ox": 0.7812301838934687, "oy": 0.5719720989220038, "term": "utilizing", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 14, "s": 0.08148383005707038, "os": -0.018108900745556697, "bg": 1.3555551265246401e-05}, {"x": 0.979708306911858, "y": 0.9730500951173113, "ox": 0.979708306911858, "oy": 0.9730500951173113, "term": "multi", "cat25k": 42, "ncat25k": 54, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 306, "ncat": 138, "s": 0.8468611287254281, "os": 0.07190558637503974, "bg": 1.6039654360621176e-05}, {"x": 0.9283449587824985, "y": 0.7536461636017755, "ox": 0.9283449587824985, "oy": 0.7536461636017755, "term": "fusion", "cat25k": 5, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 40, "s": 0.0009511731135066582, "os": -0.11273453234868026, "bg": 1.3374693669811708e-05}, {"x": 0.5554850982878884, "y": 0.2663284717818643, "ox": 0.5554850982878884, "oy": 0.2663284717818643, "term": "taking", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 4.841351337350687e-07}, {"x": 0.6065313887127457, "y": 0.7393785668991757, "ox": 0.6065313887127457, "oy": 0.7393785668991757, "term": "full", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 8, "s": 0.8015218769816107, "os": 0.05665877530829299, "bg": 2.5793130625786066e-07}, {"x": 0.7155992390615092, "y": 0.7396956246036779, "ox": 0.7155992390615092, "oy": 0.7396956246036779, "term": "advantage", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 11, "s": 0.6997463538363983, "os": 0.0401752588247765, "bg": 2.5907989162266378e-06}, {"x": 0.8792010145846544, "y": 0.9137603043753963, "ox": 0.8792010145846544, "oy": 0.9137603043753963, "term": "them", "cat25k": 13, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 95, "ncat": 26, "s": 0.9549778059606848, "os": 0.16033002367407514, "bg": 6.001516538589923e-07}, {"x": 0.9717818642993025, "y": 0.9619530754597336, "ox": 0.9717818642993025, "oy": 0.9619530754597336, "term": "multi -", "cat25k": 31, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 224, "ncat": 106, "s": 0.8493975903614458, "os": 0.07247093742270594, "bg": 0.0}, {"x": 0.6496512365250475, "y": 0.0050729232720355105, "ox": 0.6496512365250475, "oy": 0.0050729232720355105, "term": "feature fusion", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.019340519974635383, "os": -0.048514186777852374, "bg": 0.0}, {"x": 0.8864933417882055, "y": 0.8807863031071655, "ox": 0.8864933417882055, "oy": 0.8807863031071655, "term": "addition", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 27, "s": 0.8541534559289791, "os": 0.07444966608953746, "bg": 2.63352726456161e-06}, {"x": 0.6068484464172479, "y": 0.572289156626506, "ox": 0.6068484464172479, "oy": 0.572289156626506, "term": "filter", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 8, "s": 0.40076093849080535, "os": 0.014858132221476271, "bg": 9.937869360928566e-07}, {"x": 0.7400126823081801, "y": 0.8608116677235257, "ox": 0.7400126823081801, "oy": 0.8608116677235257, "term": "useful", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 12, "s": 0.9318325935320229, "os": 0.1311437758383096, "bg": 2.5353084528211644e-06}, {"x": 0.806277742549144, "y": 0.797717184527584, "ox": 0.806277742549144, "oy": 0.797717184527584, "term": "presented", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 16, "s": 0.7117945466074826, "os": 0.04164163810466062, "bg": 2.4275676111118462e-06}, {"x": 0.8868103994927077, "y": 0.8788839568801522, "ox": 0.8868103994927077, "oy": 0.8788839568801522, "term": "in addition", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 69, "ncat": 27, "s": 0.8455928979074191, "os": 0.07123423200593618, "bg": 0.0}, {"x": 0.5558021559923906, "y": 0.007609384908053266, "ox": 0.5558021559923906, "oy": 0.007609384908053266, "term": "small scale", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.03329105897273303, "os": -0.0343097417052401, "bg": 0.0}, {"x": 0.12714013950539, "y": 0.4790741915028535, "ox": 0.12714013950539, "oy": 0.4790741915028535, "term": "patches", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6540900443880786, "os": 0.0349634288541041, "bg": 2.5027631609457057e-06}, {"x": 0.8398858592263792, "y": 0.8221306277742549, "ox": 0.8398858592263792, "oy": 0.8221306277742549, "term": "generated", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 20, "s": 0.6883322764743184, "os": 0.03895622062824636, "bg": 3.465333399467756e-06}, {"x": 0.6499682942295498, "y": 0.5266328471781865, "ox": 0.6499682942295498, "oy": 0.5266328471781865, "term": "raw", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 9, "s": 0.24128091312618896, "os": 0.0029327585597682046, "bg": 2.0598072971050205e-06}, {"x": 0.958148383005707, "y": 0.9575142675967026, "ox": 0.958148383005707, "oy": 0.9575142675967026, "term": "then", "cat25k": 28, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 200, "ncat": 74, "s": 0.9587824984147115, "os": 0.1680329316985265, "bg": 1.48044050247804e-06}, {"x": 0.2206721623335447, "y": 0.32752060875079264, "ox": 0.2206721623335447, "oy": 0.32752060875079264, "term": "sufficiently", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 5.236563632289999e-06}, {"x": 0.2209892200380469, "y": 0.6946734305643627, "ox": 0.2209892200380469, "oy": 0.6946734305643627, "term": "represent", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 3, "s": 0.8386176284083703, "os": 0.06805413236281403, "bg": 2.2839832439375943e-06}, {"x": 0.826569435637286, "y": 0.8706404565630945, "ox": 0.826569435637286, "oy": 0.8706404565630945, "term": "original", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 18, "s": 0.9071020925808497, "os": 0.10782304512208048, "bg": 1.4601859882874788e-06}, {"x": 0.7815472415979708, "y": 0.7685478757133798, "ox": 0.7815472415979708, "oy": 0.7685478757133798, "term": "categorization", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 14, "s": 0.6712111604311986, "os": 0.03655347867566518, "bg": 8.688769157649896e-05}, {"x": 0.650285351934052, "y": 0.20196575776791376, "ox": 0.650285351934052, "oy": 0.20196575776791376, "term": "enhanced", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 9, "s": 0.06277742549143944, "os": -0.022790714109042085, "bg": 1.959371346588843e-06}, {"x": 0.7403297400126823, "y": 0.3725428027901078, "ox": 0.7403297400126823, "oy": 0.3725428027901078, "term": "six", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 12, "s": 0.043753963221306286, "os": -0.029627928341754706, "bg": 6.704168856113201e-07}, {"x": 0.8402029169308814, "y": 0.8709575142675967, "ox": 0.8402029169308814, "oy": 0.8709575142675967, "term": "benchmark", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 20, "s": 0.8947368421052632, "os": 0.09683403413306949, "bg": 2.7616020995973417e-05}, {"x": 0.7406467977171846, "y": 0.012048192771084338, "ox": 0.7406467977171846, "oy": 0.012048192771084338, "term": "ls", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 12, "s": 0.013950538998097653, "os": -0.058566835094166284, "bg": 3.027373511289076e-06}, {"x": 0.9350031705770451, "y": 0.920418516169943, "ox": 0.9350031705770451, "oy": 0.920418516169943, "term": "extensive", "cat25k": 14, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 104, "ncat": 46, "s": 0.865567533291059, "os": 0.07937882053637679, "bg": 1.3707947785878564e-05}, {"x": 0.9429296131896006, "y": 0.9429296131896006, "ox": 0.9429296131896006, "oy": 0.9429296131896006, "term": "better", "cat25k": 20, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 148, "ncat": 54, "s": 0.9521242866201649, "os": 0.15760927175718176, "bg": 2.5681699415516306e-06}, {"x": 0.9660748256182625, "y": 0.9689283449587826, "ox": 0.9660748256182625, "oy": 0.9689283449587826, "term": "other", "cat25k": 37, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 269, "ncat": 93, "s": 0.9711477488902981, "os": 0.19266103671248364, "bg": 7.397473245676524e-07}, {"x": 0.982561826252378, "y": 0.9784400760938491, "ox": 0.982561826252378, "oy": 0.9784400760938491, "term": "models", "cat25k": 48, "ncat25k": 60, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 352, "ncat": 153, "s": 0.858275206087508, "os": 0.07618105367301509, "bg": 1.1635331700784937e-05}, {"x": 0.323715916296766, "y": 0.13760304375396323, "ox": 0.323715916296766, "oy": 0.13760304375396323, "term": "extensive experimental", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.1274571972098922, "y": 0.20228281547241597, "ox": 0.1274571972098922, "oy": 0.20228281547241597, "term": "accuracy than", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 0.0}, {"x": 0.32403297400126824, "y": 0.13792010145846545, "ox": 0.32403297400126824, "oy": 0.13792010145846545, "term": "than other", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.5561192136968929, "y": 0.5269499048826887, "ox": 0.5561192136968929, "oy": 0.5269499048826887, "term": "other state", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 7, "s": 0.3937856689917565, "os": 0.01392176954877919, "bg": 0.0}, {"x": 0.8268864933417882, "y": 0.13823715916296767, "ox": 0.8268864933417882, "oy": 0.13823715916296767, "term": "fault", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 18, "s": 0.006975269499048827, "os": -0.07545669764319282, "bg": 3.897007487950003e-06}, {"x": 0.956563094483196, "y": 0.9524413443246671, "ox": 0.956563094483196, "oy": 0.9524413443246671, "term": "systems", "cat25k": 25, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 182, "ncat": 71, "s": 0.9413443246670894, "os": 0.1436521677679234, "bg": 2.261078109747637e-06}, {"x": 0.22130627774254916, "y": 0.13855421686746988, "ox": 0.22130627774254916, "oy": 0.13855421686746988, "term": "operation", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 3.723344534169149e-07}, {"x": 0.6876981610653139, "y": 0.6290424857324033, "ox": 0.6876981610653139, "oy": 0.6290424857324033, "term": "states", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 10, "s": 0.39124920735573876, "os": 0.013515423483269139, "bg": 2.4505305932873557e-07}, {"x": 0.4920735573874445, "y": 0.6119213696892835, "ox": 0.4920735573874445, "oy": 0.6119213696892835, "term": "relatively", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.6236525047558656, "os": 0.03227801137768983, "bg": 2.7101867790464016e-06}, {"x": 0.981293595434369, "y": 0.9844641724793912, "ox": 0.981293595434369, "oy": 0.9844641724793912, "term": "it", "cat25k": 66, "ncat25k": 58, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 481, "ncat": 148, "s": 0.9546607482561826, "os": 0.1601886859121585, "bg": 4.471465357152422e-07}, {"x": 0.49239061509194676, "y": 0.7317691819911224, "ox": 0.49239061509194676, "oy": 0.7317691819911224, "term": "hard", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 6, "s": 0.8284717818642994, "os": 0.06443235221370269, "bg": 5.6798283666394e-07}, {"x": 0.5564362714013951, "y": 0.5041217501585289, "ox": 0.5564362714013951, "oy": 0.5041217501585289, "term": "collect", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 2.8440322218958454e-06}, {"x": 0.5567533291058973, "y": 0.5044388078630311, "ox": 0.5567533291058973, "oy": 0.5044388078630311, "term": "kinds", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 1.325150318858534e-06}, {"x": 0.9720989220038047, "y": 0.9778059606848446, "ox": 0.9720989220038047, "oy": 0.9778059606848446, "term": "samples", "cat25k": 47, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 340, "ncat": 106, "s": 0.9844641724793913, "os": 0.22359633935196643, "bg": 2.9811815919924126e-05}, {"x": 0.8795180722891566, "y": 0.8554216867469879, "ox": 0.8795180722891566, "oy": 0.8554216867469879, "term": "moreover", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 26, "s": 0.7317691819911224, "os": 0.044574396664428806, "bg": 1.413282209096433e-05}, {"x": 0.9727330374128091, "y": 0.9759036144578314, "ox": 0.9727330374128091, "oy": 0.9759036144578314, "term": "not", "cat25k": 44, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 320, "ncat": 108, "s": 0.9714648065948003, "os": 0.19331472386134763, "bg": 3.250157584896686e-07}, {"x": 0.6071655041217502, "y": 0.712428662016487, "ox": 0.6071655041217502, "oy": 0.712428662016487, "term": "always", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 8, "s": 0.7504755865567534, "os": 0.04701247305748913, "bg": 5.787322800583105e-07}, {"x": 0.9381737476220672, "y": 0.9400760938490805, "ox": 0.9381737476220672, "oy": 0.9400760938490805, "term": "similar", "cat25k": 19, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 135, "ncat": 49, "s": 0.950856055802156, "os": 0.15614289247729762, "bg": 3.0825447777532544e-06}, {"x": 0.8798351299936589, "y": 0.9476854787571338, "ox": 0.8798351299936589, "oy": 0.9476854787571338, "term": "distributions", "cat25k": 23, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 165, "ncat": 26, "s": 0.9987317691819911, "os": 0.34682520052294974, "bg": 5.1593447416079946e-05}, {"x": 0.9051997463538364, "y": 0.9099556119213696, "ox": 0.9051997463538364, "oy": 0.9099556119213696, "term": "so", "cat25k": 13, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 91, "ncat": 31, "s": 0.9201014584654407, "os": 0.11999575986714248, "bg": 3.6855747764808785e-07}, {"x": 0.6074825618262524, "y": 0.5494610019023463, "ox": 0.6074825618262524, "oy": 0.5494610019023463, "term": "get", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 8, "s": 0.35954343690551677, "os": 0.01164269813787499, "bg": 8.577808915370769e-08}, {"x": 0.781864299302473, "y": 0.9140773620798985, "ox": 0.781864299302473, "oy": 0.9140773620798985, "term": "good", "cat25k": 13, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 96, "ncat": 14, "s": 0.9863665187064046, "os": 0.22947952369174235, "bg": 6.010479193977692e-07}, {"x": 0.22162333544705137, "y": 0.2025998731769182, "ox": 0.22162333544705137, "oy": 0.2025998731769182, "term": "not always", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.9286620164870006, "y": 0.9254914394419784, "ox": 0.9286620164870006, "oy": 0.9254914394419784, "term": "solve", "cat25k": 15, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 111, "ncat": 40, "s": 0.9324667089410272, "os": 0.13163845800501747, "bg": 2.207088803825835e-05}, {"x": 0.9432466708941027, "y": 0.958148383005707, "ox": 0.9432466708941027, "oy": 0.958148383005707, "term": "problems", "cat25k": 28, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 205, "ncat": 55, "s": 0.9939759036144579, "os": 0.2710858273559238, "bg": 4.286229033560366e-06}, {"x": 0.9159797083069119, "y": 0.9175649968294229, "ox": 0.9159797083069119, "oy": 0.9175649968294229, "term": "to solve", "cat25k": 14, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 99, "ncat": 34, "s": 0.9296131896005073, "os": 0.1292357160524363, "bg": 0.0}, {"x": 0.9904882688649335, "y": 0.9908053265694357, "ox": 0.9904882688649335, "oy": 0.9908053265694357, "term": "source", "cat25k": 100, "ncat25k": 102, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 727, "ncat": 259, "s": 0.852568167406468, "os": 0.07386664782163166, "bg": 1.0943654088541954e-05}, {"x": 0.9759036144578314, "y": 0.9787571337983513, "ox": 0.9759036144578314, "oy": 0.9787571337983513, "term": "one", "cat25k": 49, "ncat25k": 47, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 358, "ncat": 119, "s": 0.9657577679137603, "os": 0.17829758665771522, "bg": 9.599826212957412e-07}, {"x": 0.99143944197844, "y": 0.9933417882054534, "ox": 0.99143944197844, "oy": 0.9933417882054534, "term": "target", "cat25k": 131, "ncat25k": 107, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 953, "ncat": 274, "s": 0.8741280913126189, "os": 0.0831066040069256, "bg": 4.6803502000515944e-05}, {"x": 0.8490805326569436, "y": 0.8811033608116677, "ox": 0.8490805326569436, "oy": 0.8811033608116677, "term": "another", "cat25k": 10, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 21, "s": 0.9064679771718452, "os": 0.10741669905657043, "bg": 9.441463607803303e-07}, {"x": 0.6880152187698161, "y": 0.37285986049461, "ox": 0.6880152187698161, "oy": 0.37285986049461, "term": "transform", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 10, "s": 0.07926442612555486, "os": -0.01863891735274372, "bg": 5.6996088513889165e-06}, {"x": 0.868421052631579, "y": 0.8988585922637921, "ox": 0.868421052631579, "oy": 0.8988585922637921, "term": "matrix", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 82, "ncat": 24, "s": 0.9302473050095117, "os": 0.12951839157626938, "bg": 7.438822266009126e-06}, {"x": 0.9524413443246671, "y": 0.9530754597336716, "ox": 0.9524413443246671, "oy": 0.9530754597336716, "term": "into", "cat25k": 25, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 183, "ncat": 66, "s": 0.9632213062777426, "os": 0.17434012932405213, "bg": 1.1177286407359234e-06}, {"x": 0.9410272669625872, "y": 0.9270767279644896, "ox": 0.9410272669625872, "oy": 0.9270767279644896, "term": "low", "cat25k": 16, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 114, "ncat": 52, "s": 0.8560558021559924, "os": 0.0753506943217554, "bg": 1.7941433820811097e-06}, {"x": 0.7821813570069752, "y": 0.8094483195941662, "ox": 0.7821813570069752, "oy": 0.8094483195941662, "term": "dimensional", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 14, "s": 0.8205453392517439, "os": 0.06227695134447546, "bg": 1.078768816029835e-05}, {"x": 0.9090044388078631, "y": 0.9213696892834495, "ox": 0.9090044388078631, "oy": 0.9213696892834495, "term": "subspace", "cat25k": 14, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 105, "ncat": 32, "s": 0.9540266328471783, "os": 0.159517331543055, "bg": 0.0002938306291621537}, {"x": 0.12777425491439443, "y": 0.2666455294863665, "ox": 0.12777425491439443, "oy": 0.2666455294863665, "term": "low dimensional", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.6506024096385542, "y": 0.6949904882688649, "ox": 0.6506024096385542, "oy": 0.6949904882688649, "term": "contain", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 9, "s": 0.6569435637285986, "os": 0.03508709939578107, "bg": 2.417451826480546e-06}, {"x": 0.8065948002536462, "y": 0.8059606848446417, "ox": 0.8065948002536462, "oy": 0.8059606848446417, "term": "lack", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 16, "s": 0.7577679137603044, "os": 0.048072506271863186, "bg": 3.6745396689289584e-06}, {"x": 0.9207355738744452, "y": 0.9432466708941027, "ox": 0.9207355738744452, "oy": 0.9432466708941027, "term": "some", "cat25k": 21, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 150, "ncat": 36, "s": 0.992707672796449, "os": 0.2597258047418819, "bg": 6.775209382108882e-07}, {"x": 0.9128091312618897, "y": 0.8934686112872543, "ox": 0.9128091312618897, "oy": 0.8934686112872543, "term": "shared", "cat25k": 11, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 33, "s": 0.8265694356372861, "os": 0.0639906717077135, "bg": 7.046798300004754e-06}, {"x": 0.49270767279644895, "y": 0.26696258719086874, "ox": 0.49270767279644895, "oy": 0.26696258719086874, "term": "introducing", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 3.96707722610059e-06}, {"x": 0.8069118579581483, "y": 0.806277742549144, "ox": 0.8069118579581483, "oy": 0.806277742549144, "term": "idea", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 16, "s": 0.7577679137603044, "os": 0.048072506271863186, "bg": 1.7963115725561528e-06}, {"x": 0.7916930881420419, "y": 0.8452758402029169, "ox": 0.7916930881420419, "oy": 0.8452758402029169, "term": "regularization", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 15, "s": 0.8864933417882055, "os": 0.09215222076958411, "bg": 0.0002754517408550022}, {"x": 0.8801521876981611, "y": 0.7853519340519974, "ox": 0.8801521876981611, "oy": 0.7853519340519974, "term": "terms", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 26, "s": 0.07324032974001268, "os": -0.019734285007596913, "bg": 4.677321367277316e-07}, {"x": 0.9258084971464806, "y": 0.9264426125554851, "ox": 0.9258084971464806, "oy": 0.9264426125554851, "term": "structure", "cat25k": 16, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 113, "ncat": 38, "s": 0.9451490171211161, "os": 0.14905833716123104, "bg": 4.479213556888608e-06}, {"x": 0.9746353836398225, "y": 0.9771718452758402, "ox": 0.9746353836398225, "oy": 0.9771718452758402, "term": "information", "cat25k": 46, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 332, "ncat": 114, "s": 0.9660748256182625, "os": 0.1809830041341296, "bg": 9.56234359384057e-07}, {"x": 0.2219403931515536, "y": 0.04882688649334179, "ox": 0.2219403931515536, "oy": 0.04882688649334179, "term": "included", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.951566034164115e-07}, {"x": 0.9476854787571338, "y": 0.948002536461636, "ox": 0.9476854787571338, "oy": 0.948002536461636, "term": "process", "cat25k": 23, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 165, "ncat": 60, "s": 0.9543436905516804, "os": 0.16001201370976292, "bg": 2.5415072755672295e-06}, {"x": 0.3243500317057704, "y": 0.1388712745719721, "ox": 0.3243500317057704, "oy": 0.1388712745719721, "term": "by introducing", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.9232720355104629, "y": 0.9194673430564363, "ox": 0.9232720355104629, "oy": 0.9194673430564363, "term": "obtained", "cat25k": 14, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 103, "ncat": 37, "s": 0.924857324032974, "os": 0.12561393590332495, "bg": 8.304347207129401e-06}, {"x": 0.9093214965123653, "y": 0.891566265060241, "ox": 0.9093214965123653, "oy": 0.891566265060241, "term": "discriminative", "cat25k": 10, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 32, "s": 0.8351299936588459, "os": 0.06626974311861772, "bg": 0.0006312725402290116}, {"x": 0.8956880152187698, "y": 0.8398858592263792, "ox": 0.8956880152187698, "oy": 0.8398858592263792, "term": "ability", "cat25k": 7, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 29, "s": 0.3056436271401395, "os": 0.008798275679304635, "bg": 3.1396421110471123e-06}, {"x": 0.9736842105263158, "y": 0.9746353836398225, "ox": 0.9736842105263158, "oy": 0.9746353836398225, "term": "experiments", "cat25k": 43, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 312, "ncat": 111, "s": 0.9600507292327204, "os": 0.16946397653793155, "bg": 6.076782953977348e-05}, {"x": 0.9001268230818009, "y": 0.9023462270133165, "ox": 0.9001268230818009, "oy": 0.9023462270133165, "term": "evaluate", "cat25k": 12, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 86, "ncat": 30, "s": 0.9093214965123653, "os": 0.10941309494364154, "bg": 1.4331291144936116e-05}, {"x": 0.896005072923272, "y": 0.8896639188332276, "ox": 0.896005072923272, "oy": 0.8896639188332276, "term": "extensive experiments", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 29, "s": 0.8662016487000634, "os": 0.07953782551853292, "bg": 0.0}, {"x": 0.5570703868103994, "y": 0.3278376664552949, "ox": 0.5570703868103994, "oy": 0.3278376664552949, "term": "are conducted", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 7, "s": 0.14679771718452758, "os": -0.005370834952828522, "bg": 0.0}, {"x": 0.8566899175649968, "y": 0.7796448953709575, "ox": 0.8566899175649968, "oy": 0.7796448953709575, "term": "web", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 22, "s": 0.18960050729232722, "os": -0.000971697113176212, "bg": 1.936100559591629e-07}, {"x": 0.9162967660114141, "y": 0.9001268230818009, "ox": 0.9162967660114141, "oy": 0.9001268230818009, "term": "metric", "cat25k": 12, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 84, "ncat": 34, "s": 0.8687381103360812, "os": 0.081004204798417, "bg": 3.251176381691413e-05}, {"x": 0.32466708941027267, "y": 0.08750792644261256, "ox": 0.32466708941027267, "oy": 0.08750792644261256, "term": "growing", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 5.646785941843853e-07}, {"x": 0.7159162967660114, "y": 0.593849080532657, "ox": 0.7159162967660114, "oy": 0.593849080532657, "term": "amounts", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 11, "s": 0.22733037412809132, "os": 0.0015900498215610792, "bg": 2.079465848278348e-06}, {"x": 0.9308814204185162, "y": 0.9188332276474318, "ox": 0.9308814204185162, "oy": 0.9188332276474318, "term": "online", "cat25k": 14, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 102, "ncat": 42, "s": 0.8918833227647431, "os": 0.09492597434719624, "bg": 4.787647191994634e-07}, {"x": 0.41471147748890297, "y": 0.04914394419784401, "ox": 0.41471147748890297, "oy": 0.04914394419784401, "term": "multimedia", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 5.985836368401782e-07}, {"x": 0.7409638554216867, "y": 0.6953075459733672, "ox": 0.7409638554216867, "oy": 0.6953075459733672, "term": "content", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 12, "s": 0.4543436905516804, "os": 0.01860358291226459, "bg": 4.1245526922605247e-07}, {"x": 0.8963221306277742, "y": 0.929296131896005, "ox": 0.8963221306277742, "oy": 0.929296131896005, "term": "challenge", "cat25k": 16, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 117, "ncat": 29, "s": 0.9793912492073558, "os": 0.21137062294618567, "bg": 7.860164655913622e-06}, {"x": 0.8804692454026632, "y": 0.8747622067216233, "ox": 0.8804692454026632, "oy": 0.8747622067216233, "term": "search", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 26, "s": 0.8436905516804059, "os": 0.07029786933323912, "bg": 1.8158315446359415e-07}, {"x": 0.8731769181991123, "y": 0.8769816106531388, "ox": 0.8731769181991123, "oy": 0.8769816106531388, "term": "recommendation", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 25, "s": 0.8642993024730501, "os": 0.0790078089113459, "bg": 1.0229960716950848e-05}, {"x": 0.7412809131261889, "y": 0.7799619530754597, "ox": 0.7412809131261889, "oy": 0.7799619530754597, "term": "retrieval", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 12, "s": 0.7897907419150286, "os": 0.05397335783187873, "bg": 1.6554657851642455e-05}, {"x": 0.955294863665187, "y": 0.9483195941661382, "ox": 0.955294863665187, "oy": 0.9483195941661382, "term": "visual", "cat25k": 23, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 165, "ncat": 70, "s": 0.9109067850348763, "os": 0.11056146425921343, "bg": 9.362500004233044e-06}, {"x": 0.4150285351934052, "y": 0.04946100190234623, "ox": 0.4150285351934052, "oy": 0.04946100190234623, "term": "elements", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 4.953718979755457e-07}, {"x": 0.6509194673430564, "y": 0.7320862396956246, "ox": 0.6509194673430564, "oy": 0.7320862396956246, "term": "highly", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 9, "s": 0.7574508560558022, "os": 0.04794883573018621, "bg": 1.8566570263618609e-06}, {"x": 0.4153455928979074, "y": 0.41217501585288524, "ox": 0.4153455928979074, "oy": 0.41217501585288524, "term": "valuable", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 2.038326656114929e-06}, {"x": 0.6883322764743183, "y": 0.6826252377932783, "ox": 0.6883322764743183, "oy": 0.6826252377932783, "term": "range", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 10, "s": 0.5516804058338618, "os": 0.026377159817674294, "bg": 5.601114659157936e-07}, {"x": 0.8272035510462904, "y": 0.8633481293595434, "ox": 0.8272035510462904, "oy": 0.8633481293595434, "term": "mining", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 18, "s": 0.9013950538998098, "os": 0.10139217695487791, "bg": 9.403938334545106e-06}, {"x": 0.974001268230818, "y": 0.9850982878883957, "ox": 0.974001268230818, "oy": 0.9850982878883957, "term": "tasks", "cat25k": 67, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 488, "ncat": 112, "s": 0.9949270767279645, "os": 0.2765096639694711, "bg": 5.8522725471601724e-05}, {"x": 0.6077996195307546, "y": 0.4476854787571338, "ox": 0.6077996195307546, "oy": 0.4476854787571338, "term": "resources", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 2.022992821465145e-07}, {"x": 0.9175649968294229, "y": 0.9353202282815473, "ox": 0.9175649968294229, "oy": 0.9353202282815473, "term": "due", "cat25k": 17, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 125, "ncat": 35, "s": 0.9752694990488269, "os": 0.20412706264796296, "bg": 3.043579724554513e-06}, {"x": 0.7162333544705136, "y": 0.7980342422320862, "ox": 0.7162333544705136, "oy": 0.7980342422320862, "term": "complexity", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 11, "s": 0.8408370323398858, "os": 0.0691141655771881, "bg": 9.739956278834718e-06}, {"x": 0.41566265060240964, "y": 0.5726062143310082, "ox": 0.41566265060240964, "oy": 0.5726062143310082, "term": "variability", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.61572606214331, "os": 0.03134164870499275, "bg": 1.1503453432582573e-05}, {"x": 0.7634749524413443, "y": 0.7983512999365885, "ox": 0.7634749524413443, "oy": 0.7983512999365885, "term": "cost", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 13, "s": 0.807545973367153, "os": 0.0581251545881771, "bg": 6.787407438616446e-07}, {"x": 0.6886493341788206, "y": 0.267279644895371, "ox": 0.6886493341788206, "oy": 0.267279644895371, "term": "collecting", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 10, "s": 0.05643627140139506, "os": -0.02506978551994629, "bg": 4.242929793937872e-06}, {"x": 0.7824984147114775, "y": 0.7609384908053266, "ox": 0.7824984147114775, "oy": 0.7609384908053266, "term": "big", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 14, "s": 0.6382371591629676, "os": 0.03333804459206388, "bg": 4.517763817365119e-07}, {"x": 0.6081166772352569, "y": 0.5497780596068484, "ox": 0.6081166772352569, "oy": 0.5497780596068484, "term": "enough", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 8, "s": 0.35954343690551677, "os": 0.01164269813787499, "bg": 6.203509172437084e-07}, {"x": 0.6889663918833228, "y": 0.7612555485098288, "ox": 0.6889663918833228, "oy": 0.7612555485098288, "term": "successfully", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 10, "s": 0.7948636651870641, "os": 0.05531606657008586, "bg": 4.929586066492119e-06}, {"x": 0.9178820545339251, "y": 0.9343690551680406, "ox": 0.9178820545339251, "oy": 0.9343690551680406, "term": "due to", "cat25k": 17, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 124, "ncat": 35, "s": 0.9746353836398225, "os": 0.2009116285643617, "bg": 0.0}, {"x": 0.9004438807863031, "y": 0.8097653772986684, "ox": 0.9004438807863031, "oy": 0.8097653772986684, "term": "proposes", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 30, "s": 0.052631578947368425, "os": -0.02563513656761246, "bg": 3.4784472587485303e-05}, {"x": 0.3249841471147749, "y": 0.6122384273937856, "ox": 0.3249841471147749, "oy": 0.6122384273937856, "term": "basis", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7232086239695625, "os": 0.043267022366700825, "bg": 8.314994336158457e-07}, {"x": 0.8357641090678504, "y": 0.7228915662650602, "ox": 0.8357641090678504, "oy": 0.7228915662650602, "term": "paper proposes", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 19, "s": 0.11731135066582118, "os": -0.010211653298470014, "bg": 0.0}, {"x": 0.12809131261889664, "y": 0.08782498414711477, "ox": 0.12809131261889664, "oy": 0.08782498414711477, "term": "exploring", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 2.132387374702991e-06}, {"x": 0.7920101458465441, "y": 0.8636651870640456, "ox": 0.7920101458465441, "oy": 0.8636651870640456, "term": "joint", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 15, "s": 0.9172479391249208, "os": 0.1178756934383944, "bg": 3.99405837731086e-06}, {"x": 0.9131261889663919, "y": 0.8918833227647431, "ox": 0.9131261889663919, "oy": 0.8918833227647431, "term": "strategy", "cat25k": 10, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 33, "s": 0.8145212428662018, "os": 0.060775237624112216, "bg": 3.797833583322535e-06}, {"x": 0.8687381103360812, "y": 0.8351299936588459, "ox": 0.8687381103360812, "oy": 0.8351299936588459, "term": "build", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 24, "s": 0.5944831959416614, "os": 0.0298399349846295, "bg": 1.937132758371668e-06}, {"x": 0.7923272035510462, "y": 0.1391883322764743, "ox": 0.7923272035510462, "oy": 0.1391883322764743, "term": "dcnn", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 15, "s": 0.012682308180088777, "os": -0.05897318115967634, "bg": 0.00019921612784478464}, {"x": 0.913443246670894, "y": 0.8487634749524413, "ox": 0.913443246670894, "oy": 0.8487634749524413, "term": "extraction", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 33, "s": 0.16582117945466077, "os": -0.003533444047913503, "bg": 2.7050993402141133e-05}, {"x": 0.8176918199112239, "y": 0.8979074191502854, "ox": 0.8176918199112239, "oy": 0.8979074191502854, "term": "even", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 81, "ncat": 17, "s": 0.9568801521876982, "os": 0.16476449595420656, "bg": 7.969790233616549e-07}, {"x": 0.9613189600507293, "y": 0.9705136334812936, "ox": 0.9613189600507293, "oy": 0.9705136334812936, "term": "when", "cat25k": 39, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 280, "ncat": 77, "s": 0.9955611921369689, "os": 0.2801667785590615, "bg": 1.0970228908284133e-06}, {"x": 0.3253012048192771, "y": 0.7466708941027267, "ox": 0.3253012048192771, "oy": 0.7466708941027267, "term": "scarce", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 4, "s": 0.8712745719720989, "os": 0.08185223136991626, "bg": 2.8285969623926565e-05}, {"x": 0.8690551680405834, "y": 0.7726696258719087, "ox": 0.8690551680405834, "oy": 0.7726696258719087, "term": "feature extraction", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 24, "s": 0.09480025364616361, "os": -0.0151761421857885, "bg": 0.0}, {"x": 0.2222574508560558, "y": 0.5500951173113506, "ox": 0.2222574508560558, "oy": 0.5500951173113506, "term": "even when", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 3, "s": 0.6889663918833228, "os": 0.039115225610402454, "bg": 0.0}, {"x": 0.22257450856055802, "y": 0.5729232720355104, "ox": 0.22257450856055802, "oy": 0.5729232720355104, "term": "evidence", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.715282181357007, "os": 0.042330659694003736, "bg": 7.069047451157743e-07}, {"x": 0.9210526315789473, "y": 0.9162967660114141, "ox": 0.9210526315789473, "oy": 0.9162967660114141, "term": "outperforms", "cat25k": 14, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 98, "ncat": 36, "s": 0.9156626506024097, "os": 0.11503127097982402, "bg": 0.0005450289495787211}, {"x": 0.4930247305009512, "y": 0.448002536461636, "ox": 0.4930247305009512, "oy": 0.448002536461636, "term": "handcrafted", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 1.4710392818974639e-05}, {"x": 0.32561826252377934, "y": 0.7127457197209892, "ox": 0.32561826252377934, "oy": 0.7127457197209892, "term": "descriptors", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 4, "s": 0.8405199746353836, "os": 0.0689904950355111, "bg": 3.816239951290901e-05}, {"x": 0.8807863031071655, "y": 0.8503487634749525, "ox": 0.8807863031071655, "oy": 0.8503487634749525, "term": "achieves", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 26, "s": 0.6826252377932784, "os": 0.03814352849722624, "bg": 8.41398424456107e-05}, {"x": 0.22289156626506024, "y": 0.44831959416613826, "ox": 0.22289156626506024, "oy": 0.44831959416613826, "term": "approach outperforms", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 0.0}, {"x": 0.9055168040583386, "y": 0.9267596702599873, "ox": 0.9055168040583386, "oy": 0.9267596702599873, "term": "address", "cat25k": 16, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 113, "ncat": 31, "s": 0.9695624603677869, "os": 0.18751987562276948, "bg": 1.0988009289079922e-06}, {"x": 0.9438807863031071, "y": 0.9441978440076094, "ox": 0.9438807863031071, "oy": 0.9441978440076094, "term": "over", "cat25k": 21, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 152, "ncat": 56, "s": 0.9476854787571338, "os": 0.15305112893537332, "bg": 9.054226316415675e-07}, {"x": 0.9540266328471781, "y": 0.9657577679137603, "ox": 0.9540266328471781, "oy": 0.9657577679137603, "term": "learn", "cat25k": 34, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 245, "ncat": 68, "s": 0.9936588459099557, "os": 0.2694604430938836, "bg": 3.4843694110415467e-06}, {"x": 0.8630310716550412, "y": 0.7469879518072289, "ox": 0.8630310716550412, "oy": 0.7469879518072289, "term": "identify", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 23, "s": 0.06436271401395054, "os": -0.022543373025688138, "bg": 3.103777832469546e-06}, {"x": 0.8275206087507927, "y": 0.8814204185161699, "ox": 0.8275206087507927, "oy": 0.8814204185161699, "term": "categories", "cat25k": 10, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 18, "s": 0.9229549778059607, "os": 0.12390021554008691, "bg": 1.011087621324339e-06}, {"x": 0.8278376664552949, "y": 0.7856689917564997, "ox": 0.8278376664552949, "oy": 0.7856689917564997, "term": "provided", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 18, "s": 0.523145212428662, "os": 0.02422175894844704, "bg": 7.124252487807716e-07}, {"x": 0.9533925174381738, "y": 0.9499048826886494, "ox": 0.9533925174381738, "oy": 0.9499048826886494, "term": "at", "cat25k": 23, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 170, "ncat": 67, "s": 0.9391249207355739, "os": 0.1399067170771351, "bg": 2.0858050095193437e-07}, {"x": 0.32593532022828153, "y": 0.049778059606848446, "ox": 0.32593532022828153, "oy": 0.049778059606848446, "term": "over time", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.3262523779327838, "y": 0.13950538998097653, "ox": 0.3262523779327838, "oy": 0.13950538998097653, "term": "are provided", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.9096385542168675, "y": 0.885859226379201, "ox": 0.9096385542168675, "oy": 0.885859226379201, "term": "result", "cat25k": 10, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 32, "s": 0.8008877615726063, "os": 0.05662344086781387, "bg": 1.6450456489200607e-06}, {"x": 0.608433734939759, "y": 0.9074191502853519, "ox": 0.608433734939759, "oy": 0.9074191502853519, "term": "prior", "cat25k": 12, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 89, "ncat": 8, "s": 0.9889029803424224, "os": 0.2399385180735663, "bg": 3.0705901126555196e-06}, {"x": 0.9854153455928979, "y": 0.9895370957514268, "ox": 0.9854153455928979, "oy": 0.9895370957514268, "term": "knowledge", "cat25k": 96, "ncat25k": 72, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 696, "ncat": 183, "s": 0.9489537095751427, "os": 0.15441150489381994, "bg": 1.9537436194934267e-05}, {"x": 0.7637920101458465, "y": 0.44863665187064045, "ox": 0.7637920101458465, "oy": 0.44863665187064045, "term": "complete", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 13, "s": 0.04533925174381738, "os": -0.028691565669057638, "bg": 3.610997992512476e-07}, {"x": 0.9648065948002537, "y": 0.9720989220038047, "ox": 0.9648065948002537, "oy": 0.9720989220038047, "term": "set", "cat25k": 40, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 292, "ncat": 88, "s": 0.9892200380469246, "os": 0.2435779654429172, "bg": 2.4226902564399463e-06}, {"x": 0.8281547241597971, "y": 0.8750792644261256, "ox": 0.8281547241597971, "oy": 0.8750792644261256, "term": "possible", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 18, "s": 0.914711477488903, "os": 0.11425391328928307, "bg": 1.3760443691098826e-06}, {"x": 0.5573874445149017, "y": 0.5941661382371591, "ox": 0.5573874445149017, "oy": 0.5941661382371591, "term": "necessary", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 7, "s": 0.5180722891566265, "os": 0.023568071799583055, "bg": 7.413986023318304e-07}, {"x": 0.6512365250475587, "y": 0.761572606214331, "ox": 0.6512365250475587, "oy": 0.761572606214331, "term": "initial", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 9, "s": 0.8148383005707039, "os": 0.06081057206459135, "bg": 2.115191248261607e-06}, {"x": 0.6087507926442612, "y": 0.7618896639188333, "ox": 0.6087507926442612, "oy": 0.7618896639188333, "term": "phase", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 8, "s": 0.8354470513633481, "os": 0.06630507755909684, "bg": 2.099923902176266e-06}, {"x": 0.32656943563728597, "y": 0.6125554850982878, "ox": 0.32656943563728597, "oy": 0.6125554850982878, "term": "prior knowledge", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7232086239695625, "os": 0.043267022366700825, "bg": 0.0}, {"x": 0.41597970830691183, "y": 0.37317691819911225, "ox": 0.41597970830691183, "oy": 0.37317691819911225, "term": "training phase", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 0.0}, {"x": 0.4933417882054534, "y": 0.05009511731135067, "ox": 0.4933417882054534, "oy": 0.05009511731135067, "term": "soft", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.09099556119213698, "os": -0.01595349987632946, "bg": 6.049641696106785e-07}, {"x": 0.7415979708306912, "y": 0.41249207355738743, "ox": 0.7415979708306912, "oy": 0.41249207355738743, "term": "modal", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 12, "s": 0.05041217501585289, "os": -0.026412494258153417, "bg": 2.5647904540551134e-05}, {"x": 0.8493975903614458, "y": 0.6829422954977806, "ox": 0.8493975903614458, "oy": 0.6829422954977806, "term": "medical", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 21, "s": 0.03551046290424857, "os": -0.034062400621886146, "bg": 6.045582690174447e-07}, {"x": 0.7165504121750158, "y": 0.0212428662016487, "ox": 0.7165504121750158, "oy": 0.0212428662016487, "term": "medical image", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.019023462270133164, "os": -0.049856895516059506, "bg": 0.0}, {"x": 0.4936588459099556, "y": 0.012365250475586557, "ox": 0.4936588459099556, "oy": 0.012365250475586557, "term": "image fusion", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 0.0}, {"x": 0.8899809765377299, "y": 0.9207355738744452, "ox": 0.8899809765377299, "oy": 0.9207355738744452, "term": "fuzzy", "cat25k": 14, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 104, "ncat": 28, "s": 0.9654407102092581, "os": 0.17827991943747573, "bg": 4.8237283583153134e-05}, {"x": 0.12840837032339886, "y": 0.08814204185161699, "ox": 0.12840837032339886, "oy": 0.08814204185161699, "term": "logic", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 7.431634229165918e-07}, {"x": 0.9556119213696893, "y": 0.9648065948002537, "ox": 0.9556119213696893, "oy": 0.9648065948002537, "term": "analysis", "cat25k": 33, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 238, "ncat": 70, "s": 0.9901712111604313, "os": 0.24882512985406874, "bg": 4.920885187360708e-06}, {"x": 0.4162967660114141, "y": 0.5732403297400127, "ox": 0.4162967660114141, "oy": 0.5732403297400127, "term": "motivated", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.61572606214331, "os": 0.03134164870499275, "bg": 8.507267067039922e-06}, {"x": 0.6090678503487634, "y": 0.6832593532022828, "ox": 0.6090678503487634, "oy": 0.6832593532022828, "term": "computation", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.6769181991122385, "os": 0.037366170806685285, "bg": 1.1482849604221636e-05}, {"x": 0.8072289156626506, "y": 0.703868103994927, "ox": 0.8072289156626506, "oy": 0.703868103994927, "term": "power", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 16, "s": 0.19530754597336716, "os": -0.0001590049821561107, "bg": 3.8796036600427814e-07}, {"x": 0.7168674698795181, "y": 0.6956246036778694, "ox": 0.7168674698795181, "oy": 0.6956246036778694, "term": "increase", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 11, "s": 0.5221940393151554, "os": 0.02409808840677008, "bg": 9.244169796375631e-07}, {"x": 0.22320862396956245, "y": 0.4128091312618897, "ox": 0.22320862396956245, "oy": 0.4128091312618897, "term": "motivated by", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 0.0}, {"x": 0.5577045022194039, "y": 0.37349397590361444, "ox": 0.5577045022194039, "oy": 0.37349397590361444, "term": "meanwhile", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 7, "s": 0.17438173747622068, "os": -0.0021554008692272406, "bg": 4.55843463354684e-06}, {"x": 0.7641090678503487, "y": 0.8414711477488903, "ox": 0.7641090678503487, "oy": 0.8414711477488903, "term": "relevant", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 13, "s": 0.8985415345592898, "os": 0.09992579767499382, "bg": 2.946799191116817e-06}, {"x": 0.8902980342422321, "y": 0.924857324032974, "ox": 0.8902980342422321, "oy": 0.924857324032974, "term": "significantly", "cat25k": 15, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 109, "ncat": 28, "s": 0.9720989220038048, "os": 0.19435708985548217, "bg": 1.3115152716140949e-05}, {"x": 0.7419150285351934, "y": 0.7473050095117312, "ox": 0.7419150285351934, "oy": 0.7473050095117312, "term": "enhance", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 12, "s": 0.6816740646797718, "os": 0.03789618741387231, "bg": 3.992662905193731e-06}, {"x": 0.7422320862396956, "y": 0.7729866835764109, "ox": 0.7422320862396956, "oy": 0.7729866835764109, "term": "expert", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 12, "s": 0.7714013950538999, "os": 0.05075792374827745, "bg": 2.594779964644799e-06}, {"x": 0.7425491439441978, "y": 0.7232086239695624, "ox": 0.7425491439441978, "oy": 0.7232086239695624, "term": "intelligent", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 12, "s": 0.5827520608750792, "os": 0.02824988516306845, "bg": 5.8467320943829606e-06}, {"x": 0.9007609384908053, "y": 0.8899809765377299, "ox": 0.9007609384908053, "oy": 0.8899809765377299, "term": "reduce", "cat25k": 10, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 30, "s": 0.8528852251109702, "os": 0.07404332002402741, "bg": 5.241909767512566e-06}, {"x": 0.8633481293595434, "y": 0.8332276474318326, "ox": 0.8633481293595434, "oy": 0.8332276474318326, "term": "presents", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 23, "s": 0.6233354470513633, "os": 0.03211900639553372, "bg": 6.54020068740197e-06}, {"x": 0.4939759036144578, "y": 0.13982244768547875, "ox": 0.4939759036144578, "oy": 0.13982244768547875, "term": "aided", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 6, "s": 0.11953075459733671, "os": -0.00952263170912689, "bg": 7.896639754166323e-06}, {"x": 0.764426125554851, "y": 0.7476220672162334, "ox": 0.764426125554851, "oy": 0.7476220672162334, "term": "paper presents", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 13, "s": 0.6258719086873811, "os": 0.032401681919366807, "bg": 0.0}, {"x": 0.22352568167406467, "y": 0.050412175015852885, "ox": 0.22352568167406467, "oy": 0.050412175015852885, "term": "confirmed", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.1563405880428731e-06}, {"x": 0.2238427393785669, "y": 0.7622067216233355, "ox": 0.2238427393785669, "oy": 0.7622067216233355, "term": "cancer", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 3, "s": 0.8890298034242232, "os": 0.09377760503162431, "bg": 1.1636986532515487e-06}, {"x": 0.5580215599239061, "y": 0.012682308180088777, "ox": 0.5580215599239061, "oy": 0.012682308180088777, "term": "processed", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 1.4899162838388853e-06}, {"x": 0.6093849080532657, "y": 0.26759670259987317, "ox": 0.6093849080532657, "oy": 0.26759670259987317, "term": "investigates", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.09701965757767915, "os": -0.0140807745309353, "bg": 2.099742431595058e-05}, {"x": 0.22415979708306913, "y": 0.2679137603043754, "ox": 0.22415979708306913, "oy": 0.2679137603043754, "term": "significance", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 2.54777744144938e-06}, {"x": 0.7926442612555485, "y": 0.5735573874445149, "ox": 0.7926442612555485, "oy": 0.5735573874445149, "term": "impact", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 15, "s": 0.06150919467343057, "os": -0.0236034062400622, "bg": 1.1102322860642794e-06}, {"x": 0.8966391883322765, "y": 0.813570069752695, "ox": 0.8966391883322765, "oy": 0.813570069752695, "term": "natural", "cat25k": 6, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 29, "s": 0.08370323398858592, "os": -0.016925196989505675, "bg": 1.4050713791925278e-06}, {"x": 0.9942929613189601, "y": 0.9946100190234622, "ox": 0.9942929613189601, "oy": 0.9946100190234622, "term": "domain", "cat25k": 168, "ncat25k": 157, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1220, "ncat": 400, "s": 0.7476220672162334, "os": 0.04651779089078123, "bg": 3.6680673891453715e-05}, {"x": 0.9235890932149651, "y": 0.8966391883322765, "ox": 0.9235890932149651, "oy": 0.8966391883322765, "term": "end", "cat25k": 11, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 37, "s": 0.7783766645529487, "os": 0.05165895198049539, "bg": 1.058615987986826e-06}, {"x": 0.3268864933417882, "y": 0.032339885859226376, "ox": 0.3268864933417882, "oy": 0.032339885859226376, "term": "fuse", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 5.572125750495687e-06}, {"x": 0.5583386176284084, "y": 0.5504121750158529, "ox": 0.5583386176284084, "oy": 0.5504121750158529, "term": "modalities", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.42897907419150283, "os": 0.017137203632380478, "bg": 3.479175741516725e-05}, {"x": 0.9166138237159163, "y": 0.9448319594166138, "ox": 0.9166138237159163, "oy": 0.9448319594166138, "term": "sets", "cat25k": 21, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 154, "ncat": 34, "s": 0.9958782498414712, "os": 0.2803611179816967, "bg": 7.3169844138501954e-06}, {"x": 0.8871274571972099, "y": 0.906150919467343, "ox": 0.8871274571972099, "oy": 0.906150919467343, "term": "non", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 27, "s": 0.9327837666455295, "os": 0.13232747959436064, "bg": 1.1714340569413924e-06}, {"x": 0.8874445149017122, "y": 0.9026632847178186, "ox": 0.8874445149017122, "oy": 0.9026632847178186, "term": "non -", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 86, "ncat": 27, "s": 0.9258084971464807, "os": 0.12589661142715802, "bg": 0.0}, {"x": 0.7929613189600507, "y": 0.7688649334178821, "ox": 0.7929613189600507, "oy": 0.7688649334178821, "term": "being", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 15, "s": 0.6154090044388079, "os": 0.031058973181159674, "bg": 4.1972886305524494e-07}, {"x": 0.5586556753329106, "y": 0.0326569435637286, "ox": 0.5586556753329106, "oy": 0.0326569435637286, "term": "frequency", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.05770450221940394, "os": -0.02466343945443624, "bg": 7.989512433345828e-07}, {"x": 0.6097019657577679, "y": 0.7479391249207356, "ox": 0.6097019657577679, "oy": 0.7479391249207356, "term": "sub", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 8, "s": 0.8123018389346862, "os": 0.059874209391894286, "bg": 1.71478363487455e-06}, {"x": 0.9058338617628409, "y": 0.8731769181991123, "ox": 0.9058338617628409, "oy": 0.8731769181991123, "term": "were", "cat25k": 9, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 31, "s": 0.6965757767913761, "os": 0.03960990777711035, "bg": 3.3979615281990076e-07}, {"x": 0.8075459733671528, "y": 0.7859860494610019, "ox": 0.8075459733671528, "oy": 0.7859860494610019, "term": "selected", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 16, "s": 0.6578947368421053, "os": 0.03521076993745803, "bg": 1.8675977127972243e-06}, {"x": 0.8570069752694991, "y": 0.8956880152187698, "ox": 0.8570069752694991, "oy": 0.8956880152187698, "term": "local", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 79, "ncat": 22, "s": 0.9311984781230184, "os": 0.1308611003144765, "bg": 7.454591995086405e-07}, {"x": 0.5589727330374128, "y": 0.012999365884590995, "ox": 0.5589727330374128, "oy": 0.012999365884590995, "term": "energy", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 1.6994963619008845e-07}, {"x": 0.49429296131896006, "y": 0.5272669625871909, "ox": 0.49429296131896006, "oy": 0.5272669625871909, "term": "entropy", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.4663918833227647, "os": 0.019416275043284685, "bg": 2.055906350784105e-05}, {"x": 0.689283449587825, "y": 0.683576410906785, "ox": 0.689283449587825, "oy": 0.683576410906785, "term": "according", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 10, "s": 0.5516804058338618, "os": 0.026377159817674294, "bg": 9.892283610726933e-07}, {"x": 0.7932783766645529, "y": 0.8376664552948636, "ox": 0.7932783766645529, "oy": 0.8376664552948636, "term": "maximum", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 15, "s": 0.8725428027901078, "os": 0.08250591851878024, "bg": 2.264876303358141e-06}, {"x": 0.22447685478757134, "y": 0.3281547241597971, "ox": 0.22447685478757134, "oy": 0.3281547241597971, "term": "absolute", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 1.7042557518175124e-06}, {"x": 0.49461001902346224, "y": 0.7986683576410907, "ox": 0.49461001902346224, "oy": 0.7986683576410907, "term": "value", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 6, "s": 0.8941027266962587, "os": 0.09658669304971557, "bg": 5.156557020856451e-07}, {"x": 0.6896005072923272, "y": 0.6838934686112873, "ox": 0.6896005072923272, "oy": 0.6838934686112873, "term": "according to", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 10, "s": 0.5516804058338618, "os": 0.026377159817674294, "bg": 0.0}, {"x": 0.7428662016487001, "y": 0.3738110336081167, "ox": 0.7428662016487001, "oy": 0.3738110336081167, "term": "outperformed", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 12, "s": 0.043753963221306286, "os": -0.029627928341754706, "bg": 9.892644200746483e-05}, {"x": 0.12872542802790107, "y": 0.2682308180088776, "ox": 0.12872542802790107, "oy": 0.2682308180088776, "term": "mutual", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 1.4905188098505406e-06}, {"x": 0.8693722257450857, "y": 0.7539632213062778, "ox": 0.8693722257450857, "oy": 0.7539632213062778, "term": "in terms", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 24, "s": 0.05707038681039949, "os": -0.02482244443659236, "bg": 0.0}, {"x": 0.8405199746353836, "y": 0.7130627774254914, "ox": 0.8405199746353836, "oy": 0.7130627774254914, "term": "tuned", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 20, "s": 0.07894736842105263, "os": -0.018921592876576812, "bg": 2.4548243435411446e-05}, {"x": 0.7431832593532023, "y": 0.20291693088142043, "ox": 0.7431832593532023, "oy": 0.20291693088142043, "term": "alexnet", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 12, "s": 0.027901077996195307, "os": -0.039274230592558565, "bg": 0.0001818945617856849}, {"x": 0.22479391249207356, "y": 0.08845909955611922, "ox": 0.22479391249207356, "oy": 0.08845909955611922, "term": "stochastic", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 7.029390232029627e-06}, {"x": 0.4949270767279645, "y": 0.3284717818642993, "ox": 0.4949270767279645, "oy": 0.3284717818642993, "term": "gradient", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.19625871908687384, "os": 0.00012367054167697344, "bg": 8.858487486213979e-06}, {"x": 0.3272035510462904, "y": 0.032974001268230815, "ox": 0.3272035510462904, "oy": 0.032974001268230815, "term": "descent", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 5.105487888931813e-06}, {"x": 0.8284717818642993, "y": 0.6293595434369055, "ox": 0.8284717818642993, "oy": 0.6293595434369055, "term": "fine tuned", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 18, "s": 0.04248573240329741, "os": -0.03044062047277482, "bg": 0.0}, {"x": 0.9663918833227647, "y": 0.956563094483196, "ox": 0.9663918833227647, "oy": 0.956563094483196, "term": "first", "cat25k": 27, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 195, "ncat": 93, "s": 0.8519340519974635, "os": 0.07368997561923613, "bg": 9.95863168836292e-07}, {"x": 0.9816106531388713, "y": 0.974001268230818, "ox": 0.9816106531388713, "oy": 0.974001268230818, "term": "dataset", "cat25k": 43, "ncat25k": 58, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 311, "ncat": 148, "s": 0.7723525681674065, "os": 0.050863927069714765, "bg": 0.00027851814994773994}, {"x": 0.8811033608116677, "y": 0.9039315155358275, "ox": 0.8811033608116677, "oy": 0.9039315155358275, "term": "second", "cat25k": 12, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 26, "s": 0.9350031705770451, "os": 0.13460655100526484, "bg": 1.4162673241923766e-06}, {"x": 0.22511097019657578, "y": 0.26854787571337985, "ox": 0.22511097019657578, "oy": 0.26854787571337985, "term": "fused", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 1.635289034192007e-05}, {"x": 0.7935954343690552, "y": 0.7733037412809132, "ox": 0.7935954343690552, "oy": 0.7733037412809132, "term": "average", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 15, "s": 0.6493341788205453, "os": 0.034274407264760956, "bg": 1.009037326386396e-06}, {"x": 0.6100190234622701, "y": 0.5047558655675333, "ox": 0.6100190234622701, "oy": 0.5047558655675333, "term": "augmented", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 8, "s": 0.26981610653138866, "os": 0.005211829970672419, "bg": 2.845115470196823e-05}, {"x": 0.1290424857324033, "y": 0.140139505389981, "ox": 0.1290424857324033, "oy": 0.140139505389981, "term": "rotation", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 2.750045100739652e-06}, {"x": 0.4166138237159163, "y": 0.41312618896639186, "ox": 0.4166138237159163, "oy": 0.41312618896639186, "term": "usefulness", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 1.4841070684309404e-05}, {"x": 0.4169308814204185, "y": 0.37412809131261887, "ox": 0.4169308814204185, "oy": 0.37412809131261887, "term": "complementary", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 6.5013824042379835e-06}, {"x": 0.6515535827520609, "y": 0.5944831959416614, "ox": 0.6515535827520609, "oy": 0.5944831959416614, "term": "crucial", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 9, "s": 0.3804692454026633, "os": 0.01257906081057207, "bg": 5.735823590591944e-06}, {"x": 0.8734939759036144, "y": 0.8138871274571972, "ox": 0.8734939759036144, "oy": 0.8138871274571972, "term": "investigate", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 25, "s": 0.26949904882688647, "os": 0.005052824988516308, "bg": 1.3659560052889817e-05}, {"x": 0.7647431832593532, "y": 0.7482561826252377, "ox": 0.7647431832593532, "oy": 0.7482561826252377, "term": "depth", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 13, "s": 0.6258719086873811, "os": 0.032401681919366807, "bg": 2.865074843540443e-06}, {"x": 0.32752060875079264, "y": 0.5050729232720355, "ox": 0.32752060875079264, "oy": 0.5050729232720355, "term": "transferability", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 8.766969565465155e-05}, {"x": 0.225428027901078, "y": 0.08877615726062144, "ox": 0.225428027901078, "oy": 0.08877615726062144, "term": "incrementally", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 3.0105882388359863e-05}, {"x": 0.3278376664552949, "y": 0.4489537095751427, "ox": 0.3278376664552949, "oy": 0.4489537095751427, "term": "assess", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 2.802605707086132e-06}, {"x": 0.7171845275840203, "y": 0.7691819911223843, "ox": 0.7171845275840203, "oy": 0.7691819911223843, "term": "required", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 11, "s": 0.7875713379835131, "os": 0.05303699515918166, "bg": 5.885671421210135e-07}, {"x": 0.7435003170577045, "y": 0.7235256816740647, "ox": 0.7435003170577045, "oy": 0.7235256816740647, "term": "improvements", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 12, "s": 0.5827520608750792, "os": 0.02824988516306845, "bg": 4.430721865865592e-06}, {"x": 0.3281547241597971, "y": 0.3744451490171211, "ox": 0.3281547241597971, "oy": 0.3744451490171211, "term": "to assess", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 0.0}, {"x": 0.41724793912492075, "y": 0.02155992390615092, "ox": 0.41724793912492075, "oy": 0.02155992390615092, "term": "performance improvements", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 0.0}, {"x": 0.9289790741915028, "y": 0.9280279010779962, "ox": 0.9289790741915028, "oy": 0.9280279010779962, "term": "through", "cat25k": 16, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 115, "ncat": 40, "s": 0.9416613823715917, "os": 0.14450019433942266, "bg": 9.048338383570974e-07}, {"x": 0.49524413443246673, "y": 0.26886493341788203, "ox": 0.49524413443246673, "oy": 0.26886493341788203, "term": "wise", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 2.3649747243326337e-06}, {"x": 0.9213696892834495, "y": 0.9042485732403297, "ox": 0.9213696892834495, "oy": 0.9042485732403297, "term": "further", "cat25k": 12, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 36, "s": 0.8665187064045656, "os": 0.07966149606020989, "bg": 2.241271082776099e-06}, {"x": 0.8360811667723526, "y": 0.8142041851616995, "ox": 0.8360811667723526, "oy": 0.8142041851616995, "term": "potential", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 19, "s": 0.681991122384274, "os": 0.03801985795554927, "bg": 1.9374443126662766e-06}, {"x": 0.818008877615726, "y": 0.8817374762206721, "ox": 0.818008877615726, "oy": 0.8817374762206721, "term": "layers", "cat25k": 10, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 17, "s": 0.9299302473050095, "os": 0.12939472103459243, "bg": 1.8811986305738887e-05}, {"x": 0.807863031071655, "y": 0.8922003804692454, "ox": 0.807863031071655, "oy": 0.8922003804692454, "term": "improvement", "cat25k": 10, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 16, "s": 0.9486366518706405, "os": 0.15418183103070562, "bg": 4.924432443103e-06}, {"x": 0.3284717818642993, "y": 0.05072923272035511, "ox": 0.3284717818642993, "oy": 0.05072923272035511, "term": "performance improvement", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.6899175649968294, "y": 0.7863031071655041, "ox": 0.6899175649968294, "oy": 0.7863031071655041, "term": "random", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 10, "s": 0.8389346861128725, "os": 0.06817780290449099, "bg": 2.213717731111002e-06}, {"x": 0.8636651870640456, "y": 0.8262523779327837, "ox": 0.8636651870640456, "oy": 0.8262523779327837, "term": "weights", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 23, "s": 0.543436905516804, "os": 0.02568813822833116, "bg": 2.1116120819900307e-05}, {"x": 0.559289790741915, "y": 0.7041851616994292, "ox": 0.559289790741915, "oy": 0.7041851616994292, "term": "better than", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 7, "s": 0.7637920101458466, "os": 0.04929154446839334, "bg": 0.0}, {"x": 0.41756499682942294, "y": 0.2691819911223843, "ox": 0.41756499682942294, "oy": 0.2691819911223843, "term": "pipeline", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 1.8633279969689861e-06}, {"x": 0.6902346227013316, "y": 0.5738744451490171, "ox": 0.6902346227013316, "oy": 0.5738744451490171, "term": "classifying", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 10, "s": 0.2590361445783132, "os": 0.0038691212324652793, "bg": 5.747365625600748e-05}, {"x": 0.7438173747622068, "y": 0.7866201648700063, "ox": 0.7438173747622068, "oy": 0.7866201648700063, "term": "may be", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 12, "s": 0.8027901077996196, "os": 0.057188791915480014, "bg": 0.0}, {"x": 0.7650602409638554, "y": 0.74857324032974, "ox": 0.7650602409638554, "oy": 0.74857324032974, "term": "texture", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 13, "s": 0.6258719086873811, "os": 0.032401681919366807, "bg": 1.3564027961948774e-05}, {"x": 0.1293595434369055, "y": 0.26949904882688647, "ox": 0.1293595434369055, "oy": 0.26949904882688647, "term": "scaling", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 5.983533316313506e-06}, {"x": 0.7828154724159797, "y": 0.37476220672162336, "ox": 0.7828154724159797, "oy": 0.37476220672162336, "term": "partial", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 14, "s": 0.026949904882688648, "os": -0.0406169393307657, "bg": 3.4962528909641093e-06}, {"x": 0.7831325301204819, "y": 0.6128725428027901, "ox": 0.7831325301204819, "oy": 0.6128725428027901, "term": "least", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 14, "s": 0.11065313887127458, "os": -0.01167803257835412, "bg": 6.280239634206545e-07}, {"x": 0.5596068484464173, "y": 0.02187698161065314, "ox": 0.5596068484464173, "oy": 0.02187698161065314, "term": "square", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.04660748256182626, "os": -0.02787887353803753, "bg": 4.887222780340595e-07}, {"x": 0.6518706404565631, "y": 0.2698161065313887, "ox": 0.6518706404565631, "oy": 0.2698161065313887, "term": "coupled", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 9, "s": 0.0735573874445149, "os": -0.019575280025440796, "bg": 5.5956149227392836e-06}, {"x": 0.5599239061509195, "y": 0.75428027901078, "ox": 0.5599239061509195, "oy": 0.75428027901078, "term": "binary", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 7, "s": 0.839568801521877, "os": 0.06858414897000106, "bg": 6.550618989557434e-06}, {"x": 0.8877615726062144, "y": 0.7736207989854154, "ox": 0.8877615726062144, "oy": 0.7736207989854154, "term": "patterns", "cat25k": 5, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 27, "s": 0.037412809131261895, "os": -0.03165965866930498, "bg": 4.915370756047767e-06}, {"x": 0.7939124920735574, "y": 0.7324032974001268, "ox": 0.7939124920735574, "oy": 0.7324032974001268, "term": "variations", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 15, "s": 0.40171211160431197, "os": 0.014981802763153237, "bg": 9.618479859896386e-06}, {"x": 0.4178820545339252, "y": 0.527584020291693, "ox": 0.4178820545339252, "oy": 0.527584020291693, "term": "poses", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 5, "s": 0.5326569435637286, "os": 0.02491078053779018, "bg": 1.2531506771285762e-05}, {"x": 0.7834495878249842, "y": 0.6842105263157895, "ox": 0.7834495878249842, "oy": 0.6842105263157895, "term": "great", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 14, "s": 0.2606214331008243, "os": 0.004399137839652317, "bg": 2.6514794168039204e-07}, {"x": 0.3287888395688015, "y": 0.08909321496512365, "ox": 0.3287888395688015, "oy": 0.08909321496512365, "term": "local binary", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.8906150919467343, "y": 0.8639822447685479, "ox": 0.8906150919467343, "oy": 0.8639822447685479, "term": "issue", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 28, "s": 0.7454026632847179, "os": 0.04644712200982298, "bg": 1.4146016378024588e-06}, {"x": 0.4955611921369689, "y": 0.03329105897273304, "ox": 0.4955611921369689, "oy": 0.03329105897273304, "term": "scales", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 2.3421597267764218e-06}, {"x": 0.7175015852885225, "y": 0.6845275840202917, "ox": 0.7175015852885225, "oy": 0.6845275840202917, "term": "this issue", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 11, "s": 0.48065948002536457, "os": 0.020882654323168798, "bg": 0.0}, {"x": 0.7942295497780596, "y": 0.8145212428662016, "ox": 0.7942295497780596, "oy": 0.8145212428662016, "term": "makes", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 15, "s": 0.8126188966391884, "os": 0.05999787993357124, "bg": 1.362698888741768e-06}, {"x": 0.32910589727330375, "y": 0.4492707672796449, "ox": 0.32910589727330375, "oy": 0.4492707672796449, "term": "construction", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 4.0448996899146195e-07}, {"x": 0.49587824984147116, "y": 0.37507926442612555, "ox": 0.49587824984147116, "oy": 0.37507926442612555, "term": "quite", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 6, "s": 0.24381737476220672, "os": 0.0033391046252782552, "bg": 6.100802440239632e-07}, {"x": 0.2257450856055802, "y": 0.47939124920735576, "ox": 0.2257450856055802, "oy": 0.47939124920735576, "term": "costly", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.5916296766011414, "os": 0.0294689233595986, "bg": 6.810634654665823e-06}, {"x": 0.9010779961953076, "y": 0.914711477488903, "ox": 0.9010779961953076, "oy": 0.914711477488903, "term": "given", "cat25k": 13, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 30, "s": 0.9422954977805961, "os": 0.14478286986325573, "bg": 2.0194956866076624e-06}, {"x": 0.9682942295497781, "y": 0.9717818642993025, "ox": 0.9682942295497781, "oy": 0.9717818642993025, "term": "real", "cat25k": 40, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 291, "ncat": 97, "s": 0.9778059606848447, "os": 0.207395498392283, "bg": 2.6048505815579004e-06}, {"x": 0.9441978440076094, "y": 0.9521242866201649, "ox": 0.9441978440076094, "oy": 0.9521242866201649, "term": "world", "cat25k": 25, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 181, "ncat": 56, "s": 0.982561826252378, "os": 0.21735981060739906, "bg": 1.0968023703370884e-06}, {"x": 0.939441978440076, "y": 0.9467343056436271, "ox": 0.939441978440076, "oy": 0.9467343056436271, "term": "real world", "cat25k": 22, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 162, "ncat": 51, "s": 0.974001268230818, "os": 0.1998162609095085, "bg": 0.0}, {"x": 0.9543436905516804, "y": 0.963855421686747, "ox": 0.9543436905516804, "oy": 0.963855421686747, "term": "only", "cat25k": 32, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 231, "ncat": 68, "s": 0.9911223842739378, "os": 0.25016783859227587, "bg": 9.032205317372551e-07}, {"x": 0.744134432466709, "y": 0.7327203551046291, "ox": 0.744134432466709, "oy": 0.7327203551046291, "term": "subset", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 12, "s": 0.6185795814838301, "os": 0.03146531924666973, "bg": 1.8521406007310357e-05}, {"x": 0.9280279010779962, "y": 0.9384908053265695, "ox": 0.9280279010779962, "oy": 0.9384908053265695, "term": "classes", "cat25k": 18, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 133, "ncat": 39, "s": 0.9762206721623335, "os": 0.20465707925514998, "bg": 7.010675057086052e-06}, {"x": 0.9099556119213696, "y": 0.9299302473050095, "ox": 0.9099556119213696, "oy": 0.9299302473050095, "term": "classifiers", "cat25k": 16, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 118, "ncat": 32, "s": 0.9733671528218136, "os": 0.19810254054627044, "bg": 0.0004698497420524916}, {"x": 0.9397590361445783, "y": 0.9590995561192137, "ox": 0.9397590361445783, "oy": 0.9590995561192137, "term": "learned", "cat25k": 29, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 209, "ncat": 51, "s": 0.9971464806594801, "os": 0.3027101515847497, "bg": 2.2267456561866166e-05}, {"x": 0.49619530754597335, "y": 0.5507292327203551, "ox": 0.49619530754597335, "oy": 0.5507292327203551, "term": "maps", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 7.479122613530784e-07}, {"x": 0.8696892834495878, "y": 0.8506658211794547, "ox": 0.8696892834495878, "oy": 0.8506658211794547, "term": "promising", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 24, "s": 0.7612555485098289, "os": 0.04913253948623722, "bg": 2.9649645512864746e-05}, {"x": 0.7653772986683577, "y": 0.8021559923906151, "ox": 0.7653772986683577, "oy": 0.8021559923906151, "term": "reduction", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 13, "s": 0.8167406467977173, "os": 0.061340588671778384, "bg": 3.7690347820577914e-06}, {"x": 0.7444514901712111, "y": 0.8642993024730501, "ox": 0.7444514901712111, "oy": 0.8642993024730501, "term": "improved", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 12, "s": 0.9343690551680406, "os": 0.1343592099219109, "bg": 5.429189166885494e-06}, {"x": 0.32942295497780594, "y": 0.4134432466708941, "ox": 0.32942295497780594, "oy": 0.4134432466708941, "term": "followed", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 1.0736225280867548e-06}, {"x": 0.3297400126823082, "y": 0.3287888395688015, "ox": 0.3297400126823082, "oy": 0.3287888395688015, "term": "reasonable", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 1.121198911450401e-06}, {"x": 0.9239061509194674, "y": 0.9181991122384274, "ox": 0.9239061509194674, "oy": 0.9181991122384274, "term": "limited", "cat25k": 14, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 101, "ncat": 37, "s": 0.9194673430564363, "os": 0.11918306773612239, "bg": 2.5943265893782304e-06}, {"x": 0.4181991122384274, "y": 0.27013316423589095, "ox": 0.4181991122384274, "oy": 0.27013316423589095, "term": "requirements", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 3.0780428847093696e-07}, {"x": 0.9730500951173113, "y": 0.9708306911857958, "ox": 0.9730500951173113, "oy": 0.9708306911857958, "term": "adaptation", "cat25k": 39, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 286, "ncat": 110, "s": 0.9432466708941027, "os": 0.1460195752800254, "bg": 0.0001428182954564307}, {"x": 0.8909321496512366, "y": 0.8674698795180723, "ox": 0.8909321496512366, "oy": 0.8674698795180723, "term": "rank", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 28, "s": 0.7660114140773622, "os": 0.049662556093424265, "bg": 5.021947822507989e-06}, {"x": 0.9641724793912492, "y": 0.955294863665187, "ox": 0.9641724793912492, "oy": 0.955294863665187, "term": "domain adaptation", "cat25k": 27, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 193, "ncat": 87, "s": 0.8909321496512366, "os": 0.09473163492456094, "bg": 0.0}, {"x": 0.8081800887761572, "y": 0.7400126823081801, "ox": 0.8081800887761572, "oy": 0.7400126823081801, "term": "low rank", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 16, "s": 0.38173747622067217, "os": 0.01270273135224903, "bg": 0.0}, {"x": 0.948002536461636, "y": 0.9698795180722891, "ox": 0.948002536461636, "oy": 0.9698795180722891, "term": "distribution", "cat25k": 38, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 277, "ncat": 60, "s": 0.9993658845909956, "os": 0.35615349280944136, "bg": 1.0165090572389764e-05}, {"x": 0.7447685478757133, "y": 0.6578947368421053, "ox": 0.7447685478757133, "oy": 0.6578947368421053, "term": "alignment", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 12, "s": 0.31008243500317056, "os": 0.008957280661460731, "bg": 9.368468908523928e-06}, {"x": 0.6521876981610654, "y": 0.5053899809765378, "ox": 0.6521876981610654, "oy": 0.5053899809765378, "term": "da", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 9, "s": 0.19499048826886495, "os": -0.0002826755238330772, "bg": 1.4845103664694952e-06}, {"x": 0.7450856055802156, "y": 0.8148383005707038, "ox": 0.7450856055802156, "oy": 0.8148383005707038, "term": "leveraging", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 12, "s": 0.8595434369055168, "os": 0.07648139641708773, "bg": 4.8255357191233444e-05}, {"x": 0.9568801521876982, "y": 0.9597336715282181, "ox": 0.9568801521876982, "oy": 0.9597336715282181, "term": "existing", "cat25k": 29, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 210, "ncat": 71, "s": 0.9749524413443247, "os": 0.20152998127274652, "bg": 9.519898960332172e-06}, {"x": 0.6103360811667724, "y": 0.7913760304375397, "ox": 0.6103360811667724, "oy": 0.7913760304375397, "term": "applying", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 8, "s": 0.8715916296766011, "os": 0.08238224797710328, "bg": 6.543329896494745e-06}, {"x": 0.9311984781230184, "y": 0.9559289790741915, "ox": 0.9311984781230184, "oy": 0.9559289790741915, "term": "related", "cat25k": 27, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 194, "ncat": 42, "s": 0.9980976537729868, "os": 0.3232217942828875, "bg": 2.0401230833918897e-06}, {"x": 0.9559289790741915, "y": 0.9600507292327204, "ox": 0.9559289790741915, "oy": 0.9600507292327204, "term": "source domain", "cat25k": 29, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 212, "ncat": 70, "s": 0.9812935954343691, "os": 0.2134553549344546, "bg": 0.0}, {"x": 0.966708941027267, "y": 0.9762206721623335, "ox": 0.966708941027267, "oy": 0.9762206721623335, "term": "target domain", "cat25k": 45, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 324, "ncat": 96, "s": 0.9914394419784401, "os": 0.25147521289000385, "bg": 0.0}, {"x": 0.5602409638554217, "y": 0.6296766011414078, "ox": 0.5602409638554217, "oy": 0.6296766011414078, "term": "mainly", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 7, "s": 0.6036778693722258, "os": 0.02999893996678562, "bg": 3.5091920563266466e-06}, {"x": 0.8573240329740013, "y": 0.8303741280913126, "ox": 0.8573240329740013, "oy": 0.8303741280913126, "term": "focus", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 22, "s": 0.650285351934052, "os": 0.03439807780643794, "bg": 2.156164462810984e-06}, {"x": 0.9806594800253646, "y": 0.9828788839568802, "ox": 0.9806594800253646, "oy": 0.9828788839568802, "term": "domains", "cat25k": 59, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 431, "ncat": 142, "s": 0.9429296131896006, "os": 0.14545422423235932, "bg": 6.292297305666845e-05}, {"x": 0.6106531388712746, "y": 0.6959416613823716, "ox": 0.6106531388712746, "oy": 0.6959416613823716, "term": "exploiting", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7035510462904249, "os": 0.04058160489028657, "bg": 4.167190045892669e-05}, {"x": 0.6525047558655676, "y": 0.7625237793278377, "ox": 0.6525047558655676, "oy": 0.7625237793278377, "term": "either", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 9, "s": 0.8148383005707039, "os": 0.06081057206459135, "bg": 8.855440637317206e-07}, {"x": 0.4185161699429296, "y": 0.7739378566899175, "ox": 0.4185161699429296, "oy": 0.7739378566899175, "term": "statistical", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 5, "s": 0.8826886493341788, "os": 0.08921946220981591, "bg": 4.5312529416057975e-06}, {"x": 0.4188332276474318, "y": 0.41376030437539635, "ox": 0.4188332276474318, "oy": 0.41376030437539635, "term": "property", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 1.8748604303118205e-07}, {"x": 0.3300570703868104, "y": 0.6445783132530121, "ox": 0.3300570703868104, "oy": 0.6445783132530121, "term": "geometric", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 4, "s": 0.7663284717818644, "os": 0.0496978905339034, "bg": 1.3886928003221767e-05}, {"x": 0.22606214331008243, "y": 0.62999365884591, "ox": 0.22606214331008243, "oy": 0.62999365884591, "term": "independently", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 3, "s": 0.7786937222574509, "os": 0.051976961944807595, "bg": 6.497737617716264e-06}, {"x": 0.7656943563728599, "y": 0.8455928979074192, "ox": 0.7656943563728599, "oy": 0.8455928979074192, "term": "difference", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 13, "s": 0.9020291693088142, "os": 0.1031412317585951, "bg": 3.037219675511043e-06}, {"x": 0.8287888395688016, "y": 0.7916930881420419, "ox": 0.8287888395688016, "oy": 0.7916930881420419, "term": "focus on", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 18, "s": 0.5719720989220038, "os": 0.02743719303204832, "bg": 0.0}, {"x": 0.6905516804058338, "y": 0.3753963221306278, "ox": 0.6905516804058338, "oy": 0.3753963221306278, "term": "common subspace", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 10, "s": 0.07926442612555486, "os": -0.01863891735274372, "bg": 0.0}, {"x": 0.8084971464806595, "y": 0.7330374128091313, "ox": 0.8084971464806595, "oy": 0.7330374128091313, "term": "two domains", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 16, "s": 0.3335447051363348, "os": 0.009487297268647749, "bg": 0.0}, {"x": 0.4965123652504756, "y": 0.5741915028535194, "ox": 0.4965123652504756, "oy": 0.5741915028535194, "term": "by exploiting", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 6, "s": 0.5440710209258085, "os": 0.025847143210487256, "bg": 0.0}, {"x": 0.3303741280913126, "y": 0.5279010779961953, "ox": 0.3303741280913126, "oy": 0.5279010779961953, "term": "distribution difference", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.6049461001902346, "os": 0.030405286032295677, "bg": 0.0}, {"x": 0.8291058972733037, "y": 0.7545973367152822, "ox": 0.8291058972733037, "oy": 0.7545973367152822, "term": "properties", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 18, "s": 0.3027901077996195, "os": 0.008144588530440616, "bg": 1.7055291434506295e-06}, {"x": 0.9413443246670894, "y": 0.9578313253012049, "ox": 0.9413443246670894, "oy": 0.9578313253012049, "term": "each", "cat25k": 28, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 204, "ncat": 52, "s": 0.9961953075459734, "os": 0.284353909755839, "bg": 1.5009205127916037e-06}, {"x": 0.7178186429930248, "y": 0.6962587190868738, "ox": 0.7178186429930248, "oy": 0.6962587190868738, "term": "jointly", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 11, "s": 0.5221940393151554, "os": 0.02409808840677008, "bg": 1.6213118887814177e-05}, {"x": 0.8294229549778059, "y": 0.8354470513633482, "ox": 0.8294229549778059, "oy": 0.8354470513633482, "term": "could", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 18, "s": 0.8224476854787572, "os": 0.06280696795166248, "bg": 4.561344400335951e-07}, {"x": 0.4968294229549778, "y": 0.32910589727330375, "ox": 0.4968294229549778, "oy": 0.32910589727330375, "term": "yield", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.19625871908687384, "os": 0.00012367054167697344, "bg": 2.2290122597641057e-06}, {"x": 0.49714648065948003, "y": 0.6582117945466075, "ox": 0.49714648065948003, "oy": 0.6582117945466075, "term": "these two", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 6, "s": 0.712428662016487, "os": 0.04192431362849369, "bg": 0.0}, {"x": 0.49746353836398227, "y": 0.47970830691185795, "ox": 0.49746353836398227, "oy": 0.47970830691185795, "term": "each other", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 6, "s": 0.3823715916296766, "os": 0.012985406876082115, "bg": 0.0}, {"x": 0.6908687381103361, "y": 0.6303107165504122, "ox": 0.6908687381103361, "oy": 0.6303107165504122, "term": "inspired", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 10, "s": 0.39124920735573876, "os": 0.013515423483269139, "bg": 5.44118348461382e-06}, {"x": 0.6109701965757768, "y": 0.7742549143944197, "ox": 0.6109701965757768, "oy": 0.7742549143944197, "term": "theoretical", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 8, "s": 0.8503487634749525, "os": 0.07273594572629943, "bg": 8.506781086022932e-06}, {"x": 0.9102726696258719, "y": 0.9121750158528852, "ox": 0.9102726696258719, "oy": 0.9121750158528852, "term": "error", "cat25k": 13, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 32, "s": 0.9207355738744452, "os": 0.12093212253983956, "bg": 2.7893230337543114e-06}, {"x": 0.9315155358275206, "y": 0.9435637285986049, "ox": 0.9315155358275206, "oy": 0.9435637285986049, "term": "single", "cat25k": 21, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 150, "ncat": 42, "s": 0.9854153455928979, "os": 0.22675877177484893, "bg": 2.8447546912838504e-06}, {"x": 0.9340519974635384, "y": 0.9124920735573875, "ox": 0.9340519974635384, "oy": 0.9124920735573875, "term": "optimization", "cat25k": 13, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 45, "s": 0.7653772986683577, "os": 0.04950355111126814, "bg": 1.6933647710184297e-05}, {"x": 0.6528218135700697, "y": 0.4800253646163602, "ox": 0.6528218135700697, "oy": 0.4800253646163602, "term": "inspired by", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 9, "s": 0.16613823715916298, "os": -0.003498109607434366, "bg": 0.0}, {"x": 0.8183259353202282, "y": 0.8151553582752061, "ox": 0.8183259353202282, "oy": 0.8151553582752061, "term": "learns", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 17, "s": 0.7609384908053266, "os": 0.04900886894456026, "bg": 4.425524139092797e-05}, {"x": 0.22637920101458464, "y": 0.6306277742549143, "ox": 0.22637920101458464, "oy": 0.6306277742549143, "term": "appropriate", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 3, "s": 0.7786937222574509, "os": 0.051976961944807595, "bg": 7.152860111171468e-07}, {"x": 0.611287254280279, "y": 0.8100824350031706, "ox": 0.611287254280279, "oy": 0.8100824350031706, "term": "projection", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 8, "s": 0.8922003804692454, "os": 0.09524398431150843, "bg": 9.280646389882098e-06}, {"x": 0.8297400126823081, "y": 0.774571972098922, "ox": 0.8297400126823081, "oy": 0.774571972098922, "term": "reduced", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 18, "s": 0.4362714013950539, "os": 0.017790890781244476, "bg": 3.671871299442206e-06}, {"x": 0.6531388712745719, "y": 0.5745085605580216, "ox": 0.6531388712745719, "oy": 0.5745085605580216, "term": "projected", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 9, "s": 0.33259353202282815, "os": 0.009363626726970775, "bg": 6.957509619067651e-06}, {"x": 0.6534559289790742, "y": 0.007926442612555484, "ox": 0.6534559289790742, "oy": 0.007926442612555484, "term": "aligned", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.02219403931515536, "os": -0.045298752694251085, "bg": 5.6977799118553444e-06}, {"x": 0.22669625871908688, "y": 0.051046290424857324, "ox": 0.22669625871908688, "oy": 0.051046290424857324, "term": "reconstructed", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.2311648871774179e-05}, {"x": 0.2270133164235891, "y": 0.08941027266962587, "ox": 0.2270133164235891, "oy": 0.08941027266962587, "term": "be reduced", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.9384908053265695, "y": 0.904565630944832, "ox": 0.9384908053265695, "oy": 0.904565630944832, "term": "label", "cat25k": 12, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 49, "s": 0.3031071655041217, "os": 0.008232924631638439, "bg": 6.539896070953063e-06}, {"x": 0.6537729866835764, "y": 0.7920101458465441, "ox": 0.6537729866835764, "oy": 0.7920101458465441, "term": "introduced", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 9, "s": 0.8601775523145212, "os": 0.07688774248259778, "bg": 3.2650516632789533e-06}, {"x": 0.8408370323398858, "y": 0.8753963221306278, "ox": 0.8408370323398858, "oy": 0.8753963221306278, "term": "distance", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 20, "s": 0.9026632847178186, "os": 0.10326490230027208, "bg": 2.9057105930284787e-06}, {"x": 0.9670259987317692, "y": 0.9809765377298668, "ox": 0.9670259987317692, "oy": 0.9809765377298668, "term": "between", "cat25k": 54, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 389, "ncat": 96, "s": 0.9968294229549779, "os": 0.29649129006042185, "bg": 3.7939818569049673e-06}, {"x": 0.7945466074825618, "y": 0.673430564362714, "ox": 0.7945466074825618, "oy": 0.673430564362714, "term": "points", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 15, "s": 0.15979708306911858, "os": -0.004310801738454481, "bg": 8.887787198289065e-07}, {"x": 0.41915028535193405, "y": 0.37571337983513, "ox": 0.41915028535193405, "oy": 0.37571337983513, "term": "distance between", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 0.0}, {"x": 0.6540900443880786, "y": 0.41407736207989854, "ox": 0.6540900443880786, "oy": 0.41407736207989854, "term": "data points", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 9, "s": 0.11794546607482562, "os": -0.009928977774636936, "bg": 0.0}, {"x": 0.8411540900443881, "y": 0.6448953709575143, "ox": 0.8411540900443881, "oy": 0.6448953709575143, "term": "global", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 20, "s": 0.029486366518706404, "os": -0.03821419737818452, "bg": 9.180038923365036e-07}, {"x": 0.6116043119847813, "y": 0.20323398858592265, "ox": 0.6116043119847813, "oy": 0.20323398858592265, "term": "preserved", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 8, "s": 0.08306911857958149, "os": -0.01729620861453659, "bg": 6.546107896030864e-06}, {"x": 0.7948636651870641, "y": 0.7403297400126823, "ox": 0.7948636651870641, "oy": 0.7403297400126823, "term": "constraint", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 15, "s": 0.45212428662016485, "os": 0.01819723684675452, "bg": 1.769733564729135e-05}, {"x": 0.8088142041851617, "y": 0.8103994927076728, "ox": 0.8088142041851617, "oy": 0.8103994927076728, "term": "respectively", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 16, "s": 0.7774254914394421, "os": 0.05128794035546447, "bg": 7.027635708916266e-06}, {"x": 0.718135700697527, "y": 0.6131896005072923, "ox": 0.718135700697527, "oy": 0.6131896005072923, "term": "reconstruction", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 11, "s": 0.2691819911223842, "os": 0.004805483905162361, "bg": 8.621525794527486e-06}, {"x": 0.560558021559924, "y": 0.27045022194039314, "ox": 0.560558021559924, "oy": 0.27045022194039314, "term": "coefficient", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 7.246162144812845e-06}, {"x": 0.33069118579581486, "y": 0.05136334812935954, "ox": 0.33069118579581486, "oy": 0.05136334812935954, "term": "rank constraint", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.7660114140773621, "y": 0.6309448319594166, "ox": 0.7660114140773621, "oy": 0.6309448319594166, "term": "relationship", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 13, "s": 0.17279644895370957, "os": -0.002968093000247349, "bg": 1.4525632377036942e-06}, {"x": 0.7951807228915663, "y": 0.8417882054533925, "ox": 0.7951807228915663, "oy": 0.8417882054533925, "term": "graph", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 15, "s": 0.8823715916296766, "os": 0.08893678668598283, "bg": 8.757624924505148e-06}, {"x": 0.4194673430564363, "y": 0.7045022194039315, "ox": 0.4194673430564363, "oy": 0.7045022194039315, "term": "marginal", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 5, "s": 0.8132530120481929, "os": 0.06028055545740433, "bg": 1.443719546540815e-05}, {"x": 0.6911857958148383, "y": 0.7628408370323398, "ox": 0.6911857958148383, "oy": 0.7628408370323398, "term": "conditional", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 10, "s": 0.7948636651870641, "os": 0.05531606657008586, "bg": 1.684233534324773e-05}, {"x": 0.49778059606848446, "y": 0.2707672796448954, "ox": 0.49778059606848446, "oy": 0.2707672796448954, "term": "minimized", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 2.202189940258716e-05}, {"x": 0.6119213696892835, "y": 0.7923272035510462, "ox": 0.6119213696892835, "oy": 0.7923272035510462, "term": "shift", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 8, "s": 0.8715916296766011, "os": 0.08238224797710328, "bg": 4.789825213789608e-06}, {"x": 0.33100824350031705, "y": 0.0897273303741281, "ox": 0.33100824350031705, "oy": 0.0897273303741281, "term": "statistically", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 6.125940993265641e-06}, {"x": 0.12967660114140775, "y": 0.1404565630944832, "ox": 0.12967660114140775, "oy": 0.1404565630944832, "term": "conditional distributions", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.4197844007609385, "y": 0.2710843373493976, "ox": 0.4197844007609385, "oy": 0.2710843373493976, "term": "distributions between", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 0.0}, {"x": 0.05833861762840837, "y": 0.4143944197844008, "ox": 0.05833861762840837, "oy": 0.4143944197844008, "term": "domain shift", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 0.0}, {"x": 0.22733037412809132, "y": 0.6135066582117945, "ox": 0.22733037412809132, "oy": 0.6135066582117945, "term": "formulate", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 3, "s": 0.7606214331008244, "os": 0.048761527861206314, "bg": 2.2792336646610993e-05}, {"x": 0.9499048826886494, "y": 0.9616360177552314, "ox": 0.9499048826886494, "oy": 0.9616360177552314, "term": "classifier", "cat25k": 31, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 222, "ncat": 63, "s": 0.9930247305009512, "os": 0.261563195646797, "bg": 0.00056253238262071}, {"x": 0.9216867469879518, "y": 0.8532022828154724, "ox": 0.9216867469879518, "oy": 0.8532022828154724, "term": "design", "cat25k": 8, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 36, "s": 0.10304375396322132, "os": -0.01358609236422742, "bg": 7.102935112250838e-07}, {"x": 0.4980976537729867, "y": 0.3760304375396322, "ox": 0.4980976537729867, "oy": 0.3760304375396322, "term": "minimization", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 6, "s": 0.24381737476220672, "os": 0.0033391046252782552, "bg": 3.2150550310252814e-05}, {"x": 0.9445149017121116, "y": 0.9324667089410272, "ox": 0.9445149017121116, "oy": 0.9324667089410272, "term": "effective", "cat25k": 17, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 122, "ncat": 56, "s": 0.8649334178820546, "os": 0.07909614501254375, "bg": 4.672413788125092e-06}, {"x": 0.9701965757767914, "y": 0.9790741915028536, "ox": 0.9701965757767914, "oy": 0.9790741915028536, "term": "algorithm", "cat25k": 51, "ncat25k": 40, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 367, "ncat": 101, "s": 0.9920735573874445, "os": 0.252941592169888, "bg": 5.6092798583584925e-05}, {"x": 0.6915028535193405, "y": 0.3763474952441344, "ox": 0.6915028535193405, "oy": 0.3763474952441344, "term": "direction", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 10, "s": 0.07926442612555486, "os": -0.01863891735274372, "bg": 1.2903424466212455e-06}, {"x": 0.22764743183259353, "y": 0.4495878249841471, "ox": 0.22764743183259353, "oy": 0.4495878249841471, "term": "we formulate", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 0.0}, {"x": 0.7954977805960685, "y": 0.7631578947368421, "ox": 0.7954977805960685, "oy": 0.7631578947368421, "term": "an effective", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 15, "s": 0.579581483830057, "os": 0.02784353909755838, "bg": 0.0}, {"x": 0.7837666455294864, "y": 0.8459099556119214, "ox": 0.7837666455294864, "oy": 0.8459099556119214, "term": "functions", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 14, "s": 0.8963221306277742, "os": 0.09764672626408961, "bg": 2.651430343879564e-06}, {"x": 0.8363982244768547, "y": 0.8357641090678504, "ox": 0.8363982244768547, "oy": 0.8357641090678504, "term": "term", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 19, "s": 0.803424223208624, "os": 0.05731246245715699, "bg": 1.2015746532839173e-06}, {"x": 0.6918199112238428, "y": 0.5057070386810399, "ox": 0.6918199112238428, "oy": 0.5057070386810399, "term": "five", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 10, "s": 0.14552948636651872, "os": -0.005777181018338573, "bg": 4.981001454672738e-07}, {"x": 0.9502219403931516, "y": 0.9454660748256183, "ox": 0.9502219403931516, "oy": 0.9454660748256183, "term": "several", "cat25k": 21, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 155, "ncat": 63, "s": 0.930564362714014, "os": 0.12973039821914423, "bg": 4.0437510116333515e-06}, {"x": 0.692136968928345, "y": 0.7406467977171846, "ox": 0.692136968928345, "oy": 0.7406467977171846, "term": "comparable", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 10, "s": 0.7403297400126824, "os": 0.045669764319282, "bg": 1.0496550571068582e-05}, {"x": 0.3313253012048193, "y": 0.2714013950538998, "ox": 0.3313253012048193, "oy": 0.2714013950538998, "term": "modern", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 4.889898264793405e-07}, {"x": 0.5608750792644261, "y": 0.05168040583386176, "ox": 0.5608750792644261, "oy": 0.05168040583386176, "term": "experiments conducted", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.06785034876347495, "os": -0.021448005370834956, "bg": 0.0}, {"x": 0.6544071020925808, "y": 0.41471147748890297, "ox": 0.6544071020925808, "oy": 0.41471147748890297, "term": "conducted on", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 9, "s": 0.11794546607482562, "os": -0.009928977774636936, "bg": 0.0}, {"x": 0.6122384273937856, "y": 0.5060240963855421, "ox": 0.6122384273937856, "oy": 0.5060240963855421, "term": "several state", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 8, "s": 0.26981610653138866, "os": 0.005211829970672419, "bg": 0.0}, {"x": 0.4201014584654407, "y": 0.271718452758402, "ox": 0.4201014584654407, "oy": 0.271718452758402, "term": "art approaches", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 0.0}, {"x": 0.3316423589093215, "y": 0.6138237159162968, "ox": 0.3316423589093215, "oy": 0.6138237159162968, "term": "constrained", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7232086239695625, "os": 0.043267022366700825, "bg": 1.7159039587963414e-05}, {"x": 0.9169308814204186, "y": 0.9311984781230184, "ox": 0.9169308814204186, "oy": 0.9311984781230184, "term": "clustering", "cat25k": 17, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 120, "ncat": 34, "s": 0.9717818642993025, "os": 0.193544397724462, "bg": 0.0001059578225942672}, {"x": 0.4204185161699429, "y": 0.4499048826886493, "ox": 0.4204185161699429, "oy": 0.4499048826886493, "term": "utility", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.4032974001268231, "os": 0.015264478286986322, "bg": 1.3318334085293485e-06}, {"x": 0.9105897273303741, "y": 0.9327837666455295, "ox": 0.9105897273303741, "oy": 0.9327837666455295, "term": "function", "cat25k": 17, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 122, "ncat": 32, "s": 0.9790741915028536, "os": 0.21096427688067562, "bg": 3.2727685899817393e-06}, {"x": 0.4984147114774889, "y": 0.5510462904248573, "ox": 0.4984147114774889, "oy": 0.5510462904248573, "term": "addressing", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 4.995171334376769e-06}, {"x": 0.7184527584020292, "y": 0.7549143944197844, "ox": 0.7184527584020292, "oy": 0.7549143944197844, "term": "issues", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 11, "s": 0.7479391249207357, "os": 0.04660612699197908, "bg": 5.986542213194457e-07}, {"x": 0.8639822447685479, "y": 0.9261255548509829, "ox": 0.8639822447685479, "oy": 0.9261255548509829, "term": "labels", "cat25k": 15, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 112, "ncat": 23, "s": 0.9860494610019024, "os": 0.2282604854952122, "bg": 1.4470663142256267e-05}, {"x": 0.9584654407102092, "y": 0.9527584020291693, "ox": 0.9584654407102092, "oy": 0.9527584020291693, "term": "many", "cat25k": 25, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 182, "ncat": 74, "s": 0.9334178820545339, "os": 0.13266315677891238, "bg": 1.6040221909076573e-06}, {"x": 0.9435637285986049, "y": 0.9369055168040583, "ox": 0.9435637285986049, "oy": 0.9369055168040583, "term": "has been", "cat25k": 18, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 129, "ncat": 55, "s": 0.9042485732403297, "os": 0.10388325500865692, "bg": 0.0}, {"x": 0.6547241597970831, "y": 0.7694990488268865, "ox": 0.6547241597970831, "oy": 0.7694990488268865, "term": "data mining", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 9, "s": 0.8268864933417883, "os": 0.06402600614819265, "bg": 0.0}, {"x": 0.8091312618896639, "y": 0.6585288522511097, "ox": 0.8091312618896639, "oy": 0.6585288522511097, "term": "although", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 16, "s": 0.10494610019023462, "os": -0.013020741316561252, "bg": 1.0023765219662232e-06}, {"x": 0.6125554850982878, "y": 0.5948002536461636, "ox": 0.6125554850982878, "oy": 0.5948002536461636, "term": "considerable", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 8, "s": 0.45085605580215593, "os": 0.018073566305077567, "bg": 5.209501013527028e-06}, {"x": 0.3319594166138237, "y": 0.033608116677235254, "ox": 0.3319594166138237, "oy": 0.033608116677235254, "term": "predicts", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 7.916249597590644e-06}, {"x": 0.9181991122384274, "y": 0.8937856689917565, "ox": 0.9181991122384274, "oy": 0.8937856689917565, "term": "instances", "cat25k": 11, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 35, "s": 0.7872542802790109, "os": 0.05300166071870252, "bg": 2.5260931893821084e-05}, {"x": 0.49873176918199114, "y": 0.09004438807863031, "ox": 0.49873176918199114, "oy": 0.09004438807863031, "term": "near", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 6, "s": 0.10589727330374128, "os": -0.012738065792728175, "bg": 3.1752365935954355e-07}, {"x": 0.6128725428027901, "y": 0.14077362079898542, "ox": 0.6128725428027901, "oy": 0.14077362079898542, "term": "boundary", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 8, "s": 0.07133798351299936, "os": -0.020511642698137874, "bg": 2.4756188292783084e-06}, {"x": 0.42073557387444516, "y": 0.5748256182625238, "ox": 0.42073557387444516, "oy": 0.5748256182625238, "term": "determine", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.61572606214331, "os": 0.03134164870499275, "bg": 1.1178811494380027e-06}, {"x": 0.7187698161065313, "y": 0.5951173113506658, "ox": 0.7187698161065313, "oy": 0.5951173113506658, "term": "whole", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 11, "s": 0.22733037412809132, "os": 0.0015900498215610792, "bg": 7.257111246236442e-07}, {"x": 0.4990488268864933, "y": 0.5513633481293595, "ox": 0.4990488268864933, "oy": 0.5513633481293595, "term": "promising results", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 0.0}, {"x": 0.7190868738110336, "y": 0.4150285351934052, "ox": 0.7190868738110336, "oy": 0.4150285351934052, "term": "most existing", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 11, "s": 0.0700697526949905, "os": -0.020917988763647928, "bg": 0.0}, {"x": 0.42105263157894735, "y": 0.6965757767913761, "ox": 0.42105263157894735, "oy": 0.6965757767913761, "term": "little", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 5, "s": 0.8021559923906152, "os": 0.05706512137380305, "bg": 3.415638971614509e-07}, {"x": 0.7194039315155358, "y": 0.5282181357006975, "ox": 0.7194039315155358, "oy": 0.5282181357006975, "term": "done", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 11, "s": 0.13157894736842105, "os": -0.008056252429242787, "bg": 5.434596005428306e-07}, {"x": 0.4213696892834496, "y": 0.09036144578313253, "ox": 0.4213696892834496, "oy": 0.09036144578313253, "term": "regarding", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 4.7392794980012986e-07}, {"x": 0.8414711477488903, "y": 0.8421052631578947, "ox": 0.8414711477488903, "oy": 0.8421052631578947, "term": "develop", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 20, "s": 0.8180088776157262, "os": 0.06146425921345536, "bg": 2.6503352530833787e-06}, {"x": 0.9109067850348763, "y": 0.9086873811033608, "ox": 0.9109067850348763, "oy": 0.9086873811033608, "term": "unsupervised", "cat25k": 12, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 32, "s": 0.9115409004438808, "os": 0.11128582028903575, "bg": 0.00035056665364014626}, {"x": 0.05865567533291059, "y": 0.37666455294863666, "ox": 0.05865567533291059, "oy": 0.37666455294863666, "term": "guide", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 1.2171731787725637e-07}, {"x": 0.8814204185161699, "y": 0.8969562460367787, "ox": 0.8814204185161699, "oy": 0.8969562460367787, "term": "semi", "cat25k": 11, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 26, "s": 0.9121750158528852, "os": 0.11209851242005583, "bg": 1.2708581847553246e-05}, {"x": 0.49936588459099557, "y": 0.45022194039315155, "ox": 0.49936588459099557, "oy": 0.45022194039315155, "term": "fashion", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 9.067297436235252e-07}, {"x": 0.6131896005072923, "y": 0.7048192771084337, "ox": 0.6131896005072923, "oy": 0.7048192771084337, "term": "we develop", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 8, "s": 0.7285986049461003, "os": 0.04379703897388785, "bg": 0.0}, {"x": 0.6135066582117945, "y": 0.614140773620799, "ox": 0.6135066582117945, "oy": 0.614140773620799, "term": "unsupervised domain", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 8, "s": 0.4949270767279645, "os": 0.02128900038867885, "bg": 0.0}, {"x": 0.05897273303741281, "y": 0.27203551046290425, "ox": 0.05897273303741281, "oy": 0.27203551046290425, "term": "to guide", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.836715282181357, "y": 0.8677869372225745, "ox": 0.836715282181357, "oy": 0.8677869372225745, "term": "semi supervised", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 19, "s": 0.8972733037412809, "os": 0.0991131055439737, "bg": 0.0}, {"x": 0.8738110336081166, "y": 0.8509828788839569, "ox": 0.8738110336081166, "oy": 0.8509828788839569, "term": "re", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 25, "s": 0.7263792010145848, "os": 0.043638033991731745, "bg": 3.804411246388965e-07}, {"x": 0.49968294229549776, "y": 0.4803424223208624, "ox": 0.49968294229549776, "oy": 0.4803424223208624, "term": "formulated", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 6, "s": 0.3823715916296766, "os": 0.012985406876082115, "bg": 1.2868987578362942e-05}, {"x": 0.6924540266328472, "y": 0.5516804058338618, "ox": 0.6924540266328472, "oy": 0.5516804058338618, "term": "missing", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 10, "s": 0.2159162967660114, "os": 0.0006536871488639975, "bg": 1.5536585384800714e-06}, {"x": 0.8094483195941662, "y": 0.7133798351299937, "ox": 0.8094483195941662, "oy": 0.7133798351299937, "term": "values", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 16, "s": 0.24318325935320229, "os": 0.003056429101445171, "bg": 1.2824676090362157e-06}, {"x": 0.8300570703868104, "y": 0.842422320862397, "ox": 0.8300570703868104, "oy": 0.842422320862397, "term": "re -", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 18, "s": 0.8487634749524413, "os": 0.07245327020246635, "bg": 0.0}, {"x": 0.8642993024730501, "y": 0.8681039949270767, "ox": 0.8642993024730501, "oy": 0.8681039949270767, "term": "furthermore", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 23, "s": 0.8608116677235257, "os": 0.07713508356595172, "bg": 1.488065203938481e-05}, {"x": 0.12999365884590997, "y": 0.09067850348763475, "ox": 0.12999365884590997, "oy": 0.09067850348763475, "term": "exactly", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 5.626662678821591e-07}, {"x": 0.849714648065948, "y": 0.7136968928344959, "ox": 0.849714648065948, "oy": 0.7136968928344959, "term": "k", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 21, "s": 0.058972733037412815, "os": -0.0244160983710823, "bg": 5.105195488528536e-07}, {"x": 0.6927710843373494, "y": 0.7634749524413443, "ox": 0.6927710843373494, "oy": 0.7634749524413443, "term": "means", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 10, "s": 0.7948636651870641, "os": 0.05531606657008586, "bg": 8.476365007685379e-07}, {"x": 0.7454026632847178, "y": 0.7409638554216867, "ox": 0.7454026632847178, "oy": 0.7409638554216867, "term": "like", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 12, "s": 0.653138871274572, "os": 0.034680753330271014, "bg": 1.689655540734866e-07}, {"x": 0.13031071655041218, "y": 0.5285351934051997, "ox": 0.13031071655041218, "oy": 0.5285351934051997, "term": "update", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 2, "s": 0.7089410272669626, "os": 0.04139429702130666, "bg": 3.762521932903758e-07}, {"x": 0.3322764743183259, "y": 0.6312618896639188, "ox": 0.3322764743183259, "oy": 0.6312618896639188, "term": "rule", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 4, "s": 0.7457197209892201, "os": 0.046482456450302106, "bg": 8.943416209563504e-07}, {"x": 0.922003804692454, "y": 0.8902980342422321, "ox": 0.922003804692454, "oy": 0.8902980342422321, "term": "efficient", "cat25k": 10, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 36, "s": 0.7083069118579582, "os": 0.041076287056994454, "bg": 9.571161108595688e-06}, {"x": 0.8576410906785035, "y": 0.8611287254280279, "ox": 0.8576410906785035, "oy": 0.8611287254280279, "term": "way", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 22, "s": 0.8585922637920101, "os": 0.07619872089325466, "bg": 5.494746842131377e-07}, {"x": 0.1306277742549144, "y": 0.14109067850348764, "ox": 0.1306277742549144, "oy": 0.14109067850348764, "term": "k means", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.5611921369689283, "y": 0.6848446417247939, "ox": 0.5611921369689283, "oy": 0.6848446417247939, "term": "optimization problem", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 7, "s": 0.7213062777425492, "os": 0.042860676301190774, "bg": 0.0}, {"x": 0.74571972098922, "y": 0.6452124286620164, "ox": 0.74571972098922, "oy": 0.6452124286620164, "term": "an efficient", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 12, "s": 0.27615726062143303, "os": 0.00574184657785945, "bg": 0.0}, {"x": 0.8303741280913126, "y": 0.780279010779962, "ox": 0.8303741280913126, "oy": 0.780279010779962, "term": "widely", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 18, "s": 0.4819277108433735, "os": 0.021006324864845757, "bg": 8.340625174538751e-06}, {"x": 0.8969562460367787, "y": 0.8154724159797083, "ox": 0.8969562460367787, "oy": 0.8154724159797083, "term": "databases", "cat25k": 6, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 29, "s": 0.08370323398858592, "os": -0.016925196989505675, "bg": 6.773840106891196e-06}, {"x": 0.1309448319594166, "y": 0.20355104629042486, "ox": 0.1309448319594166, "oy": 0.20355104629042486, "term": "substantial", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 1.388146528456909e-06}, {"x": 0.9261255548509829, "y": 0.9356372859860494, "ox": 0.9261255548509829, "oy": 0.9356372859860494, "term": "experiments on", "cat25k": 17, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 125, "ncat": 38, "s": 0.9698795180722892, "os": 0.18764354616444648, "bg": 0.0}, {"x": 0.719720989220038, "y": 0.6968928344958782, "ox": 0.719720989220038, "oy": 0.6968928344958782, "term": "widely used", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 11, "s": 0.5221940393151554, "os": 0.02409808840677008, "bg": 0.0}, {"x": 0.8186429930247305, "y": 0.8065948002536462, "ox": 0.8186429930247305, "oy": 0.8065948002536462, "term": "art methods", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 17, "s": 0.7175015852885226, "os": 0.0425780007773577, "bg": 0.0}, {"x": 0.9185161699429296, "y": 0.8994927076727964, "ox": 0.9185161699429296, "oy": 0.8994927076727964, "term": "heterogeneous", "cat25k": 11, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 83, "ncat": 35, "s": 0.8484464172479391, "os": 0.07229426522031027, "bg": 0.00010417351776754385}, {"x": 0.6138237159162968, "y": 0.2723525681674065, "ox": 0.6138237159162968, "oy": 0.2723525681674065, "term": "distance metric", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.09701965757767915, "os": -0.0140807745309353, "bg": 0.0}, {"x": 0.6930881420418517, "y": 0.5063411540900444, "ox": 0.6930881420418517, "oy": 0.5063411540900444, "term": "nonlinear", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 10, "s": 0.14552948636651872, "os": -0.005777181018338573, "bg": 1.3144002404341362e-05}, {"x": 0.7460367786937222, "y": 0.7926442612555485, "ox": 0.7460367786937222, "oy": 0.7926442612555485, "term": "goal", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 12, "s": 0.8142041851616996, "os": 0.060404225999081296, "bg": 2.4015279444447147e-06}, {"x": 0.9505389980976537, "y": 0.9549778059606848, "ox": 0.9505389980976537, "oy": 0.9549778059606848, "term": "improve", "cat25k": 26, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 191, "ncat": 63, "s": 0.9774889029803424, "os": 0.2069008162255751, "bg": 8.255229598573211e-06}, {"x": 0.8912492073557388, "y": 0.9150285351934052, "ox": 0.8912492073557388, "oy": 0.9150285351934052, "term": "transferring", "cat25k": 13, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 28, "s": 0.9502219403931516, "os": 0.15577188085226673, "bg": 6.982477612082032e-05}, {"x": 0.5615091946734305, "y": 0.7051363348129359, "ox": 0.5615091946734305, "oy": 0.7051363348129359, "term": "by leveraging", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 7, "s": 0.7637920101458466, "os": 0.04929154446839334, "bg": 0.0}, {"x": 0.5, "y": 0.7054533925174382, "ox": 0.5, "oy": 0.7054533925174382, "term": "transferring knowledge", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 6, "s": 0.7923272035510464, "os": 0.05478604996289883, "bg": 0.0}, {"x": 0.13126188966391883, "y": 0.20386810399492708, "ox": 0.13126188966391883, "oy": 0.20386810399492708, "term": "other related", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 0.0}, {"x": 0.5003170577045022, "y": 0.6737476220672163, "ox": 0.5003170577045022, "oy": 0.6737476220672163, "term": "related tasks", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 6, "s": 0.7355738744451491, "os": 0.045139747712094974, "bg": 0.0}, {"x": 0.05928979074191503, "y": 0.32942295497780594, "ox": 0.05928979074191503, "oy": 0.32942295497780594, "term": "examine", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 1.8097503012103158e-06}, {"x": 0.8646163601775523, "y": 0.8906150919467343, "ox": 0.8646163601775523, "oy": 0.8906150919467343, "term": "usually", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 23, "s": 0.9131261889663919, "os": 0.11250485848556588, "bg": 2.069511294410683e-06}, {"x": 0.8417882054533925, "y": 0.8224476854787571, "ox": 0.8417882054533925, "oy": 0.8224476854787571, "term": "aims", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 20, "s": 0.6883322764743184, "os": 0.03895622062824636, "bg": 1.0479548028042332e-05}, {"x": 0.614140773620799, "y": 0.3297400126823082, "ox": 0.614140773620799, "oy": 0.3297400126823082, "term": "label information", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 8, "s": 0.11128725428027901, "os": -0.010865340447334011, "bg": 0.0}, {"x": 0.33259353202282815, "y": 0.4806594800253646, "ox": 0.33259353202282815, "oy": 0.4806594800253646, "term": "applicable", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 4, "s": 0.5206087507926442, "os": 0.023974417865093106, "bg": 1.1045159880142576e-06}, {"x": 0.6934051997463538, "y": 0.8227647431832593, "ox": 0.6934051997463538, "oy": 0.8227647431832593, "term": "scenario", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 10, "s": 0.8893468611287254, "os": 0.0939012755733013, "bg": 1.1172418943365538e-05}, {"x": 0.42168674698795183, "y": 0.7333544705136334, "ox": 0.42168674698795183, "oy": 0.7333544705136334, "term": "drawn", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 5, "s": 0.8414711477488903, "os": 0.0699268577082082, "bg": 4.781416863977589e-06}, {"x": 0.13157894736842105, "y": 0.5066582117945466, "ox": 0.13157894736842105, "oy": 0.5066582117945466, "term": "are drawn", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 2, "s": 0.6829422954977806, "os": 0.03817886293770538, "bg": 0.0}, {"x": 0.22796448953709575, "y": 0.6455294863665187, "ox": 0.22796448953709575, "oy": 0.6455294863665187, "term": "drawn from", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 3, "s": 0.7935954343690552, "os": 0.05519239602840889, "bg": 0.0}, {"x": 0.13189600507292326, "y": 0.09099556119213698, "ox": 0.13189600507292326, "oy": 0.09099556119213698, "term": "htl", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 2.562328644271914e-05}, {"x": 0.3329105897273304, "y": 0.03392517438173748, "ox": 0.3329105897273304, "oy": 0.03392517438173748, "term": "transforming", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 5.112804080472128e-06}, {"x": 0.05960684844641725, "y": 0.2726696258719087, "ox": 0.05960684844641725, "oy": 0.2726696258719087, "term": "flexibility", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 1.7013195821342575e-06}, {"x": 0.9546607482561826, "y": 0.9505389980976537, "ox": 0.9546607482561826, "oy": 0.9505389980976537, "term": "applications", "cat25k": 24, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 174, "ncat": 68, "s": 0.9397590361445783, "os": 0.14084307974983212, "bg": 4.861619329460953e-06}, {"x": 0.6144578313253012, "y": 0.4809765377298668, "ox": 0.6144578313253012, "oy": 0.4809765377298668, "term": "transformations", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 8, "s": 0.22859860494610018, "os": 0.00199639588707113, "bg": 1.5914207200901992e-05}, {"x": 0.8421052631578947, "y": 0.9090044388078631, "ox": 0.8421052631578947, "oy": 0.9090044388078631, "term": "often", "cat25k": 12, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 20, "s": 0.9641724793912493, "os": 0.17721988622310167, "bg": 2.3711380251084548e-06}, {"x": 0.22828154724159797, "y": 0.4153455928979074, "ox": 0.22828154724159797, "oy": 0.4153455928979074, "term": "restricted", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 2.2396951103046343e-06}, {"x": 0.8741280913126189, "y": 0.8712745719720989, "ox": 0.8741280913126189, "oy": 0.8712745719720989, "term": "linear", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 25, "s": 0.841154090044388, "os": 0.06936150666054203, "bg": 7.766727546304365e-06}, {"x": 0.8306911857958148, "y": 0.7637920101458465, "ox": 0.8306911857958148, "oy": 0.7637920101458465, "term": "world applications", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 18, "s": 0.35890932149651233, "os": 0.011360022614041898, "bg": 0.0}, {"x": 0.22859860494610018, "y": 0.6458465440710209, "ox": 0.22859860494610018, "oy": 0.6458465440710209, "term": "are often", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 3, "s": 0.7935954343690552, "os": 0.05519239602840889, "bg": 0.0}, {"x": 0.7463538363982245, "y": 0.7869372225745086, "ox": 0.7463538363982245, "oy": 0.7869372225745086, "term": "us", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 12, "s": 0.8027901077996196, "os": 0.057188791915480014, "bg": 8.297111054525356e-08}, {"x": 0.8649334178820546, "y": 0.8826886493341788, "ox": 0.8649334178820546, "oy": 0.8826886493341788, "term": "general", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 23, "s": 0.8979074191502854, "os": 0.09964312215116072, "bg": 6.025857937213119e-07}, {"x": 0.5618262523779328, "y": 0.48129359543436906, "ox": 0.5618262523779328, "oy": 0.48129359543436906, "term": "flexible", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.2932783766645529, "os": 0.007490901381576619, "bg": 2.204371930850456e-06}, {"x": 0.8817374762206721, "y": 0.8646163601775523, "ox": 0.8817374762206721, "oy": 0.8646163601775523, "term": "any", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 26, "s": 0.8040583386176285, "os": 0.05743613299883396, "bg": 2.5036138399908104e-07}, {"x": 0.891566265060241, "y": 0.8772986683576411, "ox": 0.891566265060241, "oy": 0.8772986683576411, "term": "/", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 28, "s": 0.8214965123652506, "os": 0.06252429242782942, "bg": 0.0}, {"x": 0.8310082435003171, "y": 0.8230818008877616, "ox": 0.8310082435003171, "oy": 0.8230818008877616, "term": "in particular", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 18, "s": 0.7685478757133799, "os": 0.04994523161725735, "bg": 0.0}, {"x": 0.9632213062777425, "y": 0.9663918833227647, "ox": 0.9632213062777425, "oy": 0.9663918833227647, "term": "algorithms", "cat25k": 34, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 249, "ncat": 83, "s": 0.9759036144578314, "os": 0.20446273983251473, "bg": 6.423525271299621e-05}, {"x": 0.8652504755865568, "y": 0.8820545339251744, "ox": 0.8652504755865568, "oy": 0.8820545339251744, "term": "help", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 23, "s": 0.8934686112872543, "os": 0.09642768806755944, "bg": 3.0427705123061506e-07}, {"x": 0.422003804692454, "y": 0.05199746353836398, "ox": 0.422003804692454, "oy": 0.05199746353836398, "term": "represented as", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 0.0}, {"x": 0.9223208623969562, "y": 0.946417247939125, "ox": 0.9223208623969562, "oy": 0.946417247939125, "term": "how", "cat25k": 22, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 157, "ncat": 36, "s": 0.9952441344324667, "os": 0.27901840924348964, "bg": 6.747318035036602e-07}, {"x": 0.8189600507292327, "y": 0.8925174381737476, "ox": 0.8189600507292327, "oy": 0.8925174381737476, "term": "generalization", "cat25k": 10, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 17, "s": 0.9448319594166138, "os": 0.14868732553620012, "bg": 0.00012996659299563913}, {"x": 0.562143310082435, "y": 0.595434369055168, "ox": 0.562143310082435, "oy": 0.595434369055168, "term": "show how", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 7, "s": 0.5180722891566265, "os": 0.023568071799583055, "bg": 0.0}, {"x": 0.3332276474318326, "y": 0.631578947368421, "ox": 0.3332276474318326, "oy": 0.631578947368421, "term": "could be", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 4, "s": 0.7457197209892201, "os": 0.046482456450302106, "bg": 0.0}, {"x": 0.9226379201014585, "y": 0.9178820545339251, "ox": 0.9226379201014585, "oy": 0.9178820545339251, "term": "various", "cat25k": 14, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 99, "ncat": 36, "s": 0.9178820545339251, "os": 0.11824670506342533, "bg": 2.954463316486181e-06}, {"x": 0.9575142675967026, "y": 0.9641724793912492, "ox": 0.9575142675967026, "oy": 0.9641724793912492, "term": "demonstrate", "cat25k": 32, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 231, "ncat": 72, "s": 0.9857324032974002, "os": 0.22818981661425392, "bg": 4.029709197299217e-05}, {"x": 0.33354470513633483, "y": 0.3769816106531389, "ox": 0.33354470513633483, "oy": 0.3769816106531389, "term": "likely", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 5.724473334589259e-07}, {"x": 0.8192771084337349, "y": 0.8614457831325302, "ox": 0.8192771084337349, "oy": 0.8614457831325302, "term": "need", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 17, "s": 0.9036144578313253, "os": 0.10367124836578213, "bg": 4.94355751095296e-07}, {"x": 0.2289156626506024, "y": 0.14140773620798985, "ox": 0.2289156626506024, "oy": 0.14140773620798985, "term": "security", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 9.555042849329066e-08}, {"x": 0.22923272035510464, "y": 0.0523145212428662, "ox": 0.22923272035510464, "oy": 0.0523145212428662, "term": "baggage", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 6.998771326811515e-06}, {"x": 0.87000634115409, "y": 0.868421052631579, "ox": 0.87000634115409, "oy": 0.868421052631579, "term": "therefore", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 24, "s": 0.8462270133164236, "os": 0.07164057807144622, "bg": 2.824430495716278e-06}, {"x": 0.333861762840837, "y": 0.2041851616994293, "ox": 0.333861762840837, "oy": 0.2041851616994293, "term": "technologies", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 4.4479942479906996e-07}, {"x": 0.42232086239695626, "y": 0.2729866835764109, "ox": 0.42232086239695626, "oy": 0.2729866835764109, "term": "enabling", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 3.212356694090913e-06}, {"x": 0.8370323398858592, "y": 0.6972098922003804, "ox": 0.8370323398858592, "oy": 0.6972098922003804, "term": "public", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 19, "s": 0.07292327203551047, "os": -0.019857955549273873, "bg": 2.6322034858322267e-07}, {"x": 0.42263792010145845, "y": 0.7140139505389981, "ox": 0.42263792010145845, "oy": 0.7140139505389981, "term": "importance", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 5, "s": 0.8246670894102728, "os": 0.0634959895410056, "bg": 2.5484478678261004e-06}, {"x": 0.865567533291059, "y": 0.8579581483830057, "ox": 0.865567533291059, "oy": 0.8579581483830057, "term": "out", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 23, "s": 0.827837666455295, "os": 0.0642733472315466, "bg": 2.2377009508868898e-07}, {"x": 0.693722257450856, "y": 0.2045022194039315, "ox": 0.693722257450856, "oy": 0.2045022194039315, "term": "x", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 10, "s": 0.04597336715282181, "os": -0.02828521960354758, "bg": 7.467960132035423e-08}, {"x": 0.6147748890298034, "y": 0.02219403931515536, "ox": 0.6147748890298034, "oy": 0.02219403931515536, "term": "ray", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.03582752060875079, "os": -0.03337337903254302, "bg": 7.420028784765e-07}, {"x": 0.6550412175015853, "y": 0.551997463538364, "ox": 0.6550412175015853, "oy": 0.551997463538364, "term": "plays", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 9, "s": 0.2771084337349397, "os": 0.006148192643369493, "bg": 2.4562381375638004e-06}, {"x": 0.8373493975903614, "y": 0.7872542802790108, "ox": 0.8373493975903614, "oy": 0.7872542802790108, "term": "major", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 19, "s": 0.45561192136968925, "os": 0.01872725345394155, "bg": 9.444470389598735e-07}, {"x": 0.7466708941027267, "y": 0.7552314521242867, "ox": 0.7466708941027267, "oy": 0.7552314521242867, "term": "role", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 12, "s": 0.7086239695624604, "os": 0.04111162149747359, "bg": 1.2445899231776869e-06}, {"x": 0.5624603677869372, "y": 0.02251109701965758, "ox": 0.5624603677869372, "oy": 0.02251109701965758, "term": "x -", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.04660748256182626, "os": -0.02787887353803753, "bg": 0.0}, {"x": 0.5627774254914394, "y": 0.013316423589093214, "ox": 0.5627774254914394, "oy": 0.013316423589093214, "term": "- ray", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 0.0}, {"x": 0.6553582752060875, "y": 0.6461636017755231, "ox": 0.6553582752060875, "oy": 0.6461636017755231, "term": "role in", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 9, "s": 0.5069752694990488, "os": 0.02222536306137593, "bg": 0.0}, {"x": 0.842422320862397, "y": 0.8941027266962587, "ox": 0.842422320862397, "oy": 0.8941027266962587, "term": "person", "cat25k": 11, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 20, "s": 0.9362714013950539, "os": 0.13541924313628492, "bg": 1.319903635876633e-06}, {"x": 0.8744451490171211, "y": 0.8687381103360812, "ox": 0.8744451490171211, "oy": 0.8687381103360812, "term": "objects", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 25, "s": 0.8335447051363348, "os": 0.06614607257694075, "bg": 5.22359694772905e-06}, {"x": 0.8579581483830057, "y": 0.8306911857958148, "ox": 0.8579581483830057, "oy": 0.8306911857958148, "term": "within", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 22, "s": 0.650285351934052, "os": 0.03439807780643794, "bg": 5.427680984197265e-07}, {"x": 0.8918833227647431, "y": 0.8880786303107165, "ox": 0.8918833227647431, "oy": 0.8880786303107165, "term": "context", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 28, "s": 0.8709575142675967, "os": 0.08181689692943714, "bg": 5.000074388361611e-06}, {"x": 0.5006341154090045, "y": 0.5957514267596703, "ox": 0.5006341154090045, "oy": 0.5957514267596703, "term": "availability", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 6, "s": 0.5900443880786304, "os": 0.02906257729408855, "bg": 6.434463779939358e-07}, {"x": 0.7469879518072289, "y": 0.6464806594800253, "ox": 0.7469879518072289, "oy": 0.6464806594800253, "term": "employ", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 12, "s": 0.27615726062143303, "os": 0.00574184657785945, "bg": 1.1537253296440491e-05}, {"x": 0.6940393151553583, "y": 0.6467977171845276, "ox": 0.6940393151553583, "oy": 0.6467977171845276, "term": "generation", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 10, "s": 0.42802790107799615, "os": 0.016730857566870434, "bg": 1.6441163651755825e-06}, {"x": 0.6150919467343057, "y": 0.4505389980976538, "ox": 0.6150919467343057, "oy": 0.4505389980976538, "term": "we employ", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 0.0}, {"x": 0.7663284717818643, "y": 0.7238427393785669, "ox": 0.7663284717818643, "oy": 0.7238427393785669, "term": "article", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 13, "s": 0.5123652504755866, "os": 0.022755379668562947, "bg": 4.7200805618959535e-07}, {"x": 0.1322130627774255, "y": 0.6588459099556119, "ox": 0.1322130627774255, "oy": 0.6588459099556119, "term": "effort", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 2, "s": 0.8256182625237795, "os": 0.06390233560651568, "bg": 1.2695610363319315e-06}, {"x": 0.6943563728598605, "y": 0.6591629676601142, "ox": 0.6943563728598605, "oy": 0.6591629676601142, "term": "made", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 10, "s": 0.4727330374128091, "os": 0.019946291650471716, "bg": 2.3645270563216472e-07}, {"x": 0.8972733037412809, "y": 0.9143944197844007, "ox": 0.8972733037412809, "oy": 0.9143944197844007, "term": "perform", "cat25k": 13, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 96, "ncat": 29, "s": 0.9438807863031072, "os": 0.14706194127415992, "bg": 8.167278673526788e-06}, {"x": 0.9622701331642359, "y": 0.9613189600507293, "ox": 0.9622701331642359, "oy": 0.9613189600507293, "term": "object", "cat25k": 30, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 220, "ncat": 80, "s": 0.9610019023462271, "os": 0.17043567365110768, "bg": 8.565256325359713e-06}, {"x": 0.7473050095117312, "y": 0.7143310082435003, "ox": 0.7473050095117312, "oy": 0.7143310082435003, "term": "this article", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 12, "s": 0.5342422320862397, "os": 0.025034451079467154, "bg": 0.0}, {"x": 0.8500317057704502, "y": 0.7748890298034242, "ox": 0.8500317057704502, "oy": 0.7748890298034242, "term": "object detection", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 21, "s": 0.21813570069752694, "os": 0.001307374297727995, "bg": 0.0}, {"x": 0.8922003804692454, "y": 0.894419784400761, "ox": 0.8922003804692454, "oy": 0.894419784400761, "term": "deep neural", "cat25k": 11, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 28, "s": 0.885859226379201, "os": 0.09146319918024098, "bg": 0.0}, {"x": 0.6556753329105898, "y": 0.6975269499048827, "ox": 0.6556753329105898, "oy": 0.6975269499048827, "term": "built", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 9, "s": 0.6569435637285986, "os": 0.03508709939578107, "bg": 1.1662976522493054e-06}, {"x": 0.22954977805960686, "y": 0.528852251109702, "ox": 0.22954977805960686, "oy": 0.528852251109702, "term": "upon", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.6610653138871275, "os": 0.03589979152680117, "bg": 3.8405362494678703e-07}, {"x": 0.5009511731135067, "y": 0.03424223208623969, "ox": 0.5009511731135067, "oy": 0.03424223208623969, "term": "look", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 1.269666154846926e-07}, {"x": 0.5012682308180089, "y": 0.14172479391249207, "ox": 0.5012682308180089, "oy": 0.14172479391249207, "term": "once", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 6, "s": 0.11953075459733671, "os": -0.00952263170912689, "bg": 2.4974820252427825e-07}, {"x": 0.8503487634749525, "y": 0.7146480659480026, "ox": 0.8503487634749525, "oy": 0.7146480659480026, "term": "faster", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 21, "s": 0.058972733037412815, "os": -0.0244160983710823, "bg": 3.147408763544244e-06}, {"x": 0.8506658211794547, "y": 0.7929613189600507, "ox": 0.8506658211794547, "oy": 0.7929613189600507, "term": "region", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 21, "s": 0.3532022828154724, "os": 0.01095367654853184, "bg": 1.1869634250355007e-06}, {"x": 0.6946734305643627, "y": 0.022828154724159798, "ox": 0.6946734305643627, "oy": 0.022828154724159798, "term": "region based", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.023779327837666456, "os": -0.04436239002155401, "bg": 0.0}, {"x": 0.9686112872542803, "y": 0.9701965757767914, "ox": 0.9686112872542803, "oy": 0.9701965757767914, "term": "also", "cat25k": 38, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 278, "ncat": 97, "s": 0.972415979708307, "os": 0.19453376205787787, "bg": 1.215438994224945e-06}, {"x": 0.7840837032339886, "y": 0.787571337983513, "ox": 0.7840837032339886, "oy": 0.787571337983513, "term": "tested", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 14, "s": 0.7450856055802156, "os": 0.04619978092646902, "bg": 4.699802342275076e-06}, {"x": 0.501585288522511, "y": 0.6318960050729233, "ox": 0.501585288522511, "oy": 0.6318960050729233, "term": "studied", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 6, "s": 0.6585288522511097, "os": 0.035493445461291115, "bg": 4.2706063620443145e-06}, {"x": 0.22986683576410907, "y": 0.20481927710843373, "ox": 0.22986683576410907, "oy": 0.20481927710843373, "term": "4", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.33417882054533926, "y": 0.450856055802156, "ox": 0.33417882054533926, "oy": 0.450856055802156, "term": "2", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 0.0}, {"x": 0.929296131896005, "y": 0.9346861128725428, "ox": 0.929296131896005, "oy": 0.9346861128725428, "term": "traditional", "cat25k": 17, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 124, "ncat": 40, "s": 0.9622701331642359, "os": 0.17343910109183422, "bg": 6.822156225256035e-06}, {"x": 0.33449587824984145, "y": 0.05263157894736842, "ox": 0.33449587824984145, "oy": 0.05263157894736842, "term": "ml", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 9.977201595494215e-07}, {"x": 0.4229549778059607, "y": 0.20513633481293594, "ox": 0.4229549778059607, "oy": 0.20513633481293594, "term": "proposal", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 9.204410227414678e-07}, {"x": 0.9521242866201649, "y": 0.9514901712111604, "ox": 0.9521242866201649, "oy": 0.9514901712111604, "term": "its", "cat25k": 25, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 180, "ncat": 65, "s": 0.9606848446417248, "os": 0.17018833256775373, "bg": 9.318089223343805e-07}, {"x": 0.6949904882688649, "y": 0.826569435637286, "ox": 0.6949904882688649, "oy": 0.826569435637286, "term": "stage", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 10, "s": 0.8953709575142677, "os": 0.09711670965690258, "bg": 2.199082565502802e-06}, {"x": 0.6154090044388079, "y": 0.7241597970830691, "ox": 0.6154090044388079, "oy": 0.7241597970830691, "term": "produce", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 8, "s": 0.7698161065313888, "os": 0.050227907141090426, "bg": 2.1339927895191527e-06}, {"x": 0.7200380469245403, "y": 0.7149651236525048, "ox": 0.7200380469245403, "oy": 0.7149651236525048, "term": "traditional machine", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 11, "s": 0.6074825618262524, "os": 0.030528956573972643, "bg": 0.0}, {"x": 0.3348129359543437, "y": 0.034559289790741916, "ox": 0.3348129359543437, "oy": 0.034559289790741916, "term": "first stage", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 0.0}, {"x": 0.8195941661382372, "y": 0.6144578313253012, "ox": 0.8195941661382372, "oy": 0.6144578313253012, "term": "imagenet", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 17, "s": 0.04629042485732404, "os": -0.0281615490618706, "bg": 0.0003291183093712108}, {"x": 0.9838300570703868, "y": 0.9857324032974001, "ox": 0.9838300570703868, "oy": 0.9857324032974001, "term": "recognition", "cat25k": 72, "ncat25k": 69, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 526, "ncat": 176, "s": 0.9346861128725428, "os": 0.1345888837850252, "bg": 6.289147592049334e-05}, {"x": 0.33512999365884594, "y": 0.14204185161699429, "ox": 0.33512999365884594, "oy": 0.14204185161699429, "term": "requiring", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 2.0323535441578343e-06}, {"x": 0.4232720355104629, "y": 0.6147748890298034, "ox": 0.4232720355104629, "oy": 0.6147748890298034, "term": "per", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 5, "s": 0.6794546607482562, "os": 0.03777251687219533, "bg": 2.3725164446528893e-07}, {"x": 0.3354470513633481, "y": 0.05294863665187064, "ox": 0.3354470513633481, "oy": 0.05294863665187064, "term": "imagenet dataset", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.5630944831959417, "y": 0.03487634749524413, "ox": 0.5630944831959417, "oy": 0.03487634749524413, "term": "% accuracy", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.05770450221940394, "os": -0.02466343945443624, "bg": 0.0}, {"x": 0.42358909321496513, "y": 0.035193405199746355, "ox": 0.42358909321496513, "oy": 0.035193405199746355, "term": "comparative", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 1.9215295375118595e-06}, {"x": 0.5634115409004439, "y": 0.6851616994292962, "ox": 0.5634115409004439, "oy": 0.6851616994292962, "term": "imagery", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 7, "s": 0.7213062777425492, "os": 0.042860676301190774, "bg": 1.2866292127603207e-05}, {"x": 0.8509828788839569, "y": 0.8379835129993659, "ox": 0.8509828788839569, "oy": 0.8379835129993659, "term": "fully", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 21, "s": 0.76569435637286, "os": 0.04953888555174728, "bg": 2.9888764459278372e-06}, {"x": 0.9318325935320229, "y": 0.9391249207355739, "ox": 0.9318325935320229, "oy": 0.9391249207355739, "term": "view", "cat25k": 18, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 134, "ncat": 42, "s": 0.9708306911857958, "os": 0.1913889968552348, "bg": 5.842222884016248e-07}, {"x": 0.2301838934686113, "y": 0.3300570703868104, "ox": 0.2301838934686113, "oy": 0.3300570703868104, "term": "multi view", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.0}, {"x": 0.9013950538998098, "y": 0.9105897273303741, "ox": 0.9013950538998098, "oy": 0.9105897273303741, "term": "support", "cat25k": 13, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 92, "ncat": 30, "s": 0.9286620164870006, "os": 0.1287056994452493, "bg": 6.528540358338149e-07}, {"x": 0.9112238427393786, "y": 0.9166138237159163, "ox": 0.9112238427393786, "oy": 0.9166138237159163, "term": "vector", "cat25k": 14, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 98, "ncat": 32, "s": 0.9369055168040583, "os": 0.137009292957846, "bg": 1.609366015017861e-05}, {"x": 0.8427393785668992, "y": 0.8690551680405834, "ox": 0.8427393785668992, "oy": 0.8690551680405834, "term": "support vector", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 20, "s": 0.8883956880152187, "os": 0.0936186000494682, "bg": 0.0}, {"x": 0.7958148383005708, "y": 0.8024730500951173, "ox": 0.7958148383005708, "oy": 0.8024730500951173, "term": "vector machine", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 15, "s": 0.770133164235891, "os": 0.05035157768276739, "bg": 0.0}, {"x": 0.9296131896005073, "y": 0.9451490171211161, "ox": 0.9296131896005073, "oy": 0.9451490171211161, "term": "present", "cat25k": 21, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 154, "ncat": 40, "s": 0.989854153455929, "os": 0.24739408501466378, "bg": 3.893556585318763e-06}, {"x": 0.655992390615092, "y": 0.6150919467343057, "ox": 0.655992390615092, "oy": 0.6150919467343057, "term": "named", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 9, "s": 0.4150285351934052, "os": 0.015794494894173353, "bg": 1.7477192845909182e-06}, {"x": 0.9695624603677869, "y": 0.9695624603677869, "ox": 0.9695624603677869, "oy": 0.9695624603677869, "term": "both", "cat25k": 37, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 272, "ncat": 99, "s": 0.963855421686747, "os": 0.1771138829016643, "bg": 3.241876388616463e-06}, {"x": 0.9299302473050095, "y": 0.9283449587824985, "ox": 0.9299302473050095, "oy": 0.9283449587824985, "term": "text", "cat25k": 16, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 115, "ncat": 40, "s": 0.9416613823715917, "os": 0.14450019433942266, "bg": 1.4831746963966513e-06}, {"x": 0.906150919467343, "y": 0.922003804692454, "ox": 0.906150919467343, "oy": 0.922003804692454, "term": "we present", "cat25k": 15, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 106, "ncat": 31, "s": 0.9590995561192137, "os": 0.1682272711211618, "bg": 0.0}, {"x": 0.858275206087508, "y": 0.8157894736842105, "ox": 0.858275206087508, "oy": 0.8157894736842105, "term": "- view", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 22, "s": 0.4968294229549778, "os": 0.02153634147203279, "bg": 0.0}, {"x": 0.5019023462270134, "y": 0.6740646797717185, "ox": 0.5019023462270134, "oy": 0.6740646797717185, "term": "text classification", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 6, "s": 0.7355738744451491, "os": 0.045139747712094974, "bg": 0.0}, {"x": 0.5022194039315155, "y": 0.2733037412809131, "ox": 0.5022194039315155, "oy": 0.2733037412809131, "term": "proved", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 3.1956248699904956e-06}, {"x": 0.13253012048192772, "y": 0.1423589093214965, "ox": 0.13253012048192772, "oy": 0.1423589093214965, "term": "which aims", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.6563094483195941, "y": 0.6594800253646164, "ox": 0.6563094483195941, "oy": 0.6594800253646164, "term": "many real", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 9, "s": 0.541851616994293, "os": 0.025440797144977212, "bg": 0.0}, {"x": 0.9331008243500317, "y": 0.9426125554850983, "ox": 0.9331008243500317, "oy": 0.9426125554850983, "term": "across", "cat25k": 20, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 146, "ncat": 44, "s": 0.9784400760938491, "os": 0.20933889261863536, "bg": 4.946099039671919e-06}, {"x": 0.13284717818642994, "y": 0.3303741280913126, "ox": 0.13284717818642994, "oy": 0.3303741280913126, "term": "dimension", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 1.9762246493132355e-06}, {"x": 0.6566265060240963, "y": 0.3772986683576411, "ox": 0.6566265060240963, "oy": 0.3772986683576411, "term": "however most", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.10367786937222576, "os": -0.013144411858238225, "bg": 0.0}, {"x": 0.5637285986049461, "y": 0.6154090044388079, "ox": 0.5637285986049461, "oy": 0.6154090044388079, "term": "extend", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 7, "s": 0.5672162333544705, "os": 0.026783505883184337, "bg": 3.6536992530533834e-06}, {"x": 0.796131896005073, "y": 0.8027901077996196, "ox": 0.796131896005073, "oy": 0.8027901077996196, "term": "settings", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 15, "s": 0.770133164235891, "os": 0.05035157768276739, "bg": 2.695430982035591e-06}, {"x": 0.2305009511731135, "y": 0.09131261889663919, "ox": 0.2305009511731135, "oy": 0.09131261889663919, "term": "goals", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 5.405746551843204e-07}, {"x": 0.8199112238427394, "y": 0.8462270133164236, "ox": 0.8199112238427394, "oy": 0.8462270133164236, "term": "hand", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 17, "s": 0.8693722257450855, "os": 0.08116320978057313, "bg": 1.1849158728244604e-06}, {"x": 0.6569435637285986, "y": 0.5291693088142042, "ox": 0.6569435637285986, "oy": 0.5291693088142042, "term": "implemented", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 9, "s": 0.24128091312618896, "os": 0.0029327585597682046, "bg": 2.6810985388786358e-06}, {"x": 0.23081800887761572, "y": 0.14267596702599875, "ox": 0.23081800887761572, "oy": 0.14267596702599875, "term": "guarantee", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 5.522818768375768e-07}, {"x": 0.23113506658211794, "y": 0.48161065313887125, "ox": 0.23113506658211794, "oy": 0.48161065313887125, "term": "principle", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.5916296766011414, "os": 0.0294689233595986, "bg": 2.134670427639713e-06}, {"x": 0.4239061509194673, "y": 0.6322130627774255, "ox": 0.4239061509194673, "oy": 0.6322130627774255, "term": "other hand", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 5, "s": 0.7067216233354471, "os": 0.04098795095579661, "bg": 0.0}, {"x": 0.8658845909955611, "y": 0.8161065313887127, "ox": 0.8658845909955611, "oy": 0.8161065313887127, "term": "baseline", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 23, "s": 0.4162967660114141, "os": 0.016041835977527286, "bg": 1.4314172725543736e-05}, {"x": 0.7666455294863666, "y": 0.7932783766645529, "ox": 0.7666455294863666, "oy": 0.7932783766645529, "term": "we evaluate", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 13, "s": 0.793278376664553, "os": 0.05490972050457579, "bg": 0.0}, {"x": 0.6572606214331008, "y": 0.6325301204819277, "ox": 0.6572606214331008, "oy": 0.6325301204819277, "term": "evaluate our", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 9, "s": 0.46544071020925804, "os": 0.019009928977774634, "bg": 0.0}, {"x": 0.5640456563094484, "y": 0.03551046290424857, "ox": 0.5640456563094484, "oy": 0.03551046290424857, "term": "coral", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.05770450221940394, "os": -0.02466343945443624, "bg": 2.6521781123254894e-06}, {"x": 0.8512999365884591, "y": 0.14299302473050096, "ox": 0.8512999365884591, "oy": 0.14299302473050096, "term": "inception", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 21, "s": 0.003804692454026633, "os": -0.0919402141267093, "bg": 2.2089482207302935e-05}, {"x": 0.7669625871908687, "y": 0.0, "ox": 0.7669625871908687, "oy": 0.0, "term": "underwater", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.007292327203551046, "os": -0.07370764283947565, "bg": 5.014258816368777e-06}, {"x": 0.33576410906785037, "y": 0.33069118579581486, "ox": 0.33576410906785037, "oy": 0.33069118579581486, "term": "difficulty", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 2.3974479645906526e-06}, {"x": 0.9461001902346227, "y": 0.939441978440076, "ox": 0.9461001902346227, "oy": 0.939441978440076, "term": "three", "cat25k": 18, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 134, "ncat": 57, "s": 0.9086873811033608, "os": 0.1089714144376524, "bg": 1.7080376410538953e-06}, {"x": 0.61572606214331, "y": 0.5294863665187064, "ox": 0.61572606214331, "oy": 0.5294863665187064, "term": "following", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 8, "s": 0.30374128091312613, "os": 0.0084272640542737, "bg": 2.26304932172972e-07}, {"x": 0.8202282815472416, "y": 0.7878883956880152, "ox": 0.8202282815472416, "oy": 0.7878883956880152, "term": "challenges", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 17, "s": 0.5941661382371591, "os": 0.029716264442952542, "bg": 5.001070318352955e-06}, {"x": 0.6953075459733672, "y": 0.6328471781864299, "ox": 0.6953075459733672, "oy": 0.6328471781864299, "term": "embedded", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 10, "s": 0.39124920735573876, "os": 0.013515423483269139, "bg": 5.16671912770281e-06}, {"x": 0.657577679137603, "y": 0.6978440076093849, "ox": 0.657577679137603, "oy": 0.6978440076093849, "term": "nature", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 9, "s": 0.6569435637285986, "os": 0.03508709939578107, "bg": 8.900981696688813e-07}, {"x": 0.8662016487000634, "y": 0.7641090678503487, "ox": 0.8662016487000634, "oy": 0.7641090678503487, "term": "do", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 23, "s": 0.09067850348763476, "os": -0.016112504858485574, "bg": 1.219790902980732e-07}, {"x": 0.7476220672162334, "y": 0.61572606214331, "ox": 0.7476220672162334, "oy": 0.61572606214331, "term": "include", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 12, "s": 0.19435637285986052, "os": -0.0006890215893431278, "bg": 3.6103020296625695e-07}, {"x": 0.8205453392517438, "y": 0.8310082435003171, "ox": 0.8205453392517438, "oy": 0.8310082435003171, "term": "about", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 17, "s": 0.819277108433735, "os": 0.061870605278965415, "bg": 1.0758253491240305e-07}, {"x": 0.5643627140139506, "y": 0.6597970830691186, "ox": 0.5643627140139506, "oy": 0.6597970830691186, "term": "characteristics", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 7, "s": 0.6705770450221941, "os": 0.0364298081339882, "bg": 2.541004955164582e-06}, {"x": 0.13316423589093215, "y": 0.20545339251743816, "ox": 0.13316423589093215, "oy": 0.20545339251743816, "term": "tend", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 1.5634262856587334e-06}, {"x": 0.7672796448953709, "y": 0.6854787571337984, "ox": 0.7672796448953709, "oy": 0.6854787571337984, "term": "together", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 13, "s": 0.33798351299936585, "os": 0.009893643334157806, "bg": 8.267556267027059e-07}, {"x": 0.33608116677235256, "y": 0.6160431198478123, "ox": 0.33608116677235256, "oy": 0.6160431198478123, "term": "groups", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7232086239695625, "os": 0.043267022366700825, "bg": 3.9668317646179777e-07}, {"x": 0.7675967025998732, "y": 0.6163601775523145, "ox": 0.7675967025998732, "oy": 0.6163601775523145, "term": "do not", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 13, "s": 0.1436271401395054, "os": -0.006183527083848631, "bg": 0.0}, {"x": 0.3363982244768548, "y": 0.2057704502219404, "ox": 0.3363982244768548, "oy": 0.2057704502219404, "term": "information about", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 0.0}, {"x": 0.13348129359543437, "y": 0.14331008243500318, "ox": 0.13348129359543437, "oy": 0.14331008243500318, "term": "tend to", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.42422320862396956, "y": 0.45117311350665823, "ox": 0.42422320862396956, "oy": 0.45117311350665823, "term": "aid", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.4032974001268231, "os": 0.015264478286986322, "bg": 7.561684190040646e-07}, {"x": 0.7964489537095751, "y": 0.8557387444514901, "ox": 0.7964489537095751, "oy": 0.8557387444514901, "term": "objective", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 15, "s": 0.9055168040583387, "os": 0.10501395710398924, "bg": 6.518796001282441e-06}, {"x": 0.13379835129993659, "y": 0.09162967660114141, "ox": 0.13379835129993659, "oy": 0.09162967660114141, "term": "develop an", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.05992390615091947, "y": 0.5298034242232086, "ox": 0.05992390615091947, "oy": 0.5298034242232086, "term": "an accurate", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 1, "s": 0.7492073557387445, "os": 0.04688880251581216, "bg": 0.0}, {"x": 0.23145212428662015, "y": 0.27362079898541536, "ox": 0.23145212428662015, "oy": 0.27362079898541536, "term": "imbalanced", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 7.891725525786213e-05}, {"x": 0.8097653772986684, "y": 0.8972733037412809, "ox": 0.8097653772986684, "oy": 0.8972733037412809, "term": "subject", "cat25k": 11, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 16, "s": 0.958148383005707, "os": 0.16704356736511078, "bg": 7.486874105903456e-07}, {"x": 0.5025364616360177, "y": 0.6981610653138871, "ox": 0.5025364616360177, "oy": 0.6981610653138871, "term": "inter", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 6, "s": 0.7777425491439443, "os": 0.05157061587929755, "bg": 4.257992817024178e-06}, {"x": 0.9448319594166138, "y": 0.9635383639822448, "ox": 0.9448319594166138, "oy": 0.9635383639822448, "term": "class", "cat25k": 32, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 230, "ncat": 56, "s": 0.9974635383639823, "os": 0.30739196494823506, "bg": 2.9897719639768647e-06}, {"x": 0.6160431198478123, "y": 0.7412809131261889, "ox": 0.6160431198478123, "oy": 0.7412809131261889, "term": "variation", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 8, "s": 0.8015218769816107, "os": 0.05665877530829299, "bg": 7.095453809704364e-06}, {"x": 0.7479391249207356, "y": 0.6601141407736208, "ox": 0.7479391249207356, "oy": 0.6601141407736208, "term": "large number", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 12, "s": 0.31008243500317056, "os": 0.008957280661460731, "bg": 0.0}, {"x": 0.4245402663284718, "y": 0.660431198478123, "ox": 0.4245402663284718, "oy": 0.660431198478123, "term": "inter -", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 5, "s": 0.7520608750792644, "os": 0.04741881912299919, "bg": 0.0}, {"x": 0.060240963855421686, "y": 0.20608750792644262, "ox": 0.060240963855421686, "oy": 0.20608750792644262, "term": "class variation", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.6163601775523145, "y": 0.6166772352568167, "ox": 0.6163601775523145, "oy": 0.6166772352568167, "term": "focused", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 8, "s": 0.4949270767279645, "os": 0.02128900038867885, "bg": 2.876945881870221e-06}, {"x": 0.5646797717184527, "y": 0.5960684844641725, "ox": 0.5646797717184527, "oy": 0.5960684844641725, "term": "focused on", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 7, "s": 0.5180722891566265, "os": 0.023568071799583055, "bg": 0.0}, {"x": 0.8747622067216233, "y": 0.7152821813570069, "ox": 0.8747622067216233, "oy": 0.7152821813570069, "term": "architectures", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 25, "s": 0.0212428662016487, "os": -0.04639412034910427, "bg": 2.9958332951919375e-05}, {"x": 0.2317691819911224, "y": 0.09194673430564362, "ox": 0.2317691819911224, "oy": 0.09194673430564362, "term": "cnn architectures", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.6166772352568167, "y": 0.09226379201014584, "ox": 0.6166772352568167, "oy": 0.09226379201014584, "term": "accuracies", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 8, "s": 0.060875079264426125, "os": -0.02372707678173916, "bg": 8.033913828240279e-05}, {"x": 0.424857324032974, "y": 0.1436271401395054, "ox": 0.424857324032974, "oy": 0.1436271401395054, "term": "have achieved", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 0.0}, {"x": 0.9369055168040583, "y": 0.8849080532656943, "ox": 0.9369055168040583, "oy": 0.8849080532656943, "term": "segmentation", "cat25k": 10, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 48, "s": 0.032974001268230815, "os": -0.03450408112787531, "bg": 9.304341522058464e-05}, {"x": 0.8313253012048193, "y": 0.8383005707038681, "ox": 0.8313253012048193, "oy": 0.8383005707038681, "term": "recommender", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 18, "s": 0.8329105897273305, "os": 0.06602240203526376, "bg": 0.0003518763808006193}, {"x": 0.50285351934052, "y": 0.023145212428662017, "ox": 0.50285351934052, "oy": 0.023145212428662017, "term": "vgg16", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.06499682942295498, "os": -0.022384368043532034, "bg": 0.0}, {"x": 0.9229549778059607, "y": 0.9029803424223208, "ox": 0.9229549778059607, "oy": 0.9029803424223208, "term": "achieve", "cat25k": 12, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 86, "ncat": 36, "s": 0.8592263792010145, "os": 0.07644606197660858, "bg": 8.852198862615796e-06}, {"x": 0.6169942929613189, "y": 0.6471147748890298, "ox": 0.6169942929613189, "oy": 0.6471147748890298, "term": "can achieve", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 8, "s": 0.5783132530120482, "os": 0.027719868555881426, "bg": 0.0}, {"x": 0.6173113506658212, "y": 0.37761572606214333, "ox": 0.6173113506658212, "oy": 0.37761572606214333, "term": "skin", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 8, "s": 0.13221306277742548, "os": -0.007649906363732729, "bg": 7.270014367729396e-07}, {"x": 0.6176284083703234, "y": 0.4514901712111604, "ox": 0.6176284083703234, "oy": 0.4514901712111604, "term": "fact", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 4.6789164314368015e-07}, {"x": 0.7679137603043754, "y": 0.7805960684844642, "ox": 0.7679137603043754, "oy": 0.7805960684844642, "term": "after", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 13, "s": 0.7599873176918199, "os": 0.04847885233737323, "bg": 2.7332725748277376e-07}, {"x": 0.8820545339251744, "y": 0.8465440710209258, "ox": 0.8820545339251744, "oy": 0.8465440710209258, "term": "processing", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 26, "s": 0.6192136968928346, "os": 0.03171266033002368, "bg": 2.9075072933115436e-06}, {"x": 0.8585922637920101, "y": 0.8715916296766011, "ox": 0.8585922637920101, "oy": 0.8715916296766011, "term": "extract", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 22, "s": 0.8788839568801522, "os": 0.08584502314405851, "bg": 1.5545327405143572e-05}, {"x": 0.6956246036778694, "y": 0.6607482561826252, "ox": 0.6956246036778694, "oy": 0.6607482561826252, "term": "parts", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 10, "s": 0.4727330374128091, "os": 0.019946291650471716, "bg": 6.741455782280959e-07}, {"x": 0.2320862396956246, "y": 0.20640456563094484, "ox": 0.2320862396956246, "oy": 0.20640456563094484, "term": "- processing", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.23240329740012683, "y": 0.20672162333544705, "ox": 0.23240329740012683, "oy": 0.20672162333544705, "term": "extract features", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.8430564362714014, "y": 0.8693722257450857, "ox": 0.8430564362714014, "oy": 0.8693722257450857, "term": "output", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 20, "s": 0.8883956880152187, "os": 0.0936186000494682, "bg": 2.9972884173963906e-06}, {"x": 0.1341154090044388, "y": 0.41566265060240964, "ox": 0.1341154090044388, "oy": 0.41566265060240964, "term": "composed", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 3.1560994146066805e-06}, {"x": 0.336715282181357, "y": 0.4819277108433735, "ox": 0.336715282181357, "oy": 0.4819277108433735, "term": "nodes", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 4, "s": 0.5206087507926442, "os": 0.023974417865093106, "bg": 3.3217877896850266e-06}, {"x": 0.13443246670894102, "y": 0.27393785668991755, "ox": 0.13443246670894102, "oy": 0.27393785668991755, "term": "representing", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 1.9276959792117266e-06}, {"x": 0.8316423589093215, "y": 0.8427393785668992, "ox": 0.8316423589093215, "oy": 0.8427393785668992, "term": "'", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 18, "s": 0.8487634749524413, "os": 0.07245327020246635, "bg": 0.0}, {"x": 0.23272035510462905, "y": 0.09258084971464807, "ox": 0.23272035510462905, "oy": 0.09258084971464807, "term": "output layer", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.13474952441344323, "y": 0.41597970830691183, "ox": 0.13474952441344323, "oy": 0.41597970830691183, "term": "composed of", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 0.0}, {"x": 0.9188332276474318, "y": 0.9004438807863031, "ox": 0.9188332276474318, "oy": 0.9004438807863031, "term": "thus", "cat25k": 12, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 84, "ncat": 35, "s": 0.8573240329740013, "os": 0.07550969930391152, "bg": 3.988043309949267e-06}, {"x": 0.8433734939759037, "y": 0.9093214965123653, "ox": 0.8433734939759037, "oy": 0.9093214965123653, "term": "able", "cat25k": 12, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 20, "s": 0.9641724793912493, "os": 0.17721988622310167, "bg": 2.0069324921731915e-06}, {"x": 0.42517438173747624, "y": 0.33100824350031705, "ox": 0.42517438173747624, "oy": 0.33100824350031705, "term": "dynamically", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 1.0048433449225266e-05}, {"x": 0.8665187064045656, "y": 0.7989854153455929, "ox": 0.8665187064045656, "oy": 0.7989854153455929, "term": "predict", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 23, "s": 0.2435003170577045, "os": 0.0031800996431221584, "bg": 1.9772217871304487e-05}, {"x": 0.5031705770450222, "y": 0.023462270133164237, "ox": 0.5031705770450222, "oy": 0.023462270133164237, "term": "lesions", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.06499682942295498, "os": -0.022384368043532034, "bg": 6.3523865757555446e-06}, {"x": 0.9137603043753963, "y": 0.8649334178820546, "ox": 0.9137603043753963, "oy": 0.8649334178820546, "term": "input", "cat25k": 9, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 33, "s": 0.46512365250475585, "os": 0.018974594537295497, "bg": 3.343628167092872e-06}, {"x": 0.6578947368421053, "y": 0.8233988585922638, "ox": 0.6578947368421053, "oy": 0.8233988585922638, "term": "is able", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 9, "s": 0.8975903614457831, "os": 0.0993957810678068, "bg": 0.0}, {"x": 0.8436905516804059, "y": 0.9077362079898541, "ox": 0.8436905516804059, "oy": 0.9077362079898541, "term": "able to", "cat25k": 12, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 89, "ncat": 20, "s": 0.9625871908687381, "os": 0.17400445213950036, "bg": 0.0}, {"x": 0.5034876347495244, "y": 0.09289790741915029, "ox": 0.5034876347495244, "oy": 0.09289790741915029, "term": "input image", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 6, "s": 0.10589727330374128, "os": -0.012738065792728175, "bg": 0.0}, {"x": 0.5649968294229549, "y": 0.575142675967026, "ox": 0.5649968294229549, "oy": 0.575142675967026, "term": "prove", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.47368421052631576, "os": 0.02035263771598176, "bg": 3.194555298243683e-06}, {"x": 0.7682308180088776, "y": 0.6169942929613189, "ox": 0.7682308180088776, "oy": 0.6169942929613189, "term": "capability", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 13, "s": 0.1436271401395054, "os": -0.006183527083848631, "bg": 4.955551978165256e-06}, {"x": 0.5653138871274572, "y": 0.5754597336715283, "ox": 0.5653138871274572, "oy": 0.5754597336715283, "term": "covariate", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.47368421052631576, "os": 0.02035263771598176, "bg": 0.00014185480614995144}, {"x": 0.5656309448319594, "y": 0.3313253012048193, "ox": 0.5656309448319594, "oy": 0.3313253012048193, "term": "covariate shift", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 7, "s": 0.14679771718452758, "os": -0.005370834952828522, "bg": 0.0}, {"x": 0.6959416613823716, "y": 0.7155992390615092, "ox": 0.6959416613823716, "oy": 0.7155992390615092, "term": "density", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 10, "s": 0.6636017755231453, "os": 0.03602346206847814, "bg": 4.198711673769577e-06}, {"x": 0.4254914394419784, "y": 0.3316423589093215, "ox": 0.4254914394419784, "oy": 0.3316423589093215, "term": "ratio", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 1.1990030289814019e-06}, {"x": 0.5038046924540266, "y": 0.7057704502219404, "ox": 0.5038046924540266, "oy": 0.7057704502219404, "term": "structural", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 6, "s": 0.7923272035510464, "os": 0.05478604996289883, "bg": 4.131936374256213e-06}, {"x": 0.7203551046290425, "y": 0.7244768547875713, "ox": 0.7203551046290425, "oy": 0.7244768547875713, "term": "risk", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 11, "s": 0.6410906785034877, "os": 0.03374439065757394, "bg": 8.940539719448918e-07}, {"x": 0.06055802155992391, "y": 0.1439441978440076, "ox": 0.06055802155992391, "oy": 0.1439441978440076, "term": "shift problem", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.33703233988585923, "y": 0.035827520608750794, "ox": 0.33703233988585923, "oy": 0.035827520608750794, "term": "appropriately", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 3.6653472774004053e-06}, {"x": 0.23303741280913126, "y": 0.5301204819277109, "ox": 0.23303741280913126, "oy": 0.5301204819277109, "term": "dealing", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.6610653138871275, "os": 0.03589979152680117, "bg": 2.2811975146353077e-06}, {"x": 0.42580849714648067, "y": 0.6331642358909322, "ox": 0.42580849714648067, "oy": 0.6331642358909322, "term": "remains", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 5, "s": 0.7067216233354471, "os": 0.04098795095579661, "bg": 2.09368593175654e-06}, {"x": 0.8925174381737476, "y": 0.9334178820545339, "ox": 0.8925174381737476, "oy": 0.9334178820545339, "term": "important", "cat25k": 17, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 123, "ncat": 28, "s": 0.9879518072289157, "os": 0.23615773294229886, "bg": 2.2151401632138744e-06}, {"x": 0.23335447051363348, "y": 0.5304375396322131, "ox": 0.23335447051363348, "oy": 0.5304375396322131, "term": "dealing with", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.6610653138871275, "os": 0.03589979152680117, "bg": 0.0}, {"x": 0.6962587190868738, "y": 0.823715916296766, "ox": 0.6962587190868738, "oy": 0.823715916296766, "term": "an important", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 10, "s": 0.8893468611287254, "os": 0.0939012755733013, "bg": 0.0}, {"x": 0.7206721623335447, "y": 0.6743817374762207, "ox": 0.7206721623335447, "oy": 0.6743817374762207, "term": "estimate", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 11, "s": 0.4353202282815472, "os": 0.017667220239567502, "bg": 3.233848476412713e-06}, {"x": 0.5659480025364616, "y": 0.7935954343690552, "ox": 0.5659480025364616, "oy": 0.7935954343690552, "term": "weight", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 7, "s": 0.8801521876981611, "os": 0.08787675347160875, "bg": 1.0359554913405197e-06}, {"x": 0.9064679771718452, "y": 0.9169308814204186, "ox": 0.9064679771718452, "oy": 0.9169308814204186, "term": "unlabeled", "cat25k": 14, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 98, "ncat": 31, "s": 0.940710209258085, "os": 0.14250379845235148, "bg": 0.00039589375316484833}, {"x": 0.5662650602409639, "y": 0.6610653138871274, "ox": 0.5662650602409639, "oy": 0.6610653138871274, "term": "final", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 7, "s": 0.6705770450221941, "os": 0.0364298081339882, "bg": 6.334287076040675e-07}, {"x": 0.7482561826252377, "y": 0.4162967660114141, "ox": 0.7482561826252377, "oy": 0.4162967660114141, "term": "hypothesis", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 12, "s": 0.05041217501585289, "os": -0.026412494258153417, "bg": 7.62802224148213e-06}, {"x": 0.3373493975903614, "y": 0.6173113506658212, "ox": 0.3373493975903614, "oy": 0.6173113506658212, "term": "minimizing", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7232086239695625, "os": 0.043267022366700825, "bg": 2.3338355747490195e-05}, {"x": 0.74857324032974, "y": 0.8696892834495878, "ox": 0.74857324032974, "oy": 0.8696892834495878, "term": "weighted", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 12, "s": 0.9381737476220672, "os": 0.13757464400551217, "bg": 2.3905505312054917e-05}, {"x": 0.7685478757133798, "y": 0.8031071655041218, "ox": 0.7685478757133798, "oy": 0.8031071655041218, "term": "empirical", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 13, "s": 0.8167406467977173, "os": 0.061340588671778384, "bg": 1.8287933538324446e-05}, {"x": 0.7967660114140773, "y": 0.7882054533925175, "ox": 0.7967660114140773, "oy": 0.7882054533925175, "term": "loss", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 15, "s": 0.7064045656309449, "os": 0.04070527543196352, "bg": 1.297346354910814e-06}, {"x": 0.42612555485098286, "y": 0.20703868103994927, "ox": 0.42612555485098286, "oy": 0.20703868103994927, "term": "poor", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 5.799033980350884e-07}, {"x": 0.8823715916296766, "y": 0.9128091312618897, "ox": 0.8823715916296766, "oy": 0.9128091312618897, "term": "few", "cat25k": 13, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 26, "s": 0.9483195941661383, "os": 0.15389915550687253, "bg": 1.7582266260299617e-06}, {"x": 0.7970830691185796, "y": 0.9223208623969562, "ox": 0.7970830691185796, "oy": 0.9223208623969562, "term": "examples", "cat25k": 15, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 107, "ncat": 15, "s": 0.9923906150919468, "os": 0.259354793116851, "bg": 6.549189066794508e-06}, {"x": 0.4264426125554851, "y": 0.764426125554851, "ox": 0.4264426125554851, "oy": 0.764426125554851, "term": "training examples", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 5, "s": 0.8731769181991123, "os": 0.08278859404261332, "bg": 0.0}, {"x": 0.8703233988585922, "y": 0.87571337983513, "ox": 0.8703233988585922, "oy": 0.87571337983513, "term": "estimation", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 24, "s": 0.87000634115409, "os": 0.0812868803222501, "bg": 2.943480167170266e-05}, {"x": 0.2336715282181357, "y": 0.5069752694990488, "ox": 0.2336715282181357, "oy": 0.5069752694990488, "term": "adaptively", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.62714013950539, "os": 0.03268435744319989, "bg": 0.00011970728418824284}, {"x": 0.33766645529486367, "y": 0.5307545973367153, "ox": 0.33766645529486367, "oy": 0.5307545973367153, "term": "estimated", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.6049461001902346, "os": 0.030405286032295677, "bg": 1.274178252865589e-06}, {"x": 0.5665821179454661, "y": 0.008243500317057704, "ox": 0.5665821179454661, "oy": 0.008243500317057704, "term": "tailored", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.03329105897273303, "os": -0.0343097417052401, "bg": 3.76016262285548e-06}, {"x": 0.7844007609384908, "y": 0.8490805326569436, "ox": 0.7844007609384908, "oy": 0.8490805326569436, "term": "simultaneously", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 14, "s": 0.8998097653772986, "os": 0.1008621603476909, "bg": 1.7992423390510256e-05}, {"x": 0.7688649334178821, "y": 0.7488902980342422, "ox": 0.7688649334178821, "oy": 0.7488902980342422, "term": "discrepancy", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 13, "s": 0.6258719086873811, "os": 0.032401681919366807, "bg": 5.660040727684366e-05}, {"x": 0.42675967025998734, "y": 0.05326569435637286, "ox": 0.42675967025998734, "oy": 0.05326569435637286, "term": "discrepancy between", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 0.0}, {"x": 0.8208623969562461, "y": 0.647431832593532, "ox": 0.8208623969562461, "oy": 0.647431832593532, "term": "comprehensive", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 17, "s": 0.06753329105897274, "os": -0.021730680894668022, "bg": 2.1177847833510137e-06}, {"x": 0.7974001268230818, "y": 0.8652504755865568, "ox": 0.7974001268230818, "oy": 0.8652504755865568, "term": "studies", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 15, "s": 0.9172479391249208, "os": 0.1178756934383944, "bg": 1.4410310954372908e-06}, {"x": 0.9242232086239696, "y": 0.8617628408370324, "ox": 0.9242232086239696, "oy": 0.8617628408370324, "term": "synthetic", "cat25k": 9, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 37, "s": 0.14331008243500318, "os": -0.006218861524327768, "bg": 2.645677977697202e-05}, {"x": 0.953709575142676, "y": 0.9445149017121116, "ox": 0.953709575142676, "oy": 0.9445149017121116, "term": "prediction", "cat25k": 21, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 153, "ncat": 67, "s": 0.9010779961953076, "os": 0.10132150807391965, "bg": 5.968082693753805e-05}, {"x": 0.060875079264426125, "y": 0.48224476854787574, "ox": 0.060875079264426125, "oy": 0.48224476854787574, "term": "both synthetic", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 0.0}, {"x": 0.844007609384908, "y": 0.9232720355104629, "ox": 0.844007609384908, "oy": 0.9232720355104629, "term": "data sets", "cat25k": 15, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 108, "ncat": 20, "s": 0.9876347495244134, "os": 0.2350976997279248, "bg": 0.0}, {"x": 0.42707672796448953, "y": 0.14426125554850983, "ox": 0.42707672796448953, "oy": 0.14426125554850983, "term": "sets demonstrate", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 0.0}, {"x": 0.8750792644261256, "y": 0.9109067850348763, "ox": 0.8750792644261256, "oy": 0.9109067850348763, "term": "demonstrate that", "cat25k": 13, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 92, "ncat": 25, "s": 0.9511731135066582, "os": 0.15617822691777677, "bg": 0.0}, {"x": 0.2339885859226379, "y": 0.05358275206087508, "ox": 0.2339885859226379, "oy": 0.05358275206087508, "term": "outperforms existing", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.13506658211794548, "y": 0.3779327837666455, "ox": 0.13506658211794548, "oy": 0.3779327837666455, "term": "prediction accuracy", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 0.0}, {"x": 0.6179454660748256, "y": 0.5310716550412174, "ox": 0.6179454660748256, "oy": 0.5310716550412174, "term": "affective", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 8, "s": 0.30374128091312613, "os": 0.0084272640542737, "bg": 3.3192266201974936e-05}, {"x": 0.6182625237793279, "y": 0.6613823715916297, "ox": 0.6182625237793279, "oy": 0.6613823715916297, "term": "interface", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 8, "s": 0.6144578313253012, "os": 0.030935302639482708, "bg": 1.074928964830978e-06}, {"x": 0.9616360177552314, "y": 0.9632213062777425, "ox": 0.9616360177552314, "oy": 0.9632213062777425, "term": "cross", "cat25k": 31, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 227, "ncat": 79, "s": 0.9701965757767914, "os": 0.18879191548001845, "bg": 8.21893899782609e-06}, {"x": 0.5041217501585289, "y": 0.816423589093215, "ox": 0.5041217501585289, "oy": 0.816423589093215, "term": "eeg", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 6, "s": 0.9096385542168675, "os": 0.1094484293841207, "bg": 9.211953503116259e-05}, {"x": 0.3379835129993659, "y": 0.6334812935954344, "ox": 0.3379835129993659, "oy": 0.6334812935954344, "term": "brain computer", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 4, "s": 0.7457197209892201, "os": 0.046482456450302106, "bg": 0.0}, {"x": 0.3383005707038681, "y": 0.507292327203551, "ox": 0.3383005707038681, "oy": 0.507292327203551, "term": "computer interface", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 0.0}, {"x": 0.901712111604312, "y": 0.9102726696258719, "ox": 0.901712111604312, "oy": 0.9102726696258719, "term": "emotion", "cat25k": 13, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 91, "ncat": 30, "s": 0.9242232086239696, "os": 0.12549026536164798, "bg": 4.8763600913854064e-05}, {"x": 0.7488902980342422, "y": 0.8313253012048193, "ox": 0.7488902980342422, "oy": 0.8313253012048193, "term": "emotion recognition", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 12, "s": 0.8830057070386811, "os": 0.08934313275149289, "bg": 0.0}, {"x": 0.4273937856689918, "y": 0.45180722891566266, "ox": 0.4273937856689918, "oy": 0.45180722891566266, "term": "introduces", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.4032974001268231, "os": 0.015264478286986322, "bg": 5.658744077491436e-06}, {"x": 0.5044388078630311, "y": 0.7159162967660114, "ox": 0.5044388078630311, "oy": 0.7159162967660114, "term": "factors", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 6, "s": 0.8065948002536463, "os": 0.058001484046500115, "bg": 1.6533833455026285e-06}, {"x": 0.9400760938490805, "y": 0.9486366518706405, "ox": 0.9400760938490805, "oy": 0.9486366518706405, "term": "human", "cat25k": 23, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 166, "ncat": 51, "s": 0.9803424223208624, "os": 0.21267799724391367, "bg": 2.6989236213756665e-06}, {"x": 0.8100824350031706, "y": 0.6616994292961319, "ox": 0.8100824350031706, "oy": 0.6616994292961319, "term": "interaction", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 16, "s": 0.10494610019023462, "os": -0.013020741316561252, "bg": 4.05139231667627e-06}, {"x": 0.42771084337349397, "y": 0.20735573874445148, "ox": 0.42771084337349397, "oy": 0.20735573874445148, "term": "human computer", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 0.0}, {"x": 0.4280279010779962, "y": 0.14457831325301204, "ox": 0.4280279010779962, "oy": 0.14457831325301204, "term": "computer interaction", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 0.0}, {"x": 0.4283449587824984, "y": 0.8386176284083703, "ox": 0.4283449587824984, "oy": 0.8386176284083703, "term": "individual", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 5, "s": 0.937856689917565, "os": 0.1374509734638352, "bg": 1.1994189635776548e-06}, {"x": 0.9191502853519341, "y": 0.937856689917565, "ox": 0.9191502853519341, "oy": 0.937856689917565, "term": "user", "cat25k": 18, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 131, "ncat": 35, "s": 0.9835129993658847, "os": 0.2202042330659694, "bg": 1.0483859250817038e-06}, {"x": 0.06119213696892835, "y": 0.2076727964489537, "ox": 0.06119213696892835, "oy": 0.2076727964489537, "term": "each individual", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.1353836398224477, "y": 0.4166138237159163, "ox": 0.1353836398224477, "oy": 0.4166138237159163, "term": "emotion classification", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 0.0}, {"x": 0.7847178186429931, "y": 0.7993024730500952, "ox": 0.7847178186429931, "oy": 0.7993024730500952, "term": "independent", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 14, "s": 0.7850348763474954, "os": 0.052630649093671614, "bg": 1.7080596549461733e-06}, {"x": 0.9562460367786937, "y": 0.9673430564362714, "ox": 0.9562460367786937, "oy": 0.9673430564362714, "term": "multiple", "cat25k": 35, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 256, "ncat": 70, "s": 0.9942929613189601, "os": 0.274548602522879, "bg": 9.818897661274114e-06}, {"x": 0.23430564362714015, "y": 0.8268864933417882, "ox": 0.23430564362714015, "oy": 0.8268864933417882, "term": "subjects", "cat25k": 7, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 3, "s": 0.9365884590995561, "os": 0.13557824811844105, "bg": 2.0479897404548467e-06}, {"x": 0.6185795814838301, "y": 0.7162333544705136, "ox": 0.6185795814838301, "oy": 0.7162333544705136, "term": "generally", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 8, "s": 0.7504755865567534, "os": 0.04701247305748913, "bg": 1.8196692077445514e-06}, {"x": 0.6965757767913761, "y": 0.7336715282181357, "ox": 0.6965757767913761, "oy": 0.7336715282181357, "term": "leads", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 10, "s": 0.7171845275840203, "os": 0.042454330235680716, "bg": 2.6676883696237097e-06}, {"x": 0.42866201648700064, "y": 0.053899809765377296, "ox": 0.42866201648700064, "oy": 0.053899809765377296, "term": "vary", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 1.0347020732936856e-06}, {"x": 0.6968928344958782, "y": 0.7247939124920736, "ox": 0.6968928344958782, "oy": 0.7247939124920736, "term": "leads to", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 10, "s": 0.6908687381103361, "os": 0.039238896152079435, "bg": 0.0}, {"x": 0.061509194673430564, "y": 0.14489537095751426, "ox": 0.061509194673430564, "oy": 0.14489537095751426, "term": "fact that", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.06182625237793278, "y": 0.3319594166138237, "ox": 0.06182625237793278, "oy": 0.3319594166138237, "term": "leveraged", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 2.6066103638828067e-05}, {"x": 0.7492073557387444, "y": 0.6620164870006341, "ox": 0.7492073557387444, "oy": 0.6620164870006341, "term": "tackle", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 12, "s": 0.31008243500317056, "os": 0.008957280661460731, "bg": 1.0784405210844859e-05}, {"x": 0.33861762840837034, "y": 0.20798985415345592, "ox": 0.33861762840837034, "oy": 0.20798985415345592, "term": "tackle this", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 0.0}, {"x": 0.6972098922003804, "y": 0.4825618262523779, "ox": 0.6972098922003804, "oy": 0.4825618262523779, "term": "reported", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 10, "s": 0.1223842739378567, "os": -0.008992615101939862, "bg": 9.359755123347439e-07}, {"x": 0.6582117945466075, "y": 0.6337983512999366, "ox": 0.6582117945466075, "oy": 0.6337983512999366, "term": "successful", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 9, "s": 0.46544071020925804, "os": 0.019009928977774634, "bg": 1.4327770253002309e-06}, {"x": 0.23462270133164237, "y": 0.0932149651236525, "ox": 0.23462270133164237, "oy": 0.0932149651236525, "term": "studies have", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.9416613823715916, "y": 0.9495878249841471, "ox": 0.9416613823715916, "oy": 0.9495878249841471, "term": "cross -", "cat25k": 23, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 168, "ncat": 53, "s": 0.9781230183893469, "os": 0.20811985442210518, "bg": 0.0}, {"x": 0.6975269499048827, "y": 0.5963855421686747, "ox": 0.6975269499048827, "oy": 0.5963855421686747, "term": "we focus", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 10, "s": 0.29201014584654406, "os": 0.007084555316066575, "bg": 0.0}, {"x": 0.5047558655675333, "y": 0.7415979708306912, "ox": 0.5047558655675333, "oy": 0.7415979708306912, "term": "can improve", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 6, "s": 0.8376664552948636, "os": 0.06764778629730397, "bg": 0.0}, {"x": 0.8516169942929613, "y": 0.7939124920735574, "ox": 0.8516169942929613, "oy": 0.7939124920735574, "term": "explore", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 21, "s": 0.3532022828154724, "os": 0.01095367654853184, "bg": 3.81594114417492e-06}, {"x": 0.42897907419150283, "y": 0.5523145212428662, "ox": 0.42897907419150283, "oy": 0.5523145212428662, "term": "efficacy", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.5798985415345593, "os": 0.02812621462139147, "bg": 1.0064070938573069e-05}, {"x": 0.8103994927076728, "y": 0.865567533291059, "ox": 0.8103994927076728, "oy": 0.865567533291059, "term": "setting", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 16, "s": 0.9128091312618897, "os": 0.1123811879438889, "bg": 3.3921400165476318e-06}, {"x": 0.7209892200380469, "y": 0.7942295497780596, "ox": 0.7209892200380469, "oy": 0.7942295497780596, "term": "collected", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 11, "s": 0.8322764743183261, "os": 0.06589873149358678, "bg": 4.8389174203161945e-06}, {"x": 0.8753963221306278, "y": 0.9337349397590361, "ox": 0.8753963221306278, "oy": 0.9337349397590361, "term": "under", "cat25k": 17, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 123, "ncat": 25, "s": 0.9917564996829423, "os": 0.25264124942581534, "bg": 9.440959605160698e-07}, {"x": 0.7850348763474952, "y": 0.8430564362714014, "ox": 0.7850348763474952, "oy": 0.8430564362714014, "term": "environments", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 14, "s": 0.8902980342422321, "os": 0.09443129218048833, "bg": 1.0027637200381022e-05}, {"x": 0.33893468611287253, "y": 0.4169308814204185, "ox": 0.33893468611287253, "oy": 0.4169308814204185, "term": "devices", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 7.106377093504728e-07}, {"x": 0.6978440076093849, "y": 0.6746987951807228, "ox": 0.6978440076093849, "oy": 0.6746987951807228, "term": "we explore", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 10, "s": 0.5171211160431198, "os": 0.023161725734072998, "bg": 0.0}, {"x": 0.4292961318960051, "y": 0.5526315789473685, "ox": 0.4292961318960051, "oy": 0.5526315789473685, "term": "efficacy of", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.5798985415345593, "os": 0.02812621462139147, "bg": 0.0}, {"x": 0.6585288522511097, "y": 0.7060875079264426, "ox": 0.6585288522511097, "oy": 0.7060875079264426, "term": "here", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 9, "s": 0.684844641724794, "os": 0.03830253347938235, "bg": 1.1563545832380337e-07}, {"x": 0.8211794546607483, "y": 0.8776157260621433, "ox": 0.8211794546607483, "oy": 0.8776157260621433, "term": "apply", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 17, "s": 0.9226379201014585, "os": 0.12296385286738985, "bg": 2.2201197832980256e-06}, {"x": 0.9020291693088142, "y": 0.8830057070386811, "ox": 0.9020291693088142, "oy": 0.8830057070386811, "term": "as well", "cat25k": 10, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 30, "s": 0.8157894736842106, "os": 0.06118158368962226, "bg": 0.0}, {"x": 0.8928344958782498, "y": 0.8734939759036144, "ox": 0.8928344958782498, "oy": 0.8734939759036144, "term": "well as", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 28, "s": 0.7986683576410908, "os": 0.056093424260626856, "bg": 0.0}, {"x": 0.7691819911223843, "y": 0.8620798985415346, "ox": 0.7691819911223843, "oy": 0.8620798985415346, "term": "experiment", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 13, "s": 0.9251743817374762, "os": 0.1256492703438041, "bg": 9.991742823730469e-06}, {"x": 0.7213062777425492, "y": 0.7251109701965758, "ox": 0.7213062777425492, "oy": 0.7251109701965758, "term": "transductive", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 11, "s": 0.6410906785034877, "os": 0.03374439065757394, "bg": 0.00035509672055187235}, {"x": 0.8880786303107165, "y": 0.8833227647431833, "ox": 0.8880786303107165, "oy": 0.8833227647431833, "term": "no", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 27, "s": 0.8614457831325301, "os": 0.07766510017313874, "bg": 2.091016173049938e-07}, {"x": 0.3392517438173748, "y": 0.5529486366518707, "ox": 0.3392517438173748, "oy": 0.5529486366518707, "term": "experiment results", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 4, "s": 0.6385542168674699, "os": 0.033620720115896965, "bg": 0.0}, {"x": 0.33956880152187696, "y": 0.09353202282815472, "ox": 0.33956880152187696, "oy": 0.09353202282815472, "term": "where no", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.6188966391883323, "y": 0.6984781230183893, "ox": 0.6188966391883323, "oy": 0.6984781230183893, "term": "insufficient", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7035510462904249, "os": 0.04058160489028657, "bg": 1.372381911927979e-05}, {"x": 0.7216233354470514, "y": 0.9048826886493342, "ox": 0.7216233354470514, "oy": 0.9048826886493342, "term": "sample", "cat25k": 12, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 11, "s": 0.9822447685478758, "os": 0.21702413342284727, "bg": 2.8328447711922118e-06}, {"x": 0.9451490171211161, "y": 0.9388078630310717, "ox": 0.9451490171211161, "oy": 0.9388078630310717, "term": "order", "cat25k": 18, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 133, "ncat": 56, "s": 0.9112238427393786, "os": 0.11125048584855657, "bg": 1.1221203652918578e-06}, {"x": 0.7853519340519974, "y": 0.8535193405199747, "ox": 0.7853519340519974, "oy": 0.8535193405199747, "term": "find", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 14, "s": 0.9058338617628409, "os": 0.10729302851489346, "bg": 2.8669611067578434e-07}, {"x": 0.810716550412175, "y": 0.9032974001268231, "ox": 0.810716550412175, "oy": 0.9032974001268231, "term": "introduce", "cat25k": 12, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 86, "ncat": 16, "s": 0.9689283449587826, "os": 0.18633617186671847, "bg": 1.529738338257241e-05}, {"x": 0.6192136968928345, "y": 0.7996195307545974, "ox": 0.6192136968928345, "oy": 0.7996195307545974, "term": "divergence", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 8, "s": 0.8782498414711477, "os": 0.08559768206070459, "bg": 6.686129215587414e-05}, {"x": 0.9353202282815473, "y": 0.9315155358275206, "ox": 0.9353202282815473, "oy": 0.9315155358275206, "term": "in order", "cat25k": 17, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 120, "ncat": 46, "s": 0.927710843373494, "os": 0.12761033179039605, "bg": 0.0}, {"x": 0.9356372859860494, "y": 0.930564362714014, "ox": 0.9356372859860494, "oy": 0.930564362714014, "term": "order to", "cat25k": 16, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 119, "ncat": 46, "s": 0.9232720355104629, "os": 0.1243948977067948, "bg": 0.0}, {"x": 0.6195307545973368, "y": 0.5532656943563729, "ox": 0.6195307545973368, "oy": 0.5532656943563729, "term": "feature representation", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 8, "s": 0.35954343690551677, "os": 0.01164269813787499, "bg": 0.0}, {"x": 0.7219403931515536, "y": 0.8779327837666455, "ox": 0.7219403931515536, "oy": 0.8779327837666455, "term": "we introduce", "cat25k": 9, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 11, "s": 0.9505389980976537, "os": 0.15593088583442283, "bg": 0.0}, {"x": 0.4296131896005073, "y": 0.41724793912492075, "ox": 0.4296131896005073, "oy": 0.41724793912492075, "term": "objective function", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 0.0}, {"x": 0.1357006975269499, "y": 0.14521242866201647, "ox": 0.1357006975269499, "oy": 0.14521242866201647, "term": "divergence between", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.8214965123652505, "y": 0.859860494610019, "ox": 0.8214965123652505, "oy": 0.859860494610019, "term": "training samples", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 17, "s": 0.8988585922637921, "os": 0.10045581428218085, "bg": 0.0}, {"x": 0.062143310082435003, "y": 0.45212428662016485, "ox": 0.062143310082435003, "oy": 0.45212428662016485, "term": "test samples", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.674698795180723, "os": 0.0372425002650083, "bg": 0.0}, {"x": 0.23493975903614459, "y": 0.14552948636651872, "ox": 0.23493975903614459, "oy": 0.14552948636651872, "term": "biased", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 7.615763939011577e-06}, {"x": 0.3398858592263792, "y": 0.7647431832593532, "ox": 0.3398858592263792, "oy": 0.7647431832593532, "term": "sampling", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 4, "s": 0.8807863031071655, "os": 0.08828309953711883, "bg": 6.616277212514909e-06}, {"x": 0.8319594166138237, "y": 0.7809131261889664, "ox": 0.8319594166138237, "oy": 0.7809131261889664, "term": "higher", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 18, "s": 0.4819277108433735, "os": 0.021006324864845757, "bg": 1.3535310131539772e-06}, {"x": 0.5050729232720355, "y": 0.4524413443246671, "ox": 0.5050729232720355, "oy": 0.4524413443246671, "term": "condition", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 5.249761677225359e-07}, {"x": 0.6588459099556119, "y": 0.8719086873811034, "ox": 0.6588459099556119, "oy": 0.8719086873811034, "term": "active", "cat25k": 9, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 9, "s": 0.9514901712111604, "os": 0.15727359457262993, "bg": 1.7553073139860515e-06}, {"x": 0.13601775523145213, "y": 0.37824984147114776, "ox": 0.13601775523145213, "oy": 0.37824984147114776, "term": "al", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 2.2218970669839836e-07}, {"x": 0.3402029169308814, "y": 0.03614457831325301, "ox": 0.3402029169308814, "oy": 0.03614457831325301, "term": "which combines", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 0.0}, {"x": 0.8706404565630945, "y": 0.8402029169308814, "ox": 0.8706404565630945, "oy": 0.8402029169308814, "term": "main", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 24, "s": 0.6645529486366519, "os": 0.03627080315183209, "bg": 7.705867517232985e-07}, {"x": 0.13633481293595434, "y": 0.48287888395688017, "ox": 0.13633481293595434, "oy": 0.48287888395688017, "term": "iteratively", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6540900443880786, "os": 0.0349634288541041, "bg": 8.078216139800659e-05}, {"x": 0.13665187064045656, "y": 0.5967025998731769, "ox": 0.13665187064045656, "oy": 0.5967025998731769, "term": "informative", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 6.656562811175401e-06}, {"x": 0.5053899809765378, "y": 0.5313887127457197, "ox": 0.5053899809765378, "oy": 0.5313887127457197, "term": "unlabelled", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.4663918833227647, "os": 0.019416275043284685, "bg": 0.0001380979114191962}, {"x": 0.9388078630310717, "y": 0.9543436905516804, "ox": 0.9388078630310717, "oy": 0.9543436905516804, "term": "same", "cat25k": 26, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 190, "ncat": 49, "s": 0.9946100190234624, "os": 0.27511395357054513, "bg": 2.0335778071076235e-06}, {"x": 0.4299302473050095, "y": 0.2742549143944198, "ox": 0.4299302473050095, "oy": 0.2742549143944198, "term": "fit", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 7.046604054770998e-07}, {"x": 0.34051997463538364, "y": 0.14584654407102093, "ox": 0.34051997463538364, "oy": 0.14584654407102093, "term": "posterior", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 1.0532727818514079e-05}, {"x": 0.7856689917564997, "y": 0.7812301838934687, "ox": 0.7856689917564997, "oy": 0.7812301838934687, "term": "probability", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 14, "s": 0.722574508560558, "os": 0.04298434684286774, "bg": 7.902758381825114e-06}, {"x": 0.6198478123018389, "y": 0.6987951807228916, "ox": 0.6198478123018389, "oy": 0.6987951807228916, "term": "combine", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7035510462904249, "os": 0.04058160489028657, "bg": 5.235601702647591e-06}, {"x": 0.5057070386810399, "y": 0.7419150285351934, "ox": 0.5057070386810399, "oy": 0.7419150285351934, "term": "basic", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 6, "s": 0.8376664552948636, "os": 0.06764778629730397, "bg": 8.933698859540459e-07}, {"x": 0.2352568167406468, "y": 0.05421686746987952, "ox": 0.2352568167406468, "oy": 0.05421686746987952, "term": "main idea", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.6201648700063411, "y": 0.48319594166138236, "ox": 0.6201648700063411, "oy": 0.48319594166138236, "term": "same time", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 8, "s": 0.22859860494610018, "os": 0.00199639588707113, "bg": 0.0}, {"x": 0.06246036778693722, "y": 0.14616360177552315, "ox": 0.06246036778693722, "oy": 0.14616360177552315, "term": "uci", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 1.3909183847728437e-05}, {"x": 0.5668991756499683, "y": 0.14648065948002537, "ox": 0.5668991756499683, "oy": 0.14648065948002537, "term": "confirm", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 7, "s": 0.09511731135066583, "os": -0.015017137203632385, "bg": 1.3110514029167746e-06}, {"x": 0.9403931515535827, "y": 0.8998097653772986, "ox": 0.9403931515535827, "oy": 0.8998097653772986, "term": "facial", "cat25k": 11, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 83, "ncat": 51, "s": 0.09321496512365252, "os": -0.015617822691777639, "bg": 2.0691037414645613e-05}, {"x": 0.9419784400760939, "y": 0.8883956880152187, "ox": 0.9419784400760939, "oy": 0.8883956880152187, "term": "expression", "cat25k": 10, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 53, "s": 0.01585288522511097, "os": -0.055545740433200236, "bg": 8.086412293180453e-06}, {"x": 0.8218135700697526, "y": 0.7752060875079264, "ox": 0.8218135700697526, "oy": 0.7752060875079264, "term": "facial expression", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 17, "s": 0.5177552314521243, "os": 0.02328539627574998, "bg": 0.0}, {"x": 0.8443246670894102, "y": 0.7492073557387444, "ox": 0.8443246670894102, "oy": 0.7492073557387444, "term": "expression recognition", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 20, "s": 0.14489537095751426, "os": -0.006059856542171657, "bg": 0.0}, {"x": 0.6591629676601142, "y": 0.09384908053265695, "ox": 0.6591629676601142, "oy": 0.09384908053265695, "term": "optical", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.044705136334812934, "os": -0.029221582276244655, "bg": 1.1897507877358558e-06}, {"x": 0.5060240963855421, "y": 0.274571972098922, "ox": 0.5060240963855421, "oy": 0.274571972098922, "term": "flow", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 6.820824548590903e-07}, {"x": 0.832276474318326, "y": 0.8560558021559924, "ox": 0.832276474318326, "oy": 0.8560558021559924, "term": "temporal", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 18, "s": 0.8820545339251744, "os": 0.08853044062047276, "bg": 2.8210310428820534e-05}, {"x": 0.5672162333544705, "y": 0.013633481293595434, "ox": 0.5672162333544705, "oy": 0.013633481293595434, "term": "spatial temporal", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 0.0}, {"x": 0.7222574508560557, "y": 0.6750158528852251, "ox": 0.7222574508560557, "oy": 0.6750158528852251, "term": "performing", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 11, "s": 0.4353202282815472, "os": 0.017667220239567502, "bg": 3.406131348656289e-06}, {"x": 0.722574508560558, "y": 0.4527584020291693, "ox": 0.722574508560558, "oy": 0.4527584020291693, "term": "commonly", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 11, "s": 0.08211794546607483, "os": -0.017702554680046646, "bg": 4.212495812779162e-06}, {"x": 0.43024730500951175, "y": 0.4835129993658846, "ox": 0.43024730500951175, "oy": 0.4835129993658846, "term": "crafted", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 5, "s": 0.45244134432466704, "os": 0.01847991237058761, "bg": 8.21670237850938e-06}, {"x": 0.5063411540900444, "y": 0.4530754597336715, "ox": 0.5063411540900444, "oy": 0.4530754597336715, "term": "traditional methods", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 0.0}, {"x": 0.43056436271401394, "y": 0.4838300570703868, "ox": 0.43056436271401394, "oy": 0.4838300570703868, "term": "hand crafted", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 5, "s": 0.45244134432466704, "os": 0.01847991237058761, "bg": 0.0}, {"x": 0.4308814204185162, "y": 0.023779327837666456, "ox": 0.4308814204185162, "oy": 0.023779327837666456, "term": "channel", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 2.663369454739025e-07}, {"x": 0.5675332910589728, "y": 0.5757767913760304, "ox": 0.5675332910589728, "oy": 0.5757767913760304, "term": "recognizing", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.47368421052631576, "os": 0.02035263771598176, "bg": 1.273196663832988e-05}, {"x": 0.567850348763475, "y": 0.7165504121750158, "ox": 0.567850348763475, "oy": 0.7165504121750158, "term": "expressions", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 7, "s": 0.7844007609384909, "os": 0.05250697855199462, "bg": 8.33448318332807e-06}, {"x": 0.43119847812301837, "y": 0.14679771718452758, "ox": 0.43119847812301837, "oy": 0.14679771718452758, "term": "static", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 8.777615729487387e-07}, {"x": 0.23557387444514902, "y": 0.3322764743183259, "ox": 0.23557387444514902, "oy": 0.3322764743183259, "term": "facial expressions", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.0}, {"x": 0.6204819277108434, "y": 0.33259353202282815, "ox": 0.6204819277108434, "oy": 0.33259353202282815, "term": "essential", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 8, "s": 0.11128725428027901, "os": -0.010865340447334011, "bg": 9.942861254566684e-07}, {"x": 0.5681674064679771, "y": 0.7999365884590995, "ox": 0.5681674064679771, "oy": 0.7999365884590995, "term": "changes", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 7, "s": 0.8852251109701966, "os": 0.09109218755521006, "bg": 7.339383273611267e-07}, {"x": 0.9603677869372226, "y": 0.9457831325301205, "ox": 0.9603677869372226, "oy": 0.9457831325301205, "term": "face", "cat25k": 21, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 156, "ncat": 76, "s": 0.8370323398858592, "os": 0.06701176636867956, "bg": 5.5877296924200475e-06}, {"x": 0.5066582117945466, "y": 0.1471147748890298, "ox": 0.5066582117945466, "oy": 0.1471147748890298, "term": "emotional", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 6, "s": 0.11953075459733671, "os": -0.00952263170912689, "bg": 2.090843881018438e-06}, {"x": 0.5684844641724794, "y": 0.37856689917564995, "ox": 0.5684844641724794, "oy": 0.37856689917564995, "term": "neutral", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 7, "s": 0.17438173747622068, "os": -0.0021554008692272406, "bg": 3.4981978757469575e-06}, {"x": 0.6207989854153456, "y": 0.6753329105897273, "ox": 0.6207989854153456, "oy": 0.6753329105897273, "term": "certain", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 8, "s": 0.648700063411541, "os": 0.03415073672308399, "bg": 8.999288892554053e-07}, {"x": 0.6594800253646164, "y": 0.3788839568801522, "ox": 0.6594800253646164, "oy": 0.3788839568801522, "term": "pretrained", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.10367786937222576, "os": -0.013144411858238225, "bg": 0.0001818945617856849}, {"x": 0.7694990488268865, "y": 0.7064045656309448, "ox": 0.7694990488268865, "oy": 0.7064045656309448, "term": "instead", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 13, "s": 0.4188332276474318, "os": 0.01632451150136037, "bg": 1.4454238111292204e-06}, {"x": 0.7228915662650602, "y": 0.5760938490805326, "ox": 0.7228915662650602, "oy": 0.5760938490805326, "term": "scratch", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 11, "s": 0.1845275840202917, "os": -0.0016253842620402165, "bg": 7.803937008181257e-06}, {"x": 0.5069752694990488, "y": 0.5317057704502219, "ox": 0.5069752694990488, "oy": 0.5317057704502219, "term": "networks cnn", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.4663918833227647, "os": 0.019416275043284685, "bg": 0.0}, {"x": 0.7232086239695624, "y": 0.5764109067850349, "ox": 0.7232086239695624, "oy": 0.5764109067850349, "term": "from scratch", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 11, "s": 0.1845275840202917, "os": -0.0016253842620402165, "bg": 0.0}, {"x": 0.7495244134432467, "y": 0.553582752060875, "ox": 0.7495244134432467, "oy": 0.553582752060875, "term": "benchmarks", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 12, "s": 0.11699429296131895, "os": -0.010335323840146987, "bg": 1.5936690963922785e-05}, {"x": 0.8668357641090678, "y": 0.8947368421052632, "ox": 0.8668357641090678, "oy": 0.8947368421052632, "term": "effectively", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 23, "s": 0.9191502853519341, "os": 0.11893572665276844, "bg": 1.0132331799619158e-05}, {"x": 0.7235256816740647, "y": 0.41756499682942294, "ox": 0.7235256816740647, "oy": 0.41756499682942294, "term": "can effectively", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 11, "s": 0.0700697526949905, "os": -0.020917988763647928, "bg": 0.0}, {"x": 0.507292327203551, "y": 0.5538998097653773, "ox": 0.507292327203551, "oy": 0.5538998097653773, "term": "rates", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 3.495473696545869e-07}, {"x": 0.4315155358275206, "y": 0.20830691185795816, "ox": 0.4315155358275206, "oy": 0.20830691185795816, "term": "achieve better", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 0.0}, {"x": 0.6981610653138871, "y": 0.7422320862396956, "ox": 0.6981610653138871, "oy": 0.7422320862396956, "term": "evolutionary", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 10, "s": 0.7403297400126824, "os": 0.045669764319282, "bg": 1.7018507424222563e-05}, {"x": 0.8110336081166772, "y": 0.7339885859226379, "ox": 0.8110336081166772, "oy": 0.7339885859226379, "term": "computing", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 16, "s": 0.3335447051363348, "os": 0.009487297268647749, "bg": 2.567463518118781e-06}, {"x": 0.8221306277742549, "y": 0.7945466074825618, "ox": 0.8221306277742549, "oy": 0.7945466074825618, "term": "co", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 17, "s": 0.6293595434369056, "os": 0.032931698526553824, "bg": 9.94038550262439e-07}, {"x": 0.13696892834495877, "y": 0.3792010145846544, "ox": 0.13696892834495877, "oy": 0.3792010145846544, "term": "evolution", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 1.0811650696571484e-06}, {"x": 0.6597970830691186, "y": 0.5320228281547241, "ox": 0.6597970830691186, "oy": 0.5320228281547241, "term": "bi", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 9, "s": 0.24128091312618896, "os": 0.0029327585597682046, "bg": 3.909496068212285e-06}, {"x": 0.9264426125554851, "y": 0.9273937856689918, "ox": 0.9264426125554851, "oy": 0.9273937856689918, "term": "challenging", "cat25k": 16, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 114, "ncat": 38, "s": 0.9473684210526316, "os": 0.15227377124483235, "bg": 3.313009906553504e-05}, {"x": 0.4318325935320228, "y": 0.6857958148383005, "ox": 0.4318325935320228, "oy": 0.6857958148383005, "term": "levels", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 5, "s": 0.7894736842105264, "os": 0.053849687290201766, "bg": 9.172716270309652e-07}, {"x": 0.8224476854787571, "y": 0.7948636651870641, "ox": 0.8224476854787571, "oy": 0.7948636651870641, "term": "co -", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 17, "s": 0.6293595434369056, "os": 0.032931698526553824, "bg": 0.0}, {"x": 0.3408370323398859, "y": 0.054533925174381735, "ox": 0.3408370323398859, "oy": 0.054533925174381735, "term": "bi level", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.5688015218769816, "y": 0.14743183259353201, "ox": 0.5688015218769816, "oy": 0.14743183259353201, "term": "optimize", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 7, "s": 0.09511731135066583, "os": -0.015017137203632385, "bg": 5.249554181611126e-06}, {"x": 0.23589093214965123, "y": 0.05485098287888396, "ox": 0.23589093214965123, "oy": 0.05485098287888396, "term": "upper", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 4.6665490177808733e-07}, {"x": 0.9067850348763475, "y": 0.8623969562460367, "ox": 0.9067850348763475, "oy": 0.8623969562460367, "term": "evaluation", "cat25k": 9, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 31, "s": 0.5668991756499683, "os": 0.026748171442705193, "bg": 3.943973858153835e-06}, {"x": 0.5691185795814838, "y": 0.5323398858592264, "ox": 0.5691185795814838, "oy": 0.5323398858592264, "term": "finding", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 7, "s": 0.3937856689917565, "os": 0.01392176954877919, "bg": 1.285711438781814e-06}, {"x": 0.8446417247939125, "y": 0.8069118579581483, "ox": 0.8446417247939125, "oy": 0.8069118579581483, "term": "corresponding", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 20, "s": 0.5459733671528219, "os": 0.026094484293841202, "bg": 7.203496783152735e-06}, {"x": 0.5076093849080533, "y": 0.48414711477488903, "ox": 0.5076093849080533, "oy": 0.48414711477488903, "term": "lower", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 6, "s": 0.3823715916296766, "os": 0.012985406876082115, "bg": 5.766234386719099e-07}, {"x": 0.43214965123652505, "y": 0.2748890298034242, "ox": 0.43214965123652505, "oy": 0.2748890298034242, "term": "computationally", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 4.344344315353057e-05}, {"x": 0.7238427393785669, "y": 0.7815472415979708, "ox": 0.7238427393785669, "oy": 0.7815472415979708, "term": "expensive", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 11, "s": 0.810716550412175, "os": 0.05946786332638422, "bg": 5.4230673156491905e-06}, {"x": 0.6601141407736208, "y": 0.3329105897273304, "ox": 0.6601141407736208, "oy": 0.3329105897273304, "term": "an optimal", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.0897273303741281, "os": -0.016359845941839507, "bg": 0.0}, {"x": 0.8826886493341788, "y": 0.9051997463538364, "ox": 0.8826886493341788, "oy": 0.9051997463538364, "term": "case", "cat25k": 12, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 26, "s": 0.9350031705770451, "os": 0.13460655100526484, "bg": 9.584637641005234e-07}, {"x": 0.5079264426125555, "y": 0.6477488902980343, "ox": 0.5079264426125555, "oy": 0.6477488902980343, "term": "continuous", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 6, "s": 0.6873811033608117, "os": 0.03870887954489241, "bg": 2.6239743031482035e-06}, {"x": 0.4324667089410273, "y": 0.5767279644895371, "ox": 0.4324667089410273, "oy": 0.5767279644895371, "term": "attracted", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.61572606214331, "os": 0.03134164870499275, "bg": 1.079400959452528e-05}, {"x": 0.23620798985415345, "y": 0.5326569435637286, "ox": 0.23620798985415345, "oy": 0.5326569435637286, "term": "has attracted", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.6610653138871275, "os": 0.03589979152680117, "bg": 0.0}, {"x": 0.5082435003170577, "y": 0.20862396956246038, "ox": 0.5082435003170577, "oy": 0.20862396956246038, "term": "besides", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 2.4043659438554906e-06}, {"x": 0.50856055802156, "y": 0.5542168674698795, "ox": 0.50856055802156, "oy": 0.5542168674698795, "term": "reusing", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 8.329674639437886e-05}, {"x": 0.34115409004438807, "y": 0.5970196575776792, "ox": 0.34115409004438807, "oy": 0.5970196575776792, "term": "captured", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 4, "s": 0.6987951807228916, "os": 0.04005158828309954, "bg": 5.774402746305946e-06}, {"x": 0.6211160431198478, "y": 0.6861128725428028, "ox": 0.6211160431198478, "oy": 0.6861128725428028, "term": "past", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.6769181991122385, "os": 0.037366170806685285, "bg": 6.423788755898407e-07}, {"x": 0.4327837666455295, "y": 0.2089410272669626, "ox": 0.4327837666455295, "oy": 0.2089410272669626, "term": "experiences", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 1.1494543540181474e-06}, {"x": 0.7241597970830691, "y": 0.5973367152821814, "ox": 0.7241597970830691, "oy": 0.5973367152821814, "term": "along", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 11, "s": 0.22733037412809132, "os": 0.0015900498215610792, "bg": 7.409561314651695e-07}, {"x": 0.7698161065313888, "y": 0.7067216233354471, "ox": 0.7698161065313888, "oy": 0.7067216233354471, "term": "literature", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 13, "s": 0.4188332276474318, "os": 0.01632451150136037, "bg": 1.7507176981802826e-06}, {"x": 0.8519340519974635, "y": 0.7951807228915663, "ox": 0.8519340519974635, "oy": 0.7951807228915663, "term": "demonstrated", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 21, "s": 0.3532022828154724, "os": 0.01095367654853184, "bg": 1.0207519712435762e-05}, {"x": 0.6984781230183893, "y": 0.8928344958782498, "ox": 0.6984781230183893, "oy": 0.8928344958782498, "term": "much", "cat25k": 10, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 10, "s": 0.9692454026632847, "os": 0.18714886399773858, "bg": 7.091108892271989e-07}, {"x": 0.23652504755865567, "y": 0.09416613823715916, "ox": 0.23652504755865567, "oy": 0.09416613823715916, "term": "promise", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 1.2307577941891125e-06}, {"x": 0.3414711477488903, "y": 0.6623335447051364, "ox": 0.3414711477488903, "oy": 0.6623335447051364, "term": "observation", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 4, "s": 0.785986049461002, "os": 0.052913324617504684, "bg": 4.498430610020929e-06}, {"x": 0.06277742549143944, "y": 0.4178820545339252, "ox": 0.06277742549143944, "oy": 0.4178820545339252, "term": "decomposition", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 9.85981109306218e-06}, {"x": 0.7859860494610019, "y": 0.008560558021559923, "ox": 0.7859860494610019, "oy": 0.008560558021559923, "term": "ii", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 14, "s": 0.008877615726062143, "os": -0.07277128016677856, "bg": 2.3999944800126953e-07}, {"x": 0.7244768547875713, "y": 0.14774889029803423, "ox": 0.7244768547875713, "oy": 0.14774889029803423, "term": "m", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 11, "s": 0.030120481927710843, "os": -0.03699515918165436, "bg": 1.1117133249189703e-07}, {"x": 0.23684210526315788, "y": 0.2092580849714648, "ox": 0.23684210526315788, "oy": 0.2092580849714648, "term": "motivation", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 3.180353050992191e-06}, {"x": 0.569435637285986, "y": 0.4844641724793913, "ox": 0.569435637285986, "oy": 0.4844641724793913, "term": "incorporate", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.2932783766645529, "os": 0.007490901381576619, "bg": 5.85763731400171e-06}, {"x": 0.9023462270133165, "y": 0.888712745719721, "ox": 0.9023462270133165, "oy": 0.888712745719721, "term": "recently", "cat25k": 10, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 30, "s": 0.8443246670894102, "os": 0.07082788594042613, "bg": 3.3620841093874755e-06}, {"x": 0.6987951807228916, "y": 0.781864299302473, "ox": 0.6987951807228916, "oy": 0.781864299302473, "term": "scheme", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 10, "s": 0.829422954977806, "os": 0.06496236882088971, "bg": 3.3636978307547725e-06}, {"x": 0.8883956880152187, "y": 0.8861762840837032, "ox": 0.8883956880152187, "oy": 0.8861762840837032, "term": "make", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 27, "s": 0.8744451490171211, "os": 0.08409596834034133, "bg": 4.934426115288367e-07}, {"x": 0.23715916296766013, "y": 0.3795180722891566, "ox": 0.23715916296766013, "oy": 0.3795180722891566, "term": "recently proposed", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 0.0}, {"x": 0.137285986049461, "y": 0.09448319594166138, "ox": 0.137285986049461, "oy": 0.09448319594166138, "term": "more effective", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.23747622067216234, "y": 0.14806594800253647, "ox": 0.23747622067216234, "oy": 0.14806594800253647, "term": "more efficient", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.4331008243500317, "y": 0.8167406467977172, "ox": 0.4331008243500317, "oy": 0.8167406467977172, "term": "hybrid", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 5, "s": 0.9153455928979074, "os": 0.1149429348786262, "bg": 7.459499023029412e-06}, {"x": 0.13760304375396323, "y": 0.5770450221940393, "ox": 0.13760304375396323, "oy": 0.5770450221940393, "term": "investigated", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7549143944197845, "os": 0.04782516518850924, "bg": 6.658018640232854e-06}, {"x": 0.5088776157260622, "y": 0.03646163601775523, "ox": 0.5088776157260622, "oy": 0.03646163601775523, "term": "production", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 2.4217437717593674e-07}, {"x": 0.4334178820545339, "y": 0.1483830057070387, "ox": 0.4334178820545339, "oy": 0.1483830057070387, "term": "management", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 8.540491382821574e-08}, {"x": 0.3417882054533925, "y": 0.4181991122384274, "ox": 0.3417882054533925, "oy": 0.4181991122384274, "term": "reveal", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 3.995632538598104e-06}, {"x": 0.8113506658211794, "y": 0.8823715916296766, "ox": 0.8113506658211794, "oy": 0.8823715916296766, "term": "allows", "cat25k": 10, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 16, "s": 0.9356372859860494, "os": 0.1348892265290979, "bg": 3.209916761632925e-06}, {"x": 0.43373493975903615, "y": 0.7343056436271401, "ox": 0.43373493975903615, "oy": 0.7343056436271401, "term": "convergence", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 5, "s": 0.8414711477488903, "os": 0.0699268577082082, "bg": 1.2214683236789269e-05}, {"x": 0.660431198478123, "y": 0.810716550412175, "ox": 0.660431198478123, "oy": 0.810716550412175, "term": "solutions", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 9, "s": 0.8849080532656943, "os": 0.08974947881700293, "bg": 9.96852813412523e-07}, {"x": 0.797717184527584, "y": 0.7954977805960685, "ox": 0.797717184527584, "oy": 0.7954977805960685, "term": "not only", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 15, "s": 0.7295497780596069, "os": 0.0439207095155648, "bg": 0.0}, {"x": 0.7498414711477489, "y": 0.7698161065313888, "ox": 0.7498414711477489, "oy": 0.7698161065313888, "term": "but also", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 12, "s": 0.75428027901078, "os": 0.04754248966467617, "bg": 0.0}, {"x": 0.43405199746353834, "y": 0.5076093849080533, "ox": 0.43405199746353834, "oy": 0.5076093849080533, "term": "forests", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 4.687371306546717e-06}, {"x": 0.770133164235891, "y": 0.8658845909955611, "ox": 0.770133164235891, "oy": 0.8658845909955611, "term": "spaces", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 13, "s": 0.9289790741915028, "os": 0.12886470442740539, "bg": 9.56514401110916e-06}, {"x": 0.5697526949904883, "y": 0.7555485098287889, "ox": 0.5697526949904883, "oy": 0.7555485098287889, "term": "practice", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 7, "s": 0.839568801521877, "os": 0.06858414897000106, "bg": 9.730774046406987e-07}, {"x": 0.4343690551680406, "y": 0.055168040583386174, "ox": 0.4343690551680406, "oy": 0.055168040583386174, "term": "correspondence", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 2.169009838135669e-06}, {"x": 0.34210526315789475, "y": 0.0948002536461636, "ox": 0.34210526315789475, "oy": 0.0948002536461636, "term": "random forests", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.06309448319594166, "y": 0.1487000634115409, "ox": 0.06309448319594166, "oy": 0.1487000634115409, "term": "heterogeneous feature", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.6607482561826252, "y": 0.7425491439441978, "ox": 0.6607482561826252, "oy": 0.7425491439441978, "term": "feature spaces", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 9, "s": 0.7758402029169309, "os": 0.051164269813787494, "bg": 0.0}, {"x": 0.8376664552948636, "y": 0.8433734939759037, "ox": 0.8376664552948636, "oy": 0.8433734939759037, "term": "different domains", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 19, "s": 0.8367152821813569, "os": 0.06695876470796086, "bg": 0.0}, {"x": 0.62143310082435, "y": 0.7070386810399493, "ox": 0.62143310082435, "oy": 0.7070386810399493, "term": "transfers", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 8, "s": 0.7285986049461003, "os": 0.04379703897388785, "bg": 6.1883858094124995e-06}, {"x": 0.7501585288522511, "y": 0.6341154090044389, "ox": 0.7501585288522511, "oy": 0.6341154090044389, "term": "rich", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 12, "s": 0.23906150919467342, "os": 0.002526412494258154, "bg": 1.816633890673049e-06}, {"x": 0.34242232086239693, "y": 0.03677869372225745, "ox": 0.34242232086239693, "oy": 0.03677869372225745, "term": "transfers knowledge", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 0.0}, {"x": 0.3427393785668992, "y": 0.7346227013316423, "ox": 0.3427393785668992, "oy": 0.7346227013316423, "term": "bridge", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 4, "s": 0.8563728598604946, "os": 0.07542136320271368, "bg": 1.9767833572237018e-06}, {"x": 0.23779327837666456, "y": 0.0554850982878884, "ox": 0.23779327837666456, "oy": 0.0554850982878884, "term": "makes use", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.8522511097019657, "y": 0.8512999365884591, "ox": 0.8522511097019657, "oy": 0.8512999365884591, "term": "key", "cat25k": 8, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 21, "s": 0.8316423589093216, "os": 0.06561605596975371, "bg": 1.1379070803313891e-06}, {"x": 0.5091946734305643, "y": 0.5329740012682308, "ox": 0.5091946734305643, "oy": 0.5329740012682308, "term": "every", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.4663918833227647, "os": 0.019416275043284685, "bg": 2.42671332357144e-07}, {"x": 0.23811033608116677, "y": 0.3332276474318326, "ox": 0.23811033608116677, "oy": 0.3332276474318326, "term": "path", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 6.068917850824525e-07}, {"x": 0.8931515535827521, "y": 0.8782498414711477, "ox": 0.8931515535827521, "oy": 0.8782498414711477, "term": "decision", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 28, "s": 0.8214965123652506, "os": 0.06252429242782942, "bg": 2.6952116154524286e-06}, {"x": 0.13792010145846545, "y": 0.33354470513633483, "ox": 0.13792010145846545, "oy": 0.33354470513633483, "term": "tree", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 4.343914602515023e-07}, {"x": 0.43468611287254283, "y": 0.09511731135066583, "ox": 0.43468611287254283, "oy": 0.09511731135066583, "term": "leading", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 4.252876384211586e-07}, {"x": 0.06341154090044387, "y": 0.20957514267596702, "ox": 0.06341154090044387, "oy": 0.20957514267596702, "term": "partition", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 2.885357531865167e-06}, {"x": 0.6217501585288523, "y": 0.6991122384273938, "ox": 0.6217501585288523, "oy": 0.6991122384273938, "term": "associated", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7035510462904249, "os": 0.04058160489028657, "bg": 1.069527753229145e-06}, {"x": 0.6991122384273938, "y": 0.45339251743817377, "ox": 0.6991122384273938, "oy": 0.45339251743817377, "term": "forest", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 10, "s": 0.10875079264426125, "os": -0.01220804918554115, "bg": 1.0934444904703578e-06}, {"x": 0.3430564362714014, "y": 0.05580215599239061, "ox": 0.3430564362714014, "oy": 0.05580215599239061, "term": "leading to", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.5095117311350665, "y": 0.5079264426125555, "ox": 0.5095117311350665, "oy": 0.5079264426125555, "term": "associated with", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 6, "s": 0.4166138237159163, "os": 0.016200840959683403, "bg": 0.0}, {"x": 0.238427393785669, "y": 0.20989220038046924, "ox": 0.238427393785669, "oy": 0.20989220038046924, "term": "random forest", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.8830057070386811, "y": 0.8662016487000634, "ox": 0.8830057070386811, "oy": 0.8662016487000634, "term": "be used", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 26, "s": 0.8040583386176285, "os": 0.05743613299883396, "bg": 0.0}, {"x": 0.8116677235256817, "y": 0.8002536461636017, "ox": 0.8116677235256817, "oy": 0.8002536461636017, "term": "transformation", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 16, "s": 0.7117945466074826, "os": 0.04164163810466062, "bg": 9.803392111261965e-06}, {"x": 0.699429296131896, "y": 0.333861762840837, "ox": 0.699429296131896, "oy": 0.333861762840837, "term": "along with", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 10, "s": 0.06626506024096385, "os": -0.021854351436345003, "bg": 0.0}, {"x": 0.6220672162333545, "y": 0.686429930247305, "ox": 0.6220672162333545, "oy": 0.686429930247305, "term": "conduct", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.6769181991122385, "os": 0.037366170806685285, "bg": 2.252665822954976e-06}, {"x": 0.5700697526949905, "y": 0.533291058972733, "ox": 0.5700697526949905, "oy": 0.533291058972733, "term": "diverse", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 7, "s": 0.3937856689917565, "os": 0.01392176954877919, "bg": 3.513076732841566e-06}, {"x": 0.5098287888395688, "y": 0.5773620798985415, "ox": 0.5098287888395688, "oy": 0.5773620798985415, "term": "varying", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 6, "s": 0.5440710209258085, "os": 0.025847143210487256, "bg": 7.856481565865913e-06}, {"x": 0.13823715916296767, "y": 0.6176284083703234, "ox": 0.13823715916296767, "oy": 0.6176284083703234, "term": "dimensions", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 2, "s": 0.7907419150285353, "os": 0.054256033355711816, "bg": 2.045457718743694e-06}, {"x": 0.6997463538363983, "y": 0.7885225110970197, "ox": 0.6997463538363983, "oy": 0.7885225110970197, "term": "sparsity", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 10, "s": 0.8389346861128725, "os": 0.06817780290449099, "bg": 0.0002809270592956757}, {"x": 0.7000634115409005, "y": 0.5545339251743817, "ox": 0.7000634115409005, "oy": 0.5545339251743817, "term": "verify", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 10, "s": 0.2159162967660114, "os": 0.0006536871488639975, "bg": 3.7851454131202332e-06}, {"x": 0.510145846544071, "y": 0.4185161699429296, "ox": 0.510145846544071, "oy": 0.4185161699429296, "term": "superiority", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.2780596068484464, "os": 0.006554538708879544, "bg": 2.430881405612393e-05}, {"x": 0.5104629042485732, "y": 0.634432466708941, "ox": 0.5104629042485732, "oy": 0.634432466708941, "term": "we conduct", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 6, "s": 0.6585288522511097, "os": 0.035493445461291115, "bg": 0.0}, {"x": 0.435003170577045, "y": 0.21020925808497146, "ox": 0.435003170577045, "oy": 0.21020925808497146, "term": "conduct extensive", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 0.0}, {"x": 0.8975903614457831, "y": 0.9235890932149651, "ox": 0.8975903614457831, "oy": 0.9235890932149651, "term": "reinforcement", "cat25k": 15, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 108, "ncat": 29, "s": 0.9682942295497781, "os": 0.18564715027737536, "bg": 0.000115928440842859}, {"x": 0.8979074191502854, "y": 0.9216867469879518, "ox": 0.8979074191502854, "oy": 0.9216867469879518, "term": "reinforcement learning", "cat25k": 14, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 105, "ncat": 29, "s": 0.9635383639822448, "os": 0.17600084802657148, "bg": 0.0}, {"x": 0.6610653138871274, "y": 0.5976537729866835, "ox": 0.6610653138871274, "oy": 0.5976537729866835, "term": "q", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 9, "s": 0.3804692454026633, "os": 0.01257906081057207, "bg": 4.148940912729231e-07}, {"x": 0.7704502219403931, "y": 0.0003170577045022194, "ox": 0.7704502219403931, "oy": 0.0003170577045022194, "term": "trading", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.007292327203551046, "os": -0.07370764283947565, "bg": 5.917565266022212e-07}, {"x": 0.5107799619530755, "y": 0.4188332276474318, "ox": 0.5107799619530755, "oy": 0.4188332276474318, "term": "maximize", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.2780596068484464, "os": 0.006554538708879544, "bg": 6.630296581890183e-06}, {"x": 0.6613823715916297, "y": 0.14901712111604312, "ox": 0.6613823715916297, "oy": 0.14901712111604312, "term": "total", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 9, "s": 0.051363348129359554, "os": -0.02600614819264337, "bg": 1.5254821153867674e-07}, {"x": 0.6616994292961319, "y": 0.14933417882054534, "ox": 0.6616994292961319, "oy": 0.14933417882054534, "term": "financial", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 9, "s": 0.051363348129359554, "os": -0.02600614819264337, "bg": 2.2886197789144826e-07}, {"x": 0.6620164870006341, "y": 0.008877615726062143, "ox": 0.6620164870006341, "oy": 0.008877615726062143, "term": "market", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.02219403931515536, "os": -0.045298752694251085, "bg": 1.3528384515403702e-07}, {"x": 0.7003804692454026, "y": 0.6626506024096386, "ox": 0.7003804692454026, "oy": 0.6626506024096386, "term": "situations", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 10, "s": 0.4727330374128091, "os": 0.019946291650471716, "bg": 4.120756840017009e-06}, {"x": 0.43532022828154726, "y": 0.5548509828788839, "ox": 0.43532022828154726, "oy": 0.5548509828788839, "term": "limitations", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.5798985415345593, "os": 0.02812621462139147, "bg": 3.5180804863394463e-06}, {"x": 0.7863031071655041, "y": 0.6629676601141408, "ox": 0.7863031071655041, "oy": 0.6629676601141408, "term": "automated", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 14, "s": 0.176284083703234, "os": -0.00203173032755026, "bg": 5.824705041918332e-06}, {"x": 0.2387444514901712, "y": 0.21052631578947367, "ox": 0.2387444514901712, "oy": 0.21052631578947367, "term": "adding", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 8.879460546133442e-07}, {"x": 0.8449587824984147, "y": 0.8760304375396322, "ox": 0.8449587824984147, "oy": 0.8760304375396322, "term": "dnn", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 20, "s": 0.9026632847178186, "os": 0.10326490230027208, "bg": 0.00030777285655662315}, {"x": 0.13855421686746988, "y": 0.27520608750792647, "ox": 0.13855421686746988, "oy": 0.27520608750792647, "term": "thereby", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 2.8021532679762216e-06}, {"x": 0.7504755865567533, "y": 0.7349397590361446, "ox": 0.7504755865567533, "oy": 0.7349397590361446, "term": "combining", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 12, "s": 0.6185795814838301, "os": 0.03146531924666973, "bg": 1.2364072478192866e-05}, {"x": 0.3433734939759036, "y": 0.41915028535193405, "ox": 0.3433734939759036, "oy": 0.41915028535193405, "term": "network dnn", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 0.0}, {"x": 0.8833227647431833, "y": 0.9489537095751427, "ox": 0.8833227647431833, "oy": 0.9489537095751427, "term": "action", "cat25k": 23, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 166, "ncat": 26, "s": 0.9990488268864933, "os": 0.350040634606551, "bg": 2.366652315755424e-06}, {"x": 0.7866201648700063, "y": 0.8272035510462904, "ox": 0.7866201648700063, "oy": 0.8272035510462904, "term": "strategies", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 14, "s": 0.8557387444514901, "os": 0.07513868767888061, "bg": 3.343056144821623e-06}, {"x": 0.6223842739378567, "y": 0.699429296131896, "ox": 0.6223842739378567, "oy": 0.699429296131896, "term": "analyze", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7035510462904249, "os": 0.04058160489028657, "bg": 7.798890240201363e-06}, {"x": 0.43563728598604945, "y": 0.5336081166772353, "ox": 0.43563728598604945, "oy": 0.5336081166772353, "term": "beneficial", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 5, "s": 0.5326569435637286, "os": 0.02491078053779018, "bg": 5.941901438169722e-06}, {"x": 0.9140773620798985, "y": 0.8563728598604946, "ox": 0.9140773620798985, "oy": 0.8563728598604946, "term": "finally", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 33, "s": 0.2767913760304375, "os": 0.006112858202890342, "bg": 3.737553464534325e-06}, {"x": 0.34369055168040585, "y": 0.09543436905516804, "ox": 0.34369055168040585, "oy": 0.09543436905516804, "term": "overfitting", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 8.26856292376385e-05}, {"x": 0.7507926442612556, "y": 0.7168674698795181, "ox": 0.7507926442612556, "oy": 0.7168674698795181, "term": "finally we", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 12, "s": 0.5342422320862397, "os": 0.025034451079467154, "bg": 0.0}, {"x": 0.7869372225745086, "y": 0.8240329740012682, "ox": 0.7869372225745086, "oy": 0.8240329740012682, "term": "four", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 14, "s": 0.8471781864299303, "os": 0.07192325359527933, "bg": 9.428271691228616e-07}, {"x": 0.1388712745719721, "y": 0.37983512999365887, "ox": 0.1388712745719721, "oy": 0.37983512999365887, "term": "experimentally", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 2.3608052369405316e-05}, {"x": 0.9194673430564363, "y": 0.927710843373494, "ox": 0.9194673430564363, "oy": 0.927710843373494, "term": "research", "cat25k": 16, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 114, "ncat": 35, "s": 0.9594166138237159, "os": 0.16875728772834883, "bg": 9.558346251240496e-07}, {"x": 0.5703868103994927, "y": 0.6997463538363983, "ox": 0.5703868103994927, "oy": 0.6997463538363983, "term": "enables", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 7, "s": 0.7447685478757134, "os": 0.046076110384792056, "bg": 4.776422343064147e-06}, {"x": 0.7006975269499048, "y": 0.45370957514267596, "ox": 0.7006975269499048, "oy": 0.45370957514267596, "term": "increases", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 10, "s": 0.10875079264426125, "os": -0.01220804918554115, "bg": 2.1788210154368106e-06}, {"x": 0.8379835129993659, "y": 0.38015218769816106, "ox": 0.8379835129993659, "oy": 0.38015218769816106, "term": "times", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 19, "s": 0.01077996195307546, "os": -0.06808946680329317, "bg": 3.0514545168916784e-07}, {"x": 0.570703868103995, "y": 0.5551680405833862, "ox": 0.570703868103995, "oy": 0.5551680405833862, "term": "fixed", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.42897907419150283, "os": 0.017137203632380478, "bg": 1.0111978226726692e-06}, {"x": 0.1391883322764743, "y": 0.5979708306911858, "ox": 0.1391883322764743, "oy": 0.5979708306911858, "term": "situation", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 1.0206175413444047e-06}, {"x": 0.4359543436905517, "y": 0.3804692454026633, "ox": 0.4359543436905517, "oy": 0.3804692454026633, "term": "3", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 0.0}, {"x": 0.13950538998097653, "y": 0.09575142675967026, "ox": 0.13950538998097653, "oy": 0.09575142675967026, "term": "30", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.23906150919467342, "y": 0.3807863031071655, "ox": 0.23906150919467342, "oy": 0.3807863031071655, "term": "outperforming", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 8.698122945068454e-05}, {"x": 0.6623335447051364, "y": 0.6867469879518072, "ox": 0.6623335447051364, "oy": 0.6867469879518072, "term": "modality", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 9, "s": 0.622701331642359, "os": 0.03187166531217979, "bg": 6.506465102574422e-05}, {"x": 0.4362714013950539, "y": 0.5339251743817375, "ox": 0.4362714013950539, "oy": 0.5339251743817375, "term": "explicit", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 5, "s": 0.5326569435637286, "os": 0.02491078053779018, "bg": 4.489892130341569e-06}, {"x": 0.7707672796448953, "y": 0.8836398224476855, "ox": 0.7707672796448953, "oy": 0.8836398224476855, "term": "mapping", "cat25k": 10, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 13, "s": 0.9492707672796449, "os": 0.15458817709621567, "bg": 8.424892608947025e-06}, {"x": 0.4365884590995561, "y": 0.5082435003170577, "ox": 0.4365884590995561, "oy": 0.5082435003170577, "term": "relation", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 1.8594901915434282e-06}, {"x": 0.87571337983513, "y": 0.6870640456563094, "ox": 0.87571337983513, "oy": 0.6870640456563094, "term": "audio", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 25, "s": 0.015535827520608751, "os": -0.056040422599908116, "bg": 8.015991998342482e-07}, {"x": 0.6227013316423589, "y": 0.5342422320862397, "ox": 0.6227013316423589, "oy": 0.5342422320862397, "term": "extracting", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 8, "s": 0.30374128091312613, "os": 0.0084272640542737, "bg": 2.4462485799526993e-05}, {"x": 0.6230183893468612, "y": 0.21084337349397592, "ox": 0.6230183893468612, "oy": 0.21084337349397592, "term": "- modal", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 8, "s": 0.08306911857958149, "os": -0.01729620861453659, "bg": 0.0}, {"x": 0.8934686112872543, "y": 0.8792010145846544, "ox": 0.8934686112872543, "oy": 0.8792010145846544, "term": "modeling", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 69, "ncat": 28, "s": 0.8319594166138238, "os": 0.0657397265114307, "bg": 1.2352366143488778e-05}, {"x": 0.9172479391249208, "y": 0.7428662016487001, "ox": 0.9172479391249208, "oy": 0.7428662016487001, "term": "noise", "cat25k": 4, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 34, "s": 0.00538998097653773, "os": -0.08619836754884988, "bg": 5.196345488950129e-06}, {"x": 0.5110970196575777, "y": 0.50856055802156, "ox": 0.5110970196575777, "oy": 0.50856055802156, "term": "solved", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 6, "s": 0.4166138237159163, "os": 0.016200840959683403, "bg": 6.33907480626923e-06}, {"x": 0.8589093214965123, "y": 0.7650602409638554, "ox": 0.8589093214965123, "oy": 0.7650602409638554, "term": "efficiency", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 22, "s": 0.11192136968928344, "os": -0.010617999363980071, "bg": 4.905995108808946e-06}, {"x": 0.5114140773620799, "y": 0.7000634115409005, "ox": 0.5114140773620799, "oy": 0.7000634115409005, "term": "so that", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 6, "s": 0.7777425491439443, "os": 0.05157061587929755, "bg": 0.0}, {"x": 0.7247939124920736, "y": 0.5776791376030438, "ox": 0.7247939124920736, "oy": 0.5776791376030438, "term": "aspects", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 11, "s": 0.1845275840202917, "os": -0.0016253842620402165, "bg": 2.0972699103532463e-06}, {"x": 0.34400760938490804, "y": 0.7653772986683577, "ox": 0.34400760938490804, "oy": 0.7653772986683577, "term": "solving", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 4, "s": 0.8807863031071655, "os": 0.08828309953711883, "bg": 7.67217994763639e-06}, {"x": 0.6626506024096386, "y": 0.48478123018389346, "ox": 0.6626506024096386, "oy": 0.48478123018389346, "term": "firstly", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 9, "s": 0.16613823715916298, "os": -0.003498109607434366, "bg": 1.753562380241736e-05}, {"x": 0.9302473050095117, "y": 0.9055168040583386, "ox": 0.9302473050095117, "oy": 0.9055168040583386, "term": "video", "cat25k": 12, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 40, "s": 0.8059606848446418, "os": 0.05768347408218791, "bg": 6.946704060501132e-07}, {"x": 0.8325935320228282, "y": 0.8389346861128726, "ox": 0.8325935320228282, "oy": 0.8389346861128726, "term": "activity", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 18, "s": 0.8329105897273305, "os": 0.06602240203526376, "bg": 1.7529677524928735e-06}, {"x": 0.8982244768547876, "y": 0.9226379201014585, "ox": 0.8982244768547876, "oy": 0.9226379201014585, "term": "speech", "cat25k": 15, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 107, "ncat": 29, "s": 0.9670259987317692, "os": 0.1824317161937741, "bg": 8.676047856059262e-06}, {"x": 0.7511097019657578, "y": 0.8072289156626506, "ox": 0.7511097019657578, "oy": 0.8072289156626506, "term": "area", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 12, "s": 0.8430564362714014, "os": 0.07005052824988517, "bg": 4.2291020861952983e-07}, {"x": 0.8709575142675967, "y": 0.8864933417882055, "ox": 0.8709575142675967, "oy": 0.8864933417882055, "term": "via", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 24, "s": 0.8991756499682942, "os": 0.10057948482385781, "bg": 1.991825159394525e-06}, {"x": 0.5710209258084972, "y": 0.024096385542168676, "ox": 0.5710209258084972, "oy": 0.024096385542168676, "term": "extractors", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.04660748256182626, "os": -0.02787887353803753, "bg": 3.909415293632629e-05}, {"x": 0.5713379835129994, "y": 0.024413443246670895, "ox": 0.5713379835129994, "oy": 0.024413443246670895, "term": "feature extractors", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.04660748256182626, "os": -0.02787887353803753, "bg": 0.0}, {"x": 0.6629676601141408, "y": 0.27552314521242866, "ox": 0.6629676601141408, "oy": 0.27552314521242866, "term": "extractor", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 9, "s": 0.0735573874445149, "os": -0.019575280025440796, "bg": 1.7269634438196067e-05}, {"x": 0.4369055168040583, "y": 0.6756499682942295, "ox": 0.4369055168040583, "oy": 0.6756499682942295, "term": "net", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 5, "s": 0.7707672796448954, "os": 0.05063425320660047, "bg": 5.12319140670028e-07}, {"x": 0.663284717818643, "y": 0.5554850982878884, "ox": 0.663284717818643, "oy": 0.5554850982878884, "term": "2d", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 9, "s": 0.2771084337349397, "os": 0.006148192643369493, "bg": 0.0}, {"x": 0.6636017755231453, "y": 0.2758402029169309, "ox": 0.6636017755231453, "oy": 0.2758402029169309, "term": "feature extractor", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 9, "s": 0.0735573874445149, "os": -0.019575280025440796, "bg": 0.0}, {"x": 0.8760304375396322, "y": 0.8275206087507927, "ox": 0.8760304375396322, "oy": 0.8275206087507927, "term": "3d", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 25, "s": 0.4004438807863031, "os": 0.014699127239320181, "bg": 0.0}, {"x": 0.7872542802790108, "y": 0.7495244134432467, "ox": 0.7872542802790108, "oy": 0.7495244134432467, "term": "sequence", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 14, "s": 0.5684844641724794, "os": 0.026907176424861318, "bg": 2.620252078842493e-06}, {"x": 0.43722257450856056, "y": 0.21116043119847813, "ox": 0.43722257450856056, "oy": 0.21116043119847813, "term": "adopt", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 2.43409788637718e-06}, {"x": 0.3443246670894103, "y": 0.03709575142675967, "ox": 0.3443246670894103, "oy": 0.03709575142675967, "term": "we adopt", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 0.0}, {"x": 0.5716550412175015, "y": 0.33417882054533926, "ox": 0.5716550412175015, "oy": 0.33417882054533926, "term": "next", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 7, "s": 0.14679771718452758, "os": -0.005370834952828522, "bg": 8.448041961762345e-08}, {"x": 0.23937856689917564, "y": 0.2761572606214331, "ox": 0.23937856689917564, "oy": 0.2761572606214331, "term": "belief", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 2.107138425784437e-06}, {"x": 0.0637285986049461, "y": 0.14965123652504755, "ox": 0.0637285986049461, "oy": 0.14965123652504755, "term": "dbn", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 5.62440240724423e-05}, {"x": 0.13982244768547875, "y": 0.14996829422954977, "ox": 0.13982244768547875, "oy": 0.14996829422954977, "term": "deep belief", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.4375396322130628, "y": 0.09606848446417247, "ox": 0.4375396322130628, "oy": 0.09606848446417247, "term": "consideration", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 1.1027066486043318e-06}, {"x": 0.5117311350665821, "y": 0.4194673430564363, "ox": 0.5117311350665821, "oy": 0.4194673430564363, "term": "denoising", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.2780596068484464, "os": 0.006554538708879544, "bg": 0.00013217299357917508}, {"x": 0.3446417247939125, "y": 0.21147748890298035, "ox": 0.3446417247939125, "oy": 0.21147748890298035, "term": "excellent", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 4.294168938089348e-07}, {"x": 0.23969562460367788, "y": 0.21179454660748256, "ox": 0.23969562460367788, "oy": 0.21179454660748256, "term": "image processing", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.7980342422320862, "y": 0.15028535193405199, "ox": 0.7980342422320862, "oy": 0.15028535193405199, "term": "decoder", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 15, "s": 0.012682308180088777, "os": -0.05897318115967634, "bg": 1.522226660211543e-05}, {"x": 0.2400126823081801, "y": 0.4540266328471782, "ox": 0.2400126823081801, "oy": 0.4540266328471782, "term": "high resolution", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 0.0}, {"x": 0.7710843373493976, "y": 0.6873811033608117, "ox": 0.7710843373493976, "oy": 0.6873811033608117, "term": "increasing", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 13, "s": 0.33798351299936585, "os": 0.009893643334157806, "bg": 2.7189199390320546e-06}, {"x": 0.7010145846544071, "y": 0.33449587824984145, "ox": 0.7010145846544071, "oy": 0.33449587824984145, "term": "frameworks", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 10, "s": 0.06626506024096385, "os": -0.021854351436345003, "bg": 1.2616230780899628e-05}, {"x": 0.8763474952441345, "y": 0.8493975903614458, "ox": 0.8763474952441345, "oy": 0.8493975903614458, "term": "still", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 25, "s": 0.7000634115409005, "os": 0.04042259990813046, "bg": 8.496602276932066e-07}, {"x": 0.8227647431832593, "y": 0.8982244768547876, "ox": 0.8227647431832593, "oy": 0.8982244768547876, "term": "those", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 81, "ncat": 17, "s": 0.9568801521876982, "os": 0.16476449595420656, "bg": 7.252674878078557e-07}, {"x": 0.7983512999365885, "y": 0.7352568167406468, "ox": 0.7983512999365885, "oy": 0.7352568167406468, "term": "practical", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 15, "s": 0.40171211160431197, "os": 0.014981802763153237, "bg": 3.1324649277990668e-06}, {"x": 0.7986683576410907, "y": 0.849714648065948, "ox": 0.7986683576410907, "oy": 0.849714648065948, "term": "especially", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 15, "s": 0.8925174381737476, "os": 0.0953676548531854, "bg": 2.3838569371571493e-06}, {"x": 0.7714013950538998, "y": 0.770133164235891, "ox": 0.7714013950538998, "oy": 0.770133164235891, "term": "would", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 13, "s": 0.7140139505389982, "os": 0.042047984170170666, "bg": 1.7106696339825764e-07}, {"x": 0.2403297400126823, "y": 0.3348129359543437, "ox": 0.2403297400126823, "oy": 0.3348129359543437, "term": "growth", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 3.4899934906635697e-07}, {"x": 0.8985415345592898, "y": 0.9153455928979074, "ox": 0.8985415345592898, "oy": 0.9153455928979074, "term": "parameters", "cat25k": 13, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 29, "s": 0.9461001902346228, "os": 0.15027737535776123, "bg": 8.357259027307243e-06}, {"x": 0.3449587824984147, "y": 0.21211160431198478, "ox": 0.3449587824984147, "oy": 0.21211160431198478, "term": "these problems", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 0.0}, {"x": 0.7251109701965758, "y": 0.6876981610653139, "ox": 0.7251109701965758, "oy": 0.6876981610653139, "term": "consists", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 11, "s": 0.48065948002536457, "os": 0.020882654323168798, "bg": 4.5043767324350305e-06}, {"x": 0.771718452758402, "y": 0.056119213696892836, "ox": 0.771718452758402, "oy": 0.056119213696892836, "term": "shallow", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 13, "s": 0.016487000634115408, "os": -0.05441503833786793, "bg": 7.502063561036097e-06}, {"x": 0.3452758402029169, "y": 0.7431832593532023, "ox": 0.3452758402029169, "oy": 0.7431832593532023, "term": "location", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 4, "s": 0.8630310716550412, "os": 0.07863679728631497, "bg": 3.404669819589525e-07}, {"x": 0.6639188332276474, "y": 0.0963855421686747, "ox": 0.6639188332276474, "oy": 0.0963855421686747, "term": "compression", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.044705136334812934, "os": -0.029221582276244655, "bg": 2.924229645968086e-06}, {"x": 0.7720355104629042, "y": 0.33512999365884594, "ox": 0.7720355104629042, "oy": 0.33512999365884594, "term": "supervision", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 13, "s": 0.028852251109701965, "os": -0.03833786791986149, "bg": 4.7357150362405524e-06}, {"x": 0.5120481927710844, "y": 0.09670259987317692, "ox": 0.5120481927710844, "oy": 0.09670259987317692, "term": "secondly", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 6, "s": 0.10589727330374128, "os": -0.012738065792728175, "bg": 7.0586996029209975e-06}, {"x": 0.6642358909321496, "y": 0.6179454660748256, "ox": 0.6642358909321496, "oy": 0.6179454660748256, "term": "constructed", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 9, "s": 0.4150285351934052, "os": 0.015794494894173353, "bg": 5.0795276242490544e-06}, {"x": 0.5123652504755866, "y": 0.09701965757767914, "ox": 0.5123652504755866, "oy": 0.09701965757767914, "term": "positions", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 6, "s": 0.10589727330374128, "os": -0.012738065792728175, "bg": 9.436331618824465e-07}, {"x": 0.725428027901078, "y": 0.3354470513633481, "ox": 0.725428027901078, "oy": 0.3354470513633481, "term": "r", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 11, "s": 0.0488268864933418, "os": -0.0273488569308505, "bg": 1.3590097427254024e-07}, {"x": 0.140139505389981, "y": 0.15060240963855423, "ox": 0.140139505389981, "oy": 0.15060240963855423, "term": "quantitative", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 2.8219893359844986e-06}, {"x": 0.5126823081800888, "y": 0.6880152187698161, "ox": 0.5126823081800888, "oy": 0.6880152187698161, "term": "reducing", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 6, "s": 0.7584020291693089, "os": 0.04835518179569627, "bg": 4.212153259986143e-06}, {"x": 0.5129993658845909, "y": 0.212428662016487, "ox": 0.5129993658845909, "oy": 0.212428662016487, "term": "choice", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 4.382554208689518e-07}, {"x": 0.437856689917565, "y": 0.15091946734305645, "ox": 0.437856689917565, "oy": 0.15091946734305645, "term": "faster r", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 0.0}, {"x": 0.6645529486366518, "y": 0.33576410906785037, "ox": 0.6645529486366518, "oy": 0.33576410906785037, "term": "r cnn", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.0897273303741281, "os": -0.016359845941839507, "bg": 0.0}, {"x": 0.7723525681674065, "y": 0.6759670259987318, "ox": 0.7723525681674065, "oy": 0.6759670259987318, "term": "better performance", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 13, "s": 0.27932783766645525, "os": 0.0066782092505565105, "bg": 0.0}, {"x": 0.43817374762206723, "y": 0.38110336081166774, "ox": 0.43817374762206723, "oy": 0.38110336081166774, "term": "which makes", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 0.0}, {"x": 0.34559289790741915, "y": 0.27647431832593533, "ox": 0.34559289790741915, "oy": 0.27647431832593533, "term": "makes it", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.6648700063411541, "y": 0.0006341154090044388, "ox": 0.6648700063411541, "oy": 0.0006341154090044388, "term": "federated", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.016804058338617627, "os": -0.051729620861453655, "bg": 6.873962461290999e-06}, {"x": 0.9143944197844007, "y": 0.9372225745085606, "ox": 0.9143944197844007, "oy": 0.9372225745085606, "term": "'s", "cat25k": 18, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 129, "ncat": 33, "s": 0.9850982878883957, "os": 0.2247623758877778, "bg": 0.0}, {"x": 0.8329105897273303, "y": 0.6883322764743183, "ox": 0.8329105897273303, "oy": 0.6883322764743183, "term": "artificial", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 18, "s": 0.08243500317057705, "os": -0.017578884138369666, "bg": 8.122306428593252e-06}, {"x": 0.7257450856055803, "y": 0.663284717818643, "ox": 0.7257450856055803, "oy": 0.663284717818643, "term": "intelligence", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 11, "s": 0.3998097653772987, "os": 0.01445178615596622, "bg": 2.2037356782759987e-06}, {"x": 0.5719720989220038, "y": 0.5088776157260622, "ox": 0.5719720989220038, "oy": 0.5088776157260622, "term": "faces", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 3.010124685253838e-06}, {"x": 0.6233354470513633, "y": 0.4543436905516804, "ox": 0.6233354470513633, "oy": 0.4543436905516804, "term": "artificial intelligence", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 0.0}, {"x": 0.3459099556119214, "y": 0.2127457197209892, "ox": 0.3459099556119214, "oy": 0.2127457197209892, "term": "exists", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 1.3779244525360407e-06}, {"x": 0.4384908053265694, "y": 0.03741280913126189, "ox": 0.4384908053265694, "oy": 0.03741280913126189, "term": "privacy", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 5.996941817541551e-08}, {"x": 0.1404565630944832, "y": 0.09733671528218135, "ox": 0.1404565630944832, "oy": 0.09733671528218135, "term": "beyond", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 3.78579675899622e-07}, {"x": 0.24064679771718453, "y": 0.09765377298668358, "ox": 0.24064679771718453, "oy": 0.09765377298668358, "term": "google", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 2.3585024976128713e-07}, {"x": 0.24096385542168675, "y": 0.45466074825618263, "ox": 0.24096385542168675, "oy": 0.45466074825618263, "term": "2016", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 0.0}, {"x": 0.43880786303107167, "y": 0.3814204185161699, "ox": 0.43880786303107167, "oy": 0.3814204185161699, "term": "includes", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 3.0169218257878494e-07}, {"x": 0.9267596702599873, "y": 0.9020291693088142, "ox": 0.9267596702599873, "oy": 0.9020291693088142, "term": "provide", "cat25k": 12, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 85, "ncat": 38, "s": 0.8202282815472417, "os": 0.06224161690399635, "bg": 1.3570766657678886e-06}, {"x": 0.572289156626506, "y": 0.534559289790742, "ox": 0.572289156626506, "oy": 0.534559289790742, "term": "survey", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 7, "s": 0.3937856689917565, "os": 0.01392176954877919, "bg": 7.591845194178523e-07}, {"x": 0.7989854153455929, "y": 0.8170577045022194, "ox": 0.7989854153455929, "oy": 0.8170577045022194, "term": "works", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 15, "s": 0.8126188966391884, "os": 0.05999787993357124, "bg": 1.2144195317522133e-06}, {"x": 0.7993024730500952, "y": 0.735573874445149, "ox": 0.7993024730500952, "oy": 0.735573874445149, "term": "building", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 15, "s": 0.40171211160431197, "os": 0.014981802763153237, "bg": 7.160519776838042e-07}, {"x": 0.8937856689917565, "y": 0.9064679771718452, "ox": 0.8937856689917565, "oy": 0.9064679771718452, "term": "among", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 28, "s": 0.9267596702599873, "os": 0.12683297409985517, "bg": 2.7730435496449142e-06}, {"x": 0.3462270133164236, "y": 0.4850982878883957, "ox": 0.3462270133164236, "oy": 0.4850982878883957, "term": "mechanisms", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 4, "s": 0.5206087507926442, "os": 0.023974417865093106, "bg": 2.7072349143252767e-06}, {"x": 0.43912492073557385, "y": 0.4197844007609385, "ox": 0.43912492073557385, "oy": 0.4197844007609385, "term": "allowing", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 1.8413826819800142e-06}, {"x": 0.9071020925808497, "y": 0.9258084971464806, "ox": 0.9071020925808497, "oy": 0.9258084971464806, "term": "without", "cat25k": 15, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 111, "ncat": 31, "s": 0.9663918833227648, "os": 0.1810890074555669, "bg": 1.2943147330173477e-06}, {"x": 0.5133164235890932, "y": 0.800570703868104, "ox": 0.5133164235890932, "oy": 0.800570703868104, "term": "age", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 6, "s": 0.8941027266962587, "os": 0.09658669304971557, "bg": 7.069971113000582e-07}, {"x": 0.14077362079898542, "y": 0.33608116677235256, "ox": 0.14077362079898542, "oy": 0.33608116677235256, "term": "age estimation", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 0.0}, {"x": 0.8592263792010146, "y": 0.8392517438173748, "ox": 0.8592263792010146, "oy": 0.8392517438173748, "term": "mean", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 22, "s": 0.7298668357641092, "os": 0.04404438005724179, "bg": 1.8375248599731562e-06}, {"x": 0.5136334812935954, "y": 0.5558021559923906, "ox": 0.5136334812935954, "oy": 0.5558021559923906, "term": "contributions", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 2.17096631067342e-06}, {"x": 0.5726062143310082, "y": 0.7888395688015218, "ox": 0.5726062143310082, "oy": 0.7888395688015218, "term": "comparison", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 7, "s": 0.8766645529486367, "os": 0.08466131938800747, "bg": 2.5616619884778673e-06}, {"x": 0.5139505389980976, "y": 0.03772986683576411, "ox": 0.5139505389980976, "oy": 0.03772986683576411, "term": "dcnns", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 9.528223029715063e-05}, {"x": 0.4394419784400761, "y": 0.4854153455928979, "ox": 0.4394419784400761, "oy": 0.4854153455928979, "term": "addressed", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 5, "s": 0.45244134432466704, "os": 0.01847991237058761, "bg": 2.437294505606996e-06}, {"x": 0.3465440710209258, "y": 0.4201014584654407, "ox": 0.3465440710209258, "oy": 0.4201014584654407, "term": "retraining", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 4.336601940756915e-05}, {"x": 0.5142675967025999, "y": 0.4549778059606848, "ox": 0.5142675967025999, "oy": 0.4549778059606848, "term": "involving", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 2.4085938629028377e-06}, {"x": 0.24128091312618896, "y": 0.21306277742549143, "ox": 0.24128091312618896, "oy": 0.21306277742549143, "term": "biological", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 1.1837591791892768e-06}, {"x": 0.346861128725428, "y": 0.15123652504755866, "ox": 0.346861128725428, "oy": 0.15123652504755866, "term": "apparent", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 2.4288311978509707e-06}, {"x": 0.8332276474318326, "y": 0.8360811667723526, "ox": 0.8332276474318326, "oy": 0.8360811667723526, "term": "shows", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 18, "s": 0.8224476854787572, "os": 0.06280696795166248, "bg": 1.622849934043498e-06}, {"x": 0.8595434369055168, "y": 0.8075459733671528, "ox": 0.8595434369055168, "oy": 0.8075459733671528, "term": "type", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 22, "s": 0.40266328471781865, "os": 0.015105473304830225, "bg": 4.769455421305333e-07}, {"x": 0.7013316423589093, "y": 0.48573240329740014, "ox": 0.7013316423589093, "oy": 0.48573240329740014, "term": "adopted", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 10, "s": 0.1223842739378567, "os": -0.008992615101939862, "bg": 2.154811111412785e-06}, {"x": 0.5729232720355104, "y": 0.7073557387444515, "ox": 0.5729232720355104, "oy": 0.7073557387444515, "term": "shows that", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 7, "s": 0.7637920101458466, "os": 0.04929154446839334, "bg": 0.0}, {"x": 0.5145846544071021, "y": 0.6480659480025365, "ox": 0.5145846544071021, "oy": 0.6480659480025365, "term": "carried", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 6, "s": 0.6873811033608117, "os": 0.03870887954489241, "bg": 2.1126705412315884e-06}, {"x": 0.5149017121116043, "y": 0.577996195307546, "ox": 0.5149017121116043, "oy": 0.577996195307546, "term": "carried out", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 6, "s": 0.5440710209258085, "os": 0.025847143210487256, "bg": 0.0}, {"x": 0.888712745719721, "y": 0.9067850348763475, "ox": 0.888712745719721, "oy": 0.9067850348763475, "term": "robot", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 27, "s": 0.9327837666455295, "os": 0.13232747959436064, "bg": 1.7847015230254815e-05}, {"x": 0.24159797083069118, "y": 0.05643627140139505, "ox": 0.24159797083069118, "oy": 0.05643627140139505, "term": "human robot", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.8335447051363348, "y": 0.024730500951173115, "ox": 0.8335447051363348, "oy": 0.024730500951173115, "term": "gaze", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 18, "s": 0.004438807863031071, "os": -0.08831843397759798, "bg": 1.8718091503390317e-05}, {"x": 0.8988585922637921, "y": 0.6483830057070387, "ox": 0.8988585922637921, "oy": 0.6483830057070387, "term": "control", "cat25k": 3, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 29, "s": 0.0050729232720355105, "os": -0.08766474682873396, "bg": 4.819469462884539e-07}, {"x": 0.5152187698161065, "y": 0.5561192136968929, "ox": 0.5152187698161065, "oy": 0.5561192136968929, "term": "multimodal", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 6.837470477796738e-05}, {"x": 0.8119847812301839, "y": 0.8722257450856056, "ox": 0.8119847812301839, "oy": 0.8722257450856056, "term": "adapt", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 16, "s": 0.9185161699429296, "os": 0.11881205611109147, "bg": 3.3017223985178935e-05}, {"x": 0.14109067850348764, "y": 0.21337983512999367, "ox": 0.14109067850348764, "oy": 0.21337983512999367, "term": "neither", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 9.619118329977646e-07}, {"x": 0.5732403297400127, "y": 0.4204185161699429, "ox": 0.5732403297400127, "oy": 0.4204185161699429, "term": "external", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 7, "s": 0.2171845275840203, "os": 0.0010600332143740482, "bg": 7.868496338611612e-07}, {"x": 0.8452758402029169, "y": 0.2136968928344959, "ox": 0.8452758402029169, "oy": 0.2136968928344959, "term": "sensors", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 20, "s": 0.006024096385542169, "os": -0.08323027454860253, "bg": 6.894645653847182e-06}, {"x": 0.2419150285351934, "y": 0.4860494610019023, "ox": 0.2419150285351934, "oy": 0.4860494610019023, "term": "nor", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.5916296766011414, "os": 0.0294689233595986, "bg": 9.003597837695942e-07}, {"x": 0.8383005707038681, "y": 0.9359543436905516, "ox": 0.8383005707038681, "oy": 0.9359543436905516, "term": "our approach", "cat25k": 17, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 125, "ncat": 19, "s": 0.9965123652504756, "os": 0.29203915056005086, "bg": 0.0}, {"x": 0.24223208623969564, "y": 0.3363982244768548, "ox": 0.24223208623969564, "oy": 0.3363982244768548, "term": "onto", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 1.0509589343426541e-06}, {"x": 0.5735573874445149, "y": 0.755865567533291, "ox": 0.5735573874445149, "oy": 0.755865567533291, "term": "people", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 7, "s": 0.839568801521877, "os": 0.06858414897000106, "bg": 1.7064338626949917e-07}, {"x": 0.34717818642993026, "y": 0.336715282181357, "ox": 0.34717818642993026, "oy": 0.336715282181357, "term": "own", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 1.2917525646240126e-07}, {"x": 0.06404565630944832, "y": 0.42073557387444516, "ox": 0.06404565630944832, "oy": 0.42073557387444516, "term": "physical", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 4.353169670322645e-07}, {"x": 0.6651870640456563, "y": 0.6182625237793279, "ox": 0.6651870640456563, "oy": 0.6182625237793279, "term": "recurrent", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 9, "s": 0.4150285351934052, "os": 0.015794494894173353, "bg": 2.4537017157509245e-05}, {"x": 0.75142675967026, "y": 0.8243500317057705, "ox": 0.75142675967026, "oy": 0.8243500317057705, "term": "combination", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 12, "s": 0.8738110336081166, "os": 0.08291226458429032, "bg": 4.143493389811255e-06}, {"x": 0.8836398224476855, "y": 0.9302473050095117, "ox": 0.8836398224476855, "oy": 0.9302473050095117, "term": "selection", "cat25k": 16, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 118, "ncat": 26, "s": 0.9870006341154091, "os": 0.2310695735133034, "bg": 3.818223753559748e-06}, {"x": 0.7996195307545974, "y": 0.8738110336081166, "ox": 0.7996195307545974, "oy": 0.8738110336081166, "term": "policy", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 15, "s": 0.9273937856689918, "os": 0.12752199568919825, "bg": 4.21180894981895e-07}, {"x": 0.4397590361445783, "y": 0.45529486366518707, "ox": 0.4397590361445783, "oy": 0.45529486366518707, "term": "simulated", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.4032974001268231, "os": 0.015264478286986322, "bg": 8.3986419837946e-06}, {"x": 0.8766645529486367, "y": 0.8566899175649968, "ox": 0.8766645529486367, "oy": 0.8566899175649968, "term": "environment", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 25, "s": 0.7688649334178821, "os": 0.05006890215893431, "bg": 1.6439922943732604e-06}, {"x": 0.14140773620798985, "y": 0.48636651870640457, "ox": 0.14140773620798985, "oy": 0.48636651870640457, "term": "realistic", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6540900443880786, "os": 0.0349634288541041, "bg": 3.7339876451134674e-06}, {"x": 0.8839568801521877, "y": 0.858275206087508, "ox": 0.8839568801521877, "oy": 0.858275206087508, "term": "scenarios", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 26, "s": 0.7545973367152823, "os": 0.047789830748030115, "bg": 2.6136091845873645e-05}, {"x": 0.44007609384908053, "y": 0.03804692454026633, "ox": 0.44007609384908053, "oy": 0.03804692454026633, "term": "involve", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 1.6383841960149746e-06}, {"x": 0.24254914394419785, "y": 0.0979708306911858, "ox": 0.24254914394419785, "oy": 0.0979708306911858, "term": "participants", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 6.49589922002114e-07}, {"x": 0.6236525047558655, "y": 0.2767913760304375, "ox": 0.6236525047558655, "oy": 0.2767913760304375, "term": "recurrent neural", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.09701965757767915, "os": -0.0140807745309353, "bg": 0.0}, {"x": 0.6239695624603678, "y": 0.00538998097653773, "ox": 0.6239695624603678, "oy": 0.00538998097653773, "term": "network architecture", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.024413443246670895, "os": -0.04301968128334688, "bg": 0.0}, {"x": 0.62428662016487, "y": 0.8516169942929613, "ox": 0.62428662016487, "oy": 0.8516169942929613, "term": "parameter", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 8, "s": 0.9372225745085606, "os": 0.13704462739832515, "bg": 5.769627217728201e-06}, {"x": 0.24286620164870007, "y": 0.056753329105897275, "ox": 0.24286620164870007, "oy": 0.056753329105897275, "term": "configuration", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 4.7592954592831173e-07}, {"x": 0.34749524413443245, "y": 0.15155358275206088, "ox": 0.34749524413443245, "oy": 0.15155358275206088, "term": "experimental evaluation", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.932149651236525, "y": 0.8839568801521877, "ox": 0.932149651236525, "oy": 0.8839568801521877, "term": "best", "cat25k": 10, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 42, "s": 0.15852885225110971, "os": -0.004752482244443662, "bg": 6.073901487278899e-07}, {"x": 0.4403931515535828, "y": 0.1518706404565631, "ox": 0.4403931515535828, "oy": 0.1518706404565631, "term": "best results", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 0.0}, {"x": 0.7517438173747623, "y": 0.7821813570069752, "ox": 0.7517438173747623, "oy": 0.7821813570069752, "term": "indicate", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 12, "s": 0.7897907419150286, "os": 0.05397335783187873, "bg": 4.087750263312434e-06}, {"x": 0.8769816106531388, "y": 0.7358909321496513, "ox": 0.8769816106531388, "oy": 0.7358909321496513, "term": "step", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 25, "s": 0.027266962587190868, "os": -0.03996325218190169, "bg": 1.296623228437865e-06}, {"x": 0.6655041217501585, "y": 0.42105263157894735, "ox": 0.6655041217501585, "oy": 0.42105263157894735, "term": "forward", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 9, "s": 0.11794546607482562, "os": -0.009928977774636936, "bg": 7.506816359864219e-07}, {"x": 0.852568167406468, "y": 0.48668357641090676, "ox": 0.852568167406468, "oy": 0.48668357641090676, "term": "autonomous", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 21, "s": 0.009511731135066582, "os": -0.0694321755415003, "bg": 1.766089318986147e-05}, {"x": 0.7520608750792644, "y": 0.8538363982244769, "ox": 0.7520608750792644, "oy": 0.8538363982244769, "term": "behavior", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 12, "s": 0.9181991122384274, "os": 0.11828203950390445, "bg": 4.3264393685215625e-06}, {"x": 0.6658211794546608, "y": 0.725428027901078, "ox": 0.6658211794546608, "oy": 0.725428027901078, "term": "indicate that", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 9, "s": 0.7339885859226379, "os": 0.04473340164658493, "bg": 0.0}, {"x": 0.24318325935320229, "y": 0.4556119213696893, "ox": 0.24318325935320229, "oy": 0.4556119213696893, "term": "published", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 3.9927411496165715e-07}, {"x": 0.14172479391249207, "y": 0.2140139505389981, "ox": 0.14172479391249207, "oy": 0.2140139505389981, "term": "published by", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 0.0}, {"x": 0.44071020925808496, "y": 0.7498414711477489, "ox": 0.44071020925808496, "oy": 0.7498414711477489, "term": "dimensionality", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 5, "s": 0.8589093214965123, "os": 0.07635772587541076, "bg": 0.00013305764722566053}, {"x": 0.3478123018389347, "y": 0.6636017755231453, "ox": 0.3478123018389347, "oy": 0.6636017755231453, "term": "dimensionality reduction", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 4, "s": 0.785986049461002, "os": 0.052913324617504684, "bg": 0.0}, {"x": 0.666138237159163, "y": 0.4559289790741915, "ox": 0.666138237159163, "oy": 0.4559289790741915, "term": "advances", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 9, "s": 0.13823715916296767, "os": -0.006713543691035655, "bg": 4.9469408332971275e-06}, {"x": 0.7726696258719087, "y": 0.6347495244134432, "ox": 0.7726696258719087, "oy": 0.6347495244134432, "term": "allow", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 13, "s": 0.17279644895370957, "os": -0.002968093000247349, "bg": 1.040374187519778e-06}, {"x": 0.5738744451490171, "y": 0.7656943563728599, "ox": 0.5738744451490171, "oy": 0.7656943563728599, "term": "describe", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 7, "s": 0.8465440710209258, "os": 0.07179958305360234, "bg": 3.3806246500902566e-06}, {"x": 0.7729866835764109, "y": 0.7561826252377932, "ox": 0.7729866835764109, "oy": 0.7561826252377932, "term": "signals", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 13, "s": 0.6601141407736208, "os": 0.03561711600296809, "bg": 7.235641600505448e-06}, {"x": 0.4410272669625872, "y": 0.05707038681039949, "ox": 0.4410272669625872, "oy": 0.05707038681039949, "term": "recent advances", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 0.0}, {"x": 0.5741915028535194, "y": 0.27710843373493976, "ox": 0.5741915028535194, "oy": 0.27710843373493976, "term": "advances in", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 0.0}, {"x": 0.14204185161699429, "y": 0.598287888395688, "ox": 0.14204185161699429, "oy": 0.598287888395688, "term": "high dimensional", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 0.0}, {"x": 0.5155358275206088, "y": 0.057387444514901714, "ox": 0.5155358275206088, "oy": 0.057387444514901714, "term": "dr", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.09099556119213698, "os": -0.01595349987632946, "bg": 5.15276664923314e-07}, {"x": 0.8941027266962587, "y": 0.8335447051363348, "ox": 0.8941027266962587, "oy": 0.8335447051363348, "term": "e.g.", "cat25k": 7, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 28, "s": 0.261572606214331, "os": 0.004646478923006264, "bg": 0.0}, {"x": 0.1423589093214965, "y": 0.09828788839568801, "ox": 0.1423589093214965, "oy": 0.09828788839568801, "term": "pca", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.1581222977146388e-05}, {"x": 0.14267596702599875, "y": 0.1521876981610653, "ox": 0.14267596702599875, "oy": 0.1521876981610653, "term": "t", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 5.14563550669626e-08}, {"x": 0.2435003170577045, "y": 0.15250475586556753, "ox": 0.2435003170577045, "oy": 0.15250475586556753, "term": "projections", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 4.336063016398004e-06}, {"x": 0.5745085605580216, "y": 0.45624603677869374, "ox": 0.5745085605580216, "oy": 0.45624603677869374, "term": "account", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 7, "s": 0.25935320228281544, "os": 0.00427546729797533, "bg": 2.0604167972006116e-07}, {"x": 0.4413443246670894, "y": 0.38173747622067217, "ox": 0.4413443246670894, "oy": 0.38173747622067217, "term": "humans", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 2.62347035281741e-06}, {"x": 0.3481293595434369, "y": 0.15282181357006974, "ox": 0.3481293595434369, "oy": 0.15282181357006974, "term": "exploration", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 2.1764542114842422e-06}, {"x": 0.3484464172479391, "y": 0.05770450221940393, "ox": 0.3484464172479391, "oy": 0.05770450221940393, "term": "these techniques", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.34876347495244137, "y": 0.15313887127457196, "ox": 0.34876347495244137, "oy": 0.15313887127457196, "term": "into account", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.6246036778693722, "y": 0.1534559289790742, "ox": 0.6246036778693722, "oy": 0.1534559289790742, "term": "interactive", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 8, "s": 0.07133798351299936, "os": -0.020511642698137874, "bg": 8.770907273337897e-07}, {"x": 0.8772986683576411, "y": 0.9251743817374762, "ox": 0.8772986683576411, "oy": 0.9251743817374762, "term": "similarity", "cat25k": 15, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 109, "ncat": 25, "s": 0.9787571337983513, "os": 0.21084060633899865, "bg": 5.206838755277996e-05}, {"x": 0.7733037412809132, "y": 0.38205453392517436, "ox": 0.7733037412809132, "oy": 0.38205453392517436, "term": "embeddings", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 13, "s": 0.03233988585922638, "os": -0.03512243383626021, "bg": 0.00014488470075513907}, {"x": 0.515852885225111, "y": 0.15377298668357642, "ox": 0.515852885225111, "oy": 0.15377298668357642, "term": "interactions", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 6, "s": 0.11953075459733671, "os": -0.00952263170912689, "bg": 2.2835103719895482e-06}, {"x": 0.44166138237159164, "y": 0.025047558655675334, "ox": 0.44166138237159164, "oy": 0.025047558655675334, "term": "trainable", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 5.959692612298819e-05}, {"x": 0.7736207989854154, "y": 0.6639188332276474, "ox": 0.7736207989854154, "oy": 0.6639188332276474, "term": "methodology", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 13, "s": 0.24571972098922004, "os": 0.0034627751669552287, "bg": 6.5363397080605705e-06}, {"x": 0.7016487000634115, "y": 0.03836398224476855, "ox": 0.7016487000634115, "oy": 0.03836398224476855, "term": "project", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.02663284717818643, "os": -0.04114695593795272, "bg": 1.2739205602926832e-07}, {"x": 0.6664552948636652, "y": 0.7076727964489538, "ox": 0.6664552948636652, "oy": 0.7076727964489538, "term": "clusters", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 9, "s": 0.684844641724794, "os": 0.03830253347938235, "bg": 1.2034340152250664e-05}, {"x": 0.6249207355738744, "y": 0.6642358909321496, "ox": 0.6249207355738744, "oy": 0.6642358909321496, "term": "unseen", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 8, "s": 0.6144578313253012, "os": 0.030935302639482708, "bg": 3.28206643005019e-05}, {"x": 0.24381737476220672, "y": 0.21433100824350032, "ox": 0.24381737476220672, "oy": 0.21433100824350032, "term": "manipulation", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 4.824142905585292e-06}, {"x": 0.24413443246670893, "y": 0.3823715916296766, "ox": 0.24413443246670893, "oy": 0.3823715916296766, "term": "kind", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 4.246211597244622e-07}, {"x": 0.24445149017121115, "y": 0.38268864933417884, "ox": 0.24445149017121115, "oy": 0.38268864933417884, "term": "kind of", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 0.0}, {"x": 0.5748256182625238, "y": 0.487000634115409, "ox": 0.5748256182625238, "oy": 0.487000634115409, "term": "semi -", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.2932783766645529, "os": 0.007490901381576619, "bg": 0.0}, {"x": 0.575142675967026, "y": 0.676284083703234, "ox": 0.575142675967026, "oy": 0.676284083703234, "term": "report", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 7, "s": 0.6968928344958782, "os": 0.03964524221758948, "bg": 2.2341041967905657e-07}, {"x": 0.7523779327837666, "y": 0.6350665821179454, "ox": 0.7523779327837666, "oy": 0.6350665821179454, "term": "competitive", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 12, "s": 0.23906150919467342, "os": 0.002526412494258154, "bg": 2.657989529553834e-06}, {"x": 0.4419784400760938, "y": 0.5348763474952442, "ox": 0.4419784400760938, "oy": 0.5348763474952442, "term": "baselines", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 5, "s": 0.5326569435637286, "os": 0.02491078053779018, "bg": 8.024496599619566e-05}, {"x": 0.6667723525681674, "y": 0.7435003170577045, "ox": 0.6667723525681674, "oy": 0.7435003170577045, "term": "wide", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 9, "s": 0.7758402029169309, "os": 0.051164269813787494, "bg": 9.329589573833222e-07}, {"x": 0.7526949904882688, "y": 0.5783132530120482, "ox": 0.7526949904882688, "oy": 0.5783132530120482, "term": "metrics", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 12, "s": 0.13792010145846542, "os": -0.007119889756545705, "bg": 1.507010637793639e-05}, {"x": 0.44229549778059607, "y": 0.5786303107165505, "ox": 0.44229549778059607, "oy": 0.5786303107165505, "term": "we report", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.61572606214331, "os": 0.03134164870499275, "bg": 0.0}, {"x": 0.2447685478757134, "y": 0.38300570703868103, "ox": 0.2447685478757134, "oy": 0.38300570703868103, "term": "wide range", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 0.0}, {"x": 0.14299302473050096, "y": 0.21464806594800254, "ox": 0.14299302473050096, "oy": 0.21464806594800254, "term": "neighbor", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 3.676453999956885e-06}, {"x": 0.7019657577679138, "y": 0.8110336081166772, "ox": 0.7019657577679138, "oy": 0.8110336081166772, "term": "improving", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 10, "s": 0.87571337983513, "os": 0.08425497332249743, "bg": 5.971173166343946e-06}, {"x": 0.7260621433100825, "y": 0.4213696892834496, "ox": 0.7260621433100825, "oy": 0.4213696892834496, "term": "precision", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 11, "s": 0.0700697526949905, "os": -0.020917988763647928, "bg": 3.5045858966702055e-06}, {"x": 0.8230818008877616, "y": 0.33703233988585923, "ox": 0.8230818008877616, "oy": 0.33703233988585923, "term": "documents", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 17, "s": 0.012048192771084338, "os": -0.06031588989788346, "bg": 8.659584065023084e-07}, {"x": 0.14331008243500318, "y": 0.09860494610019023, "ox": 0.14331008243500318, "oy": 0.09860494610019023, "term": "emerging", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.3105262927977844e-06}, {"x": 0.9343690551680406, "y": 0.8852251109701966, "ox": 0.9343690551680406, "oy": 0.8852251109701966, "term": "semantic", "cat25k": 10, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 45, "s": 0.08180088776157261, "os": -0.018020564644358833, "bg": 4.851736932188683e-05}, {"x": 0.7263792010145846, "y": 0.7958148383005708, "ox": 0.7263792010145846, "oy": 0.7958148383005708, "term": "example", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 11, "s": 0.8322764743183261, "os": 0.06589873149358678, "bg": 7.692296960219172e-07}, {"x": 0.5161699429296132, "y": 0.5564362714013951, "ox": 0.5161699429296132, "oy": 0.5564362714013951, "term": "head", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 4.2273195280232974e-07}, {"x": 0.8712745719720989, "y": 0.8890298034242232, "ox": 0.8712745719720989, "oy": 0.8890298034242232, "term": "pose", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 24, "s": 0.9039315155358275, "os": 0.1037949189074591, "bg": 3.5809649787106157e-05}, {"x": 0.34908053265694355, "y": 0.5986049461001902, "ox": 0.34908053265694355, "oy": 0.5986049461001902, "term": "just", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 4, "s": 0.6987951807228916, "os": 0.04005158828309954, "bg": 1.0365668947635718e-07}, {"x": 0.5164870006341155, "y": 0.5091946734305643, "ox": 0.5164870006341155, "oy": 0.5091946734305643, "term": "far", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 6, "s": 0.4166138237159163, "os": 0.016200840959683403, "bg": 4.716338601726514e-07}, {"x": 0.5754597336715283, "y": 0.013950538998097653, "ox": 0.5754597336715283, "oy": 0.013950538998097653, "term": "right", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 7.303234876598154e-08}, {"x": 0.7999365884590995, "y": 0.8405199746353836, "ox": 0.7999365884590995, "oy": 0.8405199746353836, "term": "up", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 15, "s": 0.87856689917565, "os": 0.08572135260238155, "bg": 1.6381588403234593e-07}, {"x": 0.4426125554850983, "y": 0.09892200380469246, "ox": 0.4426125554850983, "oy": 0.09892200380469246, "term": "down", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 1.0659712365072693e-07}, {"x": 0.5168040583386176, "y": 0.6353836398224477, "ox": 0.5168040583386176, "oy": 0.6353836398224477, "term": "for example", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 6, "s": 0.6585288522511097, "os": 0.035493445461291115, "bg": 0.0}, {"x": 0.3493975903614458, "y": 0.09923906150919468, "ox": 0.3493975903614458, "oy": 0.09923906150919468, "term": "head pose", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.7739378566899175, "y": 0.8601775523145212, "ox": 0.7739378566899175, "oy": 0.8601775523145212, "term": "manifold", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 13, "s": 0.9223208623969562, "os": 0.12243383626020282, "bg": 5.276384507251642e-05}, {"x": 0.5171211160431198, "y": 0.5567533291058973, "ox": 0.5171211160431198, "oy": 0.5567533291058973, "term": "gaussian process", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 0.0}, {"x": 0.8123018389346861, "y": 0.7079898541534559, "ox": 0.8123018389346861, "oy": 0.7079898541534559, "term": "aim", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 16, "s": 0.19530754597336716, "os": -0.0001590049821561107, "bg": 3.0155403817523347e-06}, {"x": 0.6252377932783767, "y": 0.796131896005073, "ox": 0.6252377932783767, "oy": 0.796131896005073, "term": "predictive", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 8, "s": 0.8715916296766011, "os": 0.08238224797710328, "bg": 3.470907826788684e-05}, {"x": 0.4429296131896005, "y": 0.21496512365250475, "ox": 0.4429296131896005, "oy": 0.21496512365250475, "term": "outputs", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 4.4628448244946425e-06}, {"x": 0.7742549143944197, "y": 0.88427393785669, "ox": 0.7742549143944197, "oy": 0.88427393785669, "term": "obtain", "cat25k": 10, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 13, "s": 0.9492707672796449, "os": 0.15458817709621567, "bg": 5.436362234911813e-06}, {"x": 0.8776157260621433, "y": 0.862714013950539, "ox": 0.8776157260621433, "oy": 0.862714013950539, "term": "since", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 25, "s": 0.8110336081166774, "os": 0.05971520440973818, "bg": 8.110606484816769e-07}, {"x": 0.2450856055802156, "y": 0.5789473684210527, "ox": 0.2450856055802156, "oy": 0.5789473684210527, "term": "observations", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.715282181357007, "os": 0.042330659694003736, "bg": 2.866565668002964e-06}, {"x": 0.5757767913760304, "y": 0.5095117311350665, "ox": 0.5757767913760304, "oy": 0.5095117311350665, "term": "becomes", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 1.6610466717282853e-06}, {"x": 0.44324667089410275, "y": 0.3833227647431833, "ox": 0.44324667089410275, "oy": 0.3833227647431833, "term": "inputs", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 4.259833826388217e-06}, {"x": 0.8991756499682942, "y": 0.9131261889663919, "ox": 0.8991756499682942, "oy": 0.9131261889663919, "term": "amount", "cat25k": 13, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 94, "ncat": 29, "s": 0.9394419784400762, "os": 0.14063107310695735, "bg": 2.6660688892259646e-06}, {"x": 0.349714648065948, "y": 0.3373493975903614, "ox": 0.349714648065948, "oy": 0.3373493975903614, "term": "small amount", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 0.0}, {"x": 0.894419784400761, "y": 0.913443246670894, "ox": 0.894419784400761, "oy": 0.913443246670894, "term": "amount of", "cat25k": 13, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 94, "ncat": 28, "s": 0.9435637285986049, "os": 0.14612557860146286, "bg": 0.0}, {"x": 0.517438173747622, "y": 0.7564996829422955, "ox": 0.517438173747622, "oy": 0.7564996829422955, "term": "processes", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 6, "s": 0.8532022828154724, "os": 0.07407865446450655, "bg": 2.0160598318172645e-06}, {"x": 0.6670894102726697, "y": 0.5351934051997463, "ox": 0.6670894102726697, "oy": 0.5351934051997463, "term": "predictions", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 9, "s": 0.24128091312618896, "os": 0.0029327585597682046, "bg": 8.794264110354486e-06}, {"x": 0.44356372859860493, "y": 0.6645529486366518, "ox": 0.44356372859860493, "oy": 0.6645529486366518, "term": "distributed", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 5, "s": 0.7520608750792644, "os": 0.04741881912299919, "bg": 2.0476020884976446e-06}, {"x": 0.1436271401395054, "y": 0.09955611921369689, "ox": 0.1436271401395054, "oy": 0.09955611921369689, "term": "experts", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 5.597934685537262e-07}, {"x": 0.35003170577045023, "y": 0.27742549143944195, "ox": 0.35003170577045023, "oy": 0.27742549143944195, "term": "principled", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 3.7723393219220075e-05}, {"x": 0.787571337983513, "y": 0.7257450856055803, "ox": 0.787571337983513, "oy": 0.7257450856055803, "term": "manner", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 14, "s": 0.4305643627140139, "os": 0.01726087417405746, "bg": 3.0877420500291404e-06}, {"x": 0.1439441978440076, "y": 0.45656309448319593, "ox": 0.1439441978440076, "oy": 0.45656309448319593, "term": "gaussian processes", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 2, "s": 0.6195307545973368, "os": 0.0317479947705028, "bg": 0.0}, {"x": 0.3503487634749524, "y": 0.15409004438807863, "ox": 0.3503487634749524, "oy": 0.15409004438807863, "term": "schemes", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 2.208993623003576e-06}, {"x": 0.7266962587190868, "y": 0.7501585288522511, "ox": 0.7266962587190868, "oy": 0.7501585288522511, "term": "inference", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 11, "s": 0.7257450856055803, "os": 0.0433906929083778, "bg": 2.958598780721098e-05}, {"x": 0.859860494610019, "y": 0.8975903614457831, "ox": 0.859860494610019, "oy": 0.8975903614457831, "term": "during", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 22, "s": 0.9340519974635384, "os": 0.13407653439807782, "bg": 9.874369032054182e-07}, {"x": 0.35066582117945466, "y": 0.4568801521876982, "ox": 0.35066582117945466, "oy": 0.4568801521876982, "term": "during training", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 0.0}, {"x": 0.5177552314521243, "y": 0.7362079898541535, "ox": 0.5177552314521243, "oy": 0.7362079898541535, "term": "across different", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 6, "s": 0.8284717818642994, "os": 0.06443235221370269, "bg": 0.0}, {"x": 0.3509828788839569, "y": 0.09987317691819911, "ox": 0.3509828788839569, "oy": 0.09987317691819911, "term": "predictor", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 1.3952308472856418e-05}, {"x": 0.7530120481927711, "y": 0.8113506658211794, "ox": 0.7530120481927711, "oy": 0.8113506658211794, "term": "cases", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 12, "s": 0.8516169942929613, "os": 0.07326596233348645, "bg": 1.2949981356073715e-06}, {"x": 0.24540266328471783, "y": 0.10019023462270134, "ox": 0.24540266328471783, "oy": 0.10019023462270134, "term": "humanoid", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 3.172518376812698e-05}, {"x": 0.7533291058972733, "y": 0.10050729232720355, "ox": 0.7533291058972733, "oy": 0.10050729232720355, "term": "robotics", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.02155992390615092, "os": -0.045705098759761136, "bg": 9.413144911897919e-06}, {"x": 0.8455928979074192, "y": 0.807863031071655, "ox": 0.8455928979074192, "oy": 0.807863031071655, "term": "object recognition", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 20, "s": 0.5459733671528219, "os": 0.026094484293841202, "bg": 0.0}, {"x": 0.24571972098922004, "y": 0.6766011414077362, "ox": 0.24571972098922004, "oy": 0.6766011414077362, "term": "collection", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 3, "s": 0.8186429930247305, "os": 0.061623264195611455, "bg": 5.216445459105186e-07}, {"x": 0.5760938490805326, "y": 0.5355104629042485, "ox": 0.5760938490805326, "oy": 0.5355104629042485, "term": "benefits", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 7, "s": 0.3937856689917565, "os": 0.01392176954877919, "bg": 6.249035223819677e-07}, {"x": 0.5764109067850349, "y": 0.6185795814838301, "ox": 0.5764109067850349, "oy": 0.6185795814838301, "term": "avoid", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 7, "s": 0.5672162333544705, "os": 0.026783505883184337, "bg": 1.5242610783771507e-06}, {"x": 0.5180722891566265, "y": 0.03868103994927077, "ox": 0.5180722891566265, "oy": 0.03868103994927077, "term": "currently", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 2.4634159496978526e-07}, {"x": 0.702282815472416, "y": 0.8845909955611921, "ox": 0.702282815472416, "oy": 0.8845909955611921, "term": "consider", "cat25k": 10, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 10, "s": 0.9616360177552314, "os": 0.17107169357973215, "bg": 2.983804461936988e-06}, {"x": 0.4438807863031072, "y": 0.38363982244768546, "ox": 0.4438807863031072, "oy": 0.38363982244768546, "term": "acquisition", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 1.752133957443449e-06}, {"x": 0.3512999365884591, "y": 0.8173747622067217, "ox": 0.3512999365884591, "oy": 0.8173747622067217, "term": "we consider", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 4, "s": 0.920418516169943, "os": 0.12043744037313169, "bg": 0.0}, {"x": 0.24603677869372226, "y": 0.10082435003170577, "ox": 0.24603677869372226, "oy": 0.10082435003170577, "term": "analyzing", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 4.366475684297695e-06}, {"x": 0.6255548509828789, "y": 0.45719720989220036, "ox": 0.6255548509828789, "oy": 0.45719720989220036, "term": "off", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 1.54784278083858e-07}, {"x": 0.44419784400760937, "y": 0.05802155992390615, "ox": 0.44419784400760937, "oy": 0.05802155992390615, "term": "shelf", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 1.91006775878553e-06}, {"x": 0.4445149017121116, "y": 0.4575142675967026, "ox": 0.4445149017121116, "oy": 0.4575142675967026, "term": "line", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.4032974001268231, "os": 0.015264478286986322, "bg": 1.3559781784287292e-07}, {"x": 0.35161699429296134, "y": 0.3839568801521877, "ox": 0.35161699429296134, "oy": 0.3839568801521877, "term": "image retrieval", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 0.0}, {"x": 0.6674064679771718, "y": 0.48731769181991125, "ox": 0.6674064679771718, "oy": 0.48731769181991125, "term": "ways", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 9, "s": 0.16613823715916298, "os": -0.003498109607434366, "bg": 8.226873730908318e-07}, {"x": 0.7025998731769182, "y": 0.5098287888395688, "ox": 0.7025998731769182, "oy": 0.5098287888395688, "term": "last", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 10, "s": 0.14552948636651872, "os": -0.005777181018338573, "bg": 1.244517767602026e-07}, {"x": 0.5767279644895371, "y": 0.3842739378566899, "ox": 0.5767279644895371, "oy": 0.3842739378566899, "term": "robotic", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 7, "s": 0.17438173747622068, "os": -0.0021554008692272406, "bg": 1.7658435667065998e-05}, {"x": 0.14426125554850983, "y": 0.15440710209258085, "ox": 0.14426125554850983, "oy": 0.15440710209258085, "term": "studying", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 2.0010915954653263e-06}, {"x": 0.9115409004438808, "y": 0.8541534559289791, "ox": 0.9115409004438808, "oy": 0.8541534559289791, "term": "identification", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 32, "s": 0.30342422320862394, "os": 0.008391929613794563, "bg": 7.465524518835539e-06}, {"x": 0.24635383639822447, "y": 0.15472415979708307, "ox": 0.24635383639822447, "oy": 0.15472415979708307, "term": "highlight", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 2.0856285122102646e-06}, {"x": 0.7878883956880152, "y": 0.8116677235256817, "ox": 0.7878883956880152, "oy": 0.8116677235256817, "term": "differences", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 14, "s": 0.8205453392517439, "os": 0.06227695134447546, "bg": 4.5974179790907845e-06}, {"x": 0.8126188966391883, "y": 0.8034242232086239, "ox": 0.8126188966391883, "oy": 0.8034242232086239, "term": "considered", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 16, "s": 0.7349397590361446, "os": 0.044857072188261904, "bg": 2.0135659492666787e-06}, {"x": 0.6258719086873811, "y": 0.510145846544071, "ox": 0.6258719086873811, "oy": 0.510145846544071, "term": "differences between", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 8, "s": 0.26981610653138866, "os": 0.005211829970672419, "bg": 0.0}, {"x": 0.8890298034242232, "y": 0.953709575142676, "ox": 0.8890298034242232, "oy": 0.953709575142676, "term": "specific", "cat25k": 26, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 188, "ncat": 27, "s": 1.0, "os": 0.3895622062824635, "bg": 3.9808594353349005e-06}, {"x": 0.8002536461636017, "y": 0.6886493341788206, "ox": 0.8002536461636017, "oy": 0.6886493341788206, "term": "open", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 15, "s": 0.189283449587825, "os": -0.0010953676548531854, "bg": 3.4180730043203526e-07}, {"x": 0.4448319594166138, "y": 0.025364616360177554, "ox": 0.4448319594166138, "oy": 0.025364616360177554, "term": "results confirm", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 0.0}, {"x": 0.7029169308814204, "y": 0.7260621433100825, "ox": 0.7029169308814204, "oy": 0.7260621433100825, "term": "rules", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 10, "s": 0.6908687381103361, "os": 0.039238896152079435, "bg": 8.172474934840601e-07}, {"x": 0.8459099556119214, "y": 0.8436905516804059, "ox": 0.8459099556119214, "oy": 0.8436905516804059, "term": "leverage", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 20, "s": 0.8180088776157262, "os": 0.06146425921345536, "bg": 3.46256013273771e-05}, {"x": 0.7882054533925175, "y": 0.8544705136334813, "ox": 0.7882054533925175, "oy": 0.8544705136334813, "term": "previously", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 14, "s": 0.9058338617628409, "os": 0.10729302851489346, "bg": 5.1558225635122165e-06}, {"x": 0.6261889663918834, "y": 0.7755231452124287, "ox": 0.6261889663918834, "oy": 0.7755231452124287, "term": "acquired", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 8, "s": 0.8503487634749525, "os": 0.07273594572629943, "bg": 7.003580385928406e-06}, {"x": 0.5183893468611287, "y": 0.014267596702599873, "ox": 0.5183893468611287, "oy": 0.014267596702599873, "term": "fuzzy rules", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 0.0}, {"x": 0.14457831325301204, "y": 0.10114140773620799, "ox": 0.14457831325301204, "oy": 0.10114140773620799, "term": "its ability", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.06436271401395054, "y": 0.21528218135700697, "ox": 0.06436271401395054, "oy": 0.21528218135700697, "term": "acquired knowledge", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.774571972098922, "y": 0.7758402029169309, "ox": 0.774571972098922, "oy": 0.7758402029169309, "term": "deal", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 13, "s": 0.7362079898541535, "os": 0.04526341825377195, "bg": 1.5610566080337967e-06}, {"x": 0.14489537095751426, "y": 0.5792644261255548, "ox": 0.14489537095751426, "oy": 0.5792644261255548, "term": "uncertainty", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7549143944197845, "os": 0.04782516518850924, "bg": 5.656913071024968e-06}, {"x": 0.06467977171845275, "y": 0.48763474952441344, "ox": 0.06467977171845275, "oy": 0.48763474952441344, "term": "fuzzy system", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 0.0}, {"x": 0.7748890298034242, "y": 0.7761572606214331, "ox": 0.7748890298034242, "oy": 0.7761572606214331, "term": "deal with", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 13, "s": 0.7362079898541535, "os": 0.04526341825377195, "bg": 0.0}, {"x": 0.7032339885859227, "y": 0.7568167406467977, "ox": 0.7032339885859227, "oy": 0.7568167406467977, "term": "yet", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 10, "s": 0.7812301838934687, "os": 0.052100632486484576, "bg": 7.131774238337655e-07}, {"x": 0.6265060240963856, "y": 0.15504121750158528, "ox": 0.6265060240963856, "oy": 0.15504121750158528, "term": "selecting", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 8, "s": 0.07133798351299936, "os": -0.020511642698137874, "bg": 2.988351220171819e-06}, {"x": 0.914711477488903, "y": 0.9210526315789473, "ox": 0.914711477488903, "oy": 0.9210526315789473, "term": "labeled data", "cat25k": 14, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 104, "ncat": 33, "s": 0.946417247939125, "os": 0.15080739196494825, "bg": 0.0}, {"x": 0.6268230818008877, "y": 0.02568167406467977, "ox": 0.6268230818008877, "oy": 0.02568167406467977, "term": "proposes an", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.03582752060875079, "os": -0.03337337903254302, "bg": 0.0}, {"x": 0.3519340519974635, "y": 0.1553582752060875, "ox": 0.3519340519974635, "oy": 0.1553582752060875, "term": "that combines", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.7885225110970197, "y": 0.6188966391883323, "ox": 0.7885225110970197, "oy": 0.6188966391883323, "term": "structures", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 14, "s": 0.11065313887127458, "os": -0.01167803257835412, "bg": 2.7241585541224927e-06}, {"x": 0.5770450221940393, "y": 0.21559923906150918, "ox": 0.5770450221940393, "oy": 0.21559923906150918, "term": "providing", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 7, "s": 0.11001902346227013, "os": -0.0118017031200311, "bg": 5.499363895452417e-07}, {"x": 0.7888395688015218, "y": 0.8585922637920101, "ox": 0.7888395688015218, "oy": 0.8585922637920101, "term": "exploit", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 14, "s": 0.9143944197844007, "os": 0.11372389668209605, "bg": 2.9119722591356244e-05}, {"x": 0.7270133164235891, "y": 0.579581483830057, "ox": 0.7270133164235891, "oy": 0.579581483830057, "term": "query", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 11, "s": 0.1845275840202917, "os": -0.0016253842620402165, "bg": 1.9038417145998482e-06}, {"x": 0.44514901712111604, "y": 0.33766645529486367, "ox": 0.44514901712111604, "oy": 0.33766645529486367, "term": "correct", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 7.159021510309238e-07}, {"x": 0.35225110970196577, "y": 0.4578313253012048, "ox": 0.35225110970196577, "oy": 0.4578313253012048, "term": "employing", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 1.0465484396398827e-05}, {"x": 0.35256816740646796, "y": 0.2159162967660114, "ox": 0.35256816740646796, "oy": 0.2159162967660114, "term": "through experiments", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 0.0}, {"x": 0.06499682942295498, "y": 0.15567533291058971, "ox": 0.06499682942295498, "oy": 0.15567533291058971, "term": "an active", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.7273303741280913, "y": 0.8316423589093215, "ox": 0.7273303741280913, "oy": 0.8316423589093215, "term": "additional", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 11, "s": 0.8912492073557388, "os": 0.09483763824599838, "bg": 9.062168013138695e-07}, {"x": 0.7276474318325935, "y": 0.5358275206087508, "ox": 0.7276474318325935, "oy": 0.5358275206087508, "term": "capabilities", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 11, "s": 0.13157894736842105, "os": -0.008056252429242787, "bg": 3.0761046978600523e-06}, {"x": 0.667723525681674, "y": 0.7263792010145846, "ox": 0.667723525681674, "oy": 0.7263792010145846, "term": "world datasets", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 9, "s": 0.7339885859226379, "os": 0.04473340164658493, "bg": 0.0}, {"x": 0.14521242866201647, "y": 0.2777425491439442, "ox": 0.14521242866201647, "oy": 0.2777425491439442, "term": "knowledge acquired", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.2466708941027267, "y": 0.27805960684844644, "ox": 0.2466708941027267, "oy": 0.27805960684844644, "term": "probability distribution", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 0.0}, {"x": 0.7035510462904249, "y": 0.3379835129993659, "ox": 0.7035510462904249, "oy": 0.3379835129993659, "term": "homogeneous", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 10, "s": 0.06626506024096385, "os": -0.021854351436345003, "bg": 1.9958989029684242e-05}, {"x": 0.14552948636651872, "y": 0.6192136968928345, "ox": 0.14552948636651872, "oy": 0.6192136968928345, "term": "identical", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 2, "s": 0.7907419150285353, "os": 0.054256033355711816, "bg": 4.852240316273242e-06}, {"x": 0.518706404565631, "y": 0.15599239061509196, "ox": 0.518706404565631, "oy": 0.15599239061509196, "term": "been widely", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 6, "s": 0.11953075459733671, "os": -0.00952263170912689, "bg": 0.0}, {"x": 0.2469879518072289, "y": 0.3383005707038681, "ox": 0.2469879518072289, "oy": 0.3383005707038681, "term": "regression problems", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.0}, {"x": 0.4454660748256183, "y": 0.5570703868103994, "ox": 0.4454660748256183, "oy": 0.5570703868103994, "term": "method called", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.5798985415345593, "os": 0.02812621462139147, "bg": 0.0}, {"x": 0.9074191502853519, "y": 0.9229549778059607, "ox": 0.9074191502853519, "oy": 0.9229549778059607, "term": "transferred", "cat25k": 15, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 107, "ncat": 31, "s": 0.9619530754597337, "os": 0.1714427052047631, "bg": 2.4522395255165295e-05}, {"x": 0.8528852251109702, "y": 0.9286620164870006, "ox": 0.8528852251109702, "oy": 0.9286620164870006, "term": "latent", "cat25k": 16, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 115, "ncat": 21, "s": 0.9904882688649335, "os": 0.24889579873502704, "bg": 0.0001303751593141308}, {"x": 0.24730500951173112, "y": 0.6487000634115409, "ox": 0.24730500951173112, "oy": 0.6487000634115409, "term": "minimize", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 3, "s": 0.7935954343690552, "os": 0.05519239602840889, "bg": 6.975576494507203e-06}, {"x": 0.5773620798985415, "y": 0.5989220038046924, "ox": 0.5773620798985415, "oy": 0.5989220038046924, "term": "gap", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 7, "s": 0.5180722891566265, "os": 0.023568071799583055, "bg": 3.085571467834917e-06}, {"x": 0.4457831325301205, "y": 0.45814838300570704, "ox": 0.4457831325301205, "oy": 0.45814838300570704, "term": "generated from", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.4032974001268231, "os": 0.015264478286986322, "bg": 0.0}, {"x": 0.6271401395053899, "y": 0.536144578313253, "ox": 0.6271401395053899, "oy": 0.536144578313253, "term": "known as", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 8, "s": 0.30374128091312613, "os": 0.0084272640542737, "bg": 0.0}, {"x": 0.14584654407102093, "y": 0.2783766645529486, "ox": 0.14584654407102093, "oy": 0.2783766645529486, "term": "latent feature", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.3528852251109702, "y": 0.6490171211160431, "ox": 0.3528852251109702, "oy": 0.6490171211160431, "term": "discuss", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 4, "s": 0.7663284717818644, "os": 0.0496978905339034, "bg": 1.2431163010347008e-06}, {"x": 0.5190234622701332, "y": 0.27869372225745087, "ox": 0.5190234622701332, "oy": 0.27869372225745087, "term": "affect", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 1.2988573829834014e-06}, {"x": 0.7536461636017755, "y": 0.7660114140773621, "ox": 0.7536461636017755, "oy": 0.7660114140773621, "term": "invariant", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 12, "s": 0.731135066582118, "os": 0.04432705558107487, "bg": 3.5233783651074594e-05}, {"x": 0.4461001902346227, "y": 0.27901077996195306, "ox": 0.4461001902346227, "oy": 0.27901077996195306, "term": "domain invariant", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 0.0}, {"x": 0.14616360177552315, "y": 0.4584654407102093, "ox": 0.14616360177552315, "oy": 0.4584654407102093, "term": "human action", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 2, "s": 0.6195307545973368, "os": 0.0317479947705028, "bg": 0.0}, {"x": 0.5776791376030438, "y": 0.8630310716550412, "ox": 0.5776791376030438, "oy": 0.8630310716550412, "term": "action recognition", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 7, "s": 0.9533925174381738, "os": 0.15861630331083706, "bg": 0.0}, {"x": 0.4464172479391249, "y": 0.33861762840837034, "ox": 0.4464172479391249, "oy": 0.33861762840837034, "term": "discover", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 1.2268320906610512e-06}, {"x": 0.6274571972098922, "y": 0.42168674698795183, "ox": 0.6274571972098922, "oy": 0.42168674698795183, "term": "correlations", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 8, "s": 0.15916296766011415, "os": -0.004434472280131441, "bg": 1.7326489644947228e-05}, {"x": 0.800570703868104, "y": 0.8589093214965123, "ox": 0.800570703868104, "oy": 0.8589093214965123, "term": "actions", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 15, "s": 0.9077362079898542, "os": 0.10822939118759055, "bg": 3.4491153053732875e-06}, {"x": 0.6680405833861763, "y": 0.7438173747622068, "ox": 0.6680405833861763, "oy": 0.7438173747622068, "term": "facilitate", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 9, "s": 0.7758402029169309, "os": 0.051164269813787494, "bg": 7.092259922677068e-06}, {"x": 0.3532022828154724, "y": 0.10145846544071022, "ox": 0.3532022828154724, "oy": 0.10145846544071022, "term": "among multiple", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.5193405199746354, "y": 0.45878249841471147, "ox": 0.5193405199746354, "oy": 0.45878249841471147, "term": "relatedness", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 8.766277882242589e-05}, {"x": 0.14648065948002537, "y": 0.10177552314521243, "ox": 0.14648065948002537, "oy": 0.10177552314521243, "term": "common space", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.35351934051997463, "y": 0.6648700063411541, "ox": 0.35351934051997463, "oy": 0.6648700063411541, "term": "additionally", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 4, "s": 0.785986049461002, "os": 0.052913324617504684, "bg": 6.244208078867916e-06}, {"x": 0.577996195307546, "y": 0.38459099556119214, "ox": 0.577996195307546, "oy": 0.38459099556119214, "term": "incorporated", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 7, "s": 0.17438173747622068, "os": -0.0021554008692272406, "bg": 2.266707888936801e-06}, {"x": 0.24762206721623337, "y": 0.05833861762840837, "ox": 0.24762206721623337, "oy": 0.05833861762840837, "term": "induced", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.406554277249508e-06}, {"x": 0.14679771718452758, "y": 0.10209258084971465, "ox": 0.14679771718452758, "oy": 0.10209258084971465, "term": "regularization term", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.6683576410906785, "y": 0.7663284717818643, "ox": 0.6683576410906785, "oy": 0.7663284717818643, "term": "motion", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 9, "s": 0.8148383005707039, "os": 0.06081057206459135, "bg": 1.9928434275004693e-06}, {"x": 0.3538363982244769, "y": 0.05865567533291059, "ox": 0.3538363982244769, "oy": 0.05865567533291059, "term": "&", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.5783132530120482, "y": 0.009194673430564362, "ox": 0.5783132530120482, "oy": 0.009194673430564362, "term": "v3", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.03329105897273303, "os": -0.0343097417052401, "bg": 0.0}, {"x": 0.5786303107165505, "y": 0.009511731135066582, "ox": 0.5786303107165505, "oy": 0.009511731135066582, "term": "inception v3", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.03329105897273303, "os": -0.0343097417052401, "bg": 0.0}, {"x": 0.8129359543436906, "y": 0.8363982244768547, "ox": 0.8129359543436906, "oy": 0.8363982244768547, "term": "shown", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 16, "s": 0.8522511097019657, "os": 0.07379597894067347, "bg": 1.942402602425208e-06}, {"x": 0.35415345592897907, "y": 0.5364616360177552, "ox": 0.35415345592897907, "oy": 0.5364616360177552, "term": "have shown", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.6049461001902346, "os": 0.030405286032295677, "bg": 0.0}, {"x": 0.44673430564362715, "y": 0.422003804692454, "ox": 0.44673430564362715, "oy": 0.422003804692454, "term": "shown that", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 0.0}, {"x": 0.6277742549143944, "y": 0.5573874445149017, "ox": 0.6277742549143944, "oy": 0.5573874445149017, "term": "weak", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 8, "s": 0.35954343690551677, "os": 0.01164269813787499, "bg": 3.829513025867624e-06}, {"x": 0.6280913126188966, "y": 0.7083069118579581, "ox": 0.6280913126188966, "oy": 0.7083069118579581, "term": "classical", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 8, "s": 0.7285986049461003, "os": 0.04379703897388785, "bg": 2.1059761724590904e-06}, {"x": 0.24793912492073558, "y": 0.8468611287254281, "ox": 0.24793912492073558, "oy": 0.8468611287254281, "term": "adaptive", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 3, "s": 0.9530754597336716, "os": 0.15808628670365005, "bg": 1.952186903720498e-05}, {"x": 0.9245402663284717, "y": 0.8795180722891566, "ox": 0.9245402663284717, "oy": 0.8795180722891566, "term": "ensemble", "cat25k": 10, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 69, "ncat": 37, "s": 0.41851616994292956, "os": 0.016289177060881233, "bg": 3.3800519220617424e-05}, {"x": 0.3544705136334813, "y": 0.21623335447051364, "ox": 0.3544705136334813, "oy": 0.21623335447051364, "term": "spam", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 8.356835325602875e-07}, {"x": 0.8233988585922638, "y": 0.9112238427393786, "ox": 0.8233988585922638, "oy": 0.9112238427393786, "term": "svm", "cat25k": 13, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 92, "ncat": 17, "s": 0.9743183259353203, "os": 0.20013427087382074, "bg": 0.0003649445635815459}, {"x": 0.44705136334812934, "y": 0.2793278376664553, "ox": 0.44705136334812934, "oy": 0.2793278376664553, "term": "discussed", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 1.182166218717616e-06}, {"x": 0.9026632847178186, "y": 0.8176918199112239, "ox": 0.9026632847178186, "oy": 0.8176918199112239, "term": "automatic", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 30, "s": 0.06467977171845275, "os": -0.022419702484011178, "bg": 5.211595466370565e-06}, {"x": 0.8532022828154724, "y": 0.8665187064045656, "ox": 0.8532022828154724, "oy": 0.8665187064045656, "term": "coding", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 21, "s": 0.877298668357641, "os": 0.08490866047136143, "bg": 1.4881270052622119e-05}, {"x": 0.3547875713379835, "y": 0.4590995561192137, "ox": 0.3547875713379835, "oy": 0.4590995561192137, "term": "codes", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 1.036435396952022e-06}, {"x": 0.1471147748890298, "y": 0.21655041217501586, "ox": 0.1471147748890298, "oy": 0.21655041217501586, "term": "patient", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 5.14122092592828e-07}, {"x": 0.7279644895370958, "y": 0.4879518072289157, "ox": 0.7279644895370958, "oy": 0.4879518072289157, "term": "achieving", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 11, "s": 0.09670259987317692, "os": -0.014487120596445358, "bg": 5.77491726875732e-06}, {"x": 0.911857958148383, "y": 0.33893468611287253, "ox": 0.911857958148383, "oy": 0.33893468611287253, "term": "micro", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 32, "s": 0.0003170577045022194, "os": -0.14273347231546588, "bg": 3.830588738772867e-06}, {"x": 0.35510462904248574, "y": 0.15630944831959417, "ox": 0.35510462904248574, "oy": 0.15630944831959417, "term": "f", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 8.066776100327168e-08}, {"x": 0.7891566265060241, "y": 0.818008877615726, "ox": 0.7891566265060241, "oy": 0.818008877615726, "term": "measure", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 14, "s": 0.8313253012048194, "os": 0.06549238542807674, "bg": 3.6515861933402244e-06}, {"x": 0.35542168674698793, "y": 0.1566265060240964, "ox": 0.35542168674698793, "oy": 0.1566265060240964, "term": "indicates", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 1.0934216745151562e-06}, {"x": 0.3557387444514902, "y": 0.10240963855421686, "ox": 0.3557387444514902, "oy": 0.10240963855421686, "term": "hierarchy", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 3.3295059149429286e-06}, {"x": 0.2482561826252378, "y": 0.05897273303741281, "ox": 0.2482561826252378, "oy": 0.05897273303741281, "term": "demonstrate its", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.14743183259353201, "y": 0.10272669625871908, "ox": 0.14743183259353201, "oy": 0.10272669625871908, "term": "its effectiveness", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.24857324032974001, "y": 0.05928979074191503, "ox": 0.24857324032974001, "oy": 0.05928979074191503, "term": "indicates that", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.3560558021559924, "y": 0.5798985415345593, "ox": 0.3560558021559924, "oy": 0.5798985415345593, "term": "method outperforms", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 4, "s": 0.6718452758402029, "os": 0.03683615419949825, "bg": 0.0}, {"x": 0.24889029803424223, "y": 0.1569435637285986, "ox": 0.24889029803424223, "oy": 0.1569435637285986, "term": "vital", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 1.5280393484023479e-06}, {"x": 0.703868103994927, "y": 0.6769181991122384, "ox": 0.703868103994927, "oy": 0.6769181991122384, "term": "success", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 10, "s": 0.5171211160431198, "os": 0.023161725734072998, "bg": 1.1820825998792248e-06}, {"x": 0.728281547241598, "y": 0.7171845275840203, "ox": 0.728281547241598, "oy": 0.7171845275840203, "term": "component", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 11, "s": 0.6074825618262524, "os": 0.030528956573972643, "bg": 2.0008365497614554e-06}, {"x": 0.5196575776791376, "y": 0.6651870640456563, "ox": 0.5196575776791376, "oy": 0.6651870640456563, "term": "results indicate", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 6, "s": 0.712428662016487, "os": 0.04192431362849369, "bg": 0.0}, {"x": 0.8132530120481928, "y": 0.8037412809131262, "ox": 0.8132530120481928, "oy": 0.8037412809131262, "term": "compare", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 16, "s": 0.7349397590361446, "os": 0.044857072188261904, "bg": 6.323806419017867e-07}, {"x": 0.5199746353836399, "y": 0.5367786937222575, "ox": 0.5199746353836399, "oy": 0.5367786937222575, "term": "sequential", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.4663918833227647, "os": 0.019416275043284685, "bg": 1.3066195904883775e-05}, {"x": 0.3563728598604946, "y": 0.6655041217501585, "ox": 0.3563728598604946, "oy": 0.6655041217501585, "term": "effects", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 4, "s": 0.785986049461002, "os": 0.052913324617504684, "bg": 6.777173318462279e-07}, {"x": 0.7285986049461002, "y": 0.7571337983512999, "ox": 0.7285986049461002, "oy": 0.7571337983512999, "term": "we investigate", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 11, "s": 0.7479391249207357, "os": 0.04660612699197908, "bg": 0.0}, {"x": 0.0653138871274572, "y": 0.3392517438173748, "ox": 0.0653138871274572, "oy": 0.3392517438173748, "term": "core", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 4.662418102926183e-07}, {"x": 0.5789473684210527, "y": 0.7504755865567533, "ox": 0.5789473684210527, "oy": 0.7504755865567533, "term": "variety", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 7, "s": 0.8306911857958149, "os": 0.06536871488639978, "bg": 1.5975872279131721e-06}, {"x": 0.5792644261255548, "y": 0.7507926442612556, "ox": 0.5792644261255548, "oy": 0.7507926442612556, "term": "variety of", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 7, "s": 0.8306911857958149, "os": 0.06536871488639978, "bg": 0.0}, {"x": 0.813570069752695, "y": 0.8199112238427394, "ox": 0.813570069752695, "oy": 0.8199112238427394, "term": "pedestrian", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 16, "s": 0.8062777425491441, "os": 0.05771880852266703, "bg": 3.3827155239908195e-05}, {"x": 0.520291693088142, "y": 0.3849080532656944, "ox": 0.520291693088142, "oy": 0.3849080532656944, "term": "vehicles", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 6, "s": 0.24381737476220672, "os": 0.0033391046252782552, "bg": 9.603779023020365e-07}, {"x": 0.35668991756499685, "y": 0.4594166138237159, "ox": 0.35668991756499685, "oy": 0.4594166138237159, "term": "others", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 3.2249795737856246e-07}, {"x": 0.579581483830057, "y": 0.5104629042485732, "ox": 0.579581483830057, "oy": 0.5104629042485732, "term": "lot", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 4.3137356291296926e-07}, {"x": 0.5206087507926442, "y": 0.48826886493341787, "ox": 0.5206087507926442, "oy": 0.48826886493341787, "term": "a lot", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 6, "s": 0.3823715916296766, "os": 0.012985406876082115, "bg": 0.0}, {"x": 0.06563094483195941, "y": 0.2796448953709575, "ox": 0.06563094483195941, "oy": 0.2796448953709575, "term": "establish", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 8.731480579301986e-07}, {"x": 0.7041851616994292, "y": 0.6195307545973368, "ox": 0.7041851616994292, "oy": 0.6195307545973368, "term": "whether", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 10, "s": 0.3487634749524413, "os": 0.010299989399667857, "bg": 5.890967279154336e-07}, {"x": 0.24920735573874445, "y": 0.05960684844641725, "ox": 0.24920735573874445, "oy": 0.05960684844641725, "term": "work proposes", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.06594800253646163, "y": 0.15726062143310082, "ox": 0.06594800253646163, "oy": 0.15726062143310082, "term": "it would", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.14774889029803423, "y": 0.21686746987951808, "ox": 0.14774889029803423, "oy": 0.21686746987951808, "term": "sketch", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 5.058389914490218e-06}, {"x": 0.6284083703233989, "y": 0.0057070386810399495, "ox": 0.6284083703233989, "oy": 0.0057070386810399495, "term": "siamese", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.024413443246670895, "os": -0.04301968128334688, "bg": 2.1757471878467597e-05}, {"x": 0.8008877615726062, "y": 0.8868103994927077, "ox": 0.8008877615726062, "oy": 0.8868103994927077, "term": "bayesian", "cat25k": 10, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 15, "s": 0.9457831325301205, "os": 0.15003003427440725, "bg": 0.00010208194976524052}, {"x": 0.7539632213062778, "y": 0.38522511097019657, "ox": 0.7539632213062778, "oy": 0.38522511097019657, "term": "shape", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 12, "s": 0.043753963221306286, "os": -0.029627928341754706, "bg": 1.8528431086183338e-06}, {"x": 0.14806594800253647, "y": 0.27996195307545974, "ox": 0.14806594800253647, "oy": 0.27996195307545974, "term": "rapid", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 1.2434224248954723e-06}, {"x": 0.5798985415345593, "y": 0.6198478123018389, "ox": 0.5798985415345593, "oy": 0.6198478123018389, "term": "technology", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 7, "s": 0.5672162333544705, "os": 0.026783505883184337, "bg": 2.344025939727743e-07}, {"x": 0.8601775523145212, "y": 0.05992390615091947, "ox": 0.8601775523145212, "oy": 0.05992390615091947, "term": "demand", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 22, "s": 0.0028535193405199747, "os": -0.10386558778841737, "bg": 1.241987517138319e-06}, {"x": 0.5209258084971464, "y": 0.7666455294863666, "ox": 0.5209258084971464, "oy": 0.7666455294863666, "term": "become", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 6, "s": 0.8611287254280279, "os": 0.07729408854810783, "bg": 6.792242586368686e-07}, {"x": 0.1483830057070387, "y": 0.5370957514267597, "ox": 0.1483830057070387, "oy": 0.5370957514267597, "term": "has become", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 2, "s": 0.7089410272669626, "os": 0.04139429702130666, "bg": 0.0}, {"x": 0.75428027901078, "y": 0.744134432466709, "ox": 0.75428027901078, "oy": 0.744134432466709, "term": "pattern", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 12, "s": 0.653138871274572, "os": 0.034680753330271014, "bg": 2.5614303772369877e-06}, {"x": 0.5212428662016487, "y": 0.6201648700063411, "ox": 0.5212428662016487, "oy": 0.6201648700063411, "term": "relations", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.6236525047558656, "os": 0.03227801137768983, "bg": 9.745265966355012e-07}, {"x": 0.1487000634115409, "y": 0.3855421686746988, "ox": 0.1487000634115409, "oy": 0.3855421686746988, "term": "sketches", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 9.649113436011388e-06}, {"x": 0.24952441344324666, "y": 0.15757767913760304, "ox": 0.24952441344324666, "oy": 0.15757767913760304, "term": "relations between", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.5215599239061509, "y": 0.87000634115409, "ox": 0.5215599239061509, "oy": 0.87000634115409, "term": "views", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 6, "s": 0.9613189600507293, "os": 0.17054167697254513, "bg": 1.8049543857729268e-06}, {"x": 0.4473684210526316, "y": 0.33956880152187696, "ox": 0.4473684210526316, "oy": 0.33956880152187696, "term": "feasibility", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 7.454335208409236e-06}, {"x": 0.4476854787571338, "y": 0.5107799619530755, "ox": 0.4476854787571338, "oy": 0.5107799619530755, "term": "interesting", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 8.895638843162835e-07}, {"x": 0.5802155992390615, "y": 0.6772352568167407, "ox": 0.5802155992390615, "oy": 0.6772352568167407, "term": "purpose", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 7, "s": 0.6968928344958782, "os": 0.03964524221758948, "bg": 9.91542975037317e-07}, {"x": 0.24984147114774888, "y": 0.2171845275840203, "ox": 0.24984147114774888, "oy": 0.2171845275840203, "term": "platform", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 6.289082883220614e-07}, {"x": 0.14901712111604312, "y": 0.2175015852885225, "ox": 0.14901712111604312, "oy": 0.2175015852885225, "term": "this purpose", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 0.0}, {"x": 0.5805326569435637, "y": 0.014584654407102092, "ox": 0.5805326569435637, "oy": 0.014584654407102092, "term": "discussion", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 1.7824619610585894e-07}, {"x": 0.5808497146480659, "y": 0.5374128091312619, "ox": 0.5808497146480659, "oy": 0.5374128091312619, "term": "transductive transfer", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 7, "s": 0.3937856689917565, "os": 0.01392176954877919, "bg": 0.0}, {"x": 0.9150285351934052, "y": 0.9362714013950539, "ox": 0.9150285351934052, "oy": 0.9362714013950539, "term": "there", "cat25k": 17, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 125, "ncat": 33, "s": 0.9819277108433735, "os": 0.21511607363697394, "bg": 4.5052671670122284e-07}, {"x": 0.5218769816106531, "y": 0.385859226379201, "ox": 0.5218769816106531, "oy": 0.385859226379201, "term": "whereas", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 6, "s": 0.24381737476220672, "os": 0.0033391046252782552, "bg": 2.4170365627806452e-06}, {"x": 0.35700697526949904, "y": 0.10304375396322131, "ox": 0.35700697526949904, "oy": 0.10304375396322131, "term": "transformed", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 4.272423718787517e-06}, {"x": 0.448002536461636, "y": 0.6658211794546608, "ox": 0.448002536461636, "oy": 0.6658211794546608, "term": "same distribution", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 5, "s": 0.7520608750792644, "os": 0.04741881912299919, "bg": 0.0}, {"x": 0.2501585288522511, "y": 0.280279010779962, "ox": 0.2501585288522511, "oy": 0.280279010779962, "term": "preserve", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 2.8441989390262816e-06}, {"x": 0.7752060875079264, "y": 0.7824984147114775, "ox": 0.7752060875079264, "oy": 0.7824984147114775, "term": "constraints", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 13, "s": 0.7599873176918199, "os": 0.04847885233737323, "bg": 9.479417350400621e-06}, {"x": 0.7545973367152822, "y": 0.7175015852885225, "ox": 0.7545973367152822, "oy": 0.7175015852885225, "term": "utilized", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 12, "s": 0.5342422320862397, "os": 0.025034451079467154, "bg": 1.7372325588736462e-05}, {"x": 0.7045022194039315, "y": 0.7704502219403931, "ox": 0.7045022194039315, "oy": 0.7704502219403931, "term": "capture", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 10, "s": 0.8088142041851618, "os": 0.05853150065368715, "bg": 5.095884325641409e-06}, {"x": 0.25047558655675334, "y": 0.666138237159163, "ox": 0.25047558655675334, "oy": 0.666138237159163, "term": "assumed", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 3, "s": 0.8078630310716551, "os": 0.05840783011201017, "bg": 4.3746190435916205e-06}, {"x": 0.2507926442612555, "y": 0.3398858592263792, "ox": 0.2507926442612555, "oy": 0.3398858592263792, "term": "constructing", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 8.830306115176203e-06}, {"x": 0.6686746987951807, "y": 0.7444514901712111, "ox": 0.6686746987951807, "oy": 0.7444514901712111, "term": "should", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 9, "s": 0.7758402029169309, "os": 0.051164269813787494, "bg": 2.0384877474667883e-07}, {"x": 0.25110970196575777, "y": 0.42232086239695626, "ox": 0.25110970196575777, "oy": 0.42232086239695626, "term": "class labels", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 0.0}, {"x": 0.44831959416613826, "y": 0.5377298668357641, "ox": 0.44831959416613826, "oy": 0.5377298668357641, "term": "should be", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 5, "s": 0.5326569435637286, "os": 0.02491078053779018, "bg": 0.0}, {"x": 0.8138871274571972, "y": 0.8725428027901078, "ox": 0.8138871274571972, "oy": 0.8725428027901078, "term": "assumption", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 16, "s": 0.9185161699429296, "os": 0.11881205611109147, "bg": 1.912612498804617e-05}, {"x": 0.44863665187064045, "y": 0.5577045022194039, "ox": 0.44863665187064045, "oy": 0.5577045022194039, "term": "this assumption", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.5798985415345593, "os": 0.02812621462139147, "bg": 0.0}, {"x": 0.6689917564996829, "y": 0.6493341788205453, "ox": 0.6689917564996829, "oy": 0.6493341788205453, "term": "handle", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 9, "s": 0.5069752694990488, "os": 0.02222536306137593, "bg": 1.9662312683061125e-06}, {"x": 0.6287254280279011, "y": 0.7574508560558022, "ox": 0.6287254280279011, "oy": 0.7574508560558022, "term": "discriminant", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 8, "s": 0.8240329740012683, "os": 0.06308964347549556, "bg": 0.000145174252309394}, {"x": 0.14933417882054534, "y": 0.42263792010145845, "ox": 0.14933417882054534, "oy": 0.42263792010145845, "term": "considers", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 4.402465145683441e-06}, {"x": 0.4489537095751427, "y": 0.7964489537095751, "ox": 0.4489537095751427, "oy": 0.7964489537095751, "term": "hidden", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 5, "s": 0.8969562460367787, "os": 0.09886576446061976, "bg": 3.3635644275256323e-06}, {"x": 0.25142675967026, "y": 0.7086239695624603, "ox": 0.25142675967026, "oy": 0.7086239695624603, "term": "margin", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 3, "s": 0.8459099556119214, "os": 0.07126956644641531, "bg": 6.001324937673337e-06}, {"x": 0.6290424857324033, "y": 0.5992390615091947, "ox": 0.6290424857324033, "oy": 0.5992390615091947, "term": "defined", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 8, "s": 0.45085605580215593, "os": 0.018073566305077567, "bg": 1.0923827507067668e-06}, {"x": 0.9153455928979074, "y": 0.9308814204185162, "ox": 0.9153455928979074, "oy": 0.9308814204185162, "term": "applied", "cat25k": 16, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 119, "ncat": 33, "s": 0.9727330374128091, "os": 0.19582346913536625, "bg": 6.726967141111103e-06}, {"x": 0.5221940393151554, "y": 0.4229549778059607, "ox": 0.5221940393151554, "oy": 0.4229549778059607, "term": "voc", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.2780596068484464, "os": 0.006554538708879544, "bg": 1.9773983370079985e-05}, {"x": 0.5811667723525682, "y": 0.6496512365250475, "ox": 0.5811667723525682, "oy": 0.6496512365250475, "term": "extended", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 7, "s": 0.6379201014584654, "os": 0.033214374050386915, "bg": 1.6721241081726067e-06}, {"x": 0.14965123652504755, "y": 0.10336081166772353, "ox": 0.14965123652504755, "oy": 0.10336081166772353, "term": "competitors", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 2.343672182759557e-06}, {"x": 0.7755231452124287, "y": 0.8570069752694991, "ox": 0.7755231452124287, "oy": 0.8570069752694991, "term": "collaborative", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 13, "s": 0.9162967660114141, "os": 0.11600296809300023, "bg": 1.3987315447571754e-05}, {"x": 0.8535193405199747, "y": 0.8500317057704502, "ox": 0.8535193405199747, "oy": 0.8500317057704502, "term": "filtering", "cat25k": 8, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 21, "s": 0.8211794546607484, "os": 0.06240062188615243, "bg": 1.5256878276266007e-05}, {"x": 0.14996829422954977, "y": 0.5995561192136969, "ox": 0.14996829422954977, "oy": 0.5995561192136969, "term": "cf", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 3.0605467972908038e-06}, {"x": 0.6293595434369055, "y": 0.7669625871908687, "ox": 0.6293595434369055, "oy": 0.7669625871908687, "term": "social", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 8, "s": 0.8354470513633481, "os": 0.06630507755909684, "bg": 6.151764018332257e-07}, {"x": 0.7289156626506024, "y": 0.7891566265060241, "ox": 0.7289156626506024, "oy": 0.7891566265060241, "term": "collaborative filtering", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 11, "s": 0.822130627774255, "os": 0.0626832974099855, "bg": 0.0}, {"x": 0.3573240329740013, "y": 0.5380469245402664, "ox": 0.3573240329740013, "oy": 0.5380469245402664, "term": "data sparsity", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.6049461001902346, "os": 0.030405286032295677, "bg": 0.0}, {"x": 0.8012048192771084, "y": 0.7764743183259353, "ox": 0.8012048192771084, "oy": 0.7764743183259353, "term": "recommender systems", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 15, "s": 0.6493341788205453, "os": 0.034274407264760956, "bg": 0.0}, {"x": 0.2517438173747622, "y": 0.5110970196575777, "ox": 0.2517438173747622, "oy": 0.5110970196575777, "term": "suffer", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.62714013950539, "os": 0.03268435744319989, "bg": 4.238908922691337e-06}, {"x": 0.25206087507926445, "y": 0.45973367152821815, "ox": 0.25206087507926445, "oy": 0.45973367152821815, "term": "suffer from", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 0.0}, {"x": 0.25237793278376663, "y": 0.28059606848446417, "ox": 0.25237793278376663, "oy": 0.28059606848446417, "term": "sparsity problem", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 0.0}, {"x": 0.6296766011414078, "y": 0.6499682942295498, "ox": 0.6296766011414078, "oy": 0.6499682942295498, "term": "exploited", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 8, "s": 0.5783132530120482, "os": 0.027719868555881426, "bg": 1.6091729084840006e-05}, {"x": 0.62999365884591, "y": 0.650285351934052, "ox": 0.62999365884591, "oy": 0.650285351934052, "term": "be transferred", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 8, "s": 0.5783132530120482, "os": 0.027719868555881426, "bg": 0.0}, {"x": 0.5225110970196576, "y": 0.2809131261889664, "ox": 0.5225110970196576, "oy": 0.2809131261889664, "term": "create", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 2.330628493302575e-07}, {"x": 0.9029803424223208, "y": 0.9296131896005073, "ox": 0.9029803424223208, "oy": 0.9296131896005073, "term": "auxiliary", "cat25k": 16, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 117, "ncat": 30, "s": 0.976854787571338, "os": 0.20587611745168016, "bg": 6.23303406289513e-05}, {"x": 0.15028535193405199, "y": 0.46005072923272033, "ox": 0.15028535193405199, "oy": 0.46005072923272033, "term": "recommendations", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 2, "s": 0.6195307545973368, "os": 0.0317479947705028, "bg": 8.90043126483417e-07}, {"x": 0.5814838300570704, "y": 0.2812301838934686, "ox": 0.5814838300570704, "oy": 0.2812301838934686, "term": "auxiliary domains", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 0.0}, {"x": 0.5818008877615726, "y": 0.5802155992390615, "ox": 0.5818008877615726, "oy": 0.5802155992390615, "term": "before", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.47368421052631576, "os": 0.02035263771598176, "bg": 1.8720057335791607e-07}, {"x": 0.5821179454660749, "y": 0.4885859226379201, "ox": 0.5821179454660749, "oy": 0.4885859226379201, "term": "takes", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.2932783766645529, "os": 0.007490901381576619, "bg": 7.584227543925305e-07}, {"x": 0.4492707672796449, "y": 0.28154724159797084, "ox": 0.4492707672796449, "oy": 0.28154724159797084, "term": "place", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 1.35623175743965e-07}, {"x": 0.6303107165504122, "y": 0.5805326569435637, "ox": 0.6303107165504122, "oy": 0.5805326569435637, "term": "reliable", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 8, "s": 0.40076093849080535, "os": 0.014858132221476271, "bg": 2.623560849339929e-06}, {"x": 0.7758402029169309, "y": 0.7577679137603044, "ox": 0.7758402029169309, "oy": 0.7577679137603044, "term": "correlation", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 13, "s": 0.6601141407736208, "os": 0.03561711600296809, "bg": 1.15280702378339e-05}, {"x": 0.8462270133164236, "y": 0.9007609384908053, "ox": 0.8462270133164236, "oy": 0.9007609384908053, "term": "will", "cat25k": 12, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 84, "ncat": 20, "s": 0.9527584020291693, "os": 0.15792728172149392, "bg": 1.5333300690603682e-07}, {"x": 0.5824350031705771, "y": 0.6775523145212429, "ox": 0.5824350031705771, "oy": 0.6775523145212429, "term": "will be", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 7, "s": 0.6968928344958782, "os": 0.03964524221758948, "bg": 0.0}, {"x": 0.15060240963855423, "y": 0.5114140773620799, "ox": 0.15060240963855423, "oy": 0.5114140773620799, "term": "link", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 2, "s": 0.6829422954977806, "os": 0.03817886293770538, "bg": 1.4960507190782325e-07}, {"x": 0.88427393785669, "y": 0.8893468611287254, "ox": 0.88427393785669, "oy": 0.8893468611287254, "term": "users", "cat25k": 10, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 26, "s": 0.8874445149017122, "os": 0.09280590791844812, "bg": 1.337334495494841e-06}, {"x": 0.4495878249841471, "y": 0.6506024096385542, "ox": 0.4495878249841471, "oy": 0.6506024096385542, "term": "items", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 5, "s": 0.7301838934686113, "os": 0.044203385039397906, "bg": 1.6931921596071869e-07}, {"x": 0.5228281547241598, "y": 0.5117311350665821, "ox": 0.5228281547241598, "oy": 0.5117311350665821, "term": "especially when", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 6, "s": 0.4166138237159163, "os": 0.016200840959683403, "bg": 0.0}, {"x": 0.15091946734305645, "y": 0.10367786937222574, "ox": 0.15091946734305645, "oy": 0.10367786937222574, "term": "tagging", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 6.909038288738689e-06}, {"x": 0.06626506024096386, "y": 0.15789473684210525, "ox": 0.06626506024096386, "oy": 0.15789473684210525, "term": "history", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 8.363091769237432e-08}, {"x": 0.2526949904882689, "y": 0.060240963855421686, "ox": 0.2526949904882689, "oy": 0.060240963855421686, "term": "applies", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.0704023868546024e-06}, {"x": 0.06658211794546608, "y": 0.28186429930247303, "ox": 0.06658211794546608, "oy": 0.28186429930247303, "term": "regularize", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 8.135643362979124e-05}, {"x": 0.523145212428662, "y": 0.8471781864299303, "ox": 0.523145212428662, "oy": 0.8471781864299303, "term": "factorization", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 6, "s": 0.9400760938490805, "os": 0.14160277022013357, "bg": 0.00015257323515287336}, {"x": 0.15123652504755866, "y": 0.7828154724159797, "ox": 0.15123652504755866, "oy": 0.7828154724159797, "term": "matrix factorization", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 2, "s": 0.9083703233988586, "os": 0.10891841277693368, "bg": 0.0}, {"x": 0.15155358275206088, "y": 0.3402029169308814, "ox": 0.15155358275206088, "oy": 0.3402029169308814, "term": "encoded", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 6.9826532094825505e-06}, {"x": 0.35764109067850347, "y": 0.2821813570069753, "ox": 0.35764109067850347, "oy": 0.2821813570069753, "term": "public datasets", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.7549143944197844, "y": 0.5120481927710844, "ox": 0.7549143944197844, "oy": 0.5120481927710844, "term": "rating", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 12, "s": 0.08909321496512365, "os": -0.016766192007349558, "bg": 2.6911484025040325e-07}, {"x": 0.7552314521242867, "y": 0.4603677869372226, "ox": 0.7552314521242867, "oy": 0.4603677869372226, "term": "item", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 12, "s": 0.06182625237793279, "os": -0.023197060174552135, "bg": 1.7522232166051534e-07}, {"x": 0.1518706404565631, "y": 0.28249841471147746, "ox": 0.1518706404565631, "oy": 0.28249841471147746, "term": "related but", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.3579581483830057, "y": 0.06055802155992391, "ox": 0.3579581483830057, "oy": 0.06055802155992391, "term": "limited number", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.823715916296766, "y": 0.8202282815472416, "ox": 0.823715916296766, "oy": 0.8202282815472416, "term": "computational", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 17, "s": 0.7818642993024731, "os": 0.05222430302816154, "bg": 1.87882503054209e-05}, {"x": 0.3582752060875079, "y": 0.38617628408370325, "ox": 0.3582752060875079, "oy": 0.38617628408370325, "term": "comprehensive experiments", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 0.0}, {"x": 0.06689917564996829, "y": 0.2828154724159797, "ox": 0.06689917564996829, "oy": 0.2828154724159797, "term": "comparison with", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.4499048826886493, "y": 0.15821179454660747, "ox": 0.4499048826886493, "oy": 0.15821179454660747, "term": "50", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 0.0}, {"x": 0.7048192771084337, "y": 0.7089410272669626, "ox": 0.7048192771084337, "oy": 0.7089410272669626, "term": "experiments show", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 10, "s": 0.6290424857324033, "os": 0.03280802798487686, "bg": 0.0}, {"x": 0.45022194039315155, "y": 0.10399492707672796, "ox": 0.45022194039315155, "oy": 0.10399492707672796, "term": "baseline methods", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 0.0}, {"x": 0.35859226379201015, "y": 0.21781864299302472, "ox": 0.35859226379201015, "oy": 0.21781864299302472, "term": "at least", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 0.0}, {"x": 0.6306277742549143, "y": 0.006024096385542169, "ox": 0.6306277742549143, "oy": 0.006024096385542169, "term": "bayesian optimization", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.024413443246670895, "os": -0.04301968128334688, "bg": 0.0}, {"x": 0.5827520608750792, "y": 0.5580215599239061, "ox": 0.5827520608750792, "oy": 0.5580215599239061, "term": "areas", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.42897907419150283, "os": 0.017137203632380478, "bg": 4.0910760588805336e-07}, {"x": 0.8240329740012682, "y": 0.8474952441344324, "ox": 0.8240329740012682, "oy": 0.8474952441344324, "term": "conventional", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 17, "s": 0.8693722257450855, "os": 0.08116320978057313, "bg": 1.137683212425523e-05}, {"x": 0.4505389980976538, "y": 0.10431198478123019, "ox": 0.4505389980976538, "oy": 0.10431198478123019, "term": "grid", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 1.2234099493814132e-06}, {"x": 0.5830691185795814, "y": 0.21813570069752694, "ox": 0.5830691185795814, "oy": 0.21813570069752694, "term": "box", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 7, "s": 0.11001902346227013, "os": -0.0118017031200311, "bg": 1.8883267740040777e-07}, {"x": 0.6693088142041852, "y": 0.8741280913126189, "ox": 0.6693088142041852, "oy": 0.8741280913126189, "term": "generic", "cat25k": 9, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 9, "s": 0.9552948636651871, "os": 0.16048902865623124, "bg": 5.837546907121711e-06}, {"x": 0.5234622701331643, "y": 0.34051997463538364, "ox": 0.5234622701331643, "oy": 0.34051997463538364, "term": "cold", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.19625871908687384, "os": 0.00012367054167697344, "bg": 8.2796551922984e-07}, {"x": 0.7051363348129359, "y": 0.6509194673430564, "ox": 0.7051363348129359, "oy": 0.6509194673430564, "term": "start", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 10, "s": 0.42802790107799615, "os": 0.016730857566870434, "bg": 4.070447782498065e-07}, {"x": 0.5237793278376665, "y": 0.28313253012048195, "ox": 0.5237793278376665, "oy": 0.28313253012048195, "term": "cold start", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 0.0}, {"x": 0.25301204819277107, "y": 0.060875079264426125, "ox": 0.25301204819277107, "oy": 0.060875079264426125, "term": "start problem", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.06721623335447051, "y": 0.38649334178820544, "ox": 0.06721623335447051, "oy": 0.38649334178820544, "term": "locations", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 4.4291595284798536e-07}, {"x": 0.6696258719086874, "y": 0.4889029803424223, "ox": 0.6696258719086874, "oy": 0.4889029803424223, "term": "stages", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 9, "s": 0.16613823715916298, "os": -0.003498109607434366, "bg": 3.7314239222520513e-06}, {"x": 0.5833861762840837, "y": 0.6778693722257451, "ox": 0.5833861762840837, "oy": 0.6778693722257451, "term": "already", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 7, "s": 0.6968928344958782, "os": 0.03964524221758948, "bg": 5.895602929447715e-07}, {"x": 0.1521876981610653, "y": 0.28344958782498414, "ox": 0.1521876981610653, "oy": 0.28344958782498414, "term": "assuming", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 2.7407619478092686e-06}, {"x": 0.7894736842105263, "y": 0.6889663918833228, "ox": 0.7894736842105263, "oy": 0.6889663918833228, "term": "noisy", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 14, "s": 0.2606214331008243, "os": 0.004399137839652317, "bg": 2.0233545701256756e-05}, {"x": 0.8538363982244769, "y": 0.7266962587190868, "ox": 0.8538363982244769, "oy": 0.7266962587190868, "term": "mechanism", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 21, "s": 0.06975269499048826, "os": -0.021200664287481005, "bg": 5.665865890954078e-06}, {"x": 0.5240963855421686, "y": 0.21845275840202916, "ox": 0.5240963855421686, "oy": 0.21845275840202916, "term": "compute", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 5.6263728349717335e-06}, {"x": 0.450856055802156, "y": 0.3868103994927077, "ox": 0.450856055802156, "oy": 0.3868103994927077, "term": "adjust", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 3.6379530522158605e-06}, {"x": 0.7292327203551047, "y": 0.7580849714648066, "ox": 0.7292327203551047, "oy": 0.7580849714648066, "term": "popular", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 11, "s": 0.7479391249207357, "os": 0.04660612699197908, "bg": 7.710321639665348e-07}, {"x": 0.45117311350665823, "y": 0.03899809765377299, "ox": 0.45117311350665823, "oy": 0.03899809765377299, "term": "faster than", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 0.0}, {"x": 0.4514901712111604, "y": 0.6512365250475587, "ox": 0.4514901712111604, "oy": 0.6512365250475587, "term": "empirically", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 5, "s": 0.7301838934686113, "os": 0.044203385039397906, "bg": 5.939457833561543e-05}, {"x": 0.06753329105897274, "y": 0.15852885225110971, "ox": 0.06753329105897274, "oy": 0.15852885225110971, "term": "hyperparameters", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 7.242440702516747e-05}, {"x": 0.6699429296131896, "y": 0.48922003804692454, "ox": 0.6699429296131896, "oy": 0.48922003804692454, "term": "three different", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 9, "s": 0.16613823715916298, "os": -0.003498109607434366, "bg": 0.0}, {"x": 0.3589093214965124, "y": 0.15884590995561193, "ox": 0.3589093214965124, "oy": 0.15884590995561193, "term": "outperforms state", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.6702599873176919, "y": 0.15916296766011415, "ox": 0.6702599873176919, "oy": 0.15916296766011415, "term": "biomedical", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 9, "s": 0.051363348129359554, "os": -0.02600614819264337, "bg": 7.312574940450906e-06}, {"x": 0.15250475586556753, "y": 0.6664552948636652, "ox": 0.15250475586556753, "oy": 0.6664552948636652, "term": "signal", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 2, "s": 0.8256182625237795, "os": 0.06390233560651568, "bg": 1.5348686742748927e-06}, {"x": 0.2533291058972733, "y": 0.1046290424857324, "ox": 0.2533291058972733, "oy": 0.1046290424857324, "term": "fetal", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 5.95440119564376e-06}, {"x": 0.7555485098287889, "y": 0.2837666455294864, "ox": 0.7555485098287889, "oy": 0.2837666455294864, "term": "monitoring", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 12, "s": 0.03170577045022194, "os": -0.036058796508957276, "bg": 1.0506620400604541e-06}, {"x": 0.2536461636017755, "y": 0.5123652504755866, "ox": 0.2536461636017755, "oy": 0.5123652504755866, "term": "device", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.62714013950539, "os": 0.03268435744319989, "bg": 7.443564558525124e-07}, {"x": 0.8015218769816107, "y": 0.5583386176284084, "ox": 0.8015218769816107, "oy": 0.5583386176284084, "term": "tools", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 15, "s": 0.050095117311350676, "os": -0.02681884032366348, "bg": 3.552459265550899e-07}, {"x": 0.25396322130627774, "y": 0.46068484464172477, "ox": 0.25396322130627774, "oy": 0.46068484464172477, "term": "clinical", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 7.873592711091021e-07}, {"x": 0.15282181357006974, "y": 0.10494610019023462, "ox": 0.15282181357006974, "oy": 0.10494610019023462, "term": "subjective", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 3.3967897694730885e-06}, {"x": 0.5244134432466709, "y": 0.5126823081800888, "ox": 0.5244134432466709, "oy": 0.5126823081800888, "term": "caused", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 6, "s": 0.4166138237159163, "os": 0.016200840959683403, "bg": 1.6136246840275325e-06}, {"x": 0.25428027901078, "y": 0.15948002536461636, "ox": 0.25428027901078, "oy": 0.15948002536461636, "term": "international", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 7.435695748283369e-08}, {"x": 0.7295497780596069, "y": 0.38712745719720987, "ox": 0.7295497780596069, "oy": 0.38712745719720987, "term": "consistent", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 11, "s": 0.05960684844641725, "os": -0.024133422847249217, "bg": 2.0147597797534685e-06}, {"x": 0.6705770450221941, "y": 0.7092580849714648, "ox": 0.6705770450221941, "oy": 0.7092580849714648, "term": "assessment", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 9, "s": 0.684844641724794, "os": 0.03830253347938235, "bg": 1.2386177100003936e-06}, {"x": 0.6708941027266963, "y": 0.009828788839568801, "ox": 0.6708941027266963, "oy": 0.009828788839568801, "term": "preprocessing", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.02219403931515536, "os": -0.045298752694251085, "bg": 3.447540963054898e-05}, {"x": 0.3592263792010146, "y": 0.06119213696892835, "ox": 0.3592263792010146, "oy": 0.06119213696892835, "term": "ensure", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 3.3479248172593825e-07}, {"x": 0.5247305009511731, "y": 0.28408370323398857, "ox": 0.5247305009511731, "oy": 0.28408370323398857, "term": "meaningful", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 4.571505633952116e-06}, {"x": 0.7897907419150285, "y": 0.8246670894102727, "ox": 0.7897907419150285, "oy": 0.8246670894102727, "term": "short", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 14, "s": 0.8471781864299303, "os": 0.07192325359527933, "bg": 1.1403341468807399e-06}, {"x": 0.6712111604311984, "y": 0.6667723525681674, "ox": 0.6712111604311984, "oy": 0.6667723525681674, "term": "considering", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 9, "s": 0.541851616994293, "os": 0.025440797144977212, "bg": 3.6809415625429625e-06}, {"x": 0.5250475586556753, "y": 0.061509194673430564, "ox": 0.5250475586556753, "oy": 0.061509194673430564, "term": "operating", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.09099556119213698, "os": -0.01595349987632946, "bg": 3.733792133413371e-07}, {"x": 0.5837032339885859, "y": 0.3874445149017121, "ox": 0.5837032339885859, "oy": 0.3874445149017121, "term": "characteristic", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 7, "s": 0.17438173747622068, "os": -0.0021554008692272406, "bg": 5.336721870841219e-06}, {"x": 0.15313887127457196, "y": 0.38776157260621436, "ox": 0.15313887127457196, "oy": 0.38776157260621436, "term": "curve", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 2.4774216199099048e-06}, {"x": 0.5840202916930881, "y": 0.5808497146480659, "ox": 0.5840202916930881, "oy": 0.5808497146480659, "term": "derived", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.47368421052631576, "os": 0.02035263771598176, "bg": 3.0524600479940643e-06}, {"x": 0.3595434369055168, "y": 0.2844007609384908, "ox": 0.3595434369055168, "oy": 0.2844007609384908, "term": "derived from", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.2545973367152822, "y": 0.5998731769181991, "ox": 0.2545973367152822, "oy": 0.5998731769181991, "term": "encouraging", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 3, "s": 0.7371591629676602, "os": 0.04554609377760503, "bg": 6.201185882434144e-06}, {"x": 0.5843373493975904, "y": 0.284717818642993, "ox": 0.5843373493975904, "oy": 0.284717818642993, "term": "music", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 8.20741120734939e-08}, {"x": 0.5846544071020926, "y": 0.28503487634749525, "ox": 0.5846544071020926, "oy": 0.28503487634749525, "term": "discrimination", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 2.5243701956223557e-06}, {"x": 0.6309448319594166, "y": 0.014901712111604312, "ox": 0.6309448319594166, "oy": 0.014901712111604312, "term": "analytics", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.030437539632213063, "os": -0.0365888131161443, "bg": 5.3832125495408476e-06}, {"x": 0.1534559289790742, "y": 0.10526315789473684, "ox": 0.1534559289790742, "oy": 0.10526315789473684, "term": "radio", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.5856723578943555e-07}, {"x": 0.5849714648065948, "y": 0.6357006975269499, "ox": 0.5849714648065948, "oy": 0.6357006975269499, "term": "focuses", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 7, "s": 0.6036778693722258, "os": 0.02999893996678562, "bg": 6.219094184642976e-06}, {"x": 0.2549143944197844, "y": 0.06182625237793278, "ox": 0.2549143944197844, "oy": 0.06182625237793278, "term": "streams", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.975070874869714e-06}, {"x": 0.45180722891566266, "y": 0.2187698161065314, "ox": 0.45180722891566266, "oy": 0.2187698161065314, "term": "segment", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 1.9535602449373834e-06}, {"x": 0.6715282181357007, "y": 0.38807863031071654, "ox": 0.6715282181357007, "oy": 0.38807863031071654, "term": "applications such", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.10367786937222576, "os": -0.013144411858238225, "bg": 0.0}, {"x": 0.15377298668357642, "y": 0.15979708306911858, "ox": 0.15377298668357642, "oy": 0.15979708306911858, "term": "automatic speech", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.5253646163601775, "y": 0.7270133164235891, "ox": 0.5253646163601775, "oy": 0.7270133164235891, "term": "speech recognition", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 6, "s": 0.8161065313887128, "os": 0.06121691813010141, "bg": 0.0}, {"x": 0.585288522511097, "y": 0.6001902346227014, "ox": 0.585288522511097, "oy": 0.6001902346227014, "term": "focuses on", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 7, "s": 0.5180722891566265, "os": 0.023568071799583055, "bg": 0.0}, {"x": 0.2552314521242866, "y": 0.062143310082435003, "ox": 0.2552314521242866, "oy": 0.062143310082435003, "term": "dependencies", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 5.310729295900559e-06}, {"x": 0.6718452758402029, "y": 0.7967660114140773, "ox": 0.6718452758402029, "oy": 0.7967660114140773, "term": "contribution", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 9, "s": 0.8601775523145212, "os": 0.07688774248259778, "bg": 4.734376136038917e-06}, {"x": 0.15409004438807863, "y": 0.1601141407736208, "ox": 0.15409004438807863, "oy": 0.1601141407736208, "term": "main contribution", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.45212428662016485, "y": 0.02599873176918199, "ox": 0.45212428662016485, "oy": 0.02599873176918199, "term": "deep architectures", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 0.0}, {"x": 0.25554850982878885, "y": 0.06246036778693722, "ox": 0.25554850982878885, "oy": 0.06246036778693722, "term": "run", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.5326393950856224e-07}, {"x": 0.15440710209258085, "y": 0.160431198478123, "ox": 0.15440710209258085, "oy": 0.160431198478123, "term": "handcrafted features", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.5856055802155993, "y": 0.709575142675967, "ox": 0.5856055802155993, "oy": 0.709575142675967, "term": "exploits", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 7, "s": 0.7637920101458466, "os": 0.04929154446839334, "bg": 3.177348230557466e-05}, {"x": 0.359860494610019, "y": 0.2853519340519975, "ox": 0.359860494610019, "oy": 0.2853519340519975, "term": "tune", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 2.5549718154671603e-06}, {"x": 0.25586556753329104, "y": 0.21908687381103362, "ox": 0.25586556753329104, "oy": 0.21908687381103362, "term": "almost", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 3.6021965293887084e-07}, {"x": 0.36017755231452125, "y": 0.10558021559923907, "ox": 0.36017755231452125, "oy": 0.10558021559923907, "term": "fine tune", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.15472415979708307, "y": 0.10589727330374128, "ox": 0.15472415979708307, "oy": 0.10589727330374128, "term": "relatively small", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.729866835764109, "y": 0.689283449587825, "ox": 0.729866835764109, "oy": 0.689283449587825, "term": "against", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 11, "s": 0.48065948002536457, "os": 0.020882654323168798, "bg": 5.017263998614042e-07}, {"x": 0.15504121750158528, "y": 0.1062143310082435, "ox": 0.15504121750158528, "oy": 0.1062143310082435, "term": "through extensive", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.6721623335447051, "y": 0.461001902346227, "ox": 0.6721623335447051, "oy": 0.461001902346227, "term": "10", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 9, "s": 0.13823715916296767, "os": -0.006713543691035655, "bg": 0.0}, {"x": 0.5256816740646798, "y": 0.4895370957514268, "ox": 0.5256816740646798, "oy": 0.4895370957514268, "term": "more than", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 6, "s": 0.3823715916296766, "os": 0.012985406876082115, "bg": 0.0}, {"x": 0.06785034876347495, "y": 0.3883956880152188, "ox": 0.06785034876347495, "oy": 0.3883956880152188, "term": "significantly outperform", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 0.0}, {"x": 0.525998731769182, "y": 0.21940393151553583, "ox": 0.525998731769182, "oy": 0.21940393151553583, "term": "current state", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 0.0}, {"x": 0.1553582752060875, "y": 0.16074825618262523, "ox": 0.1553582752060875, "oy": 0.16074825618262523, "term": "setup", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 8.415475385996816e-07}, {"x": 0.4524413443246671, "y": 0.6670894102726697, "ox": 0.4524413443246671, "oy": 0.6670894102726697, "term": "ltd.", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 5, "s": 0.7520608750792644, "os": 0.04741881912299919, "bg": 0.0}, {"x": 0.4527584020291693, "y": 0.6674064679771718, "ox": 0.4527584020291693, "oy": 0.6674064679771718, "term": "elsevier ltd.", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 5, "s": 0.7520608750792644, "os": 0.04741881912299919, "bg": 0.0}, {"x": 0.36049461001902344, "y": 0.16106531388712747, "ox": 0.36049461001902344, "oy": 0.16106531388712747, "term": "dl", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 2.4090810309461514e-06}, {"x": 0.15567533291058971, "y": 0.21972098922003805, "ox": 0.15567533291058971, "oy": 0.21972098922003805, "term": "electroencephalogram", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 7.474984285544401e-05}, {"x": 0.15599239061509196, "y": 0.4232720355104629, "ox": 0.15599239061509196, "oy": 0.4232720355104629, "term": "motor", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 7.344561176179571e-07}, {"x": 0.15630944831959417, "y": 0.7178186429930248, "ox": 0.15630944831959417, "oy": 0.7178186429930248, "term": "bci", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 2, "s": 0.8671528218135701, "os": 0.0799795060245221, "bg": 3.3935246077058185e-05}, {"x": 0.5859226379201015, "y": 0.388712745719721, "ox": 0.5859226379201015, "oy": 0.388712745719721, "term": "implement", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 7, "s": 0.17438173747622068, "os": -0.0021554008692272406, "bg": 1.813445947690388e-06}, {"x": 0.1566265060240964, "y": 0.3408370323398859, "ox": 0.1566265060240964, "oy": 0.3408370323398859, "term": "interface bci", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 0.0}, {"x": 0.3608116677235257, "y": 0.6360177552314521, "ox": 0.3608116677235257, "oy": 0.6360177552314521, "term": "generalized", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 4, "s": 0.7457197209892201, "os": 0.046482456450302106, "bg": 1.502049286281062e-05}, {"x": 0.4530754597336715, "y": 0.3890298034242232, "ox": 0.4530754597336715, "oy": 0.3890298034242232, "term": "connected", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 1.1855310813124386e-06}, {"x": 0.06816740646797717, "y": 0.2856689917564997, "ox": 0.06816740646797717, "oy": 0.2856689917564997, "term": "fc", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 2.0901675478306344e-06}, {"x": 0.1569435637285986, "y": 0.1613823715916297, "ox": 0.1569435637285986, "oy": 0.1613823715916297, "term": "fully connected", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.2561826252377933, "y": 0.6005072923272036, "ox": 0.2561826252377933, "oy": 0.6005072923272036, "term": "be applied", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 3, "s": 0.7371591629676602, "os": 0.04554609377760503, "bg": 0.0}, {"x": 0.06848446417247939, "y": 0.489854153455929, "ox": 0.06848446417247939, "oy": 0.489854153455929, "term": "eeg signals", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 0.0}, {"x": 0.7301838934686112, "y": 0.7273303741280913, "ox": 0.7301838934686112, "oy": 0.7273303741280913, "term": "validate", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 11, "s": 0.6410906785034877, "os": 0.03374439065757394, "bg": 2.0775825047284005e-05}, {"x": 0.755865567533291, "y": 0.6008243500317058, "ox": 0.755865567533291, "oy": 0.6008243500317058, "term": "robustness", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 12, "s": 0.1651870640456563, "os": -0.0039044556729444096, "bg": 4.993111067074645e-05}, {"x": 0.06880152187698162, "y": 0.6515535827520609, "ox": 0.06880152187698162, "oy": 0.6515535827520609, "term": "subject specific", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 1, "s": 0.8341788205453392, "os": 0.06618140701741988, "bg": 0.0}, {"x": 0.25649968294229547, "y": 0.10653138871274571, "ox": 0.25649968294229547, "oy": 0.10653138871274571, "term": "filters", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 1.1924314231198026e-06}, {"x": 0.8386176284083703, "y": 0.8519340519974635, "ox": 0.8386176284083703, "oy": 0.8519340519974635, "term": "provides", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 19, "s": 0.859860494610019, "os": 0.0766050669587647, "bg": 1.3888298915372282e-06}, {"x": 0.2568167406467977, "y": 0.10684844641724794, "ox": 0.2568167406467977, "oy": 0.10684844641724794, "term": "insights", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 2.560628644574758e-06}, {"x": 0.5862396956246037, "y": 0.2859860494610019, "ox": 0.5862396956246037, "oy": 0.2859860494610019, "term": "validated", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 1.076737839037827e-05}, {"x": 0.3611287254280279, "y": 0.06277742549143944, "ox": 0.3611287254280279, "oy": 0.06277742549143944, "term": "individuals", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 4.379895299478845e-07}, {"x": 0.586556753329106, "y": 0.7098922003804693, "ox": 0.586556753329106, "oy": 0.7098922003804693, "term": "overall", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 7, "s": 0.7637920101458466, "os": 0.04929154446839334, "bg": 8.835755424270256e-07}, {"x": 0.45339251743817377, "y": 0.5811667723525682, "ox": 0.45339251743817377, "oy": 0.5811667723525682, "term": "tested on", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.61572606214331, "os": 0.03134164870499275, "bg": 0.0}, {"x": 0.06911857958148383, "y": 0.5129993658845909, "ox": 0.06911857958148383, "oy": 0.5129993658845909, "term": "generalization performance", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 1, "s": 0.7266962587190869, "os": 0.043673368432210875, "bg": 0.0}, {"x": 0.3614457831325301, "y": 0.7831325301204819, "ox": 0.3614457831325301, "oy": 0.7831325301204819, "term": "underlying", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 4, "s": 0.8966391883322765, "os": 0.09792940178792268, "bg": 7.479112574801143e-06}, {"x": 0.7561826252377932, "y": 0.5133164235890932, "ox": 0.7561826252377932, "oy": 0.5133164235890932, "term": "having", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 12, "s": 0.08909321496512365, "os": -0.016766192007349558, "bg": 5.109908380620213e-07}, {"x": 0.5263157894736842, "y": 0.10716550412175016, "ox": 0.5263157894736842, "oy": 0.10716550412175016, "term": "specialized", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 6, "s": 0.10589727330374128, "os": -0.012738065792728175, "bg": 2.3241592510343178e-06}, {"x": 0.5266328471781865, "y": 0.6363348129359544, "ox": 0.5266328471781865, "oy": 0.6363348129359544, "term": "alternative", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 6, "s": 0.6585288522511097, "os": 0.035493445461291115, "bg": 1.0047901035638953e-06}, {"x": 0.25713379835129996, "y": 0.5586556753329106, "ox": 0.25713379835129996, "oy": 0.5586556753329106, "term": "established", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 3, "s": 0.6889663918833228, "os": 0.039115225610402454, "bg": 8.227378411688142e-07}, {"x": 0.7761572606214331, "y": 0.8668357641090678, "ox": 0.7761572606214331, "oy": 0.8668357641090678, "term": "if", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 13, "s": 0.9289790741915028, "os": 0.12886470442740539, "bg": 1.3389488554702621e-07}, {"x": 0.25745085605580215, "y": 0.06309448319594166, "ox": 0.25745085605580215, "oy": 0.06309448319594166, "term": "considered as", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.2577679137603044, "y": 0.10748256182625238, "ox": 0.2577679137603044, "oy": 0.10748256182625238, "term": "an alternative", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.930564362714014, "y": 0.9185161699429296, "ox": 0.930564362714014, "oy": 0.9185161699429296, "term": "instance", "cat25k": 14, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 101, "ncat": 41, "s": 0.896005072923272, "os": 0.0972050457581004, "bg": 1.0743113248301342e-05}, {"x": 0.36176284083703236, "y": 0.5136334812935954, "ox": 0.36176284083703236, "oy": 0.5136334812935954, "term": "bag", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 9.779300023619456e-07}, {"x": 0.45370957514267596, "y": 0.039315155358275206, "ox": 0.45370957514267596, "oy": 0.039315155358275206, "term": "likelihood", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 3.4940216416206464e-06}, {"x": 0.8893468611287254, "y": 0.8950538998097654, "ox": 0.8893468611287254, "oy": 0.8950538998097654, "term": "testing", "cat25k": 11, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 27, "s": 0.8950538998097654, "os": 0.09695770467474646, "bg": 3.662294796384421e-06}, {"x": 0.4540266328471782, "y": 0.718135700697527, "ox": 0.4540266328471782, "oy": 0.718135700697527, "term": "testing data", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 5, "s": 0.8246670894102728, "os": 0.0634959895410056, "bg": 0.0}, {"x": 0.06943563728598605, "y": 0.3893468611287254, "ox": 0.06943563728598605, "oy": 0.3893468611287254, "term": "valid", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 4.535495010728714e-07}, {"x": 0.7054533925174382, "y": 0.42358909321496513, "ox": 0.7054533925174382, "oy": 0.42358909321496513, "term": "may not", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 10, "s": 0.09353202282815473, "os": -0.015423483269142432, "bg": 0.0}, {"x": 0.2580849714648066, "y": 0.7447685478757133, "ox": 0.2580849714648066, "oy": 0.7447685478757133, "term": "different distributions", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 3, "s": 0.8747622067216233, "os": 0.08413130278082045, "bg": 0.0}, {"x": 0.2584020291693088, "y": 0.34115409004438807, "ox": 0.2584020291693088, "oy": 0.34115409004438807, "term": "put", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 2.4255867663312545e-07}, {"x": 0.258719086873811, "y": 0.10779961953075459, "ox": 0.258719086873811, "oy": 0.10779961953075459, "term": "put forward", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.6312618896639188, "y": 0.1616994292961319, "ox": 0.6312618896639188, "oy": 0.1616994292961319, "term": "attempts", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 8, "s": 0.07133798351299936, "os": -0.020511642698137874, "bg": 2.4926659147643796e-06}, {"x": 0.5269499048826887, "y": 0.6896005072923272, "ox": 0.5269499048826887, "oy": 0.6896005072923272, "term": "weighting", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 6, "s": 0.7584020291693089, "os": 0.04835518179569627, "bg": 3.355022153630658e-05}, {"x": 0.7564996829422955, "y": 0.8205453392517438, "ox": 0.7564996829422955, "oy": 0.8205453392517438, "term": "construct", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 12, "s": 0.8668357641090678, "os": 0.07969683050068901, "bg": 1.332948157279609e-05}, {"x": 0.7057704502219404, "y": 0.6011414077362079, "ox": 0.7057704502219404, "oy": 0.6011414077362079, "term": "benchmark datasets", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 10, "s": 0.29201014584654406, "os": 0.007084555316066575, "bg": 0.0}, {"x": 0.5272669625871909, "y": 0.10811667723525682, "ox": 0.5272669625871909, "oy": 0.10811667723525682, "term": "have demonstrated", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 6, "s": 0.10589727330374128, "os": -0.012738065792728175, "bg": 0.0}, {"x": 0.06975269499048826, "y": 0.22003804692454026, "ox": 0.06975269499048826, "oy": 0.22003804692454026, "term": "than those", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.36207989854153455, "y": 0.4239061509194673, "ox": 0.36207989854153455, "oy": 0.4239061509194673, "term": "nets", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 6.852780112425905e-06}, {"x": 0.7568167406467977, "y": 0.7276474318325935, "ox": 0.7568167406467977, "oy": 0.7276474318325935, "term": "iterative", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 12, "s": 0.5827520608750792, "os": 0.02824988516306845, "bg": 5.6429390038600394e-05}, {"x": 0.6724793912492073, "y": 0.6014584654407102, "ox": 0.6724793912492073, "oy": 0.6014584654407102, "term": "person re", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 9, "s": 0.3804692454026633, "os": 0.01257906081057207, "bg": 0.0}, {"x": 0.6727964489537096, "y": 0.6518706404565631, "ox": 0.6727964489537096, "oy": 0.6518706404565631, "term": "- identification", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 9, "s": 0.5069752694990488, "os": 0.02222536306137593, "bg": 0.0}, {"x": 0.15726062143310082, "y": 0.22035510462904248, "ox": 0.15726062143310082, "oy": 0.22035510462904248, "term": "progressive", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 1.3885691848600264e-06}, {"x": 0.924857324032974, "y": 0.9239061509194674, "ox": 0.924857324032974, "oy": 0.9239061509194674, "term": "representations", "cat25k": 15, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 108, "ncat": 37, "s": 0.9403931515535827, "os": 0.1416911063213314, "bg": 4.235753881666766e-05}, {"x": 0.6731135066582118, "y": 0.6521876981610654, "ox": 0.6731135066582118, "oy": 0.6521876981610654, "term": "labelled", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 9, "s": 0.5069752694990488, "os": 0.02222536306137593, "bg": 2.7816375413279625e-05}, {"x": 0.4543436905516804, "y": 0.5589727330374128, "ox": 0.4543436905516804, "oy": 0.5589727330374128, "term": "an iterative", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.5798985415345593, "os": 0.02812621462139147, "bg": 0.0}, {"x": 0.45466074825618263, "y": 0.2863031071655041, "ox": 0.45466074825618263, "oy": 0.2863031071655041, "term": "authors", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 5.647308952808771e-07}, {"x": 0.25903614457831325, "y": 0.06341154090044387, "ox": 0.25903614457831325, "oy": 0.06341154090044387, "term": "representations learned", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.673430564362714, "y": 0.6366518706404566, "ox": 0.673430564362714, "oy": 0.6366518706404566, "term": "recognize", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 9, "s": 0.46544071020925804, "os": 0.019009928977774634, "bg": 4.2667656189256655e-06}, {"x": 0.5868738110336081, "y": 0.7279644895370958, "ox": 0.5868738110336081, "oy": 0.7279644895370958, "term": "learnt", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 7, "s": 0.7980342422320863, "os": 0.055722412635595915, "bg": 3.008572806242219e-05}, {"x": 0.0700697526949905, "y": 0.38966391883322765, "ox": 0.0700697526949905, "oy": 0.38966391883322765, "term": "learnt from", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 0.0}, {"x": 0.4549778059606848, "y": 0.28662016487000636, "ox": 0.4549778059606848, "oy": 0.28662016487000636, "term": "n", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 6.846309694845553e-08}, {"x": 0.2593532022828155, "y": 0.3414711477488903, "ox": 0.2593532022828155, "oy": 0.3414711477488903, "term": "geometry", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 3.432376603302952e-06}, {"x": 0.8018389346861129, "y": 0.7767913760304376, "ox": 0.8018389346861129, "oy": 0.7767913760304376, "term": "address this", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 15, "s": 0.6493341788205453, "os": 0.034274407264760956, "bg": 0.0}, {"x": 0.07038681039949271, "y": 0.5814838300570704, "ox": 0.07038681039949271, "oy": 0.5814838300570704, "term": "similarity between", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 1, "s": 0.7882054533925175, "os": 0.05331967068301473, "bg": 0.0}, {"x": 0.6737476220672163, "y": 0.844007609384908, "ox": 0.6737476220672163, "oy": 0.844007609384908, "term": "does", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 9, "s": 0.9213696892834496, "os": 0.1219038196530158, "bg": 4.0095497163973887e-07}, {"x": 0.527584020291693, "y": 0.7102092580849715, "ox": 0.527584020291693, "oy": 0.7102092580849715, "term": "must", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 6, "s": 0.7923272035510464, "os": 0.05478604996289883, "bg": 2.3207745030648422e-07}, {"x": 0.6740646797717185, "y": 0.8208623969562461, "ox": 0.6740646797717185, "oy": 0.8208623969562461, "term": "does not", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 9, "s": 0.8931515535827521, "os": 0.0961803469842055, "bg": 0.0}, {"x": 0.3623969562460368, "y": 0.5818008877615726, "ox": 0.3623969562460368, "oy": 0.5818008877615726, "term": "demonstrates", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 4, "s": 0.6718452758402029, "os": 0.03683615419949825, "bg": 7.500152041125616e-06}, {"x": 0.362714013950539, "y": 0.10843373493975904, "ox": 0.362714013950539, "oy": 0.10843373493975904, "term": "very few", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.7764743183259353, "y": 0.0009511731135066582, "ox": 0.7764743183259353, "oy": 0.0009511731135066582, "term": "intention", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.007292327203551046, "os": -0.07370764283947565, "bg": 3.1518211646973243e-06}, {"x": 0.7767913760304376, "y": 0.0012682308180088776, "ox": 0.7767913760304376, "oy": 0.0012682308180088776, "term": "posts", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.007292327203551046, "os": -0.07370764283947565, "bg": 1.1978639892059924e-07}, {"x": 0.8389346861128726, "y": 0.9010779961953076, "ox": 0.8389346861128726, "oy": 0.9010779961953076, "term": "sources", "cat25k": 12, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 84, "ncat": 19, "s": 0.956563094483196, "os": 0.16342178721599943, "bg": 3.2808406265411784e-06}, {"x": 0.7771084337349398, "y": 0.559289790741915, "ox": 0.7771084337349398, "oy": 0.559289790741915, "term": "multiple sources", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 13, "s": 0.0928979074191503, "os": -0.01582982933465249, "bg": 0.0}, {"x": 0.7571337983512999, "y": 0.46131896005072925, "ox": 0.7571337983512999, "oy": 0.46131896005072925, "term": "identifying", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 12, "s": 0.06182625237793279, "os": -0.023197060174552135, "bg": 4.351750667449759e-06}, {"x": 0.631578947368421, "y": 0.5139505389980976, "ox": 0.631578947368421, "oy": 0.5139505389980976, "term": "exist", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 8, "s": 0.26981610653138866, "os": 0.005211829970672419, "bg": 1.8764634949273328e-06}, {"x": 0.5871908687381103, "y": 0.010145846544071021, "ox": 0.5871908687381103, "oy": 0.010145846544071021, "term": "sentences", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.03329105897273303, "os": -0.0343097417052401, "bg": 3.023017761069074e-06}, {"x": 0.3630310716550412, "y": 0.10875079264426125, "ox": 0.3630310716550412, "oy": 0.10875079264426125, "term": "post", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 5.59529729911515e-08}, {"x": 0.2596702599873177, "y": 0.0637285986049461, "ox": 0.2596702599873177, "oy": 0.0637285986049461, "term": "there exist", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.5875079264426125, "y": 0.5821179454660749, "ox": 0.5875079264426125, "oy": 0.5821179454660749, "term": "distinct", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.47368421052631576, "os": 0.02035263771598176, "bg": 4.926637628559188e-06}, {"x": 0.6318960050729233, "y": 0.7003804692454026, "ox": 0.6318960050729233, "oy": 0.7003804692454026, "term": "across domains", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7035510462904249, "os": 0.04058160489028657, "bg": 0.0}, {"x": 0.45529486366518707, "y": 0.28693722257450854, "ox": 0.45529486366518707, "oy": 0.28693722257450854, "term": "proven", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 2.2810425368051916e-06}, {"x": 0.25998731769181993, "y": 0.2206721623335447, "ox": 0.25998731769181993, "oy": 0.2206721623335447, "term": "health", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 5.4465325529889465e-08}, {"x": 0.4556119213696893, "y": 0.06404565630944832, "ox": 0.4556119213696893, "oy": 0.06404565630944832, "term": "care", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 9.75360479045994e-08}, {"x": 0.5279010779961953, "y": 0.5383639822447686, "ox": 0.5279010779961953, "oy": 0.5383639822447686, "term": "preserving", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.4663918833227647, "os": 0.019416275043284685, "bg": 1.1645796639580062e-05}, {"x": 0.2603043753963221, "y": 0.16201648700063412, "ox": 0.2603043753963221, "oy": 0.16201648700063412, "term": "sensitive", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 1.1449029208190864e-06}, {"x": 0.8465440710209258, "y": 0.8278376664552949, "ox": 0.8465440710209258, "oy": 0.8278376664552949, "term": "attributes", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 20, "s": 0.7143310082435004, "os": 0.04217165471184764, "bg": 9.359258295296435e-06}, {"x": 0.26062143310082436, "y": 0.10906785034876347, "ox": 0.26062143310082436, "oy": 0.10906785034876347, "term": "equally", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 1.8459662959781748e-06}, {"x": 0.9032974001268231, "y": 0.8573240329740013, "ox": 0.9032974001268231, "oy": 0.8573240329740013, "term": "quality", "cat25k": 8, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 30, "s": 0.5085605580215599, "os": 0.022596374686406823, "bg": 9.381233645294839e-07}, {"x": 0.15757767913760304, "y": 0.1093849080532657, "ox": 0.15757767913760304, "oy": 0.1093849080532657, "term": "targeted", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.5762490788794446e-06}, {"x": 0.07070386810399493, "y": 0.16233354470513633, "ox": 0.07070386810399493, "oy": 0.16233354470513633, "term": "suited", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 3.054032969303914e-06}, {"x": 0.4559289790741915, "y": 0.3417882054533925, "ox": 0.4559289790741915, "oy": 0.3417882054533925, "term": "variables", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 1.3194949649515337e-06}, {"x": 0.5282181357006975, "y": 0.16265060240963855, "ox": 0.5282181357006975, "oy": 0.16265060240963855, "term": "quantization", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 6, "s": 0.11953075459733671, "os": -0.00952263170912689, "bg": 3.205836454253286e-05}, {"x": 0.3633481293595434, "y": 0.5596068484464173, "ox": 0.3633481293595434, "oy": 0.5596068484464173, "term": "variants", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 4, "s": 0.6385542168674699, "os": 0.033620720115896965, "bg": 1.3098836376778503e-05}, {"x": 0.45624603677869374, "y": 0.6017755231452124, "ox": 0.45624603677869374, "oy": 0.6017755231452124, "term": "intra", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 5, "s": 0.6509194673430565, "os": 0.03455708278859405, "bg": 1.5093505777643076e-05}, {"x": 0.7060875079264426, "y": 0.7584020291693088, "ox": 0.7060875079264426, "oy": 0.7584020291693088, "term": "cluster", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 10, "s": 0.7812301838934687, "os": 0.052100632486484576, "bg": 5.904431806127445e-06}, {"x": 0.15789473684210525, "y": 0.2209892200380469, "ox": 0.15789473684210525, "oy": 0.2209892200380469, "term": "theoretical analysis", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 0.0}, {"x": 0.36366518706404566, "y": 0.10970196575776792, "ox": 0.36366518706404566, "oy": 0.10970196575776792, "term": "is necessary", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.8243500317057705, "y": 0.8703233988585922, "ox": 0.8243500317057705, "oy": 0.8703233988585922, "term": "standard", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 17, "s": 0.9102726696258719, "os": 0.1101021165329847, "bg": 1.0989892521225502e-06}, {"x": 0.6322130627774255, "y": 0.015218769816106531, "ox": 0.6322130627774255, "oy": 0.015218769816106531, "term": "company", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.030437539632213063, "os": -0.0365888131161443, "bg": 6.779595184886201e-08}, {"x": 0.3639822447685479, "y": 0.22130627774254916, "ox": 0.3639822447685479, "oy": 0.22130627774254916, "term": "understand", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 4.196874477811964e-07}, {"x": 0.3642993024730501, "y": 0.16296766011414077, "ox": 0.3642993024730501, "oy": 0.16296766011414077, "term": "to understand", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.36461636017755233, "y": 0.22162333544705137, "ox": 0.36461636017755233, "oy": 0.22162333544705137, "term": "indeed", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 9.568000719513654e-07}, {"x": 0.45656309448319593, "y": 0.02631578947368421, "ox": 0.45656309448319593, "oy": 0.02631578947368421, "term": "maintain", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 5.426106408720018e-07}, {"x": 0.6743817374762207, "y": 0.0063411540900443885, "ox": 0.6743817374762207, "oy": 0.0063411540900443885, "term": "hashing", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.019340519974635383, "os": -0.048514186777852374, "bg": 2.9491043570067767e-05}, {"x": 0.8845909955611921, "y": 0.8408370323398858, "ox": 0.8845909955611921, "oy": 0.8408370323398858, "term": "tl", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 26, "s": 0.5355104629042485, "os": 0.025281792162821115, "bg": 3.438023319372347e-05}, {"x": 0.8021559923906151, "y": 0.8478123018389346, "ox": 0.8021559923906151, "oy": 0.8478123018389346, "term": "interest", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 15, "s": 0.8864933417882055, "os": 0.09215222076958411, "bg": 1.1617881246996985e-06}, {"x": 0.7901077996195307, "y": 0.7365250475586557, "ox": 0.7901077996195307, "oy": 0.7365250475586557, "term": "sufficient", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 14, "s": 0.4762206721623335, "os": 0.02047630825765874, "bg": 4.3194560249860374e-06}, {"x": 0.26093849080532655, "y": 0.2872542802790108, "ox": 0.26093849080532655, "oy": 0.2872542802790108, "term": "sufficient training", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 0.0}, {"x": 0.3649334178820545, "y": 0.42422320862396956, "ox": 0.3649334178820545, "oy": 0.42422320862396956, "term": "approximate", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 4.732879088302717e-06}, {"x": 0.4568801521876982, "y": 0.6020925808497146, "ox": 0.4568801521876982, "oy": 0.6020925808497146, "term": "representative", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 5, "s": 0.6509194673430565, "os": 0.03455708278859405, "bg": 1.6643712767598448e-06}, {"x": 0.07102092580849714, "y": 0.16328471781864298, "ox": 0.07102092580849714, "oy": 0.16328471781864298, "term": "capacity", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 3.6766329428190843e-07}, {"x": 0.6746987951807228, "y": 0.5386810399492707, "ox": 0.6746987951807228, "oy": 0.5386810399492707, "term": "comparing", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 9, "s": 0.24128091312618896, "os": 0.0029327585597682046, "bg": 6.432633442197407e-06}, {"x": 0.2612555485098288, "y": 0.06436271401395054, "ox": 0.2612555485098288, "oy": 0.06436271401395054, "term": "comparing with", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.7574508560558022, "y": 0.4245402663284718, "ox": 0.7574508560558022, "oy": 0.4245402663284718, "term": "simulation", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 12, "s": 0.05041217501585289, "os": -0.026412494258153417, "bg": 3.0495741208748717e-06}, {"x": 0.6750158528852251, "y": 0.0015852885225110971, "ox": 0.6750158528852251, "oy": 0.0015852885225110971, "term": "roughness", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.016804058338617627, "os": -0.051729620861453655, "bg": 2.0152351779452664e-05}, {"x": 0.6325301204819277, "y": 0.287571337983513, "ox": 0.6325301204819277, "oy": 0.287571337983513, "term": "measurement", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.09701965757767915, "os": -0.0140807745309353, "bg": 1.7396350970414364e-06}, {"x": 0.7064045656309448, "y": 0.2878883956880152, "ox": 0.7064045656309448, "oy": 0.2878883956880152, "term": "index", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 10, "s": 0.05643627140139506, "os": -0.02506978551994629, "bg": 1.645703503441092e-07}, {"x": 0.5285351934051997, "y": 0.11001902346227013, "ox": 0.5285351934051997, "oy": 0.11001902346227013, "term": "surface", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 6, "s": 0.10589727330374128, "os": -0.012738065792728175, "bg": 5.173804613338298e-07}, {"x": 0.528852251109702, "y": 0.34210526315789475, "ox": 0.528852251109702, "oy": 0.34210526315789475, "term": "fitting", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.19625871908687384, "os": 0.00012367054167697344, "bg": 4.506835543980352e-06}, {"x": 0.36525047558655677, "y": 0.28820545339251746, "ox": 0.36525047558655677, "oy": 0.28820545339251746, "term": "relationship between", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.5291693088142042, "y": 0.1636017755231452, "ox": 0.5291693088142042, "oy": 0.1636017755231452, "term": "virtual", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 6, "s": 0.11953075459733671, "os": -0.00952263170912689, "bg": 5.482180592935814e-07}, {"x": 0.261572606214331, "y": 0.34242232086239693, "ox": 0.261572606214331, "oy": 0.34242232086239693, "term": "digital", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 1.3821859520612596e-07}, {"x": 0.2618896639188332, "y": 0.11033608116677235, "ox": 0.2618896639188332, "oy": 0.11033608116677235, "term": "entity", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 1.126829569788302e-06}, {"x": 0.6753329105897273, "y": 0.03963221306277743, "ox": 0.6753329105897273, "oy": 0.03963221306277743, "term": "actual", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 9, "s": 0.03202282815472415, "os": -0.035652450443447226, "bg": 6.544528163243344e-07}, {"x": 0.5878249841471148, "y": 0.6899175649968294, "ox": 0.5878249841471148, "oy": 0.6899175649968294, "term": "adapted", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 7, "s": 0.7213062777425492, "os": 0.042860676301190774, "bg": 9.43098306566965e-06}, {"x": 0.6328471781864299, "y": 0.7587190868738111, "ox": 0.6328471781864299, "oy": 0.7587190868738111, "term": "relative", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 8, "s": 0.8240329740012683, "os": 0.06308964347549556, "bg": 3.1253199732353548e-06}, {"x": 0.45719720989220036, "y": 0.5142675967025999, "ox": 0.45719720989220036, "oy": 0.5142675967025999, "term": "degree", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 6.2443699385386e-07}, {"x": 0.777425491439442, "y": 0.3427393785668992, "ox": 0.777425491439442, "oy": 0.3427393785668992, "term": "color", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 13, "s": 0.028852251109701965, "os": -0.03833786791986149, "bg": 4.24041139623284e-07}, {"x": 0.8142041851616995, "y": 0.06467977171845275, "ox": 0.8142041851616995, "oy": 0.06467977171845275, "term": "pixel", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 16, "s": 0.009194673430564362, "os": -0.07089855482138441, "bg": 4.86504799148705e-06}, {"x": 0.36556753329105895, "y": 0.06499682942295498, "ox": 0.36556753329105895, "oy": 0.06499682942295498, "term": "by comparing", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.15821179454660747, "y": 0.16391883322764744, "ox": 0.15821179454660747, "oy": 0.16391883322764744, "term": "recommendation systems", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.15852885225110971, "y": 0.5824350031705771, "ox": 0.15852885225110971, "oy": 0.5824350031705771, "term": "incorporating", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7549143944197845, "os": 0.04782516518850924, "bg": 8.981636402828958e-06}, {"x": 0.15884590995561193, "y": 0.3430564362714014, "ox": 0.15884590995561193, "oy": 0.3430564362714014, "term": "been applied", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 0.0}, {"x": 0.07133798351299937, "y": 0.2219403931515536, "ox": 0.07133798351299937, "oy": 0.2219403931515536, "term": "arises", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 5.223915293168739e-06}, {"x": 0.15916296766011415, "y": 0.11065313887127458, "ox": 0.15916296766011415, "oy": 0.11065313887127458, "term": "add", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 4.6456093068898026e-08}, {"x": 0.26220672162333547, "y": 0.0653138871274572, "ox": 0.26220672162333547, "oy": 0.0653138871274572, "term": "unavailable", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.8524134838000782e-06}, {"x": 0.15948002536461636, "y": 0.28852251109701965, "ox": 0.15948002536461636, "oy": 0.28852251109701965, "term": "forgetting", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 1.3332925938374105e-05}, {"x": 0.15979708306911858, "y": 0.4901712111604312, "ox": 0.15979708306911858, "oy": 0.4901712111604312, "term": "which uses", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6540900443880786, "os": 0.0349634288541041, "bg": 0.0}, {"x": 0.588142041851617, "y": 0.2888395688015219, "ox": 0.588142041851617, "oy": 0.2888395688015219, "term": "adaption", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 8.123360395273159e-05}, {"x": 0.6756499682942295, "y": 0.7105263157894737, "ox": 0.6756499682942295, "oy": 0.7105263157894737, "term": "multitask", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 9, "s": 0.684844641724794, "os": 0.03830253347938235, "bg": 0.0002325179494430567}, {"x": 0.5884590995561192, "y": 0.7894736842105263, "ox": 0.5884590995561192, "oy": 0.7894736842105263, "term": "assume", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 7, "s": 0.8766645529486367, "os": 0.08466131938800747, "bg": 4.015657046825573e-06}, {"x": 0.5294863665187064, "y": 0.2222574508560558, "ox": 0.5294863665187064, "oy": 0.2222574508560558, "term": "commonly used", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 0.0}, {"x": 0.3658845909955612, "y": 0.6204819277108434, "ox": 0.3658845909955612, "oy": 0.6204819277108434, "term": "old", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7232086239695625, "os": 0.043267022366700825, "bg": 2.500936187947916e-07}, {"x": 0.6331642358909322, "y": 0.6207989854153456, "ox": 0.6331642358909322, "oy": 0.6207989854153456, "term": "weakly", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 8, "s": 0.4949270767279645, "os": 0.02128900038867885, "bg": 3.605808086459817e-05}, {"x": 0.5887761572606214, "y": 0.5145846544071021, "ox": 0.5887761572606214, "oy": 0.5145846544071021, "term": "weakly supervised", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 0.0}, {"x": 0.6334812935954344, "y": 0.5599239061509195, "ox": 0.6334812935954344, "oy": 0.5599239061509195, "term": "detectors", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 8, "s": 0.35954343690551677, "os": 0.01164269813787499, "bg": 1.0261389155290336e-05}, {"x": 0.5298034242232086, "y": 0.3433734939759036, "ox": 0.5298034242232086, "oy": 0.3433734939759036, "term": "bounding", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.19625871908687384, "os": 0.00012367054167697344, "bg": 3.310800415213323e-05}, {"x": 0.26252377932783766, "y": 0.06563094483195941, "ox": 0.26252377932783766, "oy": 0.06563094483195941, "term": "boxes", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 6.918118494686598e-07}, {"x": 0.7067216233354471, "y": 0.34369055168040585, "ox": 0.7067216233354471, "oy": 0.34369055168040585, "term": "annotations", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 10, "s": 0.06626506024096385, "os": -0.021854351436345003, "bg": 1.8190306991747666e-05}, {"x": 0.8779327837666455, "y": 0.9331008243500317, "ox": 0.8779327837666455, "oy": 0.9331008243500317, "term": "previous", "cat25k": 17, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 122, "ncat": 25, "s": 0.9908053265694357, "os": 0.24942581534221409, "bg": 1.45599545673951e-06}, {"x": 0.5301204819277109, "y": 0.4904882688649334, "ox": 0.5301204819277109, "oy": 0.4904882688649334, "term": "previous work", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 6, "s": 0.3823715916296766, "os": 0.012985406876082115, "bg": 0.0}, {"x": 0.2628408370323399, "y": 0.38998097653772984, "ox": 0.2628408370323399, "oy": 0.38998097653772984, "term": "similarities", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 1.088893252527775e-05}, {"x": 0.07165504121750159, "y": 0.34400760938490804, "ox": 0.07165504121750159, "oy": 0.34400760938490804, "term": "by incorporating", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 0.0}, {"x": 0.1601141407736208, "y": 0.424857324032974, "ox": 0.1601141407736208, "oy": 0.424857324032974, "term": "knowledge about", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 0.0}, {"x": 0.4575142675967026, "y": 0.1109701965757768, "ox": 0.4575142675967026, "oy": 0.1109701965757768, "term": "exhibit", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 1.5011794579353567e-06}, {"x": 0.7305009511731135, "y": 0.2891566265060241, "ox": 0.7305009511731135, "oy": 0.2891566265060241, "term": "transferable", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 11, "s": 0.04216867469879518, "os": -0.030564291014451787, "bg": 2.2815138170106406e-05}, {"x": 0.6759670259987318, "y": 0.8443246670894102, "ox": 0.6759670259987318, "oy": 0.8443246670894102, "term": "detector", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 9, "s": 0.9213696892834496, "os": 0.1219038196530158, "bg": 1.585493788828082e-05}, {"x": 0.6337983512999366, "y": 0.6211160431198478, "ox": 0.6337983512999366, "oy": 0.6211160431198478, "term": "found", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 8, "s": 0.4949270767279645, "os": 0.02128900038867885, "bg": 2.497450823955316e-07}, {"x": 0.4578313253012048, "y": 0.7368421052631579, "ox": 0.4578313253012048, "oy": 0.7368421052631579, "term": "strong", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 5, "s": 0.8414711477488903, "os": 0.0699268577082082, "bg": 1.1764681853182489e-06}, {"x": 0.8392517438173748, "y": 0.7590361445783133, "ox": 0.8392517438173748, "oy": 0.7590361445783133, "term": "combined", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 19, "s": 0.2403297400126823, "os": 0.0026500830359351274, "bg": 3.5249419739681705e-06}, {"x": 0.45814838300570704, "y": 0.7672796448953709, "ox": 0.45814838300570704, "oy": 0.7672796448953709, "term": "surveillance", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 5, "s": 0.8731769181991123, "os": 0.08278859404261332, "bg": 7.573113440411478e-06}, {"x": 0.7577679137603044, "y": 0.6369689283449588, "ox": 0.7577679137603044, "oy": 0.6369689283449588, "term": "cameras", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 12, "s": 0.23906150919467342, "os": 0.002526412494258154, "bg": 1.056156164990107e-06}, {"x": 0.2631578947368421, "y": 0.16423589093214966, "ox": 0.2631578947368421, "oy": 0.16423589093214966, "term": "places", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 4.236700856393231e-07}, {"x": 0.6341154090044389, "y": 0.006658211794546607, "ox": 0.6341154090044389, "oy": 0.006658211794546607, "term": "stations", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.024413443246670895, "os": -0.04301968128334688, "bg": 6.968247245800276e-07}, {"x": 0.3662016487000634, "y": 0.2894736842105263, "ox": 0.3662016487000634, "oy": 0.2894736842105263, "term": "match", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 5.208006186069748e-07}, {"x": 0.26347495244134433, "y": 0.3902980342422321, "ox": 0.26347495244134433, "oy": 0.3902980342422321, "term": "pedestrians", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 2.1067829985421063e-05}, {"x": 0.7308180088776157, "y": 0.7511097019657578, "ox": 0.7308180088776157, "oy": 0.7511097019657578, "term": "camera", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 11, "s": 0.7257450856055803, "os": 0.0433906929083778, "bg": 9.82626976397836e-07}, {"x": 0.5890932149651237, "y": 0.11128725428027901, "ox": 0.5890932149651237, "oy": 0.11128725428027901, "term": "illumination", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 7, "s": 0.08021559923906152, "os": -0.01823257128723367, "bg": 1.0998295656969442e-05}, {"x": 0.4584654407102093, "y": 0.62143310082435, "ox": 0.4584654407102093, "oy": 0.62143310082435, "term": "background", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 5, "s": 0.6794546607482562, "os": 0.03777251687219533, "bg": 9.915141263496367e-07}, {"x": 0.36651870640456563, "y": 0.06594800253646163, "ox": 0.36651870640456563, "oy": 0.06594800253646163, "term": "more challenging", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.160431198478123, "y": 0.49080532656943565, "ox": 0.160431198478123, "oy": 0.49080532656943565, "term": "importantly", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6540900443880786, "os": 0.0349634288541041, "bg": 5.640002614307093e-06}, {"x": 0.5894102726696259, "y": 0.6902346227013316, "ox": 0.5894102726696259, "oy": 0.6902346227013316, "term": "rather", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 7, "s": 0.7213062777425492, "os": 0.042860676301190774, "bg": 8.931489749349513e-07}, {"x": 0.45878249841471147, "y": 0.6217501585288523, "ox": 0.45878249841471147, "oy": 0.6217501585288523, "term": "rather than", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 5, "s": 0.6794546607482562, "os": 0.03777251687219533, "bg": 0.0}, {"x": 0.8604946100190235, "y": 0.8008877615726062, "ox": 0.8604946100190235, "oy": 0.8008877615726062, "term": "extracted", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 22, "s": 0.30500951173113505, "os": 0.008674605137627661, "bg": 2.5684585946128416e-05}, {"x": 0.5304375396322131, "y": 0.46163601775523144, "ox": 0.5304375396322131, "oy": 0.46163601775523144, "term": "consistently", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 5.308307846820283e-06}, {"x": 0.833861762840837, "y": 0.7371591629676602, "ox": 0.833861762840837, "oy": 0.7371591629676602, "term": "traffic", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 18, "s": 0.18579581483830057, "os": -0.001501713720363243, "bg": 1.705172892525493e-06}, {"x": 0.4590995561192137, "y": 0.11160431198478123, "ox": 0.4590995561192137, "oy": 0.11160431198478123, "term": "sign", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 9.896030979063165e-08}, {"x": 0.8341788205453392, "y": 0.5389980976537729, "ox": 0.8341788205453392, "oy": 0.5389980976537729, "term": "driving", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 18, "s": 0.02060875079264426, "os": -0.04651779089078125, "bg": 1.7384922245686902e-06}, {"x": 0.7777425491439443, "y": 0.015535827520608751, "ox": 0.7777425491439443, "oy": 0.015535827520608751, "term": "driver", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 13, "s": 0.01109701965757768, "os": -0.06406134058867179, "bg": 6.571602818650808e-07}, {"x": 0.676284083703234, "y": 0.0019023462270133164, "ox": 0.676284083703234, "oy": 0.0019023462270133164, "term": "autonomous driving", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.016804058338617627, "os": -0.051729620861453655, "bg": 0.0}, {"x": 0.634432466708941, "y": 0.01585288522511097, "ox": 0.634432466708941, "oy": 0.01585288522511097, "term": "fcn", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.030437539632213063, "os": -0.0365888131161443, "bg": 6.19582176310557e-05}, {"x": 0.7070386810399493, "y": 0.42517438173747624, "ox": 0.7070386810399493, "oy": 0.42517438173747624, "term": "combined with", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 10, "s": 0.09353202282815473, "os": -0.015423483269142432, "bg": 0.0}, {"x": 0.7073557387444515, "y": 0.7006975269499048, "ox": 0.7073557387444515, "oy": 0.7006975269499048, "term": "memory", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 10, "s": 0.593849080532657, "os": 0.029592593901275575, "bg": 5.259015316754187e-07}, {"x": 0.3668357641090679, "y": 0.06626506024096386, "ox": 0.3668357641090679, "oy": 0.06626506024096386, "term": "allocation", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 1.668445368466566e-06}, {"x": 0.8024730500951173, "y": 0.7108433734939759, "ox": 0.8024730500951173, "oy": 0.7108433734939759, "term": "point", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 15, "s": 0.271718452758402, "os": 0.005335500512349378, "bg": 4.89306981095181e-07}, {"x": 0.6347495244134432, "y": 0.7834495878249842, "ox": 0.6347495244134432, "oy": 0.7834495878249842, "term": "effect", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 8, "s": 0.8576410906785035, "os": 0.07595137980990072, "bg": 1.213308031992348e-06}, {"x": 0.2637920101458465, "y": 0.22257450856055802, "ox": 0.2637920101458465, "oy": 0.22257450856055802, "term": "sizes", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 1.0007396717098526e-06}, {"x": 0.4594166138237159, "y": 0.039949270767279645, "ox": 0.4594166138237159, "oy": 0.039949270767279645, "term": "average precision", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 0.0}, {"x": 0.5897273303741281, "y": 0.3443246670894103, "ox": 0.5897273303741281, "oy": 0.3443246670894103, "term": "special", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 7, "s": 0.14679771718452758, "os": -0.005370834952828522, "bg": 1.472135294336952e-07}, {"x": 0.36715282181357006, "y": 0.22289156626506024, "ox": 0.36715282181357006, "oy": 0.22289156626506024, "term": "former", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 5.514496359562808e-07}, {"x": 0.3674698795180723, "y": 0.3906150919467343, "ox": 0.3674698795180723, "oy": 0.3906150919467343, "term": "latter", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 2.241696493489307e-06}, {"x": 0.7076727964489538, "y": 0.7184527584020292, "ox": 0.7076727964489538, "oy": 0.7184527584020292, "term": "making", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 10, "s": 0.6636017755231453, "os": 0.03602346206847814, "bg": 6.268604091810425e-07}, {"x": 0.5900443880786304, "y": 0.5393151553582752, "ox": 0.5900443880786304, "oy": 0.5393151553582752, "term": "mobile", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 7, "s": 0.3937856689917565, "os": 0.01392176954877919, "bg": 3.2939673355787896e-07}, {"x": 0.8782498414711477, "y": 0.8040583386176284, "ox": 0.8782498414711477, "oy": 0.8040583386176284, "term": "attribute", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 25, "s": 0.15884590995561193, "os": -0.0045934772622875375, "bg": 8.338539216118818e-06}, {"x": 0.26410906785034877, "y": 0.16455294863665187, "ox": 0.26410906785034877, "oy": 0.16455294863665187, "term": "build an", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.45973367152821815, "y": 0.2897907419150285, "ox": 0.45973367152821815, "oy": 0.2897907419150285, "term": "naturally", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 2.7753298340741306e-06}, {"x": 0.3677869372225745, "y": 0.06658211794546608, "ox": 0.3677869372225745, "oy": 0.06658211794546608, "term": "stable", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 1.0836160984391538e-06}, {"x": 0.264426125554851, "y": 0.22320862396956245, "ox": 0.264426125554851, "oy": 0.22320862396956245, "term": "strongly", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 1.440235536119577e-06}, {"x": 0.46005072923272033, "y": 0.6220672162333545, "ox": 0.46005072923272033, "oy": 0.6220672162333545, "term": "errors", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 5, "s": 0.6794546607482562, "os": 0.03777251687219533, "bg": 1.351552960342837e-06}, {"x": 0.16074825618262523, "y": 0.1648700063411541, "ox": 0.16074825618262523, "oy": 0.1648700063411541, "term": "encourage", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 8.988499439679416e-07}, {"x": 0.16106531388712747, "y": 0.22352568167406467, "ox": 0.16106531388712747, "oy": 0.22352568167406467, "term": "iterative algorithm", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 0.0}, {"x": 0.0719720989220038, "y": 0.4254914394419784, "ox": 0.0719720989220038, "oy": 0.4254914394419784, "term": "e", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 4.719230510049074e-08}, {"x": 0.07228915662650602, "y": 0.2238427393785669, "ox": 0.07228915662650602, "oy": 0.2238427393785669, "term": "platforms", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 1.4937823922043682e-06}, {"x": 0.36810399492707674, "y": 0.42580849714648067, "ox": 0.36810399492707674, "oy": 0.42580849714648067, "term": "products", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 8.200506612344646e-08}, {"x": 0.3684210526315789, "y": 0.29010779961953076, "ox": 0.3684210526315789, "oy": 0.29010779961953076, "term": "increased", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 5.62494720334154e-07}, {"x": 0.7079898541534559, "y": 0.7187698161065313, "ox": 0.7079898541534559, "oy": 0.7187698161065313, "term": "large amount", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 10, "s": 0.6636017755231453, "os": 0.03602346206847814, "bg": 0.0}, {"x": 0.4603677869372226, "y": 0.5149017121116043, "ox": 0.4603677869372226, "oy": 0.5149017121116043, "term": "users '", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 0.0}, {"x": 0.36873811033608117, "y": 0.22415979708306913, "ox": 0.36873811033608117, "oy": 0.22415979708306913, "term": "squares", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 5.566239643315364e-06}, {"x": 0.36905516804058336, "y": 0.22447685478757134, "ox": 0.36905516804058336, "oy": 0.22447685478757134, "term": "least squares", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 0.0}, {"x": 0.2647431832593532, "y": 0.728281547241598, "ox": 0.2647431832593532, "oy": 0.728281547241598, "term": "ratings", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 3, "s": 0.8617628408370324, "os": 0.07770043461361789, "bg": 1.0311232685057802e-06}, {"x": 0.1613823715916297, "y": 0.42612555485098286, "ox": 0.1613823715916297, "oy": 0.42612555485098286, "term": "records", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 2.9411122851377777e-07}, {"x": 0.46068484464172477, "y": 0.02663284717818643, "ox": 0.46068484464172477, "oy": 0.02663284717818643, "term": "competing", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 2.6317182283623783e-06}, {"x": 0.3693722257450856, "y": 0.3446417247939125, "ox": 0.3693722257450856, "oy": 0.3446417247939125, "term": "satisfactory", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 4.2758657452271e-06}, {"x": 0.1616994292961319, "y": 0.1651870640456563, "ox": 0.1616994292961319, "oy": 0.1651870640456563, "term": "helpful", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 4.3066666187191114e-07}, {"x": 0.26506024096385544, "y": 0.637285986049461, "ox": 0.26506024096385544, "oy": 0.637285986049461, "term": "referred", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 3, "s": 0.7786937222574509, "os": 0.051976961944807595, "bg": 1.715259628901521e-06}, {"x": 0.26537729866835763, "y": 0.5827520608750792, "ox": 0.26537729866835763, "oy": 0.5827520608750792, "term": "referred to", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.715282181357007, "os": 0.042330659694003736, "bg": 0.0}, {"x": 0.16201648700063412, "y": 0.11192136968928346, "ox": 0.16201648700063412, "oy": 0.11192136968928346, "term": "mostly", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 6.665359021972616e-07}, {"x": 0.461001902346227, "y": 0.29042485732403295, "ox": 0.461001902346227, "oy": 0.29042485732403295, "term": "contribute", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 1.3341626488358497e-06}, {"x": 0.2656943563728599, "y": 0.6024096385542169, "ox": 0.2656943563728599, "oy": 0.6024096385542169, "term": "hypotheses", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 3, "s": 0.7371591629676602, "os": 0.04554609377760503, "bg": 2.1410704793856804e-05}, {"x": 0.16233354470513633, "y": 0.22479391249207356, "ox": 0.16233354470513633, "oy": 0.22479391249207356, "term": "source hypotheses", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 0.0}, {"x": 0.5903614457831325, "y": 0.49112238427393784, "ox": 0.5903614457831325, "oy": 0.49112238427393784, "term": "fail", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.2932783766645529, "os": 0.007490901381576619, "bg": 1.448499203391279e-06}, {"x": 0.26601141407736206, "y": 0.11223842739378567, "ox": 0.26601141407736206, "oy": 0.11223842739378567, "term": "analogical", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 5.830223880597015e-05}, {"x": 0.6350665821179454, "y": 0.7190868738110336, "ox": 0.6350665821179454, "oy": 0.7190868738110336, "term": "particularly", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 8, "s": 0.7504755865567534, "os": 0.04701247305748913, "bg": 1.7456485346153376e-06}, {"x": 0.2663284717818643, "y": 0.16550412175015852, "ox": 0.2663284717818643, "oy": 0.16550412175015852, "term": "three real", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.6766011414077362, "y": 0.6223842739378567, "ox": 0.6766011414077362, "oy": 0.6223842739378567, "term": "datasets demonstrate", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 9, "s": 0.4150285351934052, "os": 0.015794494894173353, "bg": 0.0}, {"x": 0.36968928344958785, "y": 0.2907419150285352, "ox": 0.36968928344958785, "oy": 0.2907419150285352, "term": "affective computing", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.9391249207355739, "y": 0.8953709575142677, "ox": 0.9391249207355739, "oy": 0.8953709575142677, "term": "language", "cat25k": 11, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 49, "s": 0.07102092580849714, "os": -0.020705982120773125, "bg": 1.8306438112660716e-06}, {"x": 0.8145212428662016, "y": 0.29105897273303744, "ox": 0.8145212428662016, "oy": 0.29105897273303744, "term": "natural language", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 16, "s": 0.014267596702599873, "os": -0.05803681848697926, "bg": 0.0}, {"x": 0.6769181991122384, "y": 0.22511097019657578, "ox": 0.6769181991122384, "oy": 0.22511097019657578, "term": "language processing", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 9, "s": 0.06277742549143944, "os": -0.022790714109042085, "bg": 0.0}, {"x": 0.5906785034876347, "y": 0.11255548509828789, "ox": 0.5906785034876347, "oy": 0.11255548509828789, "term": "emotions", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 7, "s": 0.08021559923906152, "os": -0.01823257128723367, "bg": 3.839990345167133e-06}, {"x": 0.07260621433100824, "y": 0.225428027901078, "ox": 0.07260621433100824, "oy": 0.225428027901078, "term": "text mining", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.5307545973367153, "y": 0.2257450856055802, "ox": 0.5307545973367153, "oy": 0.2257450856055802, "term": "decision making", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 0.0}, {"x": 0.5909955611921369, "y": 0.6376030437539633, "ox": 0.5909955611921369, "oy": 0.6376030437539633, "term": "taken", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 7, "s": 0.6036778693722258, "os": 0.02999893996678562, "bg": 6.647507556582869e-07}, {"x": 0.37000634115409003, "y": 0.22606214331008243, "ox": 0.37000634115409003, "oy": 0.22606214331008243, "term": "regard", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 1.3773979173319676e-06}, {"x": 0.8541534559289791, "y": 0.8411540900443881, "ox": 0.8541534559289791, "oy": 0.8411540900443881, "term": "sentiment", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 21, "s": 0.7856689917564997, "os": 0.05275431963534859, "bg": 5.499881641060629e-05}, {"x": 0.16265060240963855, "y": 0.22637920101458464, "ox": 0.16265060240963855, "oy": 0.22637920101458464, "term": "subsequently", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 2.925093150920979e-06}, {"x": 0.7780596068484464, "y": 0.6525047558655676, "ox": 0.7780596068484464, "oy": 0.6525047558655676, "term": "sentiment analysis", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 13, "s": 0.19879518072289157, "os": 0.0002473410833539469, "bg": 0.0}, {"x": 0.46131896005072925, "y": 0.7010145846544071, "ox": 0.46131896005072925, "oy": 0.7010145846544071, "term": "resulting", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 5, "s": 0.8021559923906152, "os": 0.05706512137380305, "bg": 2.899285919468438e-06}, {"x": 0.8849080532656943, "y": 0.8281547241597971, "ox": 0.8849080532656943, "oy": 0.8281547241597971, "term": "face recognition", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 26, "s": 0.31135066582117943, "os": 0.009204621744814678, "bg": 0.0}, {"x": 0.16296766011414077, "y": 0.4264426125554851, "ox": 0.16296766011414077, "oy": 0.4264426125554851, "term": "biometrics", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 2.0082740892477003e-05}, {"x": 0.790424857324033, "y": 0.7837666455294864, "ox": 0.790424857324033, "oy": 0.7837666455294864, "term": "verification", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 14, "s": 0.722574508560558, "os": 0.04298434684286774, "bg": 8.579996546551389e-06}, {"x": 0.5913126188966392, "y": 0.7285986049461002, "ox": 0.5913126188966392, "oy": 0.7285986049461002, "term": "wild", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 7, "s": 0.7980342422320863, "os": 0.055722412635595915, "bg": 1.8422546548111441e-06}, {"x": 0.7580849714648066, "y": 0.7013316423589093, "ox": 0.7580849714648066, "oy": 0.7013316423589093, "term": "1", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 12, "s": 0.4543436905516804, "os": 0.01860358291226459, "bg": 0.0}, {"x": 0.8148383005707038, "y": 0.8909321496512366, "ox": 0.8148383005707038, "oy": 0.8909321496512366, "term": "videos", "cat25k": 10, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 16, "s": 0.9467343056436271, "os": 0.15096639694710434, "bg": 1.9300088443185514e-06}, {"x": 0.6353836398224477, "y": 0.4619530754597337, "ox": 0.6353836398224477, "oy": 0.4619530754597337, "term": "face verification", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 0.0}, {"x": 0.46163601775523144, "y": 0.667723525681674, "ox": 0.46163601775523144, "oy": 0.667723525681674, "term": "contrast", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 5, "s": 0.7520608750792644, "os": 0.04741881912299919, "bg": 3.1891442409800964e-06}, {"x": 0.3703233988585923, "y": 0.06689917564996829, "ox": 0.3703233988585923, "oy": 0.06689917564996829, "term": "ijb", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 8.662058451570432e-05}, {"x": 0.07292327203551047, "y": 0.5152187698161065, "ox": 0.07292327203551047, "oy": 0.5152187698161065, "term": "templates", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 1, "s": 0.7266962587190869, "os": 0.043673368432210875, "bg": 1.7694273758309542e-06}, {"x": 0.16328471781864298, "y": 0.6027266962587191, "ox": 0.16328471781864298, "oy": 0.6027266962587191, "term": "in contrast", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 0.0}, {"x": 0.7083069118579581, "y": 0.5602409638554217, "ox": 0.7083069118579581, "oy": 0.5602409638554217, "term": "template", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 10, "s": 0.2159162967660114, "os": 0.0006536871488639975, "bg": 2.312946827252362e-06}, {"x": 0.2666455294863665, "y": 0.560558021559924, "ox": 0.2666455294863665, "oy": 0.560558021559924, "term": "media", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 3, "s": 0.6889663918833228, "os": 0.039115225610402454, "bg": 1.9384906486264945e-07}, {"x": 0.37064045656309447, "y": 0.06721623335447051, "ox": 0.37064045656309447, "oy": 0.06721623335447051, "term": "template adaptation", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.4619530754597337, "y": 0.3449587824984147, "ox": 0.4619530754597337, "oy": 0.3449587824984147, "term": "evaluations", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 5.549182666087471e-06}, {"x": 0.1636017755231452, "y": 0.6030437539632213, "ox": 0.1636017755231452, "oy": 0.6030437539632213, "term": "svms", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 0.00015620229617375374}, {"x": 0.8715916296766011, "y": 0.8183259353202282, "ox": 0.8715916296766011, "oy": 0.8183259353202282, "term": "size", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 24, "s": 0.349714648065948, "os": 0.010547330483021783, "bg": 6.416363281172702e-07}, {"x": 0.8947368421052632, "y": 0.9071020925808497, "ox": 0.8947368421052632, "oy": 0.9071020925808497, "term": "negative", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 28, "s": 0.9267596702599873, "os": 0.12683297409985517, "bg": 7.5482784800695356e-06}, {"x": 0.3709575142675967, "y": 0.6379201014584654, "ox": 0.3709575142675967, "oy": 0.6379201014584654, "term": "convolutional networks", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 4, "s": 0.7457197209892201, "os": 0.046482456450302106, "bg": 0.0}, {"x": 0.6357006975269499, "y": 0.6033608116677235, "ox": 0.6357006975269499, "oy": 0.6033608116677235, "term": "disease", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 8, "s": 0.45085605580215593, "os": 0.018073566305077567, "bg": 9.214787700423894e-07}, {"x": 0.4622701331642359, "y": 0.5608750792644261, "ox": 0.4622701331642359, "oy": 0.5608750792644261, "term": "benefit", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.5798985415345593, "os": 0.02812621462139147, "bg": 9.596443474600976e-07}, {"x": 0.4625871908687381, "y": 0.6036778693722258, "ox": 0.4625871908687381, "oy": 0.6036778693722258, "term": "labeled samples", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 5, "s": 0.6509194673430565, "os": 0.03455708278859405, "bg": 0.0}, {"x": 0.26696258719086874, "y": 0.06753329105897274, "ox": 0.26696258719086874, "oy": 0.06753329105897274, "term": "small set", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.267279644895371, "y": 0.06785034876347495, "ox": 0.267279644895371, "oy": 0.06785034876347495, "term": "initially", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.624444789588825e-06}, {"x": 0.4629042485732403, "y": 0.6227013316423589, "ox": 0.4629042485732403, "oy": 0.6227013316423589, "term": "low level", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 5, "s": 0.6794546607482562, "os": 0.03777251687219533, "bg": 0.0}, {"x": 0.46322130627774255, "y": 0.2913760304375396, "ox": 0.46322130627774255, "oy": 0.2913760304375396, "term": "level features", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 0.0}, {"x": 0.5310716550412174, "y": 0.04026632847178187, "ox": 0.5310716550412174, "oy": 0.04026632847178187, "term": "diseases", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 9.055652665996493e-07}, {"x": 0.5916296766011414, "y": 0.16582117945466074, "ox": 0.5916296766011414, "oy": 0.16582117945466074, "term": "targets", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 7, "s": 0.09511731135066583, "os": -0.015017137203632385, "bg": 1.9992630716317964e-06}, {"x": 0.26759670259987317, "y": 0.4914394419784401, "ox": 0.26759670259987317, "oy": 0.4914394419784401, "term": "seen", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.5916296766011414, "os": 0.0294689233595986, "bg": 4.5803746392748223e-07}, {"x": 0.5313887127457197, "y": 0.7289156626506024, "ox": 0.5313887127457197, "oy": 0.7289156626506024, "term": "easily", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 6, "s": 0.8161065313887128, "os": 0.06121691813010141, "bg": 1.7069618319778198e-06}, {"x": 0.6772352568167407, "y": 0.5396322130627774, "ox": 0.6772352568167407, "oy": 0.5396322130627774, "term": "researchers", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 9, "s": 0.24128091312618896, "os": 0.0029327585597682046, "bg": 2.7805535782730106e-06}, {"x": 0.6775523145212429, "y": 0.22669625871908688, "ox": 0.6775523145212429, "oy": 0.22669625871908688, "term": "massive", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 9, "s": 0.06277742549143944, "os": -0.022790714109042085, "bg": 1.8287442013569281e-06}, {"x": 0.5317057704502219, "y": 0.016169942929613188, "ox": 0.5317057704502219, "oy": 0.016169942929613188, "term": "vision applications", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 0.0}, {"x": 0.3712745719720989, "y": 0.3909321496512365, "ox": 0.3712745719720989, "oy": 0.3909321496512365, "term": "subspaces", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 7.161448134330863e-05}, {"x": 0.5919467343056436, "y": 0.7840837032339886, "ox": 0.5919467343056436, "oy": 0.7840837032339886, "term": "reuse", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 7, "s": 0.8703233988585922, "os": 0.08144588530440619, "bg": 2.152412316103273e-05}, {"x": 0.2679137603043754, "y": 0.16613823715916295, "ox": 0.2679137603043754, "oy": 0.16613823715916295, "term": "reviews", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 7.144829022877469e-08}, {"x": 0.07324032974001268, "y": 0.39124920735573876, "ox": 0.07324032974001268, "oy": 0.39124920735573876, "term": "taxonomy", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 6.68918176642252e-06}, {"x": 0.2682308180088776, "y": 0.6382371591629676, "ox": 0.2682308180088776, "oy": 0.6382371591629676, "term": "define", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 3, "s": 0.7786937222574509, "os": 0.051976961944807595, "bg": 1.3767310913487788e-06}, {"x": 0.4635383639822448, "y": 0.5155358275206088, "ox": 0.4635383639822448, "oy": 0.5155358275206088, "term": "third", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 4.3877911883186385e-07}, {"x": 0.37159162967660114, "y": 0.040583386176284084, "ox": 0.37159162967660114, "oy": 0.040583386176284084, "term": "go", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 4.272314993683062e-08}, {"x": 0.7584020291693088, "y": 0.2270133164235891, "ox": 0.7584020291693088, "oy": 0.2270133164235891, "term": "semantic segmentation", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 12, "s": 0.027901077996195307, "os": -0.039274230592558565, "bg": 0.0}, {"x": 0.7783766645529486, "y": 0.8043753963221306, "ox": 0.7783766645529486, "oy": 0.8043753963221306, "term": "future", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 13, "s": 0.8167406467977173, "os": 0.061340588671778384, "bg": 9.010831248527343e-07}, {"x": 0.5320228281547241, "y": 0.5399492707672796, "ox": 0.5320228281547241, "oy": 0.5399492707672796, "term": "directions", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.4663918833227647, "os": 0.019416275043284685, "bg": 1.0429561203646636e-06}, {"x": 0.8608116677235257, "y": 0.0409004438807863, "ox": 0.8608116677235257, "oy": 0.0409004438807863, "term": "micro -", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 22, "s": 0.0022194039315155357, "os": -0.10708102187201865, "bg": 0.0}, {"x": 0.8611287254280279, "y": 0.04121750158528852, "ox": 0.8611287254280279, "oy": 0.04121750158528852, "term": "- expression", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 22, "s": 0.0022194039315155357, "os": -0.10708102187201865, "bg": 0.0}, {"x": 0.7086239695624603, "y": 0.5402663284717819, "ox": 0.7086239695624603, "oy": 0.5402663284717819, "term": "detecting", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 10, "s": 0.173430564362714, "os": -0.002561746934737291, "bg": 1.705799497168215e-05}, {"x": 0.26854787571337985, "y": 0.29169308814204187, "ox": 0.26854787571337985, "oy": 0.29169308814204187, "term": "lies", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 1.5842830348452695e-06}, {"x": 0.5323398858592264, "y": 0.49175649968294227, "ox": 0.5323398858592264, "oy": 0.49175649968294227, "term": "intensity", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 6, "s": 0.3823715916296766, "os": 0.012985406876082115, "bg": 4.358361500974042e-06}, {"x": 0.5922637920101459, "y": 0.016487000634115408, "ox": 0.5922637920101459, "oy": 0.016487000634115408, "term": "clips", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 6.046426457662666e-07}, {"x": 0.6360177552314521, "y": 0.42675967025998734, "ox": 0.6360177552314521, "oy": 0.42675967025998734, "term": "desired", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 8, "s": 0.15916296766011415, "os": -0.004434472280131441, "bg": 2.7071273054331207e-06}, {"x": 0.26886493341788203, "y": 0.06816740646797717, "ox": 0.26886493341788203, "oy": 0.06816740646797717, "term": "details", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 6.404355371531385e-08}, {"x": 0.5925808497146481, "y": 0.016804058338617627, "ox": 0.5925808497146481, "oy": 0.016804058338617627, "term": "video clips", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 0.0}, {"x": 0.16391883322764744, "y": 0.1128725428027901, "ox": 0.16391883322764744, "oy": 0.1128725428027901, "term": "much more", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.463855421686747, "y": 0.06848446417247939, "ox": 0.463855421686747, "oy": 0.06848446417247939, "term": "difficulties", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 2.058576610302839e-06}, {"x": 0.2691819911223843, "y": 0.4920735573874445, "ox": 0.2691819911223843, "oy": 0.4920735573874445, "term": "encode", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.5916296766011414, "os": 0.0294689233595986, "bg": 1.4608035961738309e-05}, {"x": 0.16423589093214966, "y": 0.22733037412809132, "ox": 0.16423589093214966, "oy": 0.22733037412809132, "term": "small sample", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 0.0}, {"x": 0.5928979074191503, "y": 0.1664552948636652, "ox": 0.5928979074191503, "oy": 0.1664552948636652, "term": "long term", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 7, "s": 0.09511731135066583, "os": -0.015017137203632385, "bg": 0.0}, {"x": 0.4641724793912492, "y": 0.603994927076728, "ox": 0.4641724793912492, "oy": 0.603994927076728, "term": "frame", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 5, "s": 0.6509194673430565, "os": 0.03455708278859405, "bg": 1.0796594788552792e-06}, {"x": 0.8151553582752061, "y": 0.42707672796448953, "ox": 0.8151553582752061, "oy": 0.42707672796448953, "term": "lstm", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 16, "s": 0.019974635383639822, "os": -0.0483905162361754, "bg": 0.0002511790256852207}, {"x": 0.3719086873811034, "y": 0.3452758402029169, "ox": 0.3719086873811034, "oy": 0.3452758402029169, "term": "long short", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 0.0}, {"x": 0.4644895370957514, "y": 0.5611921369689283, "ox": 0.4644895370957514, "oy": 0.5611921369689283, "term": "short term", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.5798985415345593, "os": 0.02812621462139147, "bg": 0.0}, {"x": 0.46480659480025366, "y": 0.34559289790741915, "ox": 0.46480659480025366, "oy": 0.34559289790741915, "term": "term memory", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 0.0}, {"x": 0.16455294863665187, "y": 0.5405833861762841, "ox": 0.16455294863665187, "oy": 0.5405833861762841, "term": "steps", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 2, "s": 0.7089410272669626, "os": 0.04139429702130666, "bg": 9.873927356944786e-07}, {"x": 0.26949904882688647, "y": 0.16677235256816741, "ox": 0.26949904882688647, "oy": 0.16677235256816741, "term": "regarded", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 3.08836297810842e-06}, {"x": 0.2698161065313887, "y": 0.16708941027266963, "ox": 0.2698161065313887, "oy": 0.16708941027266963, "term": "regarded as", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.6778693722257451, "y": 0.5830691185795814, "ox": 0.6778693722257451, "oy": 0.5830691185795814, "term": "big data", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 9, "s": 0.33259353202282815, "os": 0.009363626726970775, "bg": 0.0}, {"x": 0.8468611287254281, "y": 0.8284717818642993, "ox": 0.8468611287254281, "oy": 0.8284717818642993, "term": "performed", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 20, "s": 0.7143310082435004, "os": 0.04217165471184764, "bg": 4.541573109425765e-06}, {"x": 0.5326569435637286, "y": 0.3459099556119214, "ox": 0.5326569435637286, "oy": 0.3459099556119214, "term": "collected from", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.19625871908687384, "os": 0.00012367054167697344, "bg": 0.0}, {"x": 0.5329740012682308, "y": 0.06880152187698162, "ox": 0.5329740012682308, "oy": 0.06880152187698162, "term": "vgg", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.09099556119213698, "os": -0.01595349987632946, "bg": 0.00010394380105156478}, {"x": 0.0735573874445149, "y": 0.39156626506024095, "ox": 0.0735573874445149, "oy": 0.39156626506024095, "term": "video surveillance", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 0.0}, {"x": 0.27013316423589095, "y": 0.11318960050729232, "ox": 0.27013316423589095, "oy": 0.11318960050729232, "term": "yielding", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 9.997335710033277e-06}, {"x": 0.7089410272669626, "y": 0.16740646797717185, "ox": 0.7089410272669626, "oy": 0.16740646797717185, "term": "biometric", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 10, "s": 0.03772986683576411, "os": -0.03150065368714887, "bg": 2.039864620984654e-05}, {"x": 0.1648700063411541, "y": 0.29201014584654406, "ox": 0.1648700063411541, "oy": 0.29201014584654406, "term": "designing", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 2.4729890348696607e-06}, {"x": 0.3722257450856056, "y": 0.5409004438807863, "ox": 0.3722257450856056, "oy": 0.5409004438807863, "term": "abundant", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.6049461001902346, "os": 0.030405286032295677, "bg": 1.1436668901355461e-05}, {"x": 0.46512365250475585, "y": 0.04153455928979074, "ox": 0.46512365250475585, "oy": 0.04153455928979074, "term": "scenarios where", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 0.0}, {"x": 0.3725428027901078, "y": 0.06911857958148383, "ox": 0.3725428027901078, "oy": 0.06911857958148383, "term": "equal", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 5.703978462233644e-07}, {"x": 0.6781864299302474, "y": 0.4622701331642359, "ox": 0.6781864299302474, "oy": 0.4622701331642359, "term": "competition", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 9, "s": 0.13823715916296767, "os": -0.006713543691035655, "bg": 1.175948479140719e-06}, {"x": 0.37285986049461, "y": 0.3462270133164236, "ox": 0.37285986049461, "oy": 0.3462270133164236, "term": "error rate", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 0.0}, {"x": 0.7092580849714648, "y": 0.6385542168674698, "ox": 0.7092580849714648, "oy": 0.6385542168674698, "term": "life", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 10, "s": 0.39124920735573876, "os": 0.013515423483269139, "bg": 2.0861167305065646e-07}, {"x": 0.8471781864299303, "y": 0.6905516804058338, "ox": 0.8471781864299303, "oy": 0.6905516804058338, "term": "product", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 20, "s": 0.045656309448319596, "os": -0.028567895127380658, "bg": 2.3037592570775044e-07}, {"x": 0.6785034876347495, "y": 0.7111604311984782, "ox": 0.6785034876347495, "oy": 0.7111604311984782, "term": "predicting", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 9, "s": 0.684844641724794, "os": 0.03830253347938235, "bg": 2.8998887853463136e-05}, {"x": 0.6363348129359544, "y": 0.515852885225111, "ox": 0.6363348129359544, "oy": 0.515852885225111, "term": "early", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 8, "s": 0.26981610653138866, "os": 0.005211829970672419, "bg": 4.788311731064471e-07}, {"x": 0.1651870640456563, "y": 0.3918833227647432, "ox": 0.1651870640456563, "oy": 0.3918833227647432, "term": "determining", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 2.19802838423954e-06}, {"x": 0.16550412175015852, "y": 0.5412175015852885, "ox": 0.16550412175015852, "oy": 0.5412175015852885, "term": "consumer", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 2, "s": 0.7089410272669626, "os": 0.04139429702130666, "bg": 5.596736572361164e-07}, {"x": 0.6788205453392517, "y": 0.22764743183259353, "ox": 0.6788205453392517, "oy": 0.22764743183259353, "term": "integrated", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 9, "s": 0.06277742549143944, "os": -0.022790714109042085, "bg": 9.516008410882635e-07}, {"x": 0.8246670894102727, "y": 0.5161699429296132, "ox": 0.8246670894102727, "oy": 0.5161699429296132, "term": "historical", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 17, "s": 0.024096385542168676, "os": -0.04423871947987703, "bg": 1.5445011499981138e-06}, {"x": 0.27045022194039314, "y": 0.3465440710209258, "ox": 0.27045022194039314, "oy": 0.3465440710209258, "term": "employs", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 8.763808475729571e-06}, {"x": 0.5932149651236525, "y": 0.16772352568167406, "ox": 0.5932149651236525, "oy": 0.16772352568167406, "term": "advanced", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 7, "s": 0.09511731135066583, "os": -0.015017137203632385, "bg": 1.727582591721477e-07}, {"x": 0.4654407102092581, "y": 0.346861128725428, "ox": 0.4654407102092581, "oy": 0.346861128725428, "term": "moving", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 6.154227834322417e-07}, {"x": 0.533291058972733, "y": 0.22796448953709575, "ox": 0.533291058972733, "oy": 0.22796448953709575, "term": "industry", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 1.8628504891100243e-07}, {"x": 0.07387444514901712, "y": 0.16804058338617628, "ox": 0.07387444514901712, "oy": 0.16804058338617628, "term": "much attention", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.2707672796448954, "y": 0.8186429930247305, "ox": 0.2707672796448954, "oy": 0.8186429930247305, "term": "boosting", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 3, "s": 0.9261255548509829, "os": 0.12593194586763717, "bg": 6.030582591969525e-05}, {"x": 0.8154724159797083, "y": 0.7897907419150285, "ox": 0.8154724159797083, "oy": 0.7897907419150285, "term": "part", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 16, "s": 0.6578947368421053, "os": 0.03521076993745803, "bg": 3.6308394791342923e-07}, {"x": 0.37317691819911225, "y": 0.06943563728598605, "ox": 0.37317691819911225, "oy": 0.06943563728598605, "term": "unrelated", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 6.34055300401135e-06}, {"x": 0.2710843373493976, "y": 0.11350665821179455, "ox": 0.2710843373493976, "oy": 0.11350665821179455, "term": "ignore", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 1.3713238493033023e-06}, {"x": 0.4657577679137603, "y": 0.5164870006341155, "ox": 0.4657577679137603, "oy": 0.5164870006341155, "term": "attempt", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.49714648065948003, "os": 0.0216953464541889, "bg": 1.6481699486112382e-06}, {"x": 0.16582117945466074, "y": 0.11382371591629677, "ox": 0.16582117945466074, "oy": 0.11382371591629677, "term": "discovered", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.1058246549135938e-06}, {"x": 0.7786937222574508, "y": 0.6230183893468612, "ox": 0.7786937222574508, "oy": 0.6230183893468612, "term": "ones", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 13, "s": 0.1436271401395054, "os": -0.006183527083848631, "bg": 1.848378541184089e-06}, {"x": 0.37349397590361444, "y": 0.2923272035510463, "ox": 0.37349397590361444, "oy": 0.2923272035510463, "term": "empirical results", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.2714013950538998, "y": 0.1683576410906785, "ox": 0.2714013950538998, "oy": 0.1683576410906785, "term": "several real", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.07419150285351934, "y": 0.5168040583386176, "ox": 0.07419150285351934, "oy": 0.5168040583386176, "term": "evaluating", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 1, "s": 0.7266962587190869, "os": 0.043673368432210875, "bg": 4.285253170866782e-06}, {"x": 0.5935320228281548, "y": 0.017121116043119847, "ox": 0.5935320228281548, "oy": 0.017121116043119847, "term": "options", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 1.45022024386077e-07}, {"x": 0.3738110336081167, "y": 0.5615091946734305, "ox": 0.3738110336081167, "oy": 0.5615091946734305, "term": "generating", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 4, "s": 0.6385542168674699, "os": 0.033620720115896965, "bg": 4.845199722986717e-06}, {"x": 0.4660748256182625, "y": 0.026949904882688648, "ox": 0.4660748256182625, "oy": 0.026949904882688648, "term": "artificial neural", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 0.0}, {"x": 0.07450856055802156, "y": 0.2926442612555485, "ox": 0.07450856055802156, "oy": 0.2926442612555485, "term": "cover", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 2.3223256715094405e-07}, {"x": 0.709575142675967, "y": 0.7292327203551047, "ox": 0.709575142675967, "oy": 0.7292327203551047, "term": "speed", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 10, "s": 0.6908687381103361, "os": 0.039238896152079435, "bg": 9.596161036584993e-07}, {"x": 0.46639188332276477, "y": 0.7450856055802156, "ox": 0.46639188332276477, "oy": 0.7450856055802156, "term": "suitable", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 5, "s": 0.851299936588459, "os": 0.07314229179180948, "bg": 3.514555698654965e-06}, {"x": 0.16613823715916295, "y": 0.5833861762840837, "ox": 0.16613823715916295, "oy": 0.5833861762840837, "term": "suitable for", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7549143944197845, "os": 0.04782516518850924, "bg": 0.0}, {"x": 0.7311350665821179, "y": 0.3922003804692454, "ox": 0.7311350665821179, "oy": 0.3922003804692454, "term": "satellite", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 11, "s": 0.05960684844641725, "os": -0.024133422847249217, "bg": 1.0795550393300676e-06}, {"x": 0.46670894102726695, "y": 0.7374762206721623, "ox": 0.46670894102726695, "oy": 0.7374762206721623, "term": "reference", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 5, "s": 0.8414711477488903, "os": 0.0699268577082082, "bg": 6.083210513031445e-07}, {"x": 0.5336081166772353, "y": 0.017438173747622066, "ox": 0.5336081166772353, "oy": 0.017438173747622066, "term": "satellite image", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 0.0}, {"x": 0.6366518706404566, "y": 0.7016487000634115, "ox": 0.6366518706404566, "oy": 0.7016487000634115, "term": "real time", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7035510462904249, "os": 0.04058160489028657, "bg": 0.0}, {"x": 0.4670259987317692, "y": 0.027266962587190868, "ox": 0.4670259987317692, "oy": 0.027266962587190868, "term": "image quality", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 0.0}, {"x": 0.271718452758402, "y": 0.29296131896005073, "ox": 0.271718452758402, "oy": 0.29296131896005073, "term": "quality assessment", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 0.0}, {"x": 0.1664552948636652, "y": 0.1686746987951807, "ox": 0.1664552948636652, "oy": 0.1686746987951807, "term": "lies in", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.16677235256816741, "y": 0.34717818642993026, "ox": 0.16677235256816741, "oy": 0.34717818642993026, "term": "low resolution", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 0.0}, {"x": 0.16708941027266963, "y": 0.6043119847812302, "ox": 0.16708941027266963, "oy": 0.6043119847812302, "term": "close", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 4.070908867042747e-07}, {"x": 0.7098922003804693, "y": 0.8081800887761572, "ox": 0.7098922003804693, "oy": 0.8081800887761572, "term": "zero", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 10, "s": 0.8690551680405834, "os": 0.08103953923889615, "bg": 3.4230708581803336e-06}, {"x": 0.4673430564362714, "y": 0.027584020291693087, "ox": 0.4673430564362714, "oy": 0.027584020291693087, "term": "opinion", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 3.1988896298662555e-07}, {"x": 0.37412809131261887, "y": 0.3925174381737476, "ox": 0.37412809131261887, "oy": 0.3925174381737476, "term": "detail", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 6.998324251258035e-07}, {"x": 0.27203551046290425, "y": 0.39283449587824987, "ox": 0.27203551046290425, "oy": 0.39283449587824987, "term": "service", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 5.771804561179714e-08}, {"x": 0.16740646797717185, "y": 0.16899175649968295, "ox": 0.16740646797717185, "oy": 0.16899175649968295, "term": "explores", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 4.445091452200264e-06}, {"x": 0.3744451490171211, "y": 0.6528218135700697, "ox": 0.3744451490171211, "oy": 0.6528218135700697, "term": "inductive", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 4, "s": 0.7663284717818644, "os": 0.0496978905339034, "bg": 4.145828758846354e-05}, {"x": 0.37476220672162336, "y": 0.8576410906785035, "ox": 0.37476220672162336, "oy": 0.8576410906785035, "term": "labeling", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 4, "s": 0.9578313253012049, "os": 0.1654535175435497, "bg": 2.9202981114478464e-05}, {"x": 0.2723525681674065, "y": 0.06975269499048826, "ox": 0.2723525681674065, "oy": 0.06975269499048826, "term": "where there", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.7790107799619531, "y": 0.8119847812301839, "ox": 0.7790107799619531, "oy": 0.8119847812301839, "term": "there are", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 13, "s": 0.8383005707038681, "os": 0.06777145683898095, "bg": 0.0}, {"x": 0.37507926442612555, "y": 0.293278376664553, "ox": 0.37507926442612555, "oy": 0.293278376664553, "term": "no labeled", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.5339251743817375, "y": 0.34749524413443245, "ox": 0.5339251743817375, "oy": 0.34749524413443245, "term": "separate", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.19625871908687384, "os": 0.00012367054167697344, "bg": 9.348532140226003e-07}, {"x": 0.2726696258719087, "y": 0.16930881420418517, "ox": 0.2726696258719087, "oy": 0.16930881420418517, "term": "cause", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 3.3265002482098397e-07}, {"x": 0.7314521242866202, "y": 0.8123018389346861, "ox": 0.7314521242866202, "oy": 0.8123018389346861, "term": "overcome", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 11, "s": 0.8636651870640456, "os": 0.07876046782799194, "bg": 1.4018794615610385e-05}, {"x": 0.7317691819911224, "y": 0.7771084337349398, "ox": 0.7317691819911224, "oy": 0.7771084337349398, "term": "to overcome", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 11, "s": 0.7999365884590997, "os": 0.05625242924278294, "bg": 0.0}, {"x": 0.593849080532657, "y": 0.4625871908687381, "ox": 0.593849080532657, "oy": 0.4625871908687381, "term": "overcome this", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 7, "s": 0.25935320228281544, "os": 0.00427546729797533, "bg": 0.0}, {"x": 0.6369689283449588, "y": 0.4629042485732403, "ox": 0.6369689283449588, "oy": 0.4629042485732403, "term": "propagation", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 1.0961552355114437e-05}, {"x": 0.2729866835764109, "y": 0.11414077362079898, "ox": 0.2729866835764109, "oy": 0.11414077362079898, "term": "label propagation", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.8474952441344324, "y": 0.0700697526949905, "ox": 0.8474952441344324, "oy": 0.0700697526949905, "term": "iris", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 20, "s": 0.0034876347495244133, "os": -0.09287657679940639, "bg": 1.0184980796414574e-05}, {"x": 0.8249841471147749, "y": 0.7194039315155358, "ox": 0.8249841471147749, "oy": 0.7194039315155358, "term": "sensor", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 17, "s": 0.17406467977171847, "os": -0.0024380763930603178, "bg": 6.576002650986808e-06}, {"x": 0.3753963221306278, "y": 0.3478123018389347, "ox": 0.3753963221306278, "oy": 0.3478123018389347, "term": "involved", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 5.030265430339207e-07}, {"x": 0.5342422320862397, "y": 0.39315155358275206, "ox": 0.5342422320862397, "oy": 0.39315155358275206, "term": "decrease", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 6, "s": 0.24381737476220672, "os": 0.0033391046252782552, "bg": 2.617267582821797e-06}, {"x": 0.37571337983513, "y": 0.07038681039949271, "ox": 0.37571337983513, "oy": 0.07038681039949271, "term": "involved in", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.46766011414077363, "y": 0.1696258719086874, "ox": 0.46766011414077363, "oy": 0.1696258719086874, "term": "overall performance", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 0.0}, {"x": 0.5941661382371591, "y": 0.1144578313253012, "ox": 0.5941661382371591, "oy": 0.1144578313253012, "term": "features extracted", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 7, "s": 0.08021559923906152, "os": -0.01823257128723367, "bg": 0.0}, {"x": 0.637285986049461, "y": 0.6908687381103361, "ox": 0.637285986049461, "oy": 0.6908687381103361, "term": "extracted from", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.6769181991122385, "os": 0.037366170806685285, "bg": 0.0}, {"x": 0.16772352568167406, "y": 0.4273937856689918, "ox": 0.16772352568167406, "oy": 0.4273937856689918, "term": "speed up", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 0.0}, {"x": 0.6791376030437539, "y": 0.836715282181357, "ox": 0.6791376030437539, "oy": 0.836715282181357, "term": "training set", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 9, "s": 0.9124920735573875, "os": 0.11225751740221193, "bg": 0.0}, {"x": 0.6794546607482562, "y": 0.8084971464806595, "ox": 0.6794546607482562, "oy": 0.8084971464806595, "term": "annotated", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 9, "s": 0.8792010145846544, "os": 0.08653404473340165, "bg": 1.6721587972033144e-05}, {"x": 0.4679771718452758, "y": 0.29359543436905516, "ox": 0.4679771718452758, "oy": 0.29359543436905516, "term": "fully convolutional", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 0.0}, {"x": 0.3760304375396322, "y": 0.22828154724159797, "ox": 0.3760304375396322, "oy": 0.22828154724159797, "term": "has shown", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 0.0}, {"x": 0.6376030437539633, "y": 0.6911857958148383, "ox": 0.6376030437539633, "oy": 0.6911857958148383, "term": "alleviate", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.6769181991122385, "os": 0.037366170806685285, "bg": 3.633918999945491e-05}, {"x": 0.46829422954977806, "y": 0.6046290424857323, "ox": 0.46829422954977806, "oy": 0.6046290424857323, "term": "lead", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 5, "s": 0.6509194673430565, "os": 0.03455708278859405, "bg": 7.293270851979187e-07}, {"x": 0.4686112872542803, "y": 0.6049461001902346, "ox": 0.4686112872542803, "oy": 0.6049461001902346, "term": "lead to", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 5, "s": 0.6509194673430565, "os": 0.03455708278859405, "bg": 0.0}, {"x": 0.2733037412809131, "y": 0.11477488902980343, "ox": 0.2733037412809131, "oy": 0.11477488902980343, "term": "computational cost", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.534559289790742, "y": 0.6233354470513633, "ox": 0.534559289790742, "oy": 0.6233354470513633, "term": "greatly", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.6236525047558656, "os": 0.03227801137768983, "bg": 4.539577550275822e-06}, {"x": 0.4689283449587825, "y": 0.6236525047558655, "ox": 0.4689283449587825, "oy": 0.6236525047558655, "term": "are usually", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 5, "s": 0.6794546607482562, "os": 0.03777251687219533, "bg": 0.0}, {"x": 0.46924540266328474, "y": 0.8088142041851617, "ox": 0.46924540266328474, "oy": 0.8088142041851617, "term": "concepts", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 5, "s": 0.9080532656943564, "os": 0.10851206671142363, "bg": 4.1610178681909175e-06}, {"x": 0.5348763474952442, "y": 0.2939124920735574, "ox": 0.5348763474952442, "oy": 0.2939124920735574, "term": "involves", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 2.4410747441829956e-06}, {"x": 0.27362079898541536, "y": 0.11509194673430564, "ox": 0.27362079898541536, "oy": 0.11509194673430564, "term": "complicated", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 2.3109811004499134e-06}, {"x": 0.07482561826252378, "y": 0.2942295497780596, "ox": 0.07482561826252378, "oy": 0.2942295497780596, "term": "tumor", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 2.8715479588319224e-06}, {"x": 0.3763474952441344, "y": 0.1699429296131896, "ox": 0.3763474952441344, "oy": 0.1699429296131896, "term": "testing samples", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.27393785668991755, "y": 0.07070386810399493, "ox": 0.27393785668991755, "oy": 0.07070386810399493, "term": "comparisons", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 2.4298016795869124e-06}, {"x": 0.2742549143944198, "y": 0.46322130627774255, "ox": 0.2742549143944198, "oy": 0.46322130627774255, "term": "are presented", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 0.0}, {"x": 0.6379201014584654, "y": 0.833861762840837, "ox": 0.6379201014584654, "oy": 0.833861762840837, "term": "annotation", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 8, "s": 0.9150285351934052, "os": 0.11453658881311615, "bg": 4.5890156687983964e-06}, {"x": 0.4695624603677869, "y": 0.29454660748256184, "ox": 0.4695624603677869, "oy": 0.29454660748256184, "term": "vision tasks", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 0.0}, {"x": 0.5351934051997463, "y": 0.75142675967026, "ox": 0.5351934051997463, "oy": 0.75142675967026, "term": "relationships", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 6, "s": 0.8446417247939125, "os": 0.07086322038090527, "bg": 2.592977956396283e-06}, {"x": 0.46987951807228917, "y": 0.3481293595434369, "ox": 0.46987951807228917, "oy": 0.3481293595434369, "term": "high quality", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 0.0}, {"x": 0.7587190868738111, "y": 0.8319594166138237, "ox": 0.7587190868738111, "oy": 0.8319594166138237, "term": "concept", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 12, "s": 0.8830057070386811, "os": 0.08934313275149289, "bg": 3.822977826477919e-06}, {"x": 0.6382371591629676, "y": 0.29486366518706403, "ox": 0.6382371591629676, "oy": 0.29486366518706403, "term": "drift", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.09701965757767915, "os": -0.0140807745309353, "bg": 8.737005221574092e-06}, {"x": 0.16804058338617628, "y": 0.29518072289156627, "ox": 0.16804058338617628, "oy": 0.29518072289156627, "term": "stream", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 6.181852163272194e-07}, {"x": 0.6385542168674698, "y": 0.7019657577679138, "ox": 0.6385542168674698, "oy": 0.7019657577679138, "term": "incremental", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7035510462904249, "os": 0.04058160489028657, "bg": 2.1056823945098242e-05}, {"x": 0.1683576410906785, "y": 0.11540900443880786, "ox": 0.1683576410906785, "oy": 0.11540900443880786, "term": "questions", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.1469745776844384e-07}, {"x": 0.6797717184527584, "y": 0.5837032339885859, "ox": 0.6797717184527584, "oy": 0.5837032339885859, "term": "namely", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 9, "s": 0.33259353202282815, "os": 0.009363626726970775, "bg": 8.277395629801166e-06}, {"x": 0.07514267596702599, "y": 0.5415345592897908, "ox": 0.07514267596702599, "oy": 0.5415345592897908, "term": "diversity", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 1, "s": 0.7492073557387445, "os": 0.04688880251581216, "bg": 1.867279867543465e-06}, {"x": 0.47019657577679136, "y": 0.17025998731769182, "ox": 0.47019657577679136, "oy": 0.17025998731769182, "term": "5", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 0.0}, {"x": 0.07545973367152822, "y": 0.17057704502219403, "ox": 0.07545973367152822, "oy": 0.17057704502219403, "term": "empirical studies", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.274571972098922, "y": 0.3484464172479391, "ox": 0.274571972098922, "oy": 0.3484464172479391, "term": "been shown", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.0}, {"x": 0.7320862396956246, "y": 0.7377932783766645, "ox": 0.7320862396956246, "oy": 0.7377932783766645, "term": "positive", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 11, "s": 0.6743817374762207, "os": 0.03695982474117522, "bg": 1.4903342773654736e-06}, {"x": 0.2748890298034242, "y": 0.34876347495244137, "ox": 0.2748890298034242, "oy": 0.34876347495244137, "term": "pair", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 1.0659520118302398e-06}, {"x": 0.27520608750792647, "y": 0.4635383639822448, "ox": 0.27520608750792647, "oy": 0.4635383639822448, "term": "coefficients", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 8.185502752375301e-06}, {"x": 0.37666455294863666, "y": 0.07102092580849714, "ox": 0.37666455294863666, "oy": 0.07102092580849714, "term": "fed", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 1.6344882245747593e-06}, {"x": 0.1686746987951807, "y": 0.11572606214331008, "ox": 0.1686746987951807, "oy": 0.11572606214331008, "term": "mapping between", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.07577679137603044, "y": 0.17089410272669625, "ox": 0.07577679137603044, "oy": 0.17089410272669625, "term": "starts", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 7.694073383507242e-07}, {"x": 0.16899175649968295, "y": 0.1160431198478123, "ox": 0.16899175649968295, "oy": 0.1160431198478123, "term": "correlation between", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.27552314521242866, "y": 0.3934686112872543, "ox": 0.27552314521242866, "oy": 0.3934686112872543, "term": "pairs", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 3.104622037103959e-06}, {"x": 0.7590361445783133, "y": 0.0022194039315155357, "ox": 0.7590361445783133, "oy": 0.0022194039315155357, "term": "bf", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 12, "s": 0.010145846544071021, "os": -0.06821313734497014, "bg": 4.842952142552296e-06}, {"x": 0.7324032974001268, "y": 0.04185161699429296, "ox": 0.7324032974001268, "oy": 0.04185161699429296, "term": "svr", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.020291693088142042, "os": -0.04664146143245822, "bg": 3.1998240096794674e-05}, {"x": 0.7327203551046291, "y": 0.0025364616360177552, "ox": 0.7327203551046291, "oy": 0.0025364616360177552, "term": "ls svr", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.011414077362079899, "os": -0.06271863185046465, "bg": 0.0}, {"x": 0.5944831959416614, "y": 0.29549778059606846, "ox": 0.5944831959416614, "oy": 0.29549778059606846, "term": "driven", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 1.8936646058211807e-06}, {"x": 0.5948002536461636, "y": 0.11636017755231452, "ox": 0.5948002536461636, "oy": 0.11636017755231452, "term": "data driven", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 7, "s": 0.08021559923906152, "os": -0.01823257128723367, "bg": 0.0}, {"x": 0.5355104629042485, "y": 0.04216867469879518, "ox": 0.5355104629042485, "oy": 0.04216867469879518, "term": "inverse", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 6.3757231011864926e-06}, {"x": 0.16930881420418517, "y": 0.22859860494610018, "ox": 0.16930881420418517, "oy": 0.22859860494610018, "term": "hot", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 1.379154081049826e-07}, {"x": 0.4705136334812936, "y": 0.17121116043119847, "ox": 0.4705136334812936, "oy": 0.17121116043119847, "term": "engineering", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 3.016398312984111e-07}, {"x": 0.3769816106531389, "y": 0.5171211160431198, "ox": 0.3769816106531389, "oy": 0.5171211160431198, "term": "cope", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 7.407779168176773e-06}, {"x": 0.3772986683576411, "y": 0.517438173747622, "ox": 0.3772986683576411, "oy": 0.517438173747622, "term": "cope with", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 0.0}, {"x": 0.1696258719086874, "y": 0.2958148383005707, "ox": 0.1696258719086874, "oy": 0.2958148383005707, "term": "controlled", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 1.0214375482230243e-06}, {"x": 0.37761572606214333, "y": 0.29613189600507295, "ox": 0.37761572606214333, "oy": 0.29613189600507295, "term": "identified", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 8.369957877589127e-07}, {"x": 0.2758402029169309, "y": 0.29644895370957514, "ox": 0.2758402029169309, "oy": 0.29644895370957514, "term": "compensate", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 9.41842664458408e-06}, {"x": 0.1699429296131896, "y": 0.2967660114140774, "ox": 0.1699429296131896, "oy": 0.2967660114140774, "term": "pseudo", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 6.606912647530294e-06}, {"x": 0.7102092580849715, "y": 0.07133798351299937, "ox": 0.7102092580849715, "oy": 0.07133798351299937, "term": "industrial", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.029803424223208624, "os": -0.03793152185435143, "bg": 4.837304372230813e-07}, {"x": 0.2761572606214331, "y": 0.6915028535193405, "ox": 0.2761572606214331, "oy": 0.6915028535193405, "term": "relevance", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 3, "s": 0.8291058972733039, "os": 0.06483869827921275, "bg": 6.925960290364915e-06}, {"x": 0.5358275206087508, "y": 0.5618262523779328, "ox": 0.5358275206087508, "oy": 0.5618262523779328, "term": "fields", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 1.17276720211878e-06}, {"x": 0.4708306911857958, "y": 0.29708306911857957, "ox": 0.4708306911857958, "oy": 0.29708306911857957, "term": "statistics", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 4.321188589590404e-07}, {"x": 0.47114774889029803, "y": 0.49239061509194676, "ox": 0.47114774889029803, "oy": 0.49239061509194676, "term": "unique", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 5, "s": 0.45244134432466704, "os": 0.01847991237058761, "bg": 6.31434704418548e-07}, {"x": 0.4714648065948003, "y": 0.2289156626506024, "ox": 0.4714648065948003, "oy": 0.2289156626506024, "term": "accelerate", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 8.162865492302419e-06}, {"x": 0.47178186429930247, "y": 0.11667723525681674, "ox": 0.47178186429930247, "oy": 0.11667723525681674, "term": "to accelerate", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 0.0}, {"x": 0.7907419150285352, "y": 0.6680405833861763, "ox": 0.7907419150285352, "oy": 0.6680405833861763, "term": "less", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 14, "s": 0.176284083703234, "os": -0.00203173032755026, "bg": 5.217591877176075e-07}, {"x": 0.6388712745719721, "y": 0.3937856689917565, "ox": 0.6388712745719721, "oy": 0.3937856689917565, "term": "explored", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 8, "s": 0.13221306277742548, "os": -0.007649906363732729, "bg": 9.712549804134292e-06}, {"x": 0.17025998731769182, "y": 0.11699429296131895, "ox": 0.17025998731769182, "oy": 0.11699429296131895, "term": "adopting", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 4.5433320292287694e-06}, {"x": 0.3779327837666455, "y": 0.5840202916930881, "ox": 0.3779327837666455, "oy": 0.5840202916930881, "term": "discriminant analysis", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 4, "s": 0.6718452758402029, "os": 0.03683615419949825, "bg": 0.0}, {"x": 0.27647431832593533, "y": 0.07165504121750159, "ox": 0.27647431832593533, "oy": 0.07165504121750159, "term": "case study", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.17057704502219403, "y": 0.541851616994293, "ox": 0.17057704502219403, "oy": 0.541851616994293, "term": "test set", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 2, "s": 0.7089410272669626, "os": 0.04139429702130666, "bg": 0.0}, {"x": 0.536144578313253, "y": 0.7593532022828154, "ox": 0.536144578313253, "oy": 0.7593532022828154, "term": "they are", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 6, "s": 0.8532022828154724, "os": 0.07407865446450655, "bg": 0.0}, {"x": 0.6391883322764743, "y": 0.702282815472416, "ox": 0.6391883322764743, "oy": 0.702282815472416, "term": "regions", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7035510462904249, "os": 0.04058160489028657, "bg": 2.4058879364207683e-06}, {"x": 0.37824984147114776, "y": 0.22923272035510464, "ox": 0.37824984147114776, "oy": 0.22923272035510464, "term": "modelling", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 3.8423756995155795e-06}, {"x": 0.2767913760304375, "y": 0.0719720989220038, "ox": 0.2767913760304375, "oy": 0.0719720989220038, "term": "verify that", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.7593532022828154, "y": 0.719720989220038, "ox": 0.7593532022828154, "oy": 0.719720989220038, "term": "series", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 12, "s": 0.5342422320862397, "os": 0.025034451079467154, "bg": 5.069568075272946e-07}, {"x": 0.5951173113506658, "y": 0.49270767279644895, "ox": 0.5951173113506658, "oy": 0.49270767279644895, "term": "time series", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.2932783766645529, "os": 0.007490901381576619, "bg": 0.0}, {"x": 0.6395053899809765, "y": 0.7200380469245403, "ox": 0.6395053899809765, "oy": 0.7200380469245403, "term": "extreme", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 8, "s": 0.7504755865567534, "os": 0.04701247305748913, "bg": 2.4727806167415602e-06}, {"x": 0.5364616360177552, "y": 0.562143310082435, "ox": 0.5364616360177552, "oy": 0.562143310082435, "term": "elm", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.5088776157260622, "os": 0.022631709126885974, "bg": 1.5054294253117258e-05}, {"x": 0.6398224476854788, "y": 0.6531388712745719, "ox": 0.6398224476854788, "oy": 0.6531388712745719, "term": "extreme learning", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 8, "s": 0.5783132530120482, "os": 0.027719868555881426, "bg": 0.0}, {"x": 0.07609384908053266, "y": 0.6239695624603678, "ox": 0.07609384908053266, "oy": 0.6239695624603678, "term": "assumption that", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.8113506658211795, "os": 0.059750538850217305, "bg": 0.0}, {"x": 0.7330374128091313, "y": 0.832276474318326, "ox": 0.7330374128091313, "oy": 0.832276474318326, "term": "are available", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 11, "s": 0.8912492073557388, "os": 0.09483763824599838, "bg": 0.0}, {"x": 0.07641090678503487, "y": 0.39410272669625873, "ox": 0.07641090678503487, "oy": 0.39410272669625873, "term": "reality", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 8.161356548039988e-07}, {"x": 0.0767279644895371, "y": 0.3944197844007609, "ox": 0.0767279644895371, "oy": 0.3944197844007609, "term": "serious", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 6.744912319123178e-07}, {"x": 0.07704502219403932, "y": 0.22954977805960686, "ox": 0.07704502219403932, "oy": 0.22954977805960686, "term": "adequate", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 1.3110599972520182e-06}, {"x": 0.27710843373493976, "y": 0.22986683576410907, "ox": 0.27710843373493976, "oy": 0.22986683576410907, "term": "forecasting", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 6.540227714378446e-06}, {"x": 0.5367786937222575, "y": 0.6683576410906785, "ox": 0.5367786937222575, "oy": 0.6683576410906785, "term": "performances", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 6, "s": 0.712428662016487, "os": 0.04192431362849369, "bg": 6.313569091122796e-06}, {"x": 0.17089410272669625, "y": 0.42771084337349397, "ox": 0.17089410272669625, "oy": 0.42771084337349397, "term": "characterized", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 4.841212271827614e-06}, {"x": 0.07736207989854153, "y": 0.6388712745719721, "ox": 0.07736207989854153, "oy": 0.6388712745719721, "term": "learner", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.8230818008877616, "os": 0.06296597293381859, "bg": 1.1316224513894355e-05}, {"x": 0.17121116043119847, "y": 0.4280279010779962, "ox": 0.17121116043119847, "oy": 0.4280279010779962, "term": "characterized by", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 0.0}, {"x": 0.27742549143944195, "y": 0.07228915662650602, "ox": 0.27742549143944195, "oy": 0.07228915662650602, "term": "sufficient labeled", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.1715282181357007, "y": 0.4283449587824984, "ox": 0.1715282181357007, "oy": 0.4283449587824984, "term": "learners", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 5.079103653331536e-06}, {"x": 0.07767913760304375, "y": 0.34908053265694355, "ox": 0.07767913760304375, "oy": 0.34908053265694355, "term": "distribution differences", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 0.0}, {"x": 0.17184527584020293, "y": 0.11731135066582118, "ox": 0.17184527584020293, "oy": 0.11731135066582118, "term": "seven", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 4.0489657625528567e-07}, {"x": 0.17216233354470514, "y": 0.1176284083703234, "ox": 0.17216233354470514, "oy": 0.1176284083703234, "term": "absence", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.2070178431436056e-06}, {"x": 0.4720989220038047, "y": 0.7295497780596069, "ox": 0.4720989220038047, "oy": 0.7295497780596069, "term": "validation", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 5, "s": 0.8360811667723526, "os": 0.06671142362460691, "bg": 7.899075097296857e-06}, {"x": 0.5370957514267597, "y": 0.11794546607482562, "ox": 0.5370957514267597, "oy": 0.11794546607482562, "term": "estimator", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 6, "s": 0.10589727330374128, "os": -0.012738065792728175, "bg": 1.4626734182171476e-05}, {"x": 0.640139505389981, "y": 0.006975269499048827, "ox": 0.640139505389981, "oy": 0.006975269499048827, "term": "multiobjective", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.024413443246670895, "os": -0.04301968128334688, "bg": 6.16463690288642e-05}, {"x": 0.17247939124920736, "y": 0.11826252377932783, "ox": 0.17247939124920736, "oy": 0.11826252377932783, "term": "vector regression", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.17279644895370957, "y": 0.39473684210526316, "ox": 0.17279644895370957, "oy": 0.39473684210526316, "term": "depends", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 1.4582669518064647e-06}, {"x": 0.4724159797083069, "y": 0.463855421686747, "ox": 0.4724159797083069, "oy": 0.463855421686747, "term": "feasible", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.4032974001268231, "os": 0.015264478286986322, "bg": 8.549860423528586e-06}, {"x": 0.07799619530754598, "y": 0.2301838934686113, "ox": 0.07799619530754598, "oy": 0.2301838934686113, "term": "depends on", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.47273303741280914, "y": 0.1715282181357007, "ox": 0.47273303741280914, "oy": 0.1715282181357007, "term": "completely", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 7.510984092024578e-07}, {"x": 0.37856689917564995, "y": 0.5843373493975904, "ox": 0.37856689917564995, "oy": 0.5843373493975904, "term": "dynamics", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 4, "s": 0.6718452758402029, "os": 0.03683615419949825, "bg": 3.3416800267276287e-06}, {"x": 0.1731135066582118, "y": 0.11857958148383006, "ox": 0.1731135066582118, "oy": 0.11857958148383006, "term": "outliers", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 2.641077559644335e-05}, {"x": 0.2777425491439442, "y": 0.07260621433100824, "ox": 0.2777425491439442, "oy": 0.07260621433100824, "term": "properly", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 8.423628389948853e-07}, {"x": 0.173430564362714, "y": 0.6052631578947368, "ox": 0.173430564362714, "oy": 0.6052631578947368, "term": "genetic", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 2.704754029361211e-06}, {"x": 0.595434369055168, "y": 0.5177552314521243, "ox": 0.595434369055168, "oy": 0.5177552314521243, "term": "illustrate", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 9.067866673973529e-06}, {"x": 0.0783132530120482, "y": 0.17184527584020293, "ox": 0.0783132530120482, "oy": 0.17184527584020293, "term": "adverse", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 1.5805521834669854e-06}, {"x": 0.47305009511731133, "y": 0.4641724793912492, "ox": 0.47305009511731133, "oy": 0.4641724793912492, "term": "caused by", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.4032974001268231, "os": 0.015264478286986322, "bg": 0.0}, {"x": 0.17374762206721622, "y": 0.17216233354470514, "ox": 0.17374762206721622, "oy": 0.17216233354470514, "term": "tensor", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 1.317986696242288e-05}, {"x": 0.27805960684844644, "y": 0.6534559289790742, "ox": 0.27805960684844644, "oy": 0.6534559289790742, "term": "assume that", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 3, "s": 0.7935954343690552, "os": 0.05519239602840889, "bg": 0.0}, {"x": 0.2783766645529486, "y": 0.5624603677869372, "ox": 0.2783766645529486, "oy": 0.5624603677869372, "term": "same feature", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 3, "s": 0.6889663918833228, "os": 0.039115225610402454, "bg": 0.0}, {"x": 0.07863031071655041, "y": 0.4644895370957514, "ox": 0.07863031071655041, "oy": 0.4644895370957514, "term": "covariance", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.674698795180723, "os": 0.0372425002650083, "bg": 2.522494343306435e-05}, {"x": 0.07894736842105263, "y": 0.5180722891566265, "ox": 0.07894736842105263, "oy": 0.5180722891566265, "term": "obtaining", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 1, "s": 0.7266962587190869, "os": 0.043673368432210875, "bg": 3.8140857552310466e-06}, {"x": 0.17406467977171844, "y": 0.2974001268230818, "ox": 0.17406467977171844, "oy": 0.2974001268230818, "term": "text categorization", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.07926442612555486, "y": 0.2305009511731135, "ox": 0.07926442612555486, "oy": 0.2305009511731135, "term": "image annotation", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.8157894736842105, "y": 0.5627774254914394, "ox": 0.8157894736842105, "oy": 0.5627774254914394, "term": "handwritten", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 16, "s": 0.037095751426759666, "os": -0.03231334581816897, "bg": 5.641238782852957e-05}, {"x": 0.6404565630944832, "y": 0.4930247305009512, "ox": 0.6404565630944832, "oy": 0.4930247305009512, "term": "offline", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 8, "s": 0.22859860494610018, "os": 0.00199639588707113, "bg": 2.013313076485939e-06}, {"x": 0.17438173747622068, "y": 0.3493975903614458, "ox": 0.17438173747622068, "oy": 0.3493975903614458, "term": "pixels", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 2.4499519573844047e-06}, {"x": 0.27869372225745087, "y": 0.349714648065948, "ox": 0.27869372225745087, "oy": 0.349714648065948, "term": "feature representations", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.0}, {"x": 0.3788839568801522, "y": 0.23081800887761572, "ox": 0.3788839568801522, "oy": 0.23081800887761572, "term": "been successfully", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 0.0}, {"x": 0.5374128091312619, "y": 0.6918199112238428, "ox": 0.5374128091312619, "oy": 0.6918199112238428, "term": "variable", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 6, "s": 0.7584020291693089, "os": 0.04835518179569627, "bg": 1.8313606431555443e-06}, {"x": 0.4733671528218136, "y": 0.07292327203551047, "ox": 0.4733671528218136, "oy": 0.07292327203551047, "term": "pooling", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 1.696477111053705e-05}, {"x": 0.3792010145846544, "y": 0.5183893468611287, "ox": 0.3792010145846544, "oy": 0.5183893468611287, "term": "adapting", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 2.3195102122235866e-05}, {"x": 0.5957514267596703, "y": 0.017755231452124286, "ox": 0.5957514267596703, "oy": 0.017755231452124286, "term": "writing", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 2.3981502874980506e-07}, {"x": 0.47368421052631576, "y": 0.17247939124920736, "ox": 0.47368421052631576, "oy": 0.17247939124920736, "term": "around", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 1.4521819279220896e-07}, {"x": 0.6800887761572606, "y": 0.7454026632847178, "ox": 0.6800887761572606, "oy": 0.7454026632847178, "term": "change", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 9, "s": 0.7758402029169309, "os": 0.051164269813787494, "bg": 3.8893495791844705e-07}, {"x": 0.7333544705136334, "y": 0.6781864299302474, "ox": 0.7333544705136334, "oy": 0.6781864299302474, "term": "understanding", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 11, "s": 0.4353202282815472, "os": 0.017667220239567502, "bg": 1.4509610238819319e-06}, {"x": 0.5377298668357641, "y": 0.297717184527584, "ox": 0.5377298668357641, "oy": 0.297717184527584, "term": "clear", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 3.608540639858307e-07}, {"x": 0.6407736207989854, "y": 0.018072289156626505, "ox": 0.6407736207989854, "oy": 0.018072289156626505, "term": "weather", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.030437539632213063, "os": -0.0365888131161443, "bg": 2.1243147853680212e-07}, {"x": 0.3795180722891566, "y": 0.17279644895370957, "ox": 0.3795180722891566, "oy": 0.17279644895370957, "term": "choose", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 2.659542992110865e-07}, {"x": 0.37983512999365887, "y": 0.07324032974001268, "ox": 0.37983512999365887, "oy": 0.07324032974001268, "term": "outdoor", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 3.724877424527372e-07}, {"x": 0.27901077996195306, "y": 0.518706404565631, "ox": 0.27901077996195306, "oy": 0.518706404565631, "term": "partially", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.62714013950539, "os": 0.03268435744319989, "bg": 5.091966270815422e-06}, {"x": 0.6804058338617628, "y": 0.42866201648700064, "ox": 0.6804058338617628, "oy": 0.42866201648700064, "term": "incomplete", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 9, "s": 0.11794546607482562, "os": -0.009928977774636936, "bg": 6.139354700799247e-06}, {"x": 0.38015218769816106, "y": 0.23113506658211794, "ox": 0.38015218769816106, "oy": 0.23113506658211794, "term": "synthesis", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 2.72815107745179e-06}, {"x": 0.5380469245402664, "y": 0.7596702599873177, "ox": 0.5380469245402664, "oy": 0.7596702599873177, "term": "we apply", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 6, "s": 0.8532022828154724, "os": 0.07407865446450655, "bg": 0.0}, {"x": 0.5960684844641725, "y": 0.42897907419150283, "ox": 0.5960684844641725, "oy": 0.42897907419150283, "term": "an unsupervised", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 7, "s": 0.2171845275840203, "os": 0.0010600332143740482, "bg": 0.0}, {"x": 0.2793278376664553, "y": 0.11889663918833228, "ox": 0.2793278376664553, "oy": 0.11889663918833228, "term": "carefully", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 1.1829685649763228e-06}, {"x": 0.3804692454026633, "y": 0.6391883322764743, "ox": 0.3804692454026633, "oy": 0.6391883322764743, "term": "come", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 4, "s": 0.7457197209892201, "os": 0.046482456450302106, "bg": 3.36445395006207e-07}, {"x": 0.7336715282181357, "y": 0.5630944831959417, "ox": 0.7336715282181357, "oy": 0.5630944831959417, "term": "ground", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 11, "s": 0.15821179454660747, "os": -0.004840818345641498, "bg": 9.522231866674306e-07}, {"x": 0.6807228915662651, "y": 0.46480659480025366, "ox": 0.6807228915662651, "oy": 0.46480659480025366, "term": "truth", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 9, "s": 0.13823715916296767, "os": -0.006713543691035655, "bg": 1.2929763556786816e-06}, {"x": 0.6810399492707673, "y": 0.46512365250475585, "ox": 0.6810399492707673, "oy": 0.46512365250475585, "term": "ground truth", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 9, "s": 0.13823715916296767, "os": -0.006713543691035655, "bg": 0.0}, {"x": 0.7105263157894737, "y": 0.8481293595434369, "ox": 0.7105263157894737, "oy": 0.8481293595434369, "term": "improves", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 10, "s": 0.9197844007609385, "os": 0.11962474824211158, "bg": 2.512499200348812e-05}, {"x": 0.07958148383005707, "y": 0.39505389980976535, "ox": 0.07958148383005707, "oy": 0.39505389980976535, "term": "significantly improves", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 0.0}, {"x": 0.2796448953709575, "y": 0.6395053899809765, "ox": 0.2796448953709575, "oy": 0.6395053899809765, "term": "code", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 3, "s": 0.7786937222574509, "os": 0.051976961944807595, "bg": 1.9961990135039987e-07}, {"x": 0.7108433734939759, "y": 0.5634115409004439, "ox": 0.7108433734939759, "oy": 0.5634115409004439, "term": "spectral", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 10, "s": 0.2159162967660114, "os": 0.0006536871488639975, "bg": 1.202971080790034e-05}, {"x": 0.3807863031071655, "y": 0.23145212428662015, "ox": 0.3807863031071655, "oy": 0.23145212428662015, "term": "spectral clustering", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 0.0}, {"x": 0.27996195307545974, "y": 0.5190234622701332, "ox": 0.27996195307545974, "oy": 0.5190234622701332, "term": "sharing", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.62714013950539, "os": 0.03268435744319989, "bg": 1.2495097318394282e-06}, {"x": 0.1746987951807229, "y": 0.1731135066582118, "ox": 0.1746987951807229, "oy": 0.1731135066582118, "term": "algorithm called", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.17501585288522511, "y": 0.29803424223208624, "ox": 0.17501585288522511, "oy": 0.29803424223208624, "term": "useful information", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.38110336081166774, "y": 0.35003170577045023, "ox": 0.38110336081166774, "oy": 0.35003170577045023, "term": "together with", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 0.0}, {"x": 0.7596702599873177, "y": 0.8091312618896639, "ox": 0.7596702599873177, "oy": 0.8091312618896639, "term": "unlabeled data", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 12, "s": 0.8430564362714014, "os": 0.07005052824988517, "bg": 0.0}, {"x": 0.3814204185161699, "y": 0.5421686746987951, "ox": 0.3814204185161699, "oy": 0.5421686746987951, "term": "when compared", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.6049461001902346, "os": 0.030405286032295677, "bg": 0.0}, {"x": 0.7599873176918199, "y": 0.5424857324032974, "ox": 0.7599873176918199, "oy": 0.5424857324032974, "term": "vehicle", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 12, "s": 0.10336081166772354, "os": -0.013550757923748276, "bg": 1.0679405355979563e-06}, {"x": 0.280279010779962, "y": 0.173430564362714, "ox": 0.280279010779962, "oy": 0.173430564362714, "term": "creating", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 5.627361397513254e-07}, {"x": 0.6410906785034877, "y": 0.17374762206721622, "ox": 0.6410906785034877, "oy": 0.17374762206721622, "term": "gathered", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 8, "s": 0.07133798351299936, "os": -0.020511642698137874, "bg": 3.6156929662003894e-06}, {"x": 0.38173747622067217, "y": 0.0424857324032974, "ox": 0.38173747622067217, "oy": 0.0424857324032974, "term": "demonstration", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 1.8791947149527836e-06}, {"x": 0.6813570069752695, "y": 0.7707672796448953, "ox": 0.6813570069752695, "oy": 0.7707672796448953, "term": "dynamic", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 9, "s": 0.8268864933417883, "os": 0.06402600614819265, "bg": 3.052661980604403e-06}, {"x": 0.474001268230818, "y": 0.04280279010779962, "ox": 0.474001268230818, "oy": 0.04280279010779962, "term": "domain adaption", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 0.0}, {"x": 0.8852251109701966, "y": 0.8046924540266328, "ox": 0.8852251109701966, "oy": 0.8046924540266328, "term": "tracking", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 26, "s": 0.1176284083703234, "os": -0.01008798275679304, "bg": 5.328046657391165e-06}, {"x": 0.5383639822447686, "y": 0.62428662016487, "ox": 0.5383639822447686, "oy": 0.62428662016487, "term": "existing methods", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.6236525047558656, "os": 0.03227801137768983, "bg": 0.0}, {"x": 0.47431832593532025, "y": 0.04311984781230184, "ox": 0.47431832593532025, "oy": 0.04311984781230184, "term": "population", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 3.190661011794342e-07}, {"x": 0.38205453392517436, "y": 0.17406467977171844, "ox": 0.38205453392517436, "oy": 0.17406467977171844, "term": "framework called", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.47463538363982244, "y": 0.17438173747622068, "ox": 0.47463538363982244, "oy": 0.17438173747622068, "term": "pool", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 4.88007323263127e-07}, {"x": 0.5386810399492707, "y": 0.7517438173747623, "ox": 0.5386810399492707, "oy": 0.7517438173747623, "term": "experience", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 6, "s": 0.8446417247939125, "os": 0.07086322038090527, "bg": 5.678278233638771e-07}, {"x": 0.17533291058972733, "y": 0.2317691819911224, "ox": 0.17533291058972733, "oy": 0.2317691819911224, "term": "integration", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 6.729155850950543e-07}, {"x": 0.4749524413443247, "y": 0.2320862396956246, "ox": 0.4749524413443247, "oy": 0.2320862396956246, "term": "without any", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 0.0}, {"x": 0.17564996829422955, "y": 0.3503487634749524, "ox": 0.17564996829422955, "oy": 0.3503487634749524, "term": "particle", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 3.0374587591957605e-06}, {"x": 0.17596702599873176, "y": 0.1746987951807229, "ox": 0.17596702599873176, "oy": 0.1746987951807229, "term": "chosen", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 9.269324200453583e-07}, {"x": 0.7111604311984782, "y": 0.4654407102092581, "ox": 0.7111604311984782, "oy": 0.4654407102092581, "term": "reasoning", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 10, "s": 0.10875079264426125, "os": -0.01220804918554115, "bg": 7.707443431379509e-06}, {"x": 0.5389980976537729, "y": 0.29835129993658843, "ox": 0.5389980976537729, "oy": 0.29835129993658843, "term": "case based", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 0.0}, {"x": 0.17628408370323398, "y": 0.1192136968928345, "ox": 0.17628408370323398, "oy": 0.1192136968928345, "term": "based reasoning", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.8614457831325302, "y": 0.605580215599239, "ox": 0.8614457831325302, "oy": 0.605580215599239, "term": "rl", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 22, "s": 0.013316423589093214, "os": -0.058849510617999354, "bg": 1.2450525834618777e-05}, {"x": 0.8161065313887127, "y": 0.8763474952441345, "ox": 0.8161065313887127, "oy": 0.8763474952441345, "term": "agent", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 16, "s": 0.9239061509194674, "os": 0.12524292427829406, "bg": 2.8952837101588687e-06}, {"x": 0.1766011414077362, "y": 0.2986683576410907, "ox": 0.1766011414077362, "oy": 0.2986683576410907, "term": "an agent", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.6414077362079899, "y": 0.4657577679137603, "ox": 0.6414077362079899, "oy": 0.4657577679137603, "term": "been used", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 0.0}, {"x": 0.47526949904882687, "y": 0.043436905516804056, "ox": 0.47526949904882687, "oy": 0.043436905516804056, "term": "soccer", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 6.925658355680414e-07}, {"x": 0.7339885859226379, "y": 0.729866835764109, "ox": 0.7339885859226379, "oy": 0.729866835764109, "term": "robots", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 11, "s": 0.6410906785034877, "os": 0.03374439065757394, "bg": 1.4894337390889895e-05}, {"x": 0.28059606848446417, "y": 0.4660748256182625, "ox": 0.28059606848446417, "oy": 0.4660748256182625, "term": "significant improvement", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 0.0}, {"x": 0.2809131261889664, "y": 0.17501585288522511, "ox": 0.2809131261889664, "oy": 0.17501585288522511, "term": "ridge", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 1.3559881979718116e-06}, {"x": 0.2812301838934686, "y": 0.11953075459733671, "ox": 0.2812301838934686, "oy": 0.11953075459733671, "term": "ridge regression", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.4755865567533291, "y": 0.3953709575142676, "ox": 0.4755865567533291, "oy": 0.3953709575142676, "term": "machine svm", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.3059606848446417, "os": 0.008833610119783751, "bg": 0.0}, {"x": 0.7910589727330374, "y": 0.8798351299936589, "ox": 0.7910589727330374, "oy": 0.8798351299936589, "term": "shot", "cat25k": 10, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 69, "ncat": 14, "s": 0.9410272669625872, "os": 0.1426628034345076, "bg": 3.966985409164793e-06}, {"x": 0.17691819911223844, "y": 0.11984781230183894, "ox": 0.17691819911223844, "oy": 0.11984781230183894, "term": "edges", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 2.2340805008709188e-06}, {"x": 0.641724793912492, "y": 0.777425491439442, "ox": 0.641724793912492, "oy": 0.777425491439442, "term": "zero shot", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 8, "s": 0.8503487634749525, "os": 0.07273594572629943, "bg": 0.0}, {"x": 0.3823715916296766, "y": 0.5846544071020926, "ox": 0.3823715916296766, "oy": 0.5846544071020926, "term": "whose", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 4, "s": 0.6718452758402029, "os": 0.03683615419949825, "bg": 1.048668045680345e-06}, {"x": 0.38268864933417884, "y": 0.2989854153455929, "ox": 0.38268864933417884, "oy": 0.2989854153455929, "term": "occurs", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 1.7040598251585931e-06}, {"x": 0.5393151553582752, "y": 0.2993024730500951, "ox": 0.5393151553582752, "oy": 0.2993024730500951, "term": "numerous", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 1.605134745795061e-06}, {"x": 0.07989854153455929, "y": 0.29961953075459735, "ox": 0.07989854153455929, "oy": 0.29961953075459735, "term": "case where", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.5396322130627774, "y": 0.692136968928345, "ox": 0.5396322130627774, "oy": 0.692136968928345, "term": "generalize", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 6, "s": 0.7584020291693089, "os": 0.04835518179569627, "bg": 8.074476956830567e-05}, {"x": 0.38300570703868103, "y": 0.35066582117945466, "ox": 0.38300570703868103, "oy": 0.35066582117945466, "term": "graphs", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 4.353443290669106e-06}, {"x": 0.3833227647431833, "y": 0.5193405199746354, "ox": 0.3833227647431833, "oy": 0.5193405199746354, "term": "derive", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 1.1480904672326369e-05}, {"x": 0.28154724159797084, "y": 0.5849714648065948, "ox": 0.28154724159797084, "oy": 0.5849714648065948, "term": "allows us", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.715282181357007, "os": 0.042330659694003736, "bg": 0.0}, {"x": 0.7343056436271401, "y": 0.7777425491439443, "ox": 0.7343056436271401, "oy": 0.7777425491439443, "term": "results demonstrate", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 11, "s": 0.7999365884590997, "os": 0.05625242924278294, "bg": 0.0}, {"x": 0.6420418516169943, "y": 0.0735573874445149, "ox": 0.6420418516169943, "oy": 0.0735573874445149, "term": "pruning", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.04946100190234623, "os": -0.026942510865340445, "bg": 1.8750167412209036e-05}, {"x": 0.38363982244768546, "y": 0.4933417882054534, "ox": 0.38363982244768546, "oy": 0.4933417882054534, "term": "must be", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 4, "s": 0.5206087507926442, "os": 0.023974417865093106, "bg": 0.0}, {"x": 0.4759036144578313, "y": 0.4936588459099556, "ox": 0.4759036144578313, "oy": 0.4936588459099556, "term": "hold", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 5, "s": 0.45244134432466704, "os": 0.01847991237058761, "bg": 6.249172277603169e-07}, {"x": 0.28186429930247303, "y": 0.3509828788839569, "ox": 0.28186429930247303, "oy": 0.3509828788839569, "term": "not hold", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.0}, {"x": 0.6423589093214965, "y": 0.6058972733037413, "ox": 0.6423589093214965, "oy": 0.6058972733037413, "term": "integrate", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 8, "s": 0.45085605580215593, "os": 0.018073566305077567, "bg": 7.199698949731059e-06}, {"x": 0.0802155992390615, "y": 0.6246036778693722, "ox": 0.0802155992390615, "oy": 0.6246036778693722, "term": "knowledge leverage", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.8113506658211795, "os": 0.059750538850217305, "bg": 0.0}, {"x": 0.08053265694356374, "y": 0.17533291058972733, "ox": 0.08053265694356374, "oy": 0.17533291058972733, "term": "leverage based", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.3839568801521877, "y": 0.4292961318960051, "ox": 0.3839568801521877, "oy": 0.4292961318960051, "term": "an ensemble", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 0.0}, {"x": 0.7346227013316423, "y": 0.7710843373493976, "ox": 0.7346227013316423, "oy": 0.7710843373493976, "term": "select", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 11, "s": 0.7875713379835131, "os": 0.05303699515918166, "bg": 5.56529238323309e-07}, {"x": 0.08084971464806595, "y": 0.29993658845909954, "ox": 0.08084971464806595, "oy": 0.29993658845909954, "term": "modify", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 1.0023758585975714e-06}, {"x": 0.2821813570069753, "y": 0.3512999365884591, "ox": 0.2821813570069753, "oy": 0.3512999365884591, "term": "candidate", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 1.2805013126053096e-06}, {"x": 0.7349397590361446, "y": 0.39568801521876984, "ox": 0.7349397590361446, "oy": 0.39568801521876984, "term": "despite", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 11, "s": 0.05960684844641725, "os": -0.024133422847249217, "bg": 1.62086581436844e-06}, {"x": 0.17723525681674066, "y": 0.17564996829422955, "ox": 0.17723525681674066, "oy": 0.17564996829422955, "term": "existence", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 1.23957655569027e-06}, {"x": 0.17755231452124287, "y": 0.46639188332276477, "ox": 0.17755231452124287, "oy": 0.46639188332276477, "term": "spectrum", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 2, "s": 0.6195307545973368, "os": 0.0317479947705028, "bg": 2.0510156821940945e-06}, {"x": 0.08116677235256817, "y": 0.17596702599873176, "ox": 0.08116677235256817, "oy": 0.17596702599873176, "term": "therefore it", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.3842739378566899, "y": 0.7599873176918199, "ox": 0.3842739378566899, "oy": 0.7599873176918199, "term": "imbalance", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 4, "s": 0.8776157260621433, "os": 0.08506766545351754, "bg": 4.6391475688425075e-05}, {"x": 0.38459099556119214, "y": 0.7675967025998732, "ox": 0.38459099556119214, "oy": 0.7675967025998732, "term": "ranking", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 4, "s": 0.8807863031071655, "os": 0.08828309953711883, "bg": 5.250763683187229e-06}, {"x": 0.28249841471147746, "y": 0.6686746987951807, "ox": 0.28249841471147746, "oy": 0.6686746987951807, "term": "class imbalance", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 3, "s": 0.8078630310716551, "os": 0.05840783011201017, "bg": 0.0}, {"x": 0.47622067216233355, "y": 0.17628408370323398, "ox": 0.47622067216233355, "oy": 0.17628408370323398, "term": "fundamental", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 1.6377438994354697e-06}, {"x": 0.1778693722257451, "y": 0.35161699429296134, "ox": 0.1778693722257451, "oy": 0.35161699429296134, "term": "by combining", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 0.0}, {"x": 0.08148383005707038, "y": 0.5196575776791376, "ox": 0.08148383005707038, "oy": 0.5196575776791376, "term": "predicted", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 1, "s": 0.7266962587190869, "os": 0.043673368432210875, "bg": 4.038758300390726e-06}, {"x": 0.1781864299302473, "y": 0.1766011414077362, "ox": 0.1781864299302473, "oy": 0.1766011414077362, "term": "2014", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.5963855421686747, "y": 0.3002536461636018, "ox": 0.5963855421686747, "oy": 0.3002536461636018, "term": "verified", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.12333544705136336, "os": -0.008586269036429811, "bg": 3.264312015999738e-06}, {"x": 0.5967025998731769, "y": 0.46670894102726695, "ox": 0.5967025998731769, "oy": 0.46670894102726695, "term": "pascal", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 7, "s": 0.25935320228281544, "os": 0.00427546729797533, "bg": 1.2459133301157129e-05}, {"x": 0.0818008877615726, "y": 0.17691819911223844, "ox": 0.0818008877615726, "oy": 0.17691819911223844, "term": "object categories", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.2828154724159797, "y": 0.3519340519974635, "ox": 0.2828154724159797, "oy": 0.3519340519974635, "term": "incorporates", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 7.497398001247247e-06}, {"x": 0.08211794546607483, "y": 0.23240329740012683, "ox": 0.08211794546607483, "oy": 0.23240329740012683, "term": "formal", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 8.886697626736305e-07}, {"x": 0.08243500317057705, "y": 0.6537729866835764, "ox": 0.08243500317057705, "oy": 0.6537729866835764, "term": "priors", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 1, "s": 0.8341788205453392, "os": 0.06618140701741988, "bg": 0.00010354892363051156}, {"x": 0.4765377298668358, "y": 0.30057070386810397, "ox": 0.4765377298668358, "oy": 0.30057070386810397, "term": "be easily", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 0.0}, {"x": 0.3849080532656944, "y": 0.3008877615726062, "ox": 0.3849080532656944, "oy": 0.3008877615726062, "term": "has recently", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.28313253012048195, "y": 0.30120481927710846, "ox": 0.28313253012048195, "oy": 0.30120481927710846, "term": "relies", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 7.228796410958189e-06}, {"x": 0.28344958782498414, "y": 0.17723525681674066, "ox": 0.28344958782498414, "oy": 0.17723525681674066, "term": "relies on", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.2837666455294864, "y": 0.35225110970196577, "ox": 0.2837666455294864, "oy": 0.35225110970196577, "term": "labelled data", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.0}, {"x": 0.17850348763474952, "y": 0.7203551046290425, "ox": 0.17850348763474952, "oy": 0.7203551046290425, "term": "gender", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 2, "s": 0.8671528218135701, "os": 0.0799795060245221, "bg": 2.103128892696532e-06}, {"x": 0.5399492707672796, "y": 0.6398224476854788, "ox": 0.5399492707672796, "oy": 0.6398224476854788, "term": "above", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 6, "s": 0.6585288522511097, "os": 0.035493445461291115, "bg": 3.940178798277832e-07}, {"x": 0.476854787571338, "y": 0.4670259987317692, "ox": 0.476854787571338, "oy": 0.4670259987317692, "term": "arabic", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.4032974001268231, "os": 0.015264478286986322, "bg": 3.730973629969301e-06}, {"x": 0.7793278376664553, "y": 0.6540900443880786, "ox": 0.7793278376664553, "oy": 0.6540900443880786, "term": "character", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 13, "s": 0.19879518072289157, "os": 0.0002473410833539469, "bg": 1.3777492915354815e-06}, {"x": 0.5402663284717819, "y": 0.23272035510462905, "ox": 0.5402663284717819, "oy": 0.23272035510462905, "term": "character recognition", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.14013950538998096, "os": -0.006307197625525604, "bg": 0.0}, {"x": 0.38522511097019657, "y": 0.17755231452124287, "ox": 0.38522511097019657, "oy": 0.17755231452124287, "term": "feature vector", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.3855421686746988, "y": 0.04375396322130628, "ox": 0.3855421686746988, "oy": 0.04375396322130628, "term": "significantly better", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 0.0}, {"x": 0.28408370323398857, "y": 0.640139505389981, "ox": 0.28408370323398857, "oy": 0.640139505389981, "term": "needed", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 3, "s": 0.7786937222574509, "os": 0.051976961944807595, "bg": 6.513895324255586e-07}, {"x": 0.4771718452758402, "y": 0.027901077996195307, "ox": 0.4771718452758402, "oy": 0.027901077996195307, "term": "it requires", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 0.0}, {"x": 0.5970196575776792, "y": 0.5428027901077996, "ox": 0.5970196575776792, "oy": 0.5428027901077996, "term": "larger", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 7, "s": 0.3937856689917565, "os": 0.01392176954877919, "bg": 7.805301565712599e-07}, {"x": 0.385859226379201, "y": 0.23303741280913126, "ox": 0.385859226379201, "oy": 0.23303741280913126, "term": "applicability", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 9.259074605349751e-06}, {"x": 0.38617628408370325, "y": 0.6785034876347495, "ox": 0.38617628408370325, "oy": 0.6785034876347495, "term": "showed", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 4, "s": 0.798985415345593, "os": 0.056128758701105966, "bg": 2.5891453918524242e-06}, {"x": 0.2844007609384908, "y": 0.4296131896005073, "ox": 0.2844007609384908, "oy": 0.4296131896005073, "term": "stacked", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 1.5170255301174604e-05}, {"x": 0.38649334178820544, "y": 0.7679137603043754, "ox": 0.38649334178820544, "oy": 0.7679137603043754, "term": "kl", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 4, "s": 0.8807863031071655, "os": 0.08828309953711883, "bg": 2.2414617549151233e-05}, {"x": 0.3868103994927077, "y": 0.12016487000634116, "ox": 0.3868103994927077, "oy": 0.12016487000634116, "term": "kl divergence", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.17882054533925174, "y": 0.12048192771084337, "ox": 0.17882054533925174, "oy": 0.12048192771084337, "term": "significant improvements", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.4774889029803424, "y": 0.23335447051363348, "ox": 0.4774889029803424, "oy": 0.23335447051363348, "term": "auto", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 2.94950240156386e-07}, {"x": 0.7913760304375397, "y": 0.4939759036144578, "ox": 0.7913760304375397, "oy": 0.4939759036144578, "term": "encoder", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 14, "s": 0.04185161699429296, "os": -0.030970637079961838, "bg": 1.94898367222127e-05}, {"x": 0.284717818642993, "y": 0.585288522511097, "ox": 0.284717818642993, "oy": 0.585288522511097, "term": "component analysis", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.715282181357007, "os": 0.042330659694003736, "bg": 0.0}, {"x": 0.38712745719720987, "y": 0.12079898541534559, "ox": 0.38712745719720987, "oy": 0.12079898541534559, "term": "maximization", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 3.457135448995466e-05}, {"x": 0.08275206087507926, "y": 0.30152187698161065, "ox": 0.08275206087507926, "oy": 0.30152187698161065, "term": "intuitive", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 4.919172407378132e-06}, {"x": 0.17913760304375395, "y": 0.1778693722257451, "ox": 0.17913760304375395, "oy": 0.1778693722257451, "term": "generalizes", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 5.198694088045083e-05}, {"x": 0.5973367152821814, "y": 0.07387444514901712, "ox": 0.5973367152821814, "oy": 0.07387444514901712, "term": "anomaly", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.06785034876347495, "os": -0.021448005370834956, "bg": 1.3514201866831056e-05}, {"x": 0.08306911857958148, "y": 0.1781864299302473, "ox": 0.08306911857958148, "oy": 0.1781864299302473, "term": "behaviour", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 1.2494396610186902e-06}, {"x": 0.7603043753963221, "y": 0.7780596068484464, "ox": 0.7603043753963221, "oy": 0.7780596068484464, "term": "localization", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 12, "s": 0.7714013950538999, "os": 0.05075792374827745, "bg": 2.660435817393115e-05}, {"x": 0.7114774889029803, "y": 0.8287888395688016, "ox": 0.7114774889029803, "oy": 0.8287888395688016, "term": "category", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 10, "s": 0.8953709575142677, "os": 0.09711670965690258, "bg": 6.944324382131133e-07}, {"x": 0.5405833861762841, "y": 0.3018389346861129, "ox": 0.5405833861762841, "oy": 0.3018389346861129, "term": "aims at", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 0.0}, {"x": 0.1794546607482562, "y": 0.4299302473050095, "ox": 0.1794546607482562, "oy": 0.4299302473050095, "term": "dtl", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 7.878027131925443e-05}, {"x": 0.5976537729866835, "y": 0.01046290424857324, "ox": 0.5976537729866835, "oy": 0.01046290424857324, "term": "residual", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.03329105897273303, "os": -0.0343097417052401, "bg": 3.628833257531039e-06}, {"x": 0.3874445149017121, "y": 0.07419150285351934, "ox": 0.3874445149017121, "oy": 0.07419150285351934, "term": "correlated", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 5.992600935624785e-06}, {"x": 0.38776157260621436, "y": 0.12111604311984782, "ox": 0.38776157260621436, "oy": 0.12111604311984782, "term": "make use", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.28503487634749525, "y": 0.2336715282181357, "ox": 0.28503487634749525, "oy": 0.2336715282181357, "term": "parallel", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 1.2970697141463718e-06}, {"x": 0.2853519340519975, "y": 0.43024730500951175, "ox": 0.2853519340519975, "oy": 0.43024730500951175, "term": "occurrence", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 5.52444506517723e-06}, {"x": 0.2856689917564997, "y": 0.3021559923906151, "ox": 0.2856689917564997, "oy": 0.3021559923906151, "term": "- occurrence", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 0.0}, {"x": 0.2859860494610019, "y": 0.17850348763474952, "ox": 0.2859860494610019, "oy": 0.17850348763474952, "term": "occurrence data", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.38807863031071654, "y": 0.35256816740646796, "ox": 0.38807863031071654, "oy": 0.35256816740646796, "term": "very challenging", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 0.0}, {"x": 0.2863031071655041, "y": 0.3024730500951173, "ox": 0.2863031071655041, "oy": 0.3024730500951173, "term": "bounds", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 6.040475366825291e-06}, {"x": 0.28662016487000636, "y": 0.12143310082435003, "ox": 0.28662016487000636, "oy": 0.12143310082435003, "term": "also provide", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.6426759670259987, "y": 0.6924540266328472, "ox": 0.6426759670259987, "oy": 0.6924540266328472, "term": "activity recognition", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.6769181991122385, "os": 0.037366170806685285, "bg": 0.0}, {"x": 0.28693722257450854, "y": 0.39600507292327203, "ox": 0.28693722257450854, "oy": 0.39600507292327203, "term": "there has", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 0.0}, {"x": 0.8253012048192772, "y": 0.7603043753963221, "ox": 0.8253012048192772, "oy": 0.7603043753963221, "term": "self", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 17, "s": 0.39283449587824987, "os": 0.013639094024946119, "bg": 9.015928307954483e-07}, {"x": 0.2872542802790108, "y": 0.6404565630944832, "ox": 0.2872542802790108, "oy": 0.6404565630944832, "term": "smart", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 3, "s": 0.7786937222574509, "os": 0.051976961944807595, "bg": 1.3562093298250913e-06}, {"x": 0.5979708306911858, "y": 0.5856055802155993, "ox": 0.5979708306911858, "oy": 0.5856055802155993, "term": "challenging problem", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.47368421052631576, "os": 0.02035263771598176, "bg": 0.0}, {"x": 0.5409004438807863, "y": 0.49429296131896006, "ox": 0.5409004438807863, "oy": 0.49429296131896006, "term": "influence", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 6, "s": 0.3823715916296766, "os": 0.012985406876082115, "bg": 1.5735300475678135e-06}, {"x": 0.47780596068484465, "y": 0.6407736207989854, "ox": 0.47780596068484465, "oy": 0.6407736207989854, "term": "article we", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 5, "s": 0.7067216233354471, "os": 0.04098795095579661, "bg": 0.0}, {"x": 0.287571337983513, "y": 0.3027901077996195, "ox": 0.287571337983513, "oy": 0.3027901077996195, "term": "object classes", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 0.0}, {"x": 0.2878883956880152, "y": 0.2339885859226379, "ox": 0.2878883956880152, "oy": 0.2339885859226379, "term": "argue", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 2.7957368275073474e-06}, {"x": 0.28820545339251746, "y": 0.12175015852885225, "ox": 0.28820545339251746, "oy": 0.12175015852885225, "term": "we argue", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.28852251109701965, "y": 0.23430564362714015, "ox": 0.28852251109701965, "oy": 0.23430564362714015, "term": "argue that", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.2888395688015219, "y": 0.12206721623335447, "ox": 0.2888395688015219, "oy": 0.12206721623335447, "term": "auxiliary dataset", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.08338617628408371, "y": 0.43056436271401394, "ox": 0.08338617628408371, "oy": 0.43056436271401394, "term": "though", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 3.038959889061467e-07}, {"x": 0.2891566265060241, "y": 0.30310716550412176, "ox": 0.2891566265060241, "oy": 0.30310716550412176, "term": "extremely", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 1.0685042309274358e-06}, {"x": 0.7606214331008243, "y": 0.17882054533925174, "ox": 0.7606214331008243, "oy": 0.17882054533925174, "term": "cell", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 12, "s": 0.02631578947368421, "os": -0.04248966467615985, "bg": 3.5304978392956033e-07}, {"x": 0.2894736842105263, "y": 0.1223842739378567, "ox": 0.2894736842105263, "oy": 0.1223842739378567, "term": "subsequent", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 1.3051565691923535e-06}, {"x": 0.3883956880152188, "y": 0.07450856055802156, "ox": 0.3883956880152188, "oy": 0.07450856055802156, "term": "treatment", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 2.2164869391290622e-07}, {"x": 0.388712745719721, "y": 0.17913760304375395, "ox": 0.388712745719721, "oy": 0.17913760304375395, "term": "plays an", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.2897907419150285, "y": 0.4308814204185162, "ox": 0.2897907419150285, "oy": 0.4308814204185162, "term": "important role", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 0.0}, {"x": 0.08370323398858592, "y": 0.3528852251109702, "ox": 0.08370323398858592, "oy": 0.3528852251109702, "term": "intra class", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 0.0}, {"x": 0.29010779961953076, "y": 0.5431198478123018, "ox": 0.29010779961953076, "oy": 0.5431198478123018, "term": "boost", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.6610653138871275, "os": 0.03589979152680117, "bg": 2.8764607566616313e-06}, {"x": 0.3890298034242232, "y": 0.5637285986049461, "ox": 0.3890298034242232, "oy": 0.5637285986049461, "term": "2012", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 4, "s": 0.6385542168674699, "os": 0.033620720115896965, "bg": 0.0}, {"x": 0.3893468611287254, "y": 0.4673430564362714, "ox": 0.3893468611287254, "oy": 0.4673430564362714, "term": "pattern recognition", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 0.0}, {"x": 0.598287888395688, "y": 0.46766011414077363, "ox": 0.598287888395688, "oy": 0.46766011414077363, "term": "access", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 7, "s": 0.25935320228281544, "os": 0.00427546729797533, "bg": 1.9246818204229058e-07}, {"x": 0.47812301838934684, "y": 0.7114774889029803, "ox": 0.47812301838934684, "oy": 0.7114774889029803, "term": "might", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 5, "s": 0.8132530120481929, "os": 0.06028055545740433, "bg": 5.347293940326597e-07}, {"x": 0.7796448953709575, "y": 0.23462270133164237, "ox": 0.7796448953709575, "oy": 0.23462270133164237, "term": "side", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 13, "s": 0.023462270133164237, "os": -0.04476873608706407, "bg": 3.112041771918218e-07}, {"x": 0.08402029169308814, "y": 0.3532022828154724, "ox": 0.08402029169308814, "oy": 0.3532022828154724, "term": "consisting", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 3.268389525574195e-06}, {"x": 0.5412175015852885, "y": 0.8341788205453392, "ox": 0.5412175015852885, "oy": 0.8341788205453392, "term": "hierarchical", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 6, "s": 0.9245402663284717, "os": 0.12552559980212713, "bg": 3.557180565979188e-05}, {"x": 0.29042485732403295, "y": 0.43119847812301837, "ox": 0.29042485732403295, "oy": 0.43119847812301837, "term": "followed by", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 0.0}, {"x": 0.5415345592897908, "y": 0.5859226379201015, "ox": 0.5415345592897908, "oy": 0.5859226379201015, "term": "difference between", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 6, "s": 0.5440710209258085, "os": 0.025847143210487256, "bg": 0.0}, {"x": 0.6429930247305009, "y": 0.39632213062777427, "ox": 0.6429930247305009, "oy": 0.39632213062777427, "term": "containing", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 8, "s": 0.13221306277742548, "os": -0.007649906363732729, "bg": 1.4042444975030427e-06}, {"x": 0.4784400760938491, "y": 0.6249207355738744, "ox": 0.4784400760938491, "oy": 0.6249207355738744, "term": "generative", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 5, "s": 0.6794546607482562, "os": 0.03777251687219533, "bg": 7.334335692544648e-05}, {"x": 0.816423589093215, "y": 0.028218135700697526, "ox": 0.816423589093215, "oy": 0.028218135700697526, "term": "adversarial", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 16, "s": 0.0063411540900443885, "os": -0.07732942298858697, "bg": 6.51551751941217e-05}, {"x": 0.7609384908053266, "y": 0.028535193405199746, "ox": 0.7609384908053266, "oy": 0.028535193405199746, "term": "translation", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.016169942929613188, "os": -0.055351401010564995, "bg": 1.0465247969308568e-06}, {"x": 0.1797717184527584, "y": 0.12270133164235891, "ox": 0.1797717184527584, "oy": 0.12270133164235891, "term": "thanks", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.9644370967706375e-07}, {"x": 0.18008877615726063, "y": 0.12301838934686113, "ox": 0.18008877615726063, "oy": 0.12301838934686113, "term": "thanks to", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.8027901077996196, "y": 0.8012048192771084, "ox": 0.8027901077996196, "oy": 0.8012048192771084, "term": "appearance", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 15, "s": 0.7517438173747623, "os": 0.04713614359916611, "bg": 6.664467392427166e-06}, {"x": 0.18040583386176284, "y": 0.5640456563094484, "ox": 0.18040583386176284, "oy": 0.5640456563094484, "term": "true", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 2, "s": 0.7320862396956247, "os": 0.04460973110490796, "bg": 4.145058305011877e-07}, {"x": 0.38966391883322765, "y": 0.07482561826252378, "ox": 0.38966391883322765, "oy": 0.07482561826252378, "term": "semantics", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 4.42337345922844e-06}, {"x": 0.541851616994293, "y": 0.7714013950538998, "ox": 0.541851616994293, "oy": 0.7714013950538998, "term": "procedure", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 6, "s": 0.8681039949270767, "os": 0.08050952263170913, "bg": 2.3101847685777905e-06}, {"x": 0.18072289156626506, "y": 0.12333544705136334, "ox": 0.18072289156626506, "oy": 0.12333544705136334, "term": "gan", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 6.955809356722886e-06}, {"x": 0.18103994927076728, "y": 0.6252377932783767, "ox": 0.18103994927076728, "oy": 0.6252377932783767, "term": "abstraction", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 2, "s": 0.7907419150285353, "os": 0.054256033355711816, "bg": 1.771020862625762e-05}, {"x": 0.1813570069752695, "y": 0.12365250475586556, "ox": 0.1813570069752695, "oy": 0.12365250475586556, "term": "that utilizes", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.6816740646797718, "y": 0.7206721623335447, "ox": 0.6816740646797718, "oy": 0.7206721623335447, "term": "take", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 9, "s": 0.7114774889029803, "os": 0.041517967562983635, "bg": 2.872469305879622e-07}, {"x": 0.6433100824350032, "y": 0.044071020925808495, "ox": 0.6433100824350032, "oy": 0.044071020925808495, "term": "answers", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.042802790107799624, "os": -0.03015794494894173, "bg": 5.094874699101109e-07}, {"x": 0.38998097653772984, "y": 0.23493975903614459, "ox": 0.38998097653772984, "oy": 0.23493975903614459, "term": "queries", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 1.7422869459485628e-06}, {"x": 0.2907419150285352, "y": 0.39663918833227646, "ox": 0.2907419150285352, "oy": 0.39663918833227646, "term": "provided by", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 0.0}, {"x": 0.08433734939759036, "y": 0.1794546607482562, "ox": 0.08433734939759036, "oy": 0.1794546607482562, "term": "ontology", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 7.798912831551283e-06}, {"x": 0.29105897273303744, "y": 0.1797717184527584, "ox": 0.29105897273303744, "oy": 0.1797717184527584, "term": "continuously", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 3.7662913500271343e-06}, {"x": 0.3902980342422321, "y": 0.543436905516804, "ox": 0.3902980342422321, "oy": 0.543436905516804, "term": "feedback", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.6049461001902346, "os": 0.030405286032295677, "bg": 2.777061802756646e-07}, {"x": 0.2913760304375396, "y": 0.2352568167406468, "ox": 0.2913760304375396, "oy": 0.2352568167406468, "term": "usage", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 9.348961680943311e-07}, {"x": 0.08465440710209259, "y": 0.23557387444514902, "ox": 0.08465440710209259, "oy": 0.23557387444514902, "term": "describes", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 1.2167746006576179e-06}, {"x": 0.29169308814204187, "y": 0.30342422320862394, "ox": 0.29169308814204187, "oy": 0.30342422320862394, "term": "play", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 1.722091200519319e-07}, {"x": 0.3906150919467343, "y": 0.18008877615726063, "ox": 0.3906150919467343, "oy": 0.18008877615726063, "term": "available at", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.4787571337983513, "y": 0.028852251109701965, "ox": 0.4787571337983513, "oy": 0.028852251109701965, "term": "egocentric", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 5.106310548786542e-05}, {"x": 0.3909321496512365, "y": 0.12396956246036779, "ox": 0.3909321496512365, "oy": 0.12396956246036779, "term": "information across", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 0.0}, {"x": 0.0849714648065948, "y": 0.3037412809131262, "ox": 0.0849714648065948, "oy": 0.3037412809131262, "term": "view action", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.39124920735573876, "y": 0.30405833861762843, "ox": 0.39124920735573876, "oy": 0.30405833861762843, "term": "possibility", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 1.6319963611143992e-06}, {"x": 0.4790741915028535, "y": 0.7381103360811667, "ox": 0.4790741915028535, "oy": 0.7381103360811667, "term": "machines", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 5, "s": 0.8414711477488903, "os": 0.0699268577082082, "bg": 1.8938811566952817e-06}, {"x": 0.29201014584654406, "y": 0.5437539632213063, "ox": 0.29201014584654406, "oy": 0.5437539632213063, "term": "vector machines", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.6610653138871275, "os": 0.03589979152680117, "bg": 0.0}, {"x": 0.6436271401395054, "y": 0.5862396956246037, "ox": 0.6436271401395054, "oy": 0.5862396956246037, "term": "score", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 8, "s": 0.40076093849080535, "os": 0.014858132221476271, "bg": 7.764654696531198e-07}, {"x": 0.5421686746987951, "y": 0.3969562460367787, "ox": 0.5421686746987951, "oy": 0.3969562460367787, "term": "textual", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 6, "s": 0.24381737476220672, "os": 0.0033391046252782552, "bg": 1.4852937354023472e-05}, {"x": 0.2923272035510463, "y": 0.23589093214965123, "ox": 0.2923272035510463, "oy": 0.23589093214965123, "term": "take advantage", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.08528852251109702, "y": 0.3043753963221306, "ox": 0.08528852251109702, "oy": 0.3043753963221306, "term": "2015", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.39156626506024095, "y": 0.30469245402663286, "ox": 0.39156626506024095, "oy": 0.30469245402663286, "term": "operator", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 1.0894084096026216e-06}, {"x": 0.2926442612555485, "y": 0.12428662016487001, "ox": 0.2926442612555485, "oy": 0.12428662016487001, "term": "computer aided", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.47939124920735576, "y": 0.30500951173113505, "ox": 0.47939124920735576, "oy": 0.30500951173113505, "term": "gain", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 9.313561057145061e-07}, {"x": 0.29296131896005073, "y": 0.07514267596702599, "ox": 0.29296131896005073, "oy": 0.07514267596702599, "term": "correctly", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.3320739807247413e-06}, {"x": 0.5986049461001902, "y": 0.07545973367152822, "ox": 0.5986049461001902, "oy": 0.07545973367152822, "term": "back", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.06785034876347495, "os": -0.021448005370834956, "bg": 5.32508623301278e-08}, {"x": 0.293278376664553, "y": 0.586556753329106, "ox": 0.293278376664553, "oy": 0.586556753329106, "term": "bias", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.715282181357007, "os": 0.042330659694003736, "bg": 4.868232437740839e-06}, {"x": 0.5989220038046924, "y": 0.5199746353836399, "ox": 0.5989220038046924, "oy": 0.5199746353836399, "term": "question", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 3.5147930571987106e-07}, {"x": 0.3918833227647432, "y": 0.18040583386176284, "ox": 0.3918833227647432, "oy": 0.18040583386176284, "term": "an open", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.3922003804692454, "y": 0.23620798985415345, "ox": 0.3922003804692454, "oy": 0.23620798985415345, "term": "classified", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.26188966391883317, "os": 0.004681813363485388, "bg": 1.1339557645600793e-06}, {"x": 0.5992390615091947, "y": 0.6788205453392517, "ox": 0.5992390615091947, "oy": 0.6788205453392517, "term": "game", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 7, "s": 0.6968928344958782, "os": 0.03964524221758948, "bg": 2.815136756330546e-07}, {"x": 0.5424857324032974, "y": 0.018389346861128725, "ox": 0.5424857324032974, "oy": 0.018389346861128725, "term": "double", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 2.3220272835883794e-07}, {"x": 0.5428027901077996, "y": 0.018706404565630944, "ox": 0.5428027901077996, "oy": 0.018706404565630944, "term": "multisource", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 5.570634092899607e-05}, {"x": 0.1816740646797717, "y": 0.3053265694356373, "ox": 0.1816740646797717, "oy": 0.3053265694356373, "term": "trial", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 4.226947115449391e-07}, {"x": 0.3925174381737476, "y": 0.5868738110336081, "ox": 0.3925174381737476, "oy": 0.5868738110336081, "term": "reward", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 4, "s": 0.6718452758402029, "os": 0.03683615419949825, "bg": 5.730314935617421e-06}, {"x": 0.47970830691185795, "y": 0.5871908687381103, "ox": 0.47970830691185795, "oy": 0.5871908687381103, "term": "needs", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.61572606214331, "os": 0.03134164870499275, "bg": 3.8763858664968507e-07}, {"x": 0.4800253646163602, "y": 0.12460367786937222, "ox": 0.4800253646163602, "oy": 0.12460367786937222, "term": "prove that", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 0.0}, {"x": 0.7612555485098288, "y": 0.4679771718452758, "ox": 0.7612555485098288, "oy": 0.4679771718452758, "term": "navigation", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 12, "s": 0.06182625237793279, "os": -0.023197060174552135, "bg": 8.328813083591523e-07}, {"x": 0.6439441978440076, "y": 0.7844007609384908, "ox": 0.6439441978440076, "oy": 0.7844007609384908, "term": "agents", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 8, "s": 0.8576410906785035, "os": 0.07595137980990072, "bg": 1.8480489554596299e-06}, {"x": 0.39283449587824987, "y": 0.7301838934686112, "ox": 0.39283449587824987, "oy": 0.7301838934686112, "term": "capable", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 4, "s": 0.8481293595434369, "os": 0.0722059291191124, "bg": 4.093591296206186e-06}, {"x": 0.5431198478123018, "y": 0.6689917564996829, "ox": 0.5431198478123018, "oy": 0.6689917564996829, "term": "gained", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 6, "s": 0.712428662016487, "os": 0.04192431362849369, "bg": 6.41354472234215e-06}, {"x": 0.29359543436905516, "y": 0.07577679137603044, "ox": 0.29359543436905516, "oy": 0.07577679137603044, "term": "knowledge gained", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.5995561192136969, "y": 0.07609384908053266, "ox": 0.5995561192136969, "oy": 0.07609384908053266, "term": "centric", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.06785034876347495, "os": -0.021448005370834956, "bg": 1.2250796419563392e-05}, {"x": 0.18199112238427395, "y": 0.35351934051997463, "ox": 0.18199112238427395, "oy": 0.35351934051997463, "term": "differ", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 2.4868676648762173e-06}, {"x": 0.2939124920735574, "y": 0.520291693088142, "ox": 0.2939124920735574, "oy": 0.520291693088142, "term": "style", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.62714013950539, "os": 0.03268435744319989, "bg": 3.752894901649809e-07}, {"x": 0.2942295497780596, "y": 0.3538363982244769, "ox": 0.2942295497780596, "oy": 0.3538363982244769, "term": "environmental", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 3.421844849766041e-07}, {"x": 0.29454660748256184, "y": 0.6693088142041852, "ox": 0.29454660748256184, "oy": 0.6693088142041852, "term": "markov", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 3, "s": 0.8078630310716551, "os": 0.05840783011201017, "bg": 3.3585306055430685e-05}, {"x": 0.08560558021559923, "y": 0.23652504755865567, "ox": 0.08560558021559923, "oy": 0.23652504755865567, "term": "hmm", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 4.434462961037256e-06}, {"x": 0.18230818008877617, "y": 0.3056436271401395, "ox": 0.18230818008877617, "oy": 0.3056436271401395, "term": "hidden markov", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.18262523779327838, "y": 0.30596068484464173, "ox": 0.18262523779327838, "oy": 0.30596068484464173, "term": "markov model", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.1829422954977806, "y": 0.12492073557387444, "ox": 0.1829422954977806, "oy": 0.12492073557387444, "term": "twitter", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 6.882812470126682e-05}, {"x": 0.08592263792010146, "y": 0.23684210526315788, "ox": 0.08592263792010146, "oy": 0.23684210526315788, "term": "social media", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.18325935320228282, "y": 0.3062777425491439, "ox": 0.18325935320228282, "oy": 0.3062777425491439, "term": "light", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 1.8907981205403655e-07}, {"x": 0.29486366518706403, "y": 0.74571972098922, "ox": 0.29486366518706403, "oy": 0.74571972098922, "term": "theory", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 3, "s": 0.8747622067216233, "os": 0.08413130278082045, "bg": 1.3859308614530457e-06}, {"x": 0.29518072289156627, "y": 0.35415345592897907, "ox": 0.29518072289156627, "oy": 0.35415345592897907, "term": "corpora", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 2.7297924675276557e-05}, {"x": 0.761572606214331, "y": 0.6062143310082435, "ox": 0.761572606214331, "oy": 0.6062143310082435, "term": "planning", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 12, "s": 0.1651870640456563, "os": -0.0039044556729444096, "bg": 6.147369250138616e-07}, {"x": 0.5998731769181991, "y": 0.8249841471147749, "ox": 0.5998731769181991, "oy": 0.8249841471147749, "term": "base", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 7, "s": 0.9105897273303741, "os": 0.11038479205681778, "bg": 1.3150586292101602e-06}, {"x": 0.18357641090678503, "y": 0.4315155358275206, "ox": 0.18357641090678503, "oy": 0.4315155358275206, "term": "response", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 3.5588797073746753e-07}, {"x": 0.39315155358275206, "y": 0.4318325935320228, "ox": 0.39315155358275206, "oy": 0.4318325935320228, "term": "responses", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 1.4077925291431687e-06}, {"x": 0.18389346861128725, "y": 0.30659480025364616, "ox": 0.18389346861128725, "oy": 0.30659480025364616, "term": "because it", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.29549778059606846, "y": 0.46829422954977806, "ox": 0.29549778059606846, "oy": 0.46829422954977806, "term": "too", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 1.9282667308303095e-07}, {"x": 0.08623969562460368, "y": 0.23715916296766013, "ox": 0.08623969562460368, "oy": 0.23715916296766013, "term": "changing", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 6.190511122646097e-07}, {"x": 0.18421052631578946, "y": 0.12523779327837667, "ox": 0.18421052631578946, "oy": 0.12523779327837667, "term": "frequently", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 5.565113638229213e-07}, {"x": 0.543436905516804, "y": 0.6927710843373494, "ox": 0.543436905516804, "oy": 0.6927710843373494, "term": "policies", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 6, "s": 0.7584020291693089, "os": 0.04835518179569627, "bg": 7.270847345572115e-07}, {"x": 0.5437539632213063, "y": 0.4686112872542803, "ox": 0.5437539632213063, "oy": 0.4686112872542803, "term": "real life", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 0.0}, {"x": 0.18452758402029168, "y": 0.12555485098287889, "ox": 0.18452758402029168, "oy": 0.12555485098287889, "term": "depend", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.7216448441806205e-06}, {"x": 0.2958148383005707, "y": 0.07641090678503487, "ox": 0.2958148383005707, "oy": 0.07641090678503487, "term": "dependence", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 2.994165369749481e-06}, {"x": 0.7117945466074825, "y": 0.0028535193405199747, "ox": 0.7117945466074825, "oy": 0.0028535193405199747, "term": "defect", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.014901712111604312, "os": -0.05722412635595915, "bg": 3.974502769830981e-06}, {"x": 0.18484464172479392, "y": 0.5643627140139506, "ox": 0.18484464172479392, "oy": 0.5643627140139506, "term": "tradaboost", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 2, "s": 0.7320862396956247, "os": 0.04460973110490796, "bg": 0.00017323366623069527}, {"x": 0.18516169942929614, "y": 0.18072289156626506, "ox": 0.18516169942929614, "oy": 0.18072289156626506, "term": "unfortunately", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 1.1699478536692402e-06}, {"x": 0.18547875713379836, "y": 0.18103994927076728, "ox": 0.18547875713379836, "oy": 0.18103994927076728, "term": "irrelevant", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 5.721075813980364e-06}, {"x": 0.0865567533291059, "y": 0.3972733037412809, "ox": 0.0865567533291059, "oy": 0.3972733037412809, "term": "poorly", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 5.168615135176183e-06}, {"x": 0.29613189600507295, "y": 0.1258719086873811, "ox": 0.29613189600507295, "oy": 0.1258719086873811, "term": "approach achieves", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.29644895370957514, "y": 0.39759036144578314, "ox": 0.29644895370957514, "oy": 0.39759036144578314, "term": "advance", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.46892834495878244, "os": 0.01982262110879474, "bg": 6.671142261886363e-07}, {"x": 0.3934686112872543, "y": 0.04438807863031072, "ox": 0.3934686112872543, "oy": 0.04438807863031072, "term": "view specific", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 0.0}, {"x": 0.2967660114140774, "y": 0.1813570069752695, "ox": 0.2967660114140774, "oy": 0.1813570069752695, "term": "in advance", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.29708306911857957, "y": 0.3544705136334813, "ox": 0.29708306911857957, "oy": 0.3544705136334813, "term": "be utilized", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.0}, {"x": 0.2974001268230818, "y": 0.4689283449587825, "ox": 0.2974001268230818, "oy": 0.4689283449587825, "term": "collective", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 2.718303995770959e-06}, {"x": 0.3937856689917565, "y": 0.5875079264426125, "ox": 0.3937856689917565, "oy": 0.5875079264426125, "term": "free", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 4, "s": 0.6718452758402029, "os": 0.03683615419949825, "bg": 4.53497650464657e-08}, {"x": 0.297717184527584, "y": 0.1816740646797717, "ox": 0.297717184527584, "oy": 0.1816740646797717, "term": "across multiple", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.08687381103360811, "y": 0.23747622067216234, "ox": 0.08687381103360811, "oy": 0.23747622067216234, "term": "multiple views", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.29803424223208624, "y": 0.3547875713379835, "ox": 0.29803424223208624, "oy": 0.3547875713379835, "term": "regularizer", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.00012126671748319592}, {"x": 0.08719086873811034, "y": 0.6065313887127457, "ox": 0.08719086873811034, "oy": 0.6065313887127457, "term": "different views", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 1, "s": 0.800570703868104, "os": 0.05653510476661602, "bg": 0.0}, {"x": 0.29835129993658843, "y": 0.18199112238427395, "ox": 0.29835129993658843, "oy": 0.18199112238427395, "term": "dnns", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 9.528223029715063e-05}, {"x": 0.681991122384274, "y": 0.3979074191502854, "ox": 0.681991122384274, "oy": 0.3979074191502854, "term": "unconstrained", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.10367786937222576, "os": -0.013144411858238225, "bg": 7.549136792156089e-05}, {"x": 0.08750792644261256, "y": 0.23779327837666456, "ox": 0.08750792644261256, "oy": 0.23779327837666456, "term": "be improved", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.39410272669625873, "y": 0.044705136334812934, "ox": 0.39410272669625873, "oy": 0.044705136334812934, "term": "over fitting", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 0.0}, {"x": 0.7121116043119848, "y": 0.7847178186429931, "ox": 0.7121116043119848, "oy": 0.7847178186429931, "term": "sparse coding", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 10, "s": 0.829422954977806, "os": 0.06496236882088971, "bg": 0.0}, {"x": 0.08782498414711477, "y": 0.738427393785669, "ox": 0.08782498414711477, "oy": 0.738427393785669, "term": "breast", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 1, "s": 0.8861762840837032, "os": 0.09190487968623016, "bg": 1.5777142731348317e-06}, {"x": 0.08814204185161699, "y": 0.6544071020925808, "ox": 0.08814204185161699, "oy": 0.6544071020925808, "term": "breast cancer", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 1, "s": 0.8341788205453392, "os": 0.06618140701741988, "bg": 0.0}, {"x": 0.3944197844007609, "y": 0.6547241597970831, "ox": 0.3944197844007609, "oy": 0.6547241597970831, "term": "i", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 4, "s": 0.7663284717818644, "os": 0.0496978905339034, "bg": 1.7495793475034933e-08}, {"x": 0.5440710209258085, "y": 0.6255548509828789, "ox": 0.5440710209258085, "oy": 0.6255548509828789, "term": "smaller", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.6236525047558656, "os": 0.03227801137768983, "bg": 2.1417134508766487e-06}, {"x": 0.39473684210526316, "y": 0.35510462904248574, "ox": 0.39473684210526316, "oy": 0.35510462904248574, "term": "very limited", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 0.0}, {"x": 0.39505389980976535, "y": 0.04502219403931516, "ox": 0.39505389980976535, "oy": 0.04502219403931516, "term": "regular", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 2.8781769397009777e-07}, {"x": 0.3953709575142676, "y": 0.6258719086873811, "ox": 0.3953709575142676, "oy": 0.6258719086873811, "term": "easy", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7232086239695625, "os": 0.043267022366700825, "bg": 3.975212672685425e-07}, {"x": 0.18579581483830057, "y": 0.12618896639188332, "ox": 0.18579581483830057, "oy": 0.12618896639188332, "term": "methods assume", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.08845909955611922, "y": 0.43214965123652505, "ox": 0.08845909955611922, "oy": 0.43214965123652505, "term": "real applications", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 0.0}, {"x": 0.08877615726062144, "y": 0.18230818008877617, "ox": 0.08877615726062144, "oy": 0.18230818008877617, "term": "marginal distribution", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.2986683576410907, "y": 0.23811033608116677, "ox": 0.2986683576410907, "oy": 0.23811033608116677, "term": "conditional distribution", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.1861128725428028, "y": 0.5646797717184527, "ox": 0.1861128725428028, "oy": 0.5646797717184527, "term": "come from", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 2, "s": 0.7320862396956247, "os": 0.04460973110490796, "bg": 0.0}, {"x": 0.2989854153455929, "y": 0.46924540266328474, "ox": 0.2989854153455929, "oy": 0.46924540266328474, "term": "would be", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 0.0}, {"x": 0.186429930247305, "y": 0.5440710209258085, "ox": 0.186429930247305, "oy": 0.5440710209258085, "term": "chinese", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 2, "s": 0.7089410272669626, "os": 0.04139429702130666, "bg": 6.750935155526801e-07}, {"x": 0.39568801521876984, "y": 0.0767279644895371, "ox": 0.39568801521876984, "oy": 0.0767279644895371, "term": "promising performance", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 0.0}, {"x": 0.6442612555485099, "y": 0.07704502219403932, "ox": 0.6442612555485099, "oy": 0.07704502219403932, "term": "object tracking", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.04946100190234623, "os": -0.026942510865340445, "bg": 0.0}, {"x": 0.08909321496512365, "y": 0.18262523779327838, "ox": 0.08909321496512365, "oy": 0.18262523779327838, "term": "manifold structure", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.2993024730500951, "y": 0.4324667089410273, "ox": 0.2993024730500951, "oy": 0.4324667089410273, "term": "optimizing", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 1.0842345185710738e-05}, {"x": 0.5443880786303107, "y": 0.3069118579581484, "ox": 0.5443880786303107, "oy": 0.3069118579581484, "term": "showing", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 6.669975947441423e-07}, {"x": 0.29961953075459735, "y": 0.6068484464172479, "ox": 0.29961953075459735, "oy": 0.6068484464172479, "term": "respect", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 3, "s": 0.7371591629676602, "os": 0.04554609377760503, "bg": 9.472703406382086e-07}, {"x": 0.29993658845909954, "y": 0.6071655041217502, "ox": 0.29993658845909954, "oy": 0.6071655041217502, "term": "with respect", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 3, "s": 0.7371591629676602, "os": 0.04554609377760503, "bg": 0.0}, {"x": 0.3002536461636018, "y": 0.6074825618262524, "ox": 0.3002536461636018, "oy": 0.6074825618262524, "term": "respect to", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 3, "s": 0.7371591629676602, "os": 0.04554609377760503, "bg": 0.0}, {"x": 0.4803424223208624, "y": 0.5649968294229549, "ox": 0.4803424223208624, "oy": 0.5649968294229549, "term": "2017", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.5798985415345593, "os": 0.02812621462139147, "bg": 0.0}, {"x": 0.39600507292327203, "y": 0.39822447685478757, "ox": 0.39600507292327203, "oy": 0.39822447685478757, "term": "c 2017", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 0.0}, {"x": 0.39632213062777427, "y": 0.6550412175015853, "ox": 0.39632213062777427, "oy": 0.6550412175015853, "term": "ltd. all", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 4, "s": 0.7663284717818644, "os": 0.0496978905339034, "bg": 0.0}, {"x": 0.544705136334813, "y": 0.5206087507926442, "ox": 0.544705136334813, "oy": 0.5206087507926442, "term": "frames", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 6, "s": 0.4166138237159163, "os": 0.016200840959683403, "bg": 1.6153762673911778e-06}, {"x": 0.6823081800887761, "y": 0.6696258719086874, "ox": 0.6823081800887761, "oy": 0.6696258719086874, "term": "encoding", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 9, "s": 0.541851616994293, "os": 0.025440797144977212, "bg": 6.046392319249515e-06}, {"x": 0.18674698795180722, "y": 0.238427393785669, "ox": 0.18674698795180722, "oy": 0.238427393785669, "term": "facilitates", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 9.179307337813114e-06}, {"x": 0.39663918833227646, "y": 0.5209258084971464, "ox": 0.39663918833227646, "oy": 0.5209258084971464, "term": "oriented", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 2.3506311973656946e-06}, {"x": 0.30057070386810397, "y": 0.3072289156626506, "ox": 0.30057070386810397, "oy": 0.3072289156626506, "term": "inherent", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 5.421414922569684e-06}, {"x": 0.6826252377932783, "y": 0.029169308814204185, "ox": 0.6826252377932783, "oy": 0.029169308814204185, "term": "wind", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.028535193405199746, "os": -0.038867884527048514, "bg": 6.017714624280168e-07}, {"x": 0.3008877615726062, "y": 0.07736207989854153, "ox": 0.3008877615726062, "oy": 0.07736207989854153, "term": "investigate how", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.3969562460367787, "y": 0.5212428662016487, "ox": 0.3969562460367787, "oy": 0.5212428662016487, "term": "computed", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.5688015218769816, "os": 0.027189851948694395, "bg": 6.7340883593201796e-06}, {"x": 0.30120481927710846, "y": 0.1829422954977806, "ox": 0.30120481927710846, "oy": 0.1829422954977806, "term": "captures", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 5.680075431401729e-06}, {"x": 0.4806594800253646, "y": 0.6410906785034877, "ox": 0.4806594800253646, "oy": 0.6410906785034877, "term": "maximum mean", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 5, "s": 0.7067216233354471, "os": 0.04098795095579661, "bg": 0.0}, {"x": 0.4809765377298668, "y": 0.6414077362079899, "ox": 0.4809765377298668, "oy": 0.6414077362079899, "term": "mean discrepancy", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 5, "s": 0.7067216233354471, "os": 0.04098795095579661, "bg": 0.0}, {"x": 0.8617628408370324, "y": 0.7901077996195307, "ox": 0.8617628408370324, "oy": 0.7901077996195307, "term": "event", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 22, "s": 0.22986683576410907, "os": 0.00224373697042507, "bg": 9.70208221714655e-07}, {"x": 0.48129359543436906, "y": 0.2387444514901712, "ox": 0.48129359543436906, "oy": 0.2387444514901712, "term": "event recognition", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.18991756499682944, "os": -0.0008126921310201082, "bg": 0.0}, {"x": 0.18706404565630944, "y": 0.12650602409638553, "ox": 0.18706404565630944, "oy": 0.12650602409638553, "term": "paper addresses", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.18738110336081168, "y": 0.7606214331008243, "ox": 0.18738110336081168, "oy": 0.7606214331008243, "term": "by transferring", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 2, "s": 0.8928344958782498, "os": 0.09605667644252854, "bg": 0.0}, {"x": 0.08941027266962587, "y": 0.3985415345592898, "ox": 0.08941027266962587, "oy": 0.3985415345592898, "term": "initialization", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 1.0171937034144845e-05}, {"x": 0.1876981610653139, "y": 0.12682308180088775, "ox": 0.1876981610653139, "oy": 0.12682308180088775, "term": "sports", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 7.755439922999512e-08}, {"x": 0.1880152187698161, "y": 0.35542168674698793, "ox": 0.1880152187698161, "oy": 0.35542168674698793, "term": "photo", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 1.1435802676174873e-07}, {"x": 0.3972733037412809, "y": 0.398858592263792, "ox": 0.3972733037412809, "oy": 0.398858592263792, "term": "estimating", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.39663918833227646, "os": 0.014328115614289247, "bg": 8.66812381439705e-06}, {"x": 0.30152187698161065, "y": 0.4695624603677869, "ox": 0.30152187698161065, "oy": 0.4695624603677869, "term": "internet", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 1.2878390599950584e-07}, {"x": 0.3018389346861129, "y": 0.07767913760304375, "ox": 0.3018389346861129, "oy": 0.07767913760304375, "term": "be efficiently", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.39759036144578314, "y": 0.5653138871274572, "ox": 0.39759036144578314, "oy": 0.5653138871274572, "term": "reduces", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 4, "s": 0.6385542168674699, "os": 0.033620720115896965, "bg": 5.391061473048736e-06}, {"x": 0.3021559923906151, "y": 0.07799619530754598, "ox": 0.3021559923906151, "oy": 0.07799619530754598, "term": "requirement", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 7.663195735687011e-07}, {"x": 0.18833227647431833, "y": 0.12714013950539, "ox": 0.18833227647431833, "oy": 0.12714013950539, "term": "expand", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 7.283276104991338e-07}, {"x": 0.48161065313887125, "y": 0.04533925174381737, "ox": 0.48161065313887125, "oy": 0.04533925174381737, "term": "directional", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 6.108623543856863e-06}, {"x": 0.5450221940393152, "y": 0.7209892200380469, "ox": 0.5450221940393152, "oy": 0.7209892200380469, "term": "calibration", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 6, "s": 0.8065948002536463, "os": 0.058001484046500115, "bg": 1.0658015200461278e-05}, {"x": 0.3979074191502854, "y": 0.18325935320228282, "ox": 0.3979074191502854, "oy": 0.18325935320228282, "term": "pose classification", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.0897273303741281, "y": 0.18357641090678503, "ox": 0.0897273303741281, "oy": 0.18357641090678503, "term": "hidden layer", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.3024730500951173, "y": 0.0783132530120482, "ox": 0.3024730500951173, "oy": 0.0783132530120482, "term": "versions", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 7.21850130759141e-07}, {"x": 0.09004438807863031, "y": 0.46987951807228917, "ox": 0.09004438807863031, "oy": 0.46987951807228917, "term": "previous approaches", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.674698795180723, "os": 0.0372425002650083, "bg": 0.0}, {"x": 0.3027901077996195, "y": 0.49461001902346224, "ox": 0.3027901077996195, "oy": 0.49461001902346224, "term": "leverages", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.5916296766011414, "os": 0.0294689233595986, "bg": 5.064388067176294e-05}, {"x": 0.5453392517438174, "y": 0.029486366518706404, "ox": 0.5453392517438174, "oy": 0.029486366518706404, "term": "labeled instances", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.06499682942295498, "os": -0.022384368043532034, "bg": 0.0}, {"x": 0.18864933417882054, "y": 0.5656309448319594, "ox": 0.18864933417882054, "oy": 0.5656309448319594, "term": "user 's", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 2, "s": 0.7320862396956247, "os": 0.04460973110490796, "bg": 0.0}, {"x": 0.18896639188332276, "y": 0.641724793912492, "ox": 0.18896639188332276, "oy": 0.641724793912492, "term": "measures", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 2, "s": 0.8046924540266329, "os": 0.0574714674393131, "bg": 1.1949824181987857e-06}, {"x": 0.6445783132530121, "y": 0.30754597336715284, "ox": 0.6445783132530121, "oy": 0.30754597336715284, "term": "method achieves", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.09701965757767915, "os": -0.0140807745309353, "bg": 0.0}, {"x": 0.39822447685478757, "y": 0.5443880786303107, "ox": 0.39822447685478757, "oy": 0.5443880786303107, "term": "skills", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.6049461001902346, "os": 0.030405286032295677, "bg": 5.864734015252022e-07}, {"x": 0.4819277108433735, "y": 0.307863031071655, "ox": 0.4819277108433735, "oy": 0.307863031071655, "term": "rely", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 3.2039113349577166e-06}, {"x": 0.48224476854787574, "y": 0.07863031071655041, "ox": 0.48224476854787574, "oy": 0.07863031071655041, "term": "discrete", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 3.6599877989315826e-06}, {"x": 0.4825618262523779, "y": 0.30818008877615727, "ox": 0.4825618262523779, "oy": 0.30818008877615727, "term": "rely on", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 0.0}, {"x": 0.6001902346227014, "y": 0.5215599239061509, "ox": 0.6001902346227014, "oy": 0.5215599239061509, "term": "direct", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.35003170577045023, "os": 0.010706335465177907, "bg": 4.4893380417380583e-07}, {"x": 0.09036144578313253, "y": 0.18389346861128725, "ox": 0.09036144578313253, "oy": 0.18389346861128725, "term": "value function", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.6829422954977806, "y": 0.7520608750792644, "ox": 0.6829422954977806, "oy": 0.7520608750792644, "term": "matching", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 9, "s": 0.7920101458465442, "os": 0.05437970389738879, "bg": 2.3976483864625576e-06}, {"x": 0.30310716550412176, "y": 0.4327837666455295, "ox": 0.30310716550412176, "oy": 0.4327837666455295, "term": "review", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 9.431218647018828e-08}, {"x": 0.30342422320862394, "y": 0.6791376030437539, "ox": 0.30342422320862394, "oy": 0.6791376030437539, "term": "share", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 3, "s": 0.8186429930247305, "os": 0.061623264195611455, "bg": 4.685206379701624e-07}, {"x": 0.48287888395688017, "y": 0.045656309448319596, "ox": 0.48287888395688017, "oy": 0.045656309448319596, "term": "class label", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.09923906150919468, "os": -0.01367442846542525, "bg": 0.0}, {"x": 0.18928344958782498, "y": 0.5659480025364616, "ox": 0.18928344958782498, "oy": 0.5659480025364616, "term": "matrices", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 2, "s": 0.7320862396956247, "os": 0.04460973110490796, "bg": 1.2875315445228408e-05}, {"x": 0.09067850348763475, "y": 0.47019657577679136, "ox": 0.09067850348763475, "oy": 0.47019657577679136, "term": "integrating", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.674698795180723, "os": 0.0372425002650083, "bg": 4.810342621463556e-06}, {"x": 0.1896005072923272, "y": 0.39917564996829424, "ox": 0.1896005072923272, "oy": 0.39917564996829424, "term": "feature mapping", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 0.0}, {"x": 0.48319594166138236, "y": 0.4331008243500317, "ox": 0.48319594166138236, "oy": 0.4331008243500317, "term": "taught", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 2.2067731753479895e-06}, {"x": 0.4835129993658846, "y": 0.4334178820545339, "ox": 0.4835129993658846, "oy": 0.4334178820545339, "term": "self taught", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 0.0}, {"x": 0.3037412809131262, "y": 0.3557387444514902, "ox": 0.3037412809131262, "oy": 0.3557387444514902, "term": "taught learning", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 0.0}, {"x": 0.09099556119213698, "y": 0.23906150919467342, "ox": 0.09099556119213698, "oy": 0.23906150919467342, "term": "semisupervised", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 8.662058451570432e-05}, {"x": 0.09131261889663919, "y": 0.18421052631578946, "ox": 0.09131261889663919, "oy": 0.18421052631578946, "term": "auxiliary samples", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.30405833861762843, "y": 0.18452758402029168, "ox": 0.30405833861762843, "oy": 0.18452758402029168, "term": "fewer", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 1.9957039750522487e-06}, {"x": 0.18991756499682944, "y": 0.18484464172479392, "ox": 0.18991756499682944, "oy": 0.18484464172479392, "term": "characterize", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 8.552092290759164e-06}, {"x": 0.4838300570703868, "y": 0.6553582752060875, "ox": 0.4838300570703868, "oy": 0.6553582752060875, "term": "confidence", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 5, "s": 0.7301838934686113, "os": 0.044203385039397906, "bg": 2.444807702122185e-06}, {"x": 0.3043753963221306, "y": 0.07894736842105263, "ox": 0.3043753963221306, "oy": 0.07894736842105263, "term": "sequentially", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.6967989887078026e-05}, {"x": 0.19023462270133165, "y": 0.3560558021559924, "ox": 0.19023462270133165, "oy": 0.3560558021559924, "term": "distinguish", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 5.120872281506167e-06}, {"x": 0.3985415345592898, "y": 0.18516169942929614, "ox": 0.3985415345592898, "oy": 0.18516169942929614, "term": "improvement over", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 0.0}, {"x": 0.19055168040583387, "y": 0.6077996195307546, "ox": 0.19055168040583387, "oy": 0.6077996195307546, "term": "nearest", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 2.9512909550310454e-06}, {"x": 0.398858592263792, "y": 0.43373493975903615, "ox": 0.398858592263792, "oy": 0.43373493975903615, "term": "handling", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 1.0673492344092298e-06}, {"x": 0.5456563094483196, "y": 0.4705136334812936, "ox": 0.5456563094483196, "oy": 0.4705136334812936, "term": "limitation", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.3348129359543437, "os": 0.009769972792480826, "bg": 4.584493592253306e-06}, {"x": 0.48414711477488903, "y": 0.18547875713379836, "ox": 0.48414711477488903, "oy": 0.18547875713379836, "term": "this limitation", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.1601141407736208, "os": -0.0040281262146213935, "bg": 0.0}, {"x": 0.09162967660114141, "y": 0.23937856689917564, "ox": 0.09162967660114141, "oy": 0.23937856689917564, "term": "many applications", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.09194673430564362, "y": 0.4949270767279645, "ox": 0.09194673430564362, "oy": 0.4949270767279645, "term": "estimates", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 1.0468679510209742e-06}, {"x": 0.30469245402663286, "y": 0.18579581483830057, "ox": 0.30469245402663286, "oy": 0.18579581483830057, "term": "achieves state", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.09226379201014584, "y": 0.5662650602409639, "ox": 0.09226379201014584, "oy": 0.5662650602409639, "term": "implicit", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 1, "s": 0.7691819911223844, "os": 0.050104236599413446, "bg": 9.780424326283193e-06}, {"x": 0.19086873811033608, "y": 0.7523779327837666, "ox": 0.19086873811033608, "oy": 0.7523779327837666, "term": "personalized", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 2, "s": 0.8877615726062144, "os": 0.09284124235892725, "bg": 5.500244053686153e-06}, {"x": 0.09258084971464807, "y": 0.3563728598604946, "ox": 0.09258084971464807, "oy": 0.3563728598604946, "term": "implicitly", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 1.4485301040768878e-05}, {"x": 0.6448953709575143, "y": 0.6930881420418517, "ox": 0.6448953709575143, "oy": 0.6930881420418517, "term": "events", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.6769181991122385, "os": 0.037366170806685285, "bg": 3.375253150190774e-07}, {"x": 0.39917564996829424, "y": 0.30849714648065946, "ox": 0.39917564996829424, "oy": 0.30849714648065946, "term": "large amounts", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 0.0}, {"x": 0.09289790741915029, "y": 0.23969562460367788, "ox": 0.09289790741915029, "oy": 0.23969562460367788, "term": "adapts", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 2.7770642265528996e-05}, {"x": 0.0932149651236525, "y": 0.2400126823081801, "ox": 0.0932149651236525, "oy": 0.2400126823081801, "term": "an adaptive", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.5459733671528219, "y": 0.07926442612555486, "ox": 0.5459733671528219, "oy": 0.07926442612555486, "term": "degrees", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.09099556119213698, "os": -0.01595349987632946, "bg": 8.473929954494997e-07}, {"x": 0.09353202282815472, "y": 0.39949270767279643, "ox": 0.09353202282815472, "oy": 0.39949270767279643, "term": "quantity", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 6.778931331720017e-07}, {"x": 0.30500951173113505, "y": 0.5665821179454661, "ox": 0.30500951173113505, "oy": 0.5665821179454661, "term": "enable", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 3, "s": 0.6889663918833228, "os": 0.039115225610402454, "bg": 1.050948218029717e-06}, {"x": 0.3053265694356373, "y": 0.43405199746353834, "ox": 0.3053265694356373, "oy": 0.43405199746353834, "term": "aware", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 1.234033439066861e-06}, {"x": 0.1911857958148383, "y": 0.3088142041851617, "ox": 0.1911857958148383, "oy": 0.3088142041851617, "term": "tri", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 2.346912602245937e-06}, {"x": 0.19150285351934052, "y": 0.1861128725428028, "ox": 0.19150285351934052, "oy": 0.1861128725428028, "term": "tri factorization", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.3056436271401395, "y": 0.1274571972098922, "ox": 0.3056436271401395, "oy": 0.1274571972098922, "term": "dense", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 3.6654983501591923e-06}, {"x": 0.4844641724793913, "y": 0.30913126188966394, "ox": 0.4844641724793913, "oy": 0.30913126188966394, "term": "infer", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 2.246455654590969e-05}, {"x": 0.30596068484464173, "y": 0.6081166772352569, "ox": 0.30596068484464173, "oy": 0.6081166772352569, "term": "intrinsic", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 3, "s": 0.7371591629676602, "os": 0.04554609377760503, "bg": 1.536356370551789e-05}, {"x": 0.6452124286620164, "y": 0.029803424223208624, "ox": 0.6452124286620164, "oy": 0.029803424223208624, "term": "multiple instance", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.03582752060875079, "os": -0.03337337903254302, "bg": 0.0}, {"x": 0.6005072923272036, "y": 0.030120481927710843, "ox": 0.6005072923272036, "oy": 0.030120481927710843, "term": "safe", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.04660748256182626, "os": -0.02787887353803753, "bg": 3.4700857863566923e-07}, {"x": 0.19181991122384273, "y": 0.5218769816106531, "ox": 0.19181991122384273, "oy": 0.5218769816106531, "term": "selective", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 2, "s": 0.6829422954977806, "os": 0.03817886293770538, "bg": 6.095932709030803e-06}, {"x": 0.09384908053265695, "y": 0.30944831959416613, "ox": 0.09384908053265695, "oy": 0.30944831959416613, "term": "selects", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 6.121519394503822e-06}, {"x": 0.3062777425491439, "y": 0.186429930247305, "ox": 0.3062777425491439, "oy": 0.186429930247305, "term": "sum", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 1.0524694785047223e-06}, {"x": 0.546290424857324, "y": 0.544705136334813, "ox": 0.546290424857324, "oy": 0.544705136334813, "term": "lingual", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.4663918833227647, "os": 0.019416275043284685, "bg": 3.5245828158198606e-05}, {"x": 0.5466074825618262, "y": 0.3097653772986684, "ox": 0.5466074825618262, "oy": 0.3097653772986684, "term": "- lingual", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.16772352568167406, "os": -0.0030917635419243153, "bg": 0.0}, {"x": 0.09416613823715916, "y": 0.18674698795180722, "ox": 0.09416613823715916, "oy": 0.18674698795180722, "term": "well established", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.30659480025364616, "y": 0.12777425491439443, "ox": 0.30659480025364616, "oy": 0.12777425491439443, "term": "nowadays", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 7.198721794958088e-06}, {"x": 0.19213696892834495, "y": 0.12809131261889664, "ox": 0.19213696892834495, "oy": 0.12809131261889664, "term": "compensate for", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.3069118579581484, "y": 0.12840837032339886, "ox": 0.3069118579581484, "oy": 0.12840837032339886, "term": "missing data", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.1924540266328472, "y": 0.18706404565630944, "ox": 0.1924540266328472, "oy": 0.18706404565630944, "term": "offers", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 1.4052957065033173e-07}, {"x": 0.1927710843373494, "y": 0.35668991756499685, "ox": 0.1927710843373494, "oy": 0.35668991756499685, "term": "implementation", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 5.514836851930046e-07}, {"x": 0.6832593532022828, "y": 0.7117945466074825, "ox": 0.6832593532022828, "oy": 0.7117945466074825, "term": "languages", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 9, "s": 0.684844641724794, "os": 0.03830253347938235, "bg": 1.9677220198038992e-06}, {"x": 0.712428662016487, "y": 0.6420418516169943, "ox": 0.712428662016487, "oy": 0.6420418516169943, "term": "vectors", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 10, "s": 0.39124920735573876, "os": 0.013515423483269139, "bg": 1.4667935137474077e-05}, {"x": 0.8344958782498415, "y": 0.6934051997463538, "ox": 0.8344958782498415, "oy": 0.6934051997463538, "term": "word", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 18, "s": 0.08243500317057705, "os": -0.017578884138369666, "bg": 8.897672689342905e-07}, {"x": 0.09448319594166138, "y": 0.3998097653772987, "ox": 0.09448319594166138, "oy": 0.3998097653772987, "term": "position", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 2.97584704109583e-07}, {"x": 0.6008243500317058, "y": 0.7213062777425492, "ox": 0.6008243500317058, "oy": 0.7213062777425492, "term": "words", "cat25k": 4, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 7, "s": 0.7844007609384909, "os": 0.05250697855199462, "bg": 7.89984220174922e-07}, {"x": 0.5469245402663284, "y": 0.6699429296131896, "ox": 0.5469245402663284, "oy": 0.6699429296131896, "term": "contains", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 6, "s": 0.712428662016487, "os": 0.04192431362849369, "bg": 1.0834853643880465e-06}, {"x": 0.48478123018389346, "y": 0.7216233354470514, "ox": 0.48478123018389346, "oy": 0.7216233354470514, "term": "components", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 5, "s": 0.8246670894102728, "os": 0.0634959895410056, "bg": 1.2319810988010434e-06}, {"x": 0.0948002536461636, "y": 0.18738110336081168, "ox": 0.0948002536461636, "oy": 0.18738110336081168, "term": "maximizing", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 7.195872447564077e-06}, {"x": 0.09511731135066583, "y": 0.2403297400126823, "ox": 0.09511731135066583, "oy": 0.2403297400126823, "term": "performs better", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.5472415979708307, "y": 0.019023462270133164, "ox": 0.5472415979708307, "oy": 0.019023462270133164, "term": "marine", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 4.816181986179003e-07}, {"x": 0.5475586556753329, "y": 0.1876981610653139, "ox": 0.5475586556753329, "oy": 0.1876981610653139, "term": "probabilities", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 6, "s": 0.11953075459733671, "os": -0.00952263170912689, "bg": 1.1155991843376248e-05}, {"x": 0.39949270767279643, "y": 0.12872542802790107, "ox": 0.39949270767279643, "oy": 0.12872542802790107, "term": "sentence", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 1.5078030521360602e-06}, {"x": 0.4850982878883957, "y": 0.35700697526949904, "ox": 0.4850982878883957, "oy": 0.35700697526949904, "term": "texts", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 2.5960074541109037e-06}, {"x": 0.6455294863665187, "y": 0.6794546607482562, "ox": 0.6455294863665187, "oy": 0.6794546607482562, "term": "group", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 8, "s": 0.648700063411541, "os": 0.03415073672308399, "bg": 2.0492191451999654e-07}, {"x": 0.6011414077362079, "y": 0.8015218769816107, "ox": 0.6011414077362079, "oy": 0.8015218769816107, "term": "gp", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 7, "s": 0.8852251109701966, "os": 0.09109218755521006, "bg": 8.766269076794068e-06}, {"x": 0.6014584654407102, "y": 0.4343690551680406, "ox": 0.6014584654407102, "oy": 0.4343690551680406, "term": "scalable", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 7, "s": 0.2171845275840203, "os": 0.0010600332143740482, "bg": 1.2066492406405499e-05}, {"x": 0.19308814204185162, "y": 0.1880152187698161, "ox": 0.19308814204185162, "oy": 0.1880152187698161, "term": "spoken", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 1.9009000001140543e-06}, {"x": 0.6017755231452124, "y": 0.07958148383005707, "ox": 0.6017755231452124, "oy": 0.07958148383005707, "term": "english", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.06785034876347495, "os": -0.021448005370834956, "bg": 1.5063518134138095e-07}, {"x": 0.3072289156626506, "y": 0.1290424857324033, "ox": 0.3072289156626506, "oy": 0.1290424857324033, "term": "digit", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 2.7693401368635588e-06}, {"x": 0.5478757133798351, "y": 0.608433734939759, "ox": 0.5478757133798351, "oy": 0.608433734939759, "term": "convolution", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 6, "s": 0.5900443880786304, "os": 0.02906257729408855, "bg": 7.694254019877811e-05}, {"x": 0.3998097653772987, "y": 0.4708306911857958, "ox": 0.3998097653772987, "oy": 0.4708306911857958, "term": "convolution neural", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 0.0}, {"x": 0.30754597336715284, "y": 0.07989854153455929, "ox": 0.30754597336715284, "oy": 0.07989854153455929, "term": "ocr", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 7.443236021292619e-06}, {"x": 0.19340519974635384, "y": 0.5221940393151554, "ox": 0.19340519974635384, "oy": 0.5221940393151554, "term": "intermediate", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 2, "s": 0.6829422954977806, "os": 0.03817886293770538, "bg": 2.687980159122452e-06}, {"x": 0.307863031071655, "y": 0.0802155992390615, "ox": 0.307863031071655, "oy": 0.0802155992390615, "term": "candidates", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 8.869284922279689e-07}, {"x": 0.19372225745085606, "y": 0.3573240329740013, "ox": 0.19372225745085606, "oy": 0.3573240329740013, "term": "good performance", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 0.0}, {"x": 0.30818008877615727, "y": 0.18833227647431833, "ox": 0.30818008877615727, "oy": 0.18833227647431833, "term": "proposing", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 7.653638087286262e-06}, {"x": 0.19403931515535827, "y": 0.35764109067850347, "ox": 0.19403931515535827, "oy": 0.35764109067850347, "term": "limited amount", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 0.0}, {"x": 0.1943563728598605, "y": 0.18864933417882054, "ox": 0.1943563728598605, "oy": 0.18864933417882054, "term": "qualitative", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 4.480363016933084e-06}, {"x": 0.09543436905516804, "y": 0.40012682308180086, "ox": 0.09543436905516804, "oy": 0.40012682308180086, "term": "represents", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 1.3056479719972656e-06}, {"x": 0.6020925808497146, "y": 0.7783766645529486, "ox": 0.6020925808497146, "oy": 0.7783766645529486, "term": "rgb", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 7, "s": 0.8623969562460367, "os": 0.07823045122080491, "bg": 2.0548332245984082e-05}, {"x": 0.30849714648065946, "y": 0.7460367786937222, "ox": 0.30849714648065946, "oy": 0.7460367786937222, "term": "what", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 3, "s": 0.8747622067216233, "os": 0.08413130278082045, "bg": 8.614043485677669e-08}, {"x": 0.4854153455928979, "y": 0.7305009511731135, "ox": 0.4854153455928979, "oy": 0.7305009511731135, "term": "observed", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 5, "s": 0.8360811667723526, "os": 0.06671142362460691, "bg": 2.937898861740465e-06}, {"x": 0.48573240329740014, "y": 0.43468611287254283, "ox": 0.48573240329740014, "oy": 0.43468611287254283, "term": "presence", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.3617628408370323, "os": 0.01204904420338504, "bg": 1.2485192475021898e-06}, {"x": 0.09575142675967026, "y": 0.31008243500317056, "ox": 0.09575142675967026, "oy": 0.31008243500317056, "term": "be effectively", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.1946734305643627, "y": 0.6702599873176919, "ox": 0.1946734305643627, "oy": 0.6702599873176919, "term": "cognitive", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 2, "s": 0.8256182625237795, "os": 0.06390233560651568, "bg": 6.15905620517806e-06}, {"x": 0.19499048826886492, "y": 0.1293595434369055, "ox": 0.19499048826886492, "oy": 0.1293595434369055, "term": "encoder decoder", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.09606848446417247, "y": 0.3579581483830057, "ox": 0.09606848446417247, "oy": 0.3579581483830057, "term": "extracts", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 5.8249260234395025e-06}, {"x": 0.5481927710843374, "y": 0.04597336715282181, "ox": 0.5481927710843374, "oy": 0.04597336715282181, "term": "feature vectors", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 0.0}, {"x": 0.19530754597336716, "y": 0.3582752060875079, "ox": 0.19530754597336716, "oy": 0.3582752060875079, "term": "generates", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 4.7932125161102636e-06}, {"x": 0.0963855421686747, "y": 0.35859226379201015, "ox": 0.0963855421686747, "oy": 0.35859226379201015, "term": "sometimes", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 5.038432004691116e-07}, {"x": 0.40012682308180086, "y": 0.3103994927076728, "ox": 0.40012682308180086, "oy": 0.3103994927076728, "term": "relying", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 4.311918651342724e-06}, {"x": 0.4004438807863031, "y": 0.18896639188332276, "ox": 0.4004438807863031, "oy": 0.18896639188332276, "term": "evolved", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 4.987386069399477e-06}, {"x": 0.19562460367786938, "y": 0.47114774889029803, "ox": 0.19562460367786938, "oy": 0.47114774889029803, "term": "starting", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 2, "s": 0.6195307545973368, "os": 0.0317479947705028, "bg": 4.6844507153624693e-07}, {"x": 0.3088142041851617, "y": 0.12967660114140775, "ox": 0.3088142041851617, "oy": 0.12967660114140775, "term": "detects", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 9.622587677410445e-06}, {"x": 0.40076093849080535, "y": 0.7387444514901712, "ox": 0.40076093849080535, "oy": 0.7387444514901712, "term": "body", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 4, "s": 0.8563728598604946, "os": 0.07542136320271368, "bg": 5.117267334796097e-07}, {"x": 0.1959416613823716, "y": 0.5878249841471148, "ox": 0.1959416613823716, "oy": 0.5878249841471148, "term": "principal", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7549143944197845, "os": 0.04782516518850924, "bg": 1.4896244465247168e-06}, {"x": 0.40107799619530754, "y": 0.4714648065948003, "ox": 0.40107799619530754, "oy": 0.4714648065948003, "term": "follow", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 5.201267826370238e-07}, {"x": 0.4013950538998098, "y": 0.6556753329105898, "ox": 0.4013950538998098, "oy": 0.6556753329105898, "term": "d", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 4, "s": 0.7663284717818644, "os": 0.0496978905339034, "bg": 1.4928481501649985e-07}, {"x": 0.30913126188966394, "y": 0.12999365884590997, "ox": 0.30913126188966394, "oy": 0.12999365884590997, "term": "many cases", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 0.0}, {"x": 0.1962587190868738, "y": 0.435003170577045, "ox": 0.1962587190868738, "oy": 0.435003170577045, "term": "desirable", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 4.894356936967861e-06}, {"x": 0.5485098287888396, "y": 0.655992390615092, "ox": 0.5485098287888396, "oy": 0.655992390615092, "term": "accurately", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 6, "s": 0.6873811033608117, "os": 0.03870887954489241, "bg": 8.5059665689116e-06}, {"x": 0.5488268864933418, "y": 0.08053265694356374, "ox": 0.5488268864933418, "oy": 0.08053265694356374, "term": "trajectory", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.09099556119213698, "os": -0.01595349987632946, "bg": 1.2955570490449317e-05}, {"x": 0.549143944197844, "y": 0.18928344958782498, "ox": 0.549143944197844, "oy": 0.18928344958782498, "term": "trajectories", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 6, "s": 0.11953075459733671, "os": -0.00952263170912689, "bg": 2.5448437829466385e-05}, {"x": 0.683576410906785, "y": 0.01077996195307546, "ox": 0.683576410906785, "oy": 0.01077996195307546, "term": "intersection", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.02219403931515536, "os": -0.045298752694251085, "bg": 3.3661306664602107e-06}, {"x": 0.40171211160431197, "y": 0.7786937222574508, "ox": 0.40171211160431197, "oy": 0.7786937222574508, "term": "feature selection", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 4, "s": 0.8906150919467343, "os": 0.0947139677043214, "bg": 0.0}, {"x": 0.09670259987317692, "y": 0.4004438807863031, "ox": 0.09670259987317692, "oy": 0.4004438807863031, "term": "nonnegative", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 5.045633877162103e-05}, {"x": 0.19657577679137603, "y": 0.13031071655041218, "ox": 0.19657577679137603, "oy": 0.13031071655041218, "term": "result shows", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.4860494610019023, "y": 0.030437539632213063, "ox": 0.4860494610019023, "oy": 0.030437539632213063, "term": "demonstrations", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.08433734939759037, "os": -0.016889862549026538, "bg": 4.660950787351112e-06}, {"x": 0.19689283449587824, "y": 0.40076093849080535, "ox": 0.19689283449587824, "oy": 0.40076093849080535, "term": "skill", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 1.74281394241195e-06}, {"x": 0.30944831959416613, "y": 0.6423589093214965, "ox": 0.30944831959416613, "oy": 0.6423589093214965, "term": "quickly", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 3, "s": 0.7786937222574509, "os": 0.051976961944807595, "bg": 1.2378546345215622e-06}, {"x": 0.19720989220038046, "y": 0.6426759670259987, "ox": 0.19720989220038046, "oy": 0.6426759670259987, "term": "rgb d", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 2, "s": 0.8046924540266329, "os": 0.0574714674393131, "bg": 0.0}, {"x": 0.6024096385542169, "y": 0.046290424857324035, "ox": 0.6024096385542169, "oy": 0.046290424857324035, "term": "business", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.05770450221940394, "os": -0.02466343945443624, "bg": 3.765502863585696e-08}, {"x": 0.6838934686112873, "y": 0.8018389346861129, "ox": 0.6838934686112873, "oy": 0.8018389346861129, "term": "topic", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 9, "s": 0.8677869372225745, "os": 0.08010317656619909, "bg": 6.700947193627902e-07}, {"x": 0.09701965757767914, "y": 0.43532022828154726, "ox": 0.09701965757767914, "oy": 0.43532022828154726, "term": "failure", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 7.87074084697492e-07}, {"x": 0.6458465440710209, "y": 0.47178186429930247, "ox": 0.6458465440710209, "oy": 0.47178186429930247, "term": "scores", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.18611287254280282, "os": -0.0012190381965301589, "bg": 1.8925610307919682e-06}, {"x": 0.19752694990488268, "y": 0.6797717184527584, "ox": 0.19752694990488268, "oy": 0.6797717184527584, "term": "give", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 2, "s": 0.8373493975903614, "os": 0.06711776969011696, "bg": 3.831974693695893e-07}, {"x": 0.7618896639188333, "y": 0.0031705770450221942, "ox": 0.7618896639188333, "oy": 0.0031705770450221942, "term": "yawning", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 12, "s": 0.010145846544071021, "os": -0.06821313734497014, "bg": 4.519569736961042e-05}, {"x": 0.3097653772986684, "y": 0.24064679771718453, "ox": 0.3097653772986684, "oy": 0.24064679771718453, "term": "fatigue", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 4.57857550977192e-06}, {"x": 0.19784400760938492, "y": 0.310716550412175, "ox": 0.19784400760938492, "oy": 0.310716550412175, "term": "broad", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 1.1110586444529006e-06}, {"x": 0.19816106531388714, "y": 0.1896005072923272, "ox": 0.19816106531388714, "oy": 0.1896005072923272, "term": "a broad", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.4020291693088142, "y": 0.6800887761572606, "ox": 0.4020291693088142, "oy": 0.6800887761572606, "term": "unlike", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 4, "s": 0.798985415345593, "os": 0.056128758701105966, "bg": 4.4057126597241745e-06}, {"x": 0.31008243500317056, "y": 0.31103360811667724, "ox": 0.31008243500317056, "oy": 0.31103360811667724, "term": "demonstrating", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 7.270717910686501e-06}, {"x": 0.4023462270133164, "y": 0.1306277742549144, "ox": 0.4023462270133164, "oy": 0.1306277742549144, "term": "mnist", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 9.528223029715063e-05}, {"x": 0.48636651870640457, "y": 0.1309448319594166, "ox": 0.48636651870640457, "oy": 0.1309448319594166, "term": "preliminary", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 1.7994070503917195e-06}, {"x": 0.3103994927076728, "y": 0.43563728598604945, "ox": 0.3103994927076728, "oy": 0.43563728598604945, "term": "gestures", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 2.0471365996677244e-05}, {"x": 0.40266328471781865, "y": 0.3589093214965124, "ox": 0.40266328471781865, "oy": 0.3589093214965124, "term": "primary", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 4.589245543636061e-07}, {"x": 0.310716550412175, "y": 0.18991756499682944, "ox": 0.310716550412175, "oy": 0.18991756499682944, "term": "svm classifier", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.19847812301838935, "y": 0.13126188966391883, "ox": 0.19847812301838935, "oy": 0.13126188966391883, "term": "switching", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.7739726283907762e-06}, {"x": 0.31103360811667724, "y": 0.31135066582117943, "ox": 0.31103360811667724, "oy": 0.31135066582117943, "term": "uncertain", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.3849080532656943, "os": 0.013391752941592169, "bg": 6.2084333448268934e-06}, {"x": 0.19879518072289157, "y": 0.24096385542168675, "ox": 0.19879518072289157, "oy": 0.24096385542168675, "term": "library", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 1.3554536133953517e-07}, {"x": 0.19911223842739378, "y": 0.5450221940393152, "ox": 0.19911223842739378, "oy": 0.5450221940393152, "term": "entities", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 2, "s": 0.7089410272669626, "os": 0.04139429702130666, "bg": 3.274665968990464e-06}, {"x": 0.40298034242232084, "y": 0.49524413443246673, "ox": 0.40298034242232084, "oy": 0.49524413443246673, "term": "outcomes", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 4, "s": 0.5206087507926442, "os": 0.023974417865093106, "bg": 2.6213433639726984e-06}, {"x": 0.31135066582117943, "y": 0.4359543436905517, "ox": 0.31135066582117943, "oy": 0.4359543436905517, "term": "engine", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 4.0863780116494215e-07}, {"x": 0.4032974001268231, "y": 0.3116677235256817, "ox": 0.4032974001268231, "oy": 0.3116677235256817, "term": "deployed", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 5.470659388345112e-06}, {"x": 0.3116677235256817, "y": 0.24128091312618896, "ox": 0.3116677235256817, "oy": 0.24128091312618896, "term": "feed", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 3.8159030909435124e-07}, {"x": 0.3119847812301839, "y": 0.24159797083069118, "ox": 0.3119847812301839, "oy": 0.24159797083069118, "term": "entire", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 4.2466668663779723e-07}, {"x": 0.199429296131896, "y": 0.40107799619530754, "ox": 0.199429296131896, "oy": 0.40107799619530754, "term": "convnet", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 0.00012126671748319592}, {"x": 0.48668357641090676, "y": 0.08084971464806595, "ox": 0.48668357641090676, "oy": 0.08084971464806595, "term": "extracted by", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 0.0}, {"x": 0.5494610019023463, "y": 0.6087507926442612, "ox": 0.5494610019023463, "oy": 0.6087507926442612, "term": "pedestrian detection", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 6, "s": 0.5900443880786304, "os": 0.02906257729408855, "bg": 0.0}, {"x": 0.7127457197209892, "y": 0.0034876347495244133, "ox": 0.7127457197209892, "oy": 0.0034876347495244133, "term": "listings", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.014901712111604312, "os": -0.05722412635595915, "bg": 2.2978184844492318e-07}, {"x": 0.09733671528218135, "y": 0.4013950538998098, "ox": 0.09733671528218135, "oy": 0.4013950538998098, "term": "sense", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 4.918603915571936e-07}, {"x": 0.3123018389346861, "y": 0.13157894736842105, "ox": 0.3123018389346861, "oy": 0.13157894736842105, "term": "session", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.24635383639822447, "os": 0.0037454506907883094, "bg": 3.6117779356124736e-07}, {"x": 0.7130627774254914, "y": 0.6261889663918834, "ox": 0.7130627774254914, "oy": 0.6261889663918834, "term": "autoencoder", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 10, "s": 0.3487634749524413, "os": 0.010299989399667857, "bg": 0.0002684993915457337}, {"x": 0.09765377298668358, "y": 0.4362714013950539, "ox": 0.09765377298668358, "oy": 0.4362714013950539, "term": "described", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 6.203479442998443e-07}, {"x": 0.19974635383639822, "y": 0.40171211160431197, "ox": 0.19974635383639822, "oy": 0.40171211160431197, "term": "are represented", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 0.0}, {"x": 0.20006341154090043, "y": 0.19023462270133165, "ox": 0.20006341154090043, "oy": 0.19023462270133165, "term": "youtube", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 5.6813823939640995e-05}, {"x": 0.0979708306911858, "y": 0.19055168040583387, "ox": 0.0979708306911858, "oy": 0.19055168040583387, "term": "nearest neighbor", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.09828788839568801, "y": 0.4365884590995561, "ox": 0.09828788839568801, "oy": 0.4365884590995561, "term": "recommender system", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 0.0}, {"x": 0.5497780596068484, "y": 0.019340519974635383, "ox": 0.5497780596068484, "oy": 0.019340519974635383, "term": "workshop", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 5.177960311106813e-07}, {"x": 0.4036144578313253, "y": 0.19086873811033608, "ox": 0.4036144578313253, "oy": 0.19086873811033608, "term": "ideas", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.21845275840202916, "os": 0.0014663792798841023, "bg": 3.55342081825776e-07}, {"x": 0.31261889663918835, "y": 0.1911857958148383, "ox": 0.31261889663918835, "oy": 0.1911857958148383, "term": "profile", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 1.088647378296126e-07}, {"x": 0.09860494610019023, "y": 0.3592263792010146, "ox": 0.09860494610019023, "oy": 0.3592263792010146, "term": "services", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 4.267139291221549e-08}, {"x": 0.09892200380469246, "y": 0.4020291693088142, "ox": 0.09892200380469246, "oy": 0.4020291693088142, "term": "tweets", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 0.0001126052967799216}, {"x": 0.09923906150919468, "y": 0.3119847812301839, "ox": 0.09923906150919468, "oy": 0.3119847812301839, "term": "even if", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.20038046924540268, "y": 0.5453392517438174, "ox": 0.20038046924540268, "oy": 0.5453392517438174, "term": "keyword", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 2, "s": 0.7089410272669626, "os": 0.04139429702130666, "bg": 9.626225323488034e-07}, {"x": 0.09955611921369689, "y": 0.588142041851617, "ox": 0.09955611921369689, "oy": 0.588142041851617, "term": "student", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 1, "s": 0.7882054533925175, "os": 0.05331967068301473, "bg": 2.7812337117482236e-07}, {"x": 0.31293595434369054, "y": 0.08116677235256817, "ox": 0.31293595434369054, "oy": 0.08116677235256817, "term": "companies", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.5210800012787212e-07}, {"x": 0.2006975269499049, "y": 0.6090678503487634, "ox": 0.2006975269499049, "oy": 0.6090678503487634, "term": "try", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 2, "s": 0.7726696258719088, "os": 0.051040599272110534, "bg": 3.838935305494444e-07}, {"x": 0.2010145846544071, "y": 0.19150285351934052, "ox": 0.2010145846544071, "oy": 0.19150285351934052, "term": "is formulated", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.4039315155358275, "y": 0.3123018389346861, "ox": 0.4039315155358275, "oy": 0.3123018389346861, "term": "signs", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.29549778059606846, "os": 0.007897247447086676, "bg": 8.714498494772686e-07}, {"x": 0.20133164235890932, "y": 0.13189600507292326, "ox": 0.20133164235890932, "oy": 0.13189600507292326, "term": "gains", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.7360345902963191e-06}, {"x": 0.20164870006341154, "y": 0.19181991122384273, "ox": 0.20164870006341154, "oy": 0.19181991122384273, "term": "acquire", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 2.2137161409349103e-06}, {"x": 0.20196575776791376, "y": 0.1322130627774255, "ox": 0.20196575776791376, "oy": 0.1322130627774255, "term": "to acquire", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.09987317691819911, "y": 0.4955611921369689, "ox": 0.09987317691819911, "oy": 0.4955611921369689, "term": "when there", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 0.0}, {"x": 0.20228281547241597, "y": 0.4023462270133164, "ox": 0.20228281547241597, "oy": 0.4023462270133164, "term": "perform well", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 0.0}, {"x": 0.6461636017755231, "y": 0.5668991756499683, "ox": 0.6461636017755231, "oy": 0.5668991756499683, "term": "activities", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 8, "s": 0.35954343690551677, "os": 0.01164269813787499, "bg": 3.912242443349488e-07}, {"x": 0.40424857324032976, "y": 0.13253012048192772, "ox": 0.40424857324032976, "oy": 0.13253012048192772, "term": "reach", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 6.124924346744105e-07}, {"x": 0.2025998731769182, "y": 0.2419150285351934, "ox": 0.2025998731769182, "oy": 0.2419150285351934, "term": "rise", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 7.390936320095808e-07}, {"x": 0.20291693088142043, "y": 0.4369055168040583, "ox": 0.20291693088142043, "oy": 0.4369055168040583, "term": "can help", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 0.0}, {"x": 0.3132530120481928, "y": 0.19213696892834495, "ox": 0.3132530120481928, "oy": 0.19213696892834495, "term": "10 %", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.40456563094483194, "y": 0.6093849080532657, "ox": 0.40456563094483194, "oy": 0.6093849080532657, "term": "behaviors", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 4, "s": 0.6987951807228916, "os": 0.04005158828309954, "bg": 9.14545270943563e-06}, {"x": 0.31357006975269497, "y": 0.49587824984147116, "ox": 0.31357006975269497, "oy": 0.49587824984147116, "term": "transportation", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.5916296766011414, "os": 0.0294689233595986, "bg": 6.747418269084791e-07}, {"x": 0.10019023462270134, "y": 0.3595434369055168, "ox": 0.10019023462270134, "oy": 0.3595434369055168, "term": "most important", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 0.0}, {"x": 0.3138871274571972, "y": 0.359860494610019, "ox": 0.3138871274571972, "oy": 0.359860494610019, "term": "progress", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 5.844818645505863e-07}, {"x": 0.487000634115409, "y": 0.36017755231452125, "ox": 0.487000634115409, "oy": 0.36017755231452125, "term": "pascal voc", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.27235256816740644, "os": 0.005618176036182469, "bg": 0.0}, {"x": 0.10050729232720355, "y": 0.43722257450856056, "ox": 0.10050729232720355, "oy": 0.43722257450856056, "term": "2013", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 0.0}, {"x": 0.20323398858592265, "y": 0.13284717818642994, "ox": 0.20323398858592265, "oy": 0.13284717818642994, "term": "much better", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.5500951173113506, "y": 0.019657577679137603, "ox": 0.5500951173113506, "oy": 0.019657577679137603, "term": "pixel level", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 0.0}, {"x": 0.20355104629042486, "y": 0.24223208623969564, "ox": 0.20355104629042486, "oy": 0.24223208623969564, "term": "bootstrapping", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 4.3952970321755726e-05}, {"x": 0.20386810399492708, "y": 0.40266328471781865, "ox": 0.20386810399492708, "oy": 0.40266328471781865, "term": "two stage", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 0.0}, {"x": 0.2041851616994293, "y": 0.40298034242232084, "ox": 0.2041851616994293, "oy": 0.40298034242232084, "term": "forms", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 4.3892307089391387e-07}, {"x": 0.10082435003170577, "y": 0.24254914394419785, "ox": 0.10082435003170577, "oy": 0.24254914394419785, "term": "bottleneck", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 1.9834304222525027e-05}, {"x": 0.4048826886493342, "y": 0.6265060240963856, "ox": 0.4048826886493342, "oy": 0.6265060240963856, "term": "programming", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7232086239695625, "os": 0.043267022366700825, "bg": 9.83065710078193e-07}, {"x": 0.4051997463538364, "y": 0.5672162333544705, "ox": 0.4051997463538364, "oy": 0.5672162333544705, "term": "perspective", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 4, "s": 0.6385542168674699, "os": 0.033620720115896965, "bg": 2.0197694111890178e-06}, {"x": 0.5504121750158529, "y": 0.7219403931515536, "ox": 0.5504121750158529, "oy": 0.7219403931515536, "term": "unknown", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 6, "s": 0.8065948002536463, "os": 0.058001484046500115, "bg": 1.4416092544064898e-06}, {"x": 0.48731769181991125, "y": 0.08148383005707038, "ox": 0.48731769181991125, "oy": 0.08148383005707038, "term": "prone", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 6.806989664514419e-06}, {"x": 0.2045022194039315, "y": 0.4032974001268231, "ox": 0.2045022194039315, "oy": 0.4032974001268231, "term": "previous methods", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 0.0}, {"x": 0.10114140773620799, "y": 0.6268230818008877, "ox": 0.10114140773620799, "oy": 0.6268230818008877, "term": "experiments demonstrate", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.8113506658211795, "os": 0.059750538850217305, "bg": 0.0}, {"x": 0.10145846544071022, "y": 0.6429930247305009, "ox": 0.10145846544071022, "oy": 0.6429930247305009, "term": "one shot", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.8230818008877616, "os": 0.06296597293381859, "bg": 0.0}, {"x": 0.4055168040583386, "y": 0.5884590995561192, "ox": 0.4055168040583386, "oy": 0.5884590995561192, "term": "one class", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 4, "s": 0.6718452758402029, "os": 0.03683615419949825, "bg": 0.0}, {"x": 0.6027266962587191, "y": 0.019974635383639822, "ox": 0.6027266962587191, "oy": 0.019974635383639822, "term": "decision boundary", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.03804692454026633, "os": -0.031094307621638815, "bg": 0.0}, {"x": 0.20481927710843373, "y": 0.4720989220038047, "ox": 0.20481927710843373, "oy": 0.4720989220038047, "term": "latent space", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 2, "s": 0.6195307545973368, "os": 0.0317479947705028, "bg": 0.0}, {"x": 0.20513633481293594, "y": 0.24286620164870007, "ox": 0.20513633481293594, "oy": 0.24286620164870007, "term": "grained", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 1.815117120432196e-05}, {"x": 0.20545339251743816, "y": 0.4724159797083069, "ox": 0.20545339251743816, "oy": 0.4724159797083069, "term": "car", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 2, "s": 0.6195307545973368, "os": 0.0317479947705028, "bg": 1.2077692535101322e-07}, {"x": 0.2057704502219404, "y": 0.24318325935320229, "ox": 0.2057704502219404, "oy": 0.24318325935320229, "term": "fine grained", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 0.0}, {"x": 0.10177552314521243, "y": 0.31261889663918835, "ox": 0.10177552314521243, "oy": 0.31261889663918835, "term": "loss function", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.20608750792644262, "y": 0.5887761572606214, "ox": 0.20608750792644262, "oy": 0.5887761572606214, "term": "bound", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7549143944197845, "os": 0.04782516518850924, "bg": 1.8638050218540014e-06}, {"x": 0.10209258084971465, "y": 0.2435003170577045, "ox": 0.10209258084971465, "oy": 0.2435003170577045, "term": "fisher", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 1.8412193217187379e-06}, {"x": 0.5507292327203551, "y": 0.771718452758402, "ox": 0.5507292327203551, "oy": 0.771718452758402, "term": "probabilistic", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 6, "s": 0.8681039949270767, "os": 0.08050952263170913, "bg": 5.057721243693653e-05}, {"x": 0.31420418516169946, "y": 0.36049461001902344, "ox": 0.31420418516169946, "oy": 0.36049461001902344, "term": "numbers", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.41946734305643624, "os": 0.016607187025193457, "bg": 4.2873743745716076e-07}, {"x": 0.20640456563094484, "y": 0.49619530754597335, "ox": 0.20640456563094484, "oy": 0.49619530754597335, "term": "existing approaches", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6540900443880786, "os": 0.0349634288541041, "bg": 0.0}, {"x": 0.10240963855421686, "y": 0.24381737476220672, "ox": 0.10240963855421686, "oy": 0.24381737476220672, "term": "performs well", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.20672162333544705, "y": 0.31293595434369054, "ox": 0.20672162333544705, "oy": 0.31293595434369054, "term": "relying on", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.10272669625871908, "y": 0.47273303741280914, "ox": 0.10272669625871908, "oy": 0.47273303741280914, "term": "consistency", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.674698795180723, "os": 0.0372425002650083, "bg": 4.592918882622447e-06}, {"x": 0.20703868103994927, "y": 0.5225110970196576, "ox": 0.20703868103994927, "oy": 0.5225110970196576, "term": "spatio", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 2, "s": 0.6829422954977806, "os": 0.03817886293770538, "bg": 7.682769894639348e-05}, {"x": 0.20735573874445148, "y": 0.5228281547241598, "ox": 0.20735573874445148, "oy": 0.5228281547241598, "term": "spatio temporal", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 2, "s": 0.6829422954977806, "os": 0.03817886293770538, "bg": 0.0}, {"x": 0.6030437539632213, "y": 0.5890932149651237, "ox": 0.6030437539632213, "oy": 0.5890932149651237, "term": "universal", "cat25k": 3, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.47368421052631576, "os": 0.02035263771598176, "bg": 1.3784188697908956e-06}, {"x": 0.2076727964489537, "y": 0.4965123652504756, "ox": 0.2076727964489537, "oy": 0.4965123652504756, "term": "parametric", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6540900443880786, "os": 0.0349634288541041, "bg": 1.8571613507243748e-05}, {"x": 0.10304375396322131, "y": 0.24413443246670893, "ox": 0.10304375396322131, "oy": 0.24413443246670893, "term": "families", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 4.212122843361513e-07}, {"x": 0.10336081166772353, "y": 0.24445149017121115, "ox": 0.10336081166772353, "oy": 0.24445149017121115, "term": "smooth", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 1.0601947291464866e-06}, {"x": 0.20798985415345592, "y": 0.1924540266328472, "ox": 0.20798985415345592, "oy": 0.1924540266328472, "term": "can successfully", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.6033608116677235, "y": 0.030754597336715282, "ox": 0.6033608116677235, "oy": 0.030754597336715282, "term": "cells", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.04660748256182626, "os": -0.02787887353803753, "bg": 4.95953772419393e-07}, {"x": 0.10367786937222574, "y": 0.4375396322130628, "ox": 0.10367786937222574, "oy": 0.4375396322130628, "term": "relationships between", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 0.0}, {"x": 0.20830691185795816, "y": 0.4036144578313253, "ox": 0.20830691185795816, "oy": 0.4036144578313253, "term": "discovery", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 1.2205372656835443e-06}, {"x": 0.10399492707672796, "y": 0.47305009511731133, "ox": 0.10399492707672796, "oy": 0.47305009511731133, "term": "functional", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.674698795180723, "os": 0.0372425002650083, "bg": 7.126934552932384e-07}, {"x": 0.31452124286620164, "y": 0.5456563094483196, "ox": 0.31452124286620164, "oy": 0.5456563094483196, "term": "regularized", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.6610653138871275, "os": 0.03589979152680117, "bg": 0.00011589130554452977}, {"x": 0.3148383005707039, "y": 0.2447685478757134, "ox": 0.3148383005707039, "oy": 0.2447685478757134, "term": "yields", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 4.07179386950715e-06}, {"x": 0.20862396956246038, "y": 0.1927710843373494, "ox": 0.20862396956246038, "oy": 0.1927710843373494, "term": "more accurate", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.40583386176284086, "y": 0.5675332910589728, "ox": 0.40583386176284086, "oy": 0.5675332910589728, "term": "tests", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 4, "s": 0.6385542168674699, "os": 0.033620720115896965, "bg": 1.0336640658973998e-06}, {"x": 0.3151553582752061, "y": 0.2450856055802156, "ox": 0.3151553582752061, "oy": 0.2450856055802156, "term": "an improvement", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.3154724159797083, "y": 0.567850348763475, "ox": 0.3154724159797083, "oy": 0.567850348763475, "term": "indoor", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 3, "s": 0.6889663918833228, "os": 0.039115225610402454, "bg": 2.9781277786995794e-06}, {"x": 0.2089410272669626, "y": 0.4039315155358275, "ox": 0.2089410272669626, "oy": 0.4039315155358275, "term": "majority", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 9.760354027561287e-07}, {"x": 0.40615091946734305, "y": 0.04660748256182625, "ox": 0.40615091946734305, "oy": 0.04660748256182625, "term": "labelling", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 4, "s": 0.12650602409638553, "os": -0.008179922970919754, "bg": 8.334525633528128e-06}, {"x": 0.10431198478123019, "y": 0.24540266328471783, "ox": 0.10431198478123019, "oy": 0.24540266328471783, "term": "setting where", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.5510462904248573, "y": 0.3608116677235257, "ox": 0.5510462904248573, "oy": 0.3608116677235257, "term": "games", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.19625871908687384, "os": 0.00012367054167697344, "bg": 1.1105239306556849e-07}, {"x": 0.1046290424857324, "y": 0.40424857324032976, "ox": 0.1046290424857324, "oy": 0.40424857324032976, "term": "optimisation", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.608433734939759, "os": 0.030811632097805727, "bg": 1.0782645912008633e-05}, {"x": 0.10494610019023462, "y": 0.19308814204185162, "ox": 0.10494610019023462, "oy": 0.19308814204185162, "term": "dialogue", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 1.5061147001731446e-06}, {"x": 0.10526315789473684, "y": 0.4968294229549778, "ox": 0.10526315789473684, "oy": 0.4968294229549778, "term": "ai", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 2.5641559840188977e-06}, {"x": 0.2092580849714648, "y": 0.3132530120481928, "ox": 0.2092580849714648, "oy": 0.3132530120481928, "term": "simply", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 3.8558004533296727e-07}, {"x": 0.3157894736842105, "y": 0.24571972098922004, "ox": 0.3157894736842105, "oy": 0.24571972098922004, "term": "existing algorithms", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 0.0}, {"x": 0.10558021559923907, "y": 0.5459733671528219, "ox": 0.10558021559923907, "oy": 0.5459733671528219, "term": "adaboost", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 1, "s": 0.7492073557387445, "os": 0.04688880251581216, "bg": 0.00013907990913445935}, {"x": 0.20957514267596702, "y": 0.13316423589093215, "ox": 0.20957514267596702, "oy": 0.13316423589093215, "term": "otl", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 4.548831708389562e-05}, {"x": 0.20989220038046924, "y": 0.4733671528218136, "ox": 0.20989220038046924, "oy": 0.4733671528218136, "term": "inductive transfer", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 2, "s": 0.6195307545973368, "os": 0.0317479947705028, "bg": 0.0}, {"x": 0.6036778693722258, "y": 0.0818008877615726, "ox": 0.6036778693722258, "oy": 0.0818008877615726, "term": "false", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.06785034876347495, "os": -0.021448005370834956, "bg": 7.71345007640024e-07}, {"x": 0.48763474952441344, "y": 0.13348129359543437, "ox": 0.48763474952441344, "oy": 0.13348129359543437, "term": "id", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 1.9965855890694926e-07}, {"x": 0.10589727330374128, "y": 0.24603677869372226, "ox": 0.10589727330374128, "oy": 0.24603677869372226, "term": "perception", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 2.336888948817108e-06}, {"x": 0.1062143310082435, "y": 0.5681674064679771, "ox": 0.1062143310082435, "oy": 0.5681674064679771, "term": "cm", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 1, "s": 0.7691819911223844, "os": 0.050104236599413446, "bg": 1.1000690553874925e-06}, {"x": 0.5513633481293595, "y": 0.031071655041217502, "ox": 0.5513633481293595, "oy": 0.031071655041217502, "term": "cloud", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.06499682942295498, "os": -0.022384368043532034, "bg": 1.7288905916540227e-06}, {"x": 0.21020925808497146, "y": 0.13379835129993659, "ox": 0.21020925808497146, "oy": 0.13379835129993659, "term": "dedicated", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 5.271255900146013e-07}, {"x": 0.21052631578947367, "y": 0.19340519974635384, "ox": 0.21052631578947367, "oy": 0.19340519974635384, "term": "dramatically", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 3.485357056532317e-06}, {"x": 0.10653138871274571, "y": 0.6433100824350032, "ox": 0.10653138871274571, "oy": 0.6433100824350032, "term": "counting", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.8230818008877616, "os": 0.06296597293381859, "bg": 5.5068307446839334e-06}, {"x": 0.10684844641724794, "y": 0.24635383639822447, "ox": 0.10684844641724794, "oy": 0.24635383639822447, "term": "aerial", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 2.9575832286102407e-06}, {"x": 0.31610653138871275, "y": 0.08211794546607483, "ox": 0.31610653138871275, "oy": 0.08211794546607483, "term": "fingerprint", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 7.604302514362628e-06}, {"x": 0.10716550412175016, "y": 0.19372225745085606, "ox": 0.10716550412175016, "oy": 0.19372225745085606, "term": "regressors", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 5.418014899540974e-05}, {"x": 0.4879518072289157, "y": 0.31357006975269497, "ox": 0.4879518072289157, "oy": 0.31357006975269497, "term": "software", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.23081800887761572, "os": 0.0024027419525811805, "bg": 8.091751070710618e-08}, {"x": 0.10748256182625238, "y": 0.2466708941027267, "ox": 0.10748256182625238, "oy": 0.2466708941027267, "term": "by reusing", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.10779961953075459, "y": 0.19403931515535827, "ox": 0.10779961953075459, "oy": 0.19403931515535827, "term": "knowledge base", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.4064679771718453, "y": 0.693722257450856, "ox": 0.4064679771718453, "oy": 0.693722257450856, "term": "resource", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 4, "s": 0.8100824350031707, "os": 0.05934419278470726, "bg": 5.988323069430763e-07}, {"x": 0.10811667723525682, "y": 0.1943563728598605, "ox": 0.10811667723525682, "oy": 0.1943563728598605, "term": "era", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 1.0488589987382224e-06}, {"x": 0.31642358909321494, "y": 0.08243500317057705, "ox": 0.31642358909321494, "oy": 0.08243500317057705, "term": "graph based", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.10843373493975904, "y": 0.3611287254280279, "ox": 0.10843373493975904, "oy": 0.3611287254280279, "term": "rare", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 8.012324825123499e-07}, {"x": 0.3167406467977172, "y": 0.2469879518072289, "ox": 0.3167406467977172, "oy": 0.2469879518072289, "term": "composite", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 2.468908215868167e-06}, {"x": 0.31705770450221943, "y": 0.24730500951173112, "ox": 0.31705770450221943, "oy": 0.24730500951173112, "term": "parking", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 6.876040377140501e-07}, {"x": 0.10875079264426125, "y": 0.24762206721623337, "ox": 0.10875079264426125, "oy": 0.24762206721623337, "term": "2011", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.6842105263157895, "y": 0.7308180088776157, "ox": 0.6842105263157895, "oy": 0.7308180088776157, "term": "corpus", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 9, "s": 0.7339885859226379, "os": 0.04473340164658493, "bg": 1.4529758342176926e-05}, {"x": 0.10906785034876347, "y": 0.3138871274571972, "ox": 0.10906785034876347, "oy": 0.3138871274571972, "term": "internal", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 4.170921480545173e-07}, {"x": 0.1093849080532657, "y": 0.523145212428662, "ox": 0.1093849080532657, "oy": 0.523145212428662, "term": "transition", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 1, "s": 0.7266962587190869, "os": 0.043673368432210875, "bg": 1.6779181773070944e-06}, {"x": 0.10970196575776792, "y": 0.3614457831325301, "ox": 0.10970196575776792, "oy": 0.3614457831325301, "term": "produced", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 6.112853826306467e-07}, {"x": 0.11001902346227013, "y": 0.1946734305643627, "ox": 0.11001902346227013, "oy": 0.1946734305643627, "term": "shapes", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 2.45802109637573e-06}, {"x": 0.21084337349397592, "y": 0.1341154090044388, "ox": 0.21084337349397592, "oy": 0.1341154090044388, "term": "connection", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 2.925243168151461e-07}, {"x": 0.4067850348763475, "y": 0.08275206087507926, "ox": 0.4067850348763475, "oy": 0.08275206087507926, "term": "blocks", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 9.942756073321264e-07}, {"x": 0.40710209258084973, "y": 0.13443246670894102, "ox": 0.40710209258084973, "oy": 0.13443246670894102, "term": "chemical", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 4, "s": 0.17691819911223844, "os": -0.001749054803717183, "bg": 5.48325168190022e-07}, {"x": 0.3173747622067216, "y": 0.08306911857958148, "ox": 0.3173747622067216, "oy": 0.08306911857958148, "term": "city", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 4.605986913991992e-08}, {"x": 0.31769181991122386, "y": 0.19499048826886492, "ox": 0.31769181991122386, "oy": 0.19499048826886492, "term": "examination", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 9.998952382489015e-07}, {"x": 0.603994927076728, "y": 0.03138871274571972, "ox": 0.603994927076728, "oy": 0.03138871274571972, "term": "cad", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.04660748256182626, "os": -0.02787887353803753, "bg": 1.8864441239966585e-06}, {"x": 0.6845275840202917, "y": 0.003804692454026633, "ox": 0.6845275840202917, "oy": 0.003804692454026633, "term": "af", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.016804058338617627, "os": -0.051729620861453655, "bg": 1.6096716225176964e-06}, {"x": 0.11033608116677235, "y": 0.49714648065948003, "ox": 0.11033608116677235, "oy": 0.49714648065948003, "term": "patients", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 5.434119587039516e-07}, {"x": 0.21116043119847813, "y": 0.5684844641724794, "ox": 0.21116043119847813, "oy": 0.5684844641724794, "term": "auc", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 2, "s": 0.7320862396956247, "os": 0.04460973110490796, "bg": 5.261094991700622e-05}, {"x": 0.11065313887127458, "y": 0.19530754597336716, "ox": 0.11065313887127458, "oy": 0.19530754597336716, "term": "with insufficient", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.21147748890298035, "y": 0.13474952441344323, "ox": 0.21147748890298035, "oy": 0.13474952441344323, "term": "mapping function", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 0.0}, {"x": 0.1109701965757768, "y": 0.49746353836398227, "ox": 0.1109701965757768, "oy": 0.49746353836398227, "term": "sentiment classification", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 0.0}, {"x": 0.11128725428027901, "y": 0.31420418516169946, "ox": 0.11128725428027901, "oy": 0.31420418516169946, "term": "modeled", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 8.668283695588789e-06}, {"x": 0.4074191502853519, "y": 0.47368421052631576, "ox": 0.4074191502853519, "oy": 0.47368421052631576, "term": "criterion", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 5.645563966748883e-06}, {"x": 0.21179454660748256, "y": 0.24793912492073558, "ox": 0.21179454660748256, "oy": 0.24793912492073558, "term": "imitation", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 9.957242694665227e-06}, {"x": 0.40773620798985416, "y": 0.08338617628408371, "ox": 0.40773620798985416, "oy": 0.08338617628408371, "term": "subsets", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 1.1422266337553098e-05}, {"x": 0.21211160431198478, "y": 0.36176284083703236, "ox": 0.21211160431198478, "oy": 0.36176284083703236, "term": "pairwise", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 3.268269627216138e-05}, {"x": 0.212428662016487, "y": 0.31452124286620164, "ox": 0.212428662016487, "oy": 0.31452124286620164, "term": "however there", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 0.0}, {"x": 0.2127457197209892, "y": 0.19562460367786938, "ox": 0.2127457197209892, "oy": 0.19562460367786938, "term": "graphical", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 2.8556293735748626e-06}, {"x": 0.40805326569435635, "y": 0.08370323398858592, "ox": 0.40805326569435635, "oy": 0.08370323398858592, "term": "independence", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 4, "s": 0.1483830057070387, "os": -0.004964488887318468, "bg": 1.365824569804528e-06}, {"x": 0.31800887761572605, "y": 0.1959416613823716, "ox": 0.31800887761572605, "oy": 0.1959416613823716, "term": "which allows", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 0.0}, {"x": 0.11160431198478123, "y": 0.546290424857324, "ox": 0.11160431198478123, "oy": 0.546290424857324, "term": "family", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 1, "s": 0.7492073557387445, "os": 0.04688880251581216, "bg": 1.4151224028794123e-07}, {"x": 0.11192136968928346, "y": 0.2482561826252378, "ox": 0.11192136968928346, "oy": 0.2482561826252378, "term": "compressed", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 2.1655057029675437e-06}, {"x": 0.21306277742549143, "y": 0.24857324032974001, "ox": 0.21306277742549143, "oy": 0.24857324032974001, "term": "descriptor", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 9.044442334074566e-06}, {"x": 0.6043119847812302, "y": 0.01109701965757768, "ox": 0.6043119847812302, "oy": 0.01109701965757768, "term": "tc", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.03329105897273303, "os": -0.0343097417052401, "bg": 1.626629883142909e-06}, {"x": 0.11223842739378567, "y": 0.3148383005707039, "ox": 0.11223842739378567, "oy": 0.3148383005707039, "term": "20", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.6848446417247939, "y": 0.3151553582752061, "ox": 0.6848446417247939, "oy": 0.3151553582752061, "term": "document", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 9, "s": 0.0735573874445149, "os": -0.019575280025440796, "bg": 3.840918340125307e-07}, {"x": 0.21337983512999367, "y": 0.5894102726696259, "ox": 0.21337983512999367, "oy": 0.5894102726696259, "term": "assumptions", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7549143944197845, "os": 0.04782516518850924, "bg": 5.2188649043388274e-06}, {"x": 0.4083703233988586, "y": 0.474001268230818, "ox": 0.4083703233988586, "oy": 0.474001268230818, "term": "unit", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.476854787571338, "os": 0.020758983781491817, "bg": 3.832018732695841e-07}, {"x": 0.3183259353202283, "y": 0.47431832593532025, "ox": 0.3183259353202283, "oy": 0.47431832593532025, "term": "units", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 5.991906661179726e-07}, {"x": 0.2136968928344959, "y": 0.1962587190868738, "ox": 0.2136968928344959, "oy": 0.1962587190868738, "term": "convolutional layers", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 0.0}, {"x": 0.3186429930247305, "y": 0.47463538363982244, "ox": 0.3186429930247305, "oy": 0.47463538363982244, "term": "gives", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.5466074825618262, "os": 0.02625348927599731, "bg": 7.410629174404689e-07}, {"x": 0.6851616994292962, "y": 0.08402029169308814, "ox": 0.6851616994292962, "oy": 0.08402029169308814, "term": "handwriting", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 9, "s": 0.03677869372225745, "os": -0.032437016359845944, "bg": 1.595791366236778e-05}, {"x": 0.5516804058338618, "y": 0.046924540266328474, "ox": 0.5516804058338618, "oy": 0.046924540266328474, "term": "handwriting recognition", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 0.0}, {"x": 0.11255548509828789, "y": 0.19657577679137603, "ox": 0.11255548509828789, "oy": 0.19657577679137603, "term": "low resource", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.3189600507292327, "y": 0.6097019657577679, "ox": 0.3189600507292327, "oy": 0.6097019657577679, "term": "target language", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 3, "s": 0.7371591629676602, "os": 0.04554609377760503, "bg": 0.0}, {"x": 0.3192771084337349, "y": 0.437856689917565, "ox": 0.3192771084337349, "oy": 0.437856689917565, "term": "separately", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.5126823081800888, "os": 0.023038055192396028, "bg": 3.3933975300519817e-06}, {"x": 0.40868738110336084, "y": 0.43817374762206723, "ox": 0.40868738110336084, "oy": 0.43817374762206723, "term": "characters", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.43119847812301837, "os": 0.017543549697890536, "bg": 8.715751901507903e-07}, {"x": 0.2140139505389981, "y": 0.5897273303741281, "ox": 0.2140139505389981, "oy": 0.5897273303741281, "term": "gesture", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7549143944197845, "os": 0.04782516518850924, "bg": 1.7192997537717136e-05}, {"x": 0.21433100824350032, "y": 0.6940393151553583, "ox": 0.21433100824350032, "oy": 0.6940393151553583, "term": "dependent", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 2, "s": 0.844007609384908, "os": 0.07033320377371825, "bg": 2.8729517136849154e-06}, {"x": 0.31959416613823716, "y": 0.19689283449587824, "ox": 0.31959416613823716, "oy": 0.19689283449587824, "term": "ctc", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.2796448953709575, "os": 0.006960884774389595, "bg": 1.4053467054201667e-05}, {"x": 0.1128725428027901, "y": 0.24889029803424223, "ox": 0.1128725428027901, "oy": 0.24889029803424223, "term": "initialized", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 1.2115227932869522e-05}, {"x": 0.6464806594800253, "y": 0.13506658211794548, "ox": 0.6464806594800253, "oy": 0.13506658211794548, "term": "facial images", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 8, "s": 0.060875079264426125, "os": -0.02372707678173916, "bg": 0.0}, {"x": 0.11318960050729232, "y": 0.36207989854153455, "ox": 0.11318960050729232, "oy": 0.36207989854153455, "term": "minimum", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 4.777778932407687e-07}, {"x": 0.21464806594800254, "y": 0.4384908053265694, "ox": 0.21464806594800254, "oy": 0.4384908053265694, "term": "mmd", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 5.8516946507708636e-05}, {"x": 0.551997463538364, "y": 0.04724159797083069, "ox": 0.551997463538364, "oy": 0.04724159797083069, "term": "smile", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.075142675967026, "os": -0.019168933959930745, "bg": 1.5575849786504657e-06}, {"x": 0.5523145212428662, "y": 0.020291693088142042, "ox": 0.5523145212428662, "oy": 0.020291693088142042, "term": "smile detection", "cat25k": 0, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.05294863665187064, "os": -0.02559980212713332, "bg": 0.0}, {"x": 0.11350665821179455, "y": 0.43880786303107167, "ox": 0.11350665821179455, "oy": 0.43880786303107167, "term": "previous works", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 0.0}, {"x": 0.6854787571337984, "y": 0.004121750158528852, "ox": 0.6854787571337984, "oy": 0.004121750158528852, "term": "incipient", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.016804058338617627, "os": -0.051729620861453655, "bg": 3.581355463457242e-05}, {"x": 0.6857958148383005, "y": 0.004438807863031071, "ox": 0.6857958148383005, "oy": 0.004438807863031071, "term": "incipient fault", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.016804058338617627, "os": -0.051729620861453655, "bg": 0.0}, {"x": 0.409004438807863, "y": 0.7463538363982245, "ox": 0.409004438807863, "oy": 0.7463538363982245, "term": "speaker", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 4, "s": 0.8630310716550412, "os": 0.07863679728631497, "bg": 2.3253006468405074e-06}, {"x": 0.21496512365250475, "y": 0.43912492073557385, "ox": 0.21496512365250475, "oy": 0.43912492073557385, "term": "recorded", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 1.1428813066333404e-06}, {"x": 0.21528218135700697, "y": 0.3623969562460368, "ox": 0.21528218135700697, "oy": 0.3623969562460368, "term": "module", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.5009511731135067, "os": 0.022101692519698953, "bg": 5.861375449750665e-07}, {"x": 0.11382371591629677, "y": 0.4394419784400761, "ox": 0.11382371591629677, "oy": 0.4394419784400761, "term": "strength", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 9.86633442490229e-07}, {"x": 0.11414077362079898, "y": 0.19720989220038046, "ox": 0.11414077362079898, "oy": 0.19720989220038046, "term": "by minimizing", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.21559923906150918, "y": 0.1353836398224477, "ox": 0.21559923906150918, "oy": 0.1353836398224477, "term": "mixed", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 7.024130659444506e-07}, {"x": 0.2159162967660114, "y": 0.3154724159797083, "ox": 0.2159162967660114, "oy": 0.3154724159797083, "term": "arbitrary", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.45592897907419144, "os": 0.018886258436097664, "bg": 3.596600493213814e-06}, {"x": 0.1144578313253012, "y": 0.3157894736842105, "ox": 0.1144578313253012, "oy": 0.3157894736842105, "term": "nonparametric", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 3.6484547964742664e-05}, {"x": 0.11477488902980343, "y": 0.4397590361445783, "ox": 0.11477488902980343, "oy": 0.4397590361445783, "term": "sampled", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 1.0339615757722125e-05}, {"x": 0.11509194673430564, "y": 0.31610653138871275, "ox": 0.11509194673430564, "oy": 0.31610653138871275, "term": "sampled from", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.11540900443880786, "y": 0.19752694990488268, "ox": 0.11540900443880786, "oy": 0.19752694990488268, "term": "itself", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 3.508251710121659e-07}, {"x": 0.21623335447051364, "y": 0.19784400760938492, "ox": 0.21623335447051364, "oy": 0.19784400760938492, "term": "simulations", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.36651870640456563, "os": 0.01245539026889509, "bg": 3.738792968078186e-06}, {"x": 0.21655041217501586, "y": 0.1357006975269499, "ox": 0.21655041217501586, "oy": 0.1357006975269499, "term": "becoming", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 7.924608793982403e-07}, {"x": 0.21686746987951808, "y": 0.6271401395053899, "ox": 0.21686746987951808, "oy": 0.6271401395053899, "term": "showed that", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 2, "s": 0.7907419150285353, "os": 0.054256033355711816, "bg": 0.0}, {"x": 0.11572606214331008, "y": 0.31642358909321494, "ox": 0.11572606214331008, "oy": 0.31642358909321494, "term": "between them", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.48826886493341787, "y": 0.13601775523145213, "ox": 0.48826886493341787, "oy": 0.13601775523145213, "term": "natural images", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 5, "s": 0.13316423589093215, "os": -0.007243560298222679, "bg": 0.0}, {"x": 0.2171845275840203, "y": 0.40456563094483194, "ox": 0.2171845275840203, "oy": 0.40456563094483194, "term": "hilbert", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.5358275206087508, "os": 0.025317126603300235, "bg": 2.4542630549264074e-05}, {"x": 0.1160431198478123, "y": 0.24920735573874445, "ox": 0.1160431198478123, "oy": 0.24920735573874445, "term": "hilbert space", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.40932149651236527, "y": 0.362714013950539, "ox": 0.40932149651236527, "oy": 0.362714013950539, "term": "quadratic", "cat25k": 2, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.35447051363348125, "os": 0.011112681530687965, "bg": 1.755355002995806e-05}, {"x": 0.3199112238427394, "y": 0.08433734939759036, "ox": 0.3199112238427394, "oy": 0.08433734939759036, "term": "deep autoencoder", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 0.0}, {"x": 0.6467977171845276, "y": 0.007292327203551046, "ox": 0.6467977171845276, "oy": 0.007292327203551046, "term": "radar", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.024413443246670895, "os": -0.04301968128334688, "bg": 1.1831778922422048e-06}, {"x": 0.11636017755231452, "y": 0.44007609384908053, "ox": 0.11636017755231452, "oy": 0.44007609384908053, "term": "multiclass", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 9.987088121785406e-05}, {"x": 0.2175015852885225, "y": 0.49778059606848446, "ox": 0.2175015852885225, "oy": 0.49778059606848446, "term": "is capable", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6540900443880786, "os": 0.0349634288541041, "bg": 0.0}, {"x": 0.11667723525681674, "y": 0.4403931515535828, "ox": 0.11667723525681674, "oy": 0.4403931515535828, "term": "8", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 0.0}, {"x": 0.11699429296131895, "y": 0.3630310716550412, "ox": 0.11699429296131895, "oy": 0.3630310716550412, "term": "significantly outperforms", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 0.0}, {"x": 0.11731135066582118, "y": 0.4749524413443247, "ox": 0.11731135066582118, "oy": 0.4749524413443247, "term": "detected", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.674698795180723, "os": 0.0372425002650083, "bg": 2.69233962069422e-06}, {"x": 0.1176284083703234, "y": 0.24952441344324666, "ox": 0.1176284083703234, "oy": 0.24952441344324666, "term": "added", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 1.6857535266848798e-07}, {"x": 0.11794546607482562, "y": 0.19816106531388714, "ox": 0.11794546607482562, "oy": 0.19816106531388714, "term": "divided", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 1.3280147816898634e-06}, {"x": 0.11826252377932783, "y": 0.19847812301838935, "ox": 0.11826252377932783, "oy": 0.19847812301838935, "term": "balance", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 4.2977265337028067e-07}, {"x": 0.4885859226379201, "y": 0.08465440710209259, "ox": 0.4885859226379201, "oy": 0.08465440710209259, "term": "plant", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 5, "s": 0.11223842739378567, "os": -0.010458994381823964, "bg": 4.1138027431584647e-07}, {"x": 0.11857958148383006, "y": 0.44071020925808496, "ox": 0.11857958148383006, "oy": 0.44071020925808496, "term": "we discuss", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 0.0}, {"x": 0.21781864299302472, "y": 0.13633481293595434, "ox": 0.21781864299302472, "oy": 0.13633481293595434, "term": "randomly", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 4.41688309393827e-06}, {"x": 0.21813570069752694, "y": 0.4410272669625872, "ox": 0.21813570069752694, "oy": 0.4410272669625872, "term": "digits", "cat25k": 2, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.5837032339885859, "os": 0.028532560686901524, "bg": 6.863154195857948e-06}, {"x": 0.11889663918833228, "y": 0.6804058338617628, "ox": 0.11889663918833228, "oy": 0.6804058338617628, "term": "mismatch", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 1, "s": 0.8500317057704502, "os": 0.07261227518462245, "bg": 3.531598639247879e-05}, {"x": 0.1192136968928345, "y": 0.3633481293595434, "ox": 0.1192136968928345, "oy": 0.3633481293595434, "term": "predictors", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.5726062143310082, "os": 0.027596198014204446, "bg": 2.1973615181570728e-05}, {"x": 0.3202282815472416, "y": 0.0849714648065948, "ox": 0.3202282815472416, "oy": 0.0849714648065948, "term": "syntactic", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 3, "s": 0.19974635383639824, "os": 0.0005300166071870241, "bg": 1.2362076993762646e-05}, {"x": 0.6471147748890298, "y": 0.5234622701331643, "ox": 0.6471147748890298, "oy": 0.5234622701331643, "term": "convnets", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 8, "s": 0.26981610653138866, "os": 0.005211829970672419, "bg": 0.00020787679835084405}, {"x": 0.11953075459733671, "y": 0.4413443246670894, "ox": 0.11953075459733671, "oy": 0.4413443246670894, "term": "trees", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 1.0141616078112466e-06}, {"x": 0.11984781230183894, "y": 0.19879518072289157, "ox": 0.11984781230183894, "oy": 0.19879518072289157, "term": "determined", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 5.167689218385857e-07}, {"x": 0.12016487000634116, "y": 0.4980976537729867, "ox": 0.12016487000634116, "oy": 0.4980976537729867, "term": "convex", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 1.5462657199654796e-05}, {"x": 0.12048192771084337, "y": 0.5900443880786304, "ox": 0.12048192771084337, "oy": 0.5900443880786304, "term": "numerical", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 1, "s": 0.7882054533925175, "os": 0.05331967068301473, "bg": 4.782863383127325e-06}, {"x": 0.12079898541534559, "y": 0.19911223842739378, "ox": 0.12079898541534559, "oy": 0.19911223842739378, "term": "same class", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.12111604311984782, "y": 0.3167406467977172, "ox": 0.12111604311984782, "oy": 0.3167406467977172, "term": "call", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 1.1008820156621283e-07}, {"x": 0.12143310082435003, "y": 0.31705770450221943, "ox": 0.12143310082435003, "oy": 0.31705770450221943, "term": "we call", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.12175015852885225, "y": 0.4984147114774889, "ox": 0.12175015852885225, "oy": 0.4984147114774889, "term": "interests", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.7003804692454028, "os": 0.04045793434860959, "bg": 8.107566941709962e-07}, {"x": 0.12206721623335447, "y": 0.24984147114774888, "ox": 0.12206721623335447, "oy": 0.24984147114774888, "term": "which contains", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.647431832593532, "y": 0.04755865567533291, "ox": 0.647431832593532, "oy": 0.04755865567533291, "term": "white", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.042802790107799624, "os": -0.03015794494894173, "bg": 1.2375376154317972e-07}, {"x": 0.7799619530754597, "y": 0.04787571337983513, "ox": 0.7799619530754597, "oy": 0.04787571337983513, "term": "blood", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 13, "s": 0.014584654407102092, "os": -0.05763047242146921, "bg": 5.846129672157818e-07}, {"x": 0.21845275840202916, "y": 0.2501585288522511, "ox": 0.21845275840202916, "oy": 0.2501585288522511, "term": "seek", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.40615091946734305, "os": 0.015670824352496376, "bg": 9.028886979963299e-07}, {"x": 0.1223842739378567, "y": 0.199429296131896, "ox": 0.1223842739378567, "oy": 0.199429296131896, "term": "greater", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 3.2197621847875183e-07}, {"x": 0.12270133164235891, "y": 0.19974635383639822, "ox": 0.12270133164235891, "oy": 0.19974635383639822, "term": "observe", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 2.311423775407684e-06}, {"x": 0.12301838934686113, "y": 0.20006341154090043, "ox": 0.12301838934686113, "oy": 0.20006341154090043, "term": "study how", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 0.0}, {"x": 0.12333544705136334, "y": 0.3173747622067216, "ox": 0.12333544705136334, "oy": 0.3173747622067216, "term": "how much", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 0.0}, {"x": 0.12365250475586556, "y": 0.44166138237159164, "ox": 0.12365250475586556, "oy": 0.44166138237159164, "term": "multiple kernel", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.6426759670259987, "os": 0.034027066181407016, "bg": 0.0}, {"x": 0.12396956246036779, "y": 0.25047558655675334, "ox": 0.12396956246036779, "oy": 0.25047558655675334, "term": "kernel extreme", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.4825618262523779, "os": 0.021165329847001868, "bg": 0.0}, {"x": 0.12428662016487001, "y": 0.20038046924540268, "ox": 0.12428662016487001, "oy": 0.20038046924540268, "term": "landmark", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.43658845909955607, "os": 0.017949895763400586, "bg": 3.6910114847872913e-06}, {"x": 0.32054533925174383, "y": 0.2507926442612555, "ox": 0.32054533925174383, "oy": 0.2507926442612555, "term": "temperature", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.3386176284083703, "os": 0.01017631885799088, "bg": 5.597517762090008e-07}, {"x": 0.40963855421686746, "y": 0.6563094483195941, "ox": 0.40963855421686746, "oy": 0.6563094483195941, "term": "meta", "cat25k": 3, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 4, "s": 0.7663284717818644, "os": 0.0496978905339034, "bg": 4.165946626508998e-06}, {"x": 0.2187698161065314, "y": 0.13665187064045656, "ox": 0.2187698161065314, "oy": 0.13665187064045656, "term": "ensembles", "cat25k": 1, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.3116677235256817, "os": 0.009239956185293805, "bg": 1.218917601170161e-05}, {"x": 0.12460367786937222, "y": 0.31769181991122386, "ox": 0.12460367786937222, "oy": 0.31769181991122386, "term": "backgrounds", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.5234622701331643, "os": 0.024380763930603157, "bg": 2.9290381917289815e-06}, {"x": 0.0, "y": 0.31800887761572605, "ox": 0.0, "oy": 0.31800887761572605, "term": "length", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 2.8694202867153444e-07}, {"x": 0.0003170577045022194, "y": 0.5688015218769816, "ox": 0.0003170577045022194, "oy": 0.5688015218769816, "term": "preference", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 0, "s": 0.7958148383005708, "os": 0.05559874209391894, "bg": 3.2571492163389465e-06}, {"x": 0.0006341154090044388, "y": 0.4048826886493342, "ox": 0.0006341154090044388, "oy": 0.4048826886493342, "term": "customers", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 2.838604606317239e-07}, {"x": 0.0009511731135066582, "y": 0.5237793278376665, "ox": 0.0009511731135066582, "oy": 0.5237793278376665, "term": "abstract", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 0, "s": 0.7615726062143311, "os": 0.04916787392671637, "bg": 5.695695574184673e-07}, {"x": 0.0012682308180088776, "y": 0.4051997463538364, "ox": 0.0012682308180088776, "oy": 0.4051997463538364, "term": "previously learned", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 0.0}, {"x": 0.0015852885225110971, "y": 0.49873176918199114, "ox": 0.0015852885225110971, "oy": 0.49873176918199114, "term": "sets show", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 0.0}, {"x": 0.0019023462270133164, "y": 0.4419784400760938, "ox": 0.0019023462270133164, "oy": 0.4419784400760938, "term": "tca", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 2.316828012945722e-05}, {"x": 0.0022194039315155357, "y": 0.4055168040583386, "ox": 0.0022194039315155357, "oy": 0.4055168040583386, "term": "distances", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 4.341953437614993e-06}, {"x": 0.0025364616360177552, "y": 0.25110970196575777, "ox": 0.0025364616360177552, "oy": 0.25110970196575777, "term": "generic detector", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.0028535193405199747, "y": 0.36366518706404566, "ox": 0.0028535193405199747, "oy": 0.36366518706404566, "term": "scene specific", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.0031705770450221942, "y": 0.25142675967026, "ox": 0.0031705770450221942, "oy": 0.25142675967026, "term": "inc.", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.0034876347495244133, "y": 0.2517438173747622, "ox": 0.0034876347495244133, "oy": 0.2517438173747622, "term": "crafted features", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.003804692454026633, "y": 0.25206087507926445, "ox": 0.003804692454026633, "oy": 0.25206087507926445, "term": "wide variety", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.004121750158528852, "y": 0.3639822447685479, "ox": 0.004121750158528852, "oy": 0.3639822447685479, "term": "conformal", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 2.691602323097496e-05}, {"x": 0.004438807863031071, "y": 0.25237793278376663, "ox": 0.004438807863031071, "oy": 0.25237793278376663, "term": "it uses", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.004755865567533291, "y": 0.4990488268864933, "ox": 0.004755865567533291, "oy": 0.4990488268864933, "term": "latent variable", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 0.0}, {"x": 0.0050729232720355105, "y": 0.44229549778059607, "ox": 0.0050729232720355105, "oy": 0.44229549778059607, "term": "might be", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 0.0}, {"x": 0.00538998097653773, "y": 0.5466074825618262, "ox": 0.00538998097653773, "oy": 0.5466074825618262, "term": "factor", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.7824984147114775, "os": 0.05238330801031765, "bg": 8.724419776689237e-07}, {"x": 0.0057070386810399495, "y": 0.5691185795814838, "ox": 0.0057070386810399495, "oy": 0.5691185795814838, "term": "prototype", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 0, "s": 0.7958148383005708, "os": 0.05559874209391894, "bg": 5.971675348542637e-06}, {"x": 0.006024096385542169, "y": 0.569435637285986, "ox": 0.006024096385542169, "oy": 0.569435637285986, "term": "approximation", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 0, "s": 0.7958148383005708, "os": 0.05559874209391894, "bg": 8.934685217389578e-06}, {"x": 0.0063411540900443885, "y": 0.47526949904882687, "ox": 0.0063411540900443885, "oy": 0.47526949904882687, "term": "mismatch between", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 0.0}, {"x": 0.006658211794546607, "y": 0.3183259353202283, "ox": 0.006658211794546607, "oy": 0.3183259353202283, "term": "euclidean", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 2.091973620212649e-05}, {"x": 0.006975269499048827, "y": 0.6705770450221941, "ox": 0.006975269499048827, "oy": 0.6705770450221941, "term": "we describe", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 0, "s": 0.8547875713379836, "os": 0.07489134659552667, "bg": 0.0}, {"x": 0.007292327203551046, "y": 0.49936588459099557, "ox": 0.007292327203551046, "oy": 0.49936588459099557, "term": "variant", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 6.4160788647304506e-06}, {"x": 0.007609384908053266, "y": 0.5240963855421686, "ox": 0.007609384908053266, "oy": 0.5240963855421686, "term": "cues", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 0, "s": 0.7615726062143311, "os": 0.04916787392671637, "bg": 1.1358977408058839e-05}, {"x": 0.007926442612555484, "y": 0.3186429930247305, "ox": 0.007926442612555484, "oy": 0.3186429930247305, "term": "registration", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 2.93012357048977e-07}, {"x": 0.008243500317057704, "y": 0.5469245402663284, "ox": 0.008243500317057704, "oy": 0.5469245402663284, "term": "preferences", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.7824984147114775, "os": 0.05238330801031765, "bg": 1.6784970263697313e-06}, {"x": 0.008560558021559923, "y": 0.3189600507292327, "ox": 0.008560558021559923, "oy": 0.3189600507292327, "term": "customer", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 1.0832019319123095e-07}, {"x": 0.008877615726062143, "y": 0.4755865567533291, "ox": 0.008877615726062143, "oy": 0.4755865567533291, "term": "few labeled", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 0.0}, {"x": 0.009194673430564362, "y": 0.40583386176284086, "ox": 0.009194673430564362, "oy": 0.40583386176284086, "term": "confident", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 3.0894663101422543e-06}, {"x": 0.009511731135066582, "y": 0.3642993024730501, "ox": 0.009511731135066582, "oy": 0.3642993024730501, "term": "bases", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 2.473506496777246e-06}, {"x": 0.009828788839568801, "y": 0.3192771084337349, "ox": 0.009828788839568801, "oy": 0.3192771084337349, "term": "complex tasks", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 0.0}, {"x": 0.010145846544071021, "y": 0.40615091946734305, "ox": 0.010145846544071021, "oy": 0.40615091946734305, "term": "applications where", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 0.0}, {"x": 0.01046290424857324, "y": 0.5472415979708307, "ox": 0.01046290424857324, "oy": 0.5472415979708307, "term": "zsl", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.7824984147114775, "os": 0.05238330801031765, "bg": 0.00013015848709899702}, {"x": 0.01077996195307546, "y": 0.7720355104629042, "ox": 0.01077996195307546, "oy": 0.7720355104629042, "term": "formulation", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 0, "s": 0.9140773620798985, "os": 0.1134765555987421, "bg": 1.4126701997809971e-05}, {"x": 0.01109701965757768, "y": 0.6274571972098922, "ox": 0.01109701965757768, "oy": 0.6274571972098922, "term": "contextual", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.8303741280913127, "os": 0.06524504434472281, "bg": 2.2351327748694446e-05}, {"x": 0.011414077362079899, "y": 0.36461636017755233, "ox": 0.011414077362079899, "oy": 0.36461636017755233, "term": "for evaluating", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.011731135066582118, "y": 0.3649334178820545, "ox": 0.011731135066582118, "oy": 0.3649334178820545, "term": "clothing", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 3.0266828507984585e-07}, {"x": 0.012048192771084338, "y": 0.5903614457831325, "ox": 0.012048192771084338, "oy": 0.5903614457831325, "term": "node", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 0, "s": 0.809131261889664, "os": 0.058814176177520223, "bg": 1.687627068880899e-06}, {"x": 0.012365250475586557, "y": 0.2526949904882689, "ox": 0.012365250475586557, "oy": 0.2526949904882689, "term": "instance weights", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.012682308180088777, "y": 0.31959416613823716, "ox": 0.012682308180088777, "oy": 0.31959416613823716, "term": "push", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 9.521071535666484e-07}, {"x": 0.012999365884590995, "y": 0.3199112238427394, "ox": 0.012999365884590995, "oy": 0.3199112238427394, "term": "spn", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 3.828784418378931e-05}, {"x": 0.013316423589093214, "y": 0.36525047558655677, "ox": 0.013316423589093214, "oy": 0.36525047558655677, "term": "gallery", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 1.326079925284317e-07}, {"x": 0.013633481293595434, "y": 0.4426125554850983, "ox": 0.013633481293595434, "oy": 0.4426125554850983, "term": "minimal", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 2.163277531866742e-06}, {"x": 0.013950538998097653, "y": 0.25301204819277107, "ox": 0.013950538998097653, "oy": 0.25301204819277107, "term": "with minimal", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.014267596702599873, "y": 0.3202282815472416, "ox": 0.014267596702599873, "oy": 0.3202282815472416, "term": "correspondences", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 3.355113612534704e-05}, {"x": 0.014584654407102092, "y": 0.32054533925174383, "ox": 0.014584654407102092, "oy": 0.32054533925174383, "term": "description", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 1.0176452719282241e-07}, {"x": 0.014901712111604312, "y": 0.2533291058972733, "ox": 0.014901712111604312, "oy": 0.2533291058972733, "term": "explain", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 7.138662895125983e-07}, {"x": 0.015218769816106531, "y": 0.320862396956246, "ox": 0.015218769816106531, "oy": 0.320862396956246, "term": "detailed", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 3.8002766145342185e-07}, {"x": 0.015535827520608751, "y": 0.4064679771718453, "ox": 0.015535827520608751, "oy": 0.4064679771718453, "term": "focusing", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 2.6611911180972307e-06}, {"x": 0.01585288522511097, "y": 0.2536461636017755, "ox": 0.01585288522511097, "oy": 0.2536461636017755, "term": "focusing on", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.016169942929613188, "y": 0.4429296131896005, "ox": 0.016169942929613188, "oy": 0.4429296131896005, "term": "theoretically", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 1.435546449037825e-05}, {"x": 0.016487000634115408, "y": 0.4067850348763475, "ox": 0.016487000634115408, "oy": 0.4067850348763475, "term": "measured", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 1.3368934062021938e-06}, {"x": 0.016804058338617627, "y": 0.32117945466074826, "ox": 0.016804058338617627, "oy": 0.32117945466074826, "term": "situations where", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 0.0}, {"x": 0.017121116043119847, "y": 0.4759036144578313, "ox": 0.017121116043119847, "oy": 0.4759036144578313, "term": "au", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 6.657750986987805e-07}, {"x": 0.017438173747622066, "y": 0.25396322130627774, "ox": 0.017438173747622066, "oy": 0.25396322130627774, "term": "facial action", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.017755231452124286, "y": 0.44324667089410275, "ox": 0.017755231452124286, "oy": 0.44324667089410275, "term": "e.", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 0.0}, {"x": 0.018072289156626505, "y": 0.36556753329105895, "ox": 0.018072289156626505, "oy": 0.36556753329105895, "term": "g.", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.018389346861128725, "y": 0.3658845909955612, "ox": 0.018389346861128725, "oy": 0.3658845909955612, "term": "e. g.", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.018706404565630944, "y": 0.5244134432466709, "ox": 0.018706404565630944, "oy": 0.5244134432466709, "term": "person specific", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 0, "s": 0.7615726062143311, "os": 0.04916787392671637, "bg": 0.0}, {"x": 0.019023462270133164, "y": 0.25428027901078, "ox": 0.019023462270133164, "oy": 0.25428027901078, "term": "stm", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 1.3590208405845902e-05}, {"x": 0.019340519974635383, "y": 0.6100190234622701, "ox": 0.019340519974635383, "oy": 0.6100190234622701, "term": "- negative", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 0, "s": 0.8195941661382372, "os": 0.06202961026112152, "bg": 0.0}, {"x": 0.019657577679137603, "y": 0.32149651236525045, "ox": 0.019657577679137603, "oy": 0.32149651236525045, "term": "histopathological", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 5.0978402999569234e-05}, {"x": 0.019974635383639822, "y": 0.44356372859860493, "ox": 0.019974635383639822, "oy": 0.44356372859860493, "term": "program", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 8.471321439280901e-08}, {"x": 0.020291693088142042, "y": 0.40710209258084973, "ox": 0.020291693088142042, "oy": 0.40710209258084973, "term": "induction", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 3.6934697147025485e-06}, {"x": 0.02060875079264426, "y": 0.49968294229549776, "ox": 0.02060875079264426, "oy": 0.49968294229549776, "term": "notion", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 3.942268370707789e-06}, {"x": 0.02092580849714648, "y": 0.4074191502853519, "ox": 0.02092580849714648, "oy": 0.4074191502853519, "term": "labeled examples", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 0.0}, {"x": 0.0212428662016487, "y": 0.5247305009511731, "ox": 0.0212428662016487, "oy": 0.5247305009511731, "term": "dual", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 0, "s": 0.7615726062143311, "os": 0.04916787392671637, "bg": 1.0750424658571553e-06}, {"x": 0.02155992390615092, "y": 0.5475586556753329, "ox": 0.02155992390615092, "oy": 0.5475586556753329, "term": "- agent", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.7824984147114775, "os": 0.05238330801031765, "bg": 0.0}, {"x": 0.02187698161065314, "y": 0.47622067216233355, "ox": 0.02187698161065314, "oy": 0.47622067216233355, "term": "extension", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 8.070305038795829e-07}, {"x": 0.02219403931515536, "y": 0.4765377298668358, "ox": 0.02219403931515536, "oy": 0.4765377298668358, "term": "team", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 1.6286766116646486e-07}, {"x": 0.02251109701965758, "y": 0.40773620798985416, "ox": 0.02251109701965758, "oy": 0.40773620798985416, "term": "food", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 1.330561898562806e-07}, {"x": 0.022828154724159798, "y": 0.6807228915662651, "ox": 0.022828154724159798, "oy": 0.6807228915662651, "term": "mci", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 0, "s": 0.8620798985415346, "os": 0.07810678067912795, "bg": 2.2224484180834406e-05}, {"x": 0.023145212428662017, "y": 0.3218135700697527, "ox": 0.023145212428662017, "oy": 0.3218135700697527, "term": "healthcare", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 6.953972352396721e-07}, {"x": 0.023462270133164237, "y": 0.3662016487000634, "ox": 0.023462270133164237, "oy": 0.3662016487000634, "term": "pedestrian detector", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.023779327837666456, "y": 0.32213062777425494, "ox": 0.023779327837666456, "oy": 0.32213062777425494, "term": "comes", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 3.1013668576617607e-07}, {"x": 0.024096385542168676, "y": 0.40805326569435635, "ox": 0.024096385542168676, "oy": 0.40805326569435635, "term": "hairstyle", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 3.589278824152257e-05}, {"x": 0.024413443246670895, "y": 0.5, "ox": 0.024413443246670895, "oy": 0.5, "term": "site", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 3.552224825367565e-08}, {"x": 0.024730500951173115, "y": 0.5697526949904883, "ox": 0.024730500951173115, "oy": 0.5697526949904883, "term": "load", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 0, "s": 0.7958148383005708, "os": 0.05559874209391894, "bg": 9.442096917669425e-07}, {"x": 0.025047558655675334, "y": 0.32244768547875713, "ox": 0.025047558655675334, "oy": 0.32244768547875713, "term": "mkl", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 6.989487810333259e-05}, {"x": 0.025364616360177554, "y": 0.6708941027266963, "ox": 0.025364616360177554, "oy": 0.6708941027266963, "term": "tsk", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 0, "s": 0.8547875713379836, "os": 0.07489134659552667, "bg": 6.451716963176826e-05}, {"x": 0.02568167406467977, "y": 0.5003170577045022, "ox": 0.02568167406467977, "oy": 0.5003170577045022, "term": "saliency", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 0.00010998801130676755}, {"x": 0.02599873176918199, "y": 0.4083703233988586, "ox": 0.02599873176918199, "oy": 0.4083703233988586, "term": "extends", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 2.9497727445914686e-06}, {"x": 0.02631578947368421, "y": 0.36651870640456563, "ox": 0.02631578947368421, "oy": 0.36651870640456563, "term": "human pose", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.02663284717818643, "y": 0.476854787571338, "ox": 0.02663284717818643, "oy": 0.476854787571338, "term": "slns", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 0.00012126671748319592}, {"x": 0.026949904882688648, "y": 0.2545973367152822, "ox": 0.026949904882688648, "oy": 0.2545973367152822, "term": "large margin", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.027266962587190868, "y": 0.2549143944197844, "ox": 0.027266962587190868, "oy": 0.2549143944197844, "term": "keepaway", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 7.795886370625101e-05}, {"x": 0.027584020291693087, "y": 0.4438807863031072, "ox": 0.027584020291693087, "oy": 0.4438807863031072, "term": "but related", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 0.0}, {"x": 0.027901077996195307, "y": 0.2552314521242866, "ox": 0.027901077996195307, "oy": 0.2552314521242866, "term": "mappings", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 1.2700598903797197e-05}, {"x": 0.028218135700697526, "y": 0.3227647431832594, "ox": 0.028218135700697526, "oy": 0.3227647431832594, "term": "embedding space", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 0.0}, {"x": 0.028535193405199746, "y": 0.44419784400760937, "ox": 0.028535193405199746, "oy": 0.44419784400760937, "term": "polarity", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 1.7009194778177205e-05}, {"x": 0.028852251109701965, "y": 0.25554850982878885, "ox": 0.028852251109701965, "oy": 0.25554850982878885, "term": "hdd", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 3.890018795274146e-06}, {"x": 0.029169308814204185, "y": 0.32308180088776156, "ox": 0.029169308814204185, "oy": 0.32308180088776156, "term": "mode", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 2.960933911718216e-07}, {"x": 0.029486366518706404, "y": 0.5700697526949905, "ox": 0.029486366518706404, "oy": 0.5700697526949905, "term": "feedbacks", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 0, "s": 0.7958148383005708, "os": 0.05559874209391894, "bg": 3.305008189075847e-05}, {"x": 0.029803424223208624, "y": 0.25586556753329104, "ox": 0.029803424223208624, "oy": 0.25586556753329104, "term": "implicit feedbacks", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.030120481927710843, "y": 0.3668357641090679, "ox": 0.030120481927710843, "oy": 0.3668357641090679, "term": "by applying", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.030437539632213063, "y": 0.5703868103994927, "ox": 0.030437539632213063, "oy": 0.5703868103994927, "term": "relational", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 0, "s": 0.7958148383005708, "os": 0.05559874209391894, "bg": 1.0947187120269446e-05}, {"x": 0.030754597336715282, "y": 0.7025998731769182, "ox": 0.030754597336715282, "oy": 0.7025998731769182, "term": "acoustic", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 0, "s": 0.8763474952441345, "os": 0.08453764884633053, "bg": 4.869986686899358e-06}, {"x": 0.031071655041217502, "y": 0.5478757133798351, "ox": 0.031071655041217502, "oy": 0.5478757133798351, "term": "children", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.7824984147114775, "os": 0.05238330801031765, "bg": 1.644347025588545e-07}, {"x": 0.03138871274571972, "y": 0.40868738110336084, "ox": 0.03138871274571972, "oy": 0.40868738110336084, "term": "children 's", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 0.0}, {"x": 0.03170577045022194, "y": 0.2561826252377933, "ox": 0.03170577045022194, "oy": 0.2561826252377933, "term": "'s speech", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.03202282815472416, "y": 0.25649968294229547, "ox": 0.03202282815472416, "oy": 0.25649968294229547, "term": "bank", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 1.9609928027206375e-07}, {"x": 0.032339885859226376, "y": 0.4771718452758402, "ox": 0.032339885859226376, "oy": 0.4771718452758402, "term": "contextual information", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 0.0}, {"x": 0.0326569435637286, "y": 0.5006341154090045, "ox": 0.0326569435637286, "oy": 0.5006341154090045, "term": "sleep", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 1.009338602619153e-06}, {"x": 0.032974001268230815, "y": 0.2568167406467977, "ox": 0.032974001268230815, "oy": 0.2568167406467977, "term": "manual labeling", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.03329105897273304, "y": 0.25713379835129996, "ox": 0.03329105897273304, "oy": 0.25713379835129996, "term": "monocular", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 2.7882466111185983e-05}, {"x": 0.033608116677235254, "y": 0.36715282181357006, "ox": 0.033608116677235254, "oy": 0.36715282181357006, "term": "naive", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 1.0580066577473498e-05}, {"x": 0.03392517438173748, "y": 0.25745085605580215, "ox": 0.03392517438173748, "oy": 0.25745085605580215, "term": "bayes", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 2.2486451912722587e-05}, {"x": 0.03424223208623969, "y": 0.3674698795180723, "ox": 0.03424223208623969, "oy": 0.3674698795180723, "term": "encodings", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 2.600334497574006e-05}, {"x": 0.034559289790741916, "y": 0.5906785034876347, "ox": 0.034559289790741916, "oy": 0.5906785034876347, "term": "gait", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 0, "s": 0.809131261889664, "os": 0.058814176177520223, "bg": 3.605915219239645e-05}, {"x": 0.03487634749524413, "y": 0.3233988585922638, "ox": 0.03487634749524413, "oy": 0.3233988585922638, "term": "geo", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 2.443880261109055e-06}, {"x": 0.035193405199746355, "y": 0.2577679137603044, "ox": 0.035193405199746355, "oy": 0.2577679137603044, "term": "alzheimer", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 3.7206587302259903e-06}, {"x": 0.03551046290424857, "y": 0.2580849714648066, "ox": 0.03551046290424857, "oy": 0.2580849714648066, "term": "alzheimer 's", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.035827520608750794, "y": 0.570703868103995, "ox": 0.035827520608750794, "oy": 0.570703868103995, "term": "ad", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 0, "s": 0.7958148383005708, "os": 0.05559874209391894, "bg": 4.5566696486272296e-07}, {"x": 0.03614457831325301, "y": 0.3677869372225745, "ox": 0.03614457831325301, "oy": 0.3677869372225745, "term": "stress", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 8.419190580548347e-07}, {"x": 0.03646163601775523, "y": 0.2584020291693088, "ox": 0.03646163601775523, "oy": 0.2584020291693088, "term": "ptsd", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 1.687622880040953e-05}, {"x": 0.03677869372225745, "y": 0.409004438807863, "ox": 0.03677869372225745, "oy": 0.409004438807863, "term": "canonical", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 1.0842591325113746e-05}, {"x": 0.03709575142675967, "y": 0.323715916296766, "ox": 0.03709575142675967, "oy": 0.323715916296766, "term": "augment", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 1.5977171816908e-05}, {"x": 0.03741280913126189, "y": 0.5250475586556753, "ox": 0.03741280913126189, "oy": 0.5250475586556753, "term": "super", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 0, "s": 0.7615726062143311, "os": 0.04916787392671637, "bg": 4.780815662430191e-07}, {"x": 0.03772986683576411, "y": 0.40932149651236527, "ox": 0.03772986683576411, "oy": 0.40932149651236527, "term": "super -", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 0.0}, {"x": 0.03804692454026633, "y": 0.40963855421686746, "ox": 0.03804692454026633, "oy": 0.40963855421686746, "term": "- resolution", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 0.0}, {"x": 0.03836398224476855, "y": 0.4445149017121116, "ox": 0.03836398224476855, "oy": 0.4445149017121116, "term": "arousal", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 2.6892870182943923e-05}, {"x": 0.03868103994927077, "y": 0.258719086873811, "ox": 0.03868103994927077, "oy": 0.258719086873811, "term": "decoding", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 8.84207175635966e-06}, {"x": 0.03899809765377299, "y": 0.4774889029803424, "ox": 0.03899809765377299, "oy": 0.4774889029803424, "term": "sound", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 2.7932479097602156e-07}, {"x": 0.039315155358275206, "y": 0.36810399492707674, "ox": 0.039315155358275206, "oy": 0.36810399492707674, "term": "object class", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.03963221306277743, "y": 0.3684210526315789, "ox": 0.03963221306277743, "oy": 0.3684210526315789, "term": "asr", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 2.1775236013864885e-05}, {"x": 0.039949270767279645, "y": 0.36873811033608117, "ox": 0.039949270767279645, "oy": 0.36873811033608117, "term": "pose estimation", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.04026632847178187, "y": 0.4448319594166138, "ox": 0.04026632847178187, "oy": 0.4448319594166138, "term": "educational", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 4.600643608807132e-07}, {"x": 0.040583386176284084, "y": 0.25903614457831325, "ox": 0.040583386176284084, "oy": 0.25903614457831325, "term": "significantly improve", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.0409004438807863, "y": 0.44514901712111604, "ox": 0.0409004438807863, "oy": 0.44514901712111604, "term": "crowd", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 2.2336552918343664e-06}, {"x": 0.04121750158528852, "y": 0.36905516804058336, "ox": 0.04121750158528852, "oy": 0.36905516804058336, "term": "thermal", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 1.5258859970343713e-06}, {"x": 0.04153455928979074, "y": 0.32403297400126824, "ox": 0.04153455928979074, "oy": 0.32403297400126824, "term": "finds", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 1.2112557148558696e-06}, {"x": 0.04185161699429296, "y": 0.47780596068484465, "ox": 0.04185161699429296, "oy": 0.47780596068484465, "term": "malware", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 1.21630826116571e-05}, {"x": 0.04216867469879518, "y": 0.2593532022828155, "ox": 0.04216867469879518, "oy": 0.2593532022828155, "term": "behavioral", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 2.30285513096593e-06}, {"x": 0.0424857324032974, "y": 0.3693722257450856, "ox": 0.0424857324032974, "oy": 0.3693722257450856, "term": "gender classification", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.04280279010779962, "y": 0.2596702599873177, "ox": 0.04280279010779962, "oy": 0.2596702599873177, "term": "discriminative information", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.04311984781230184, "y": 0.5909955611921369, "ox": 0.04311984781230184, "oy": 0.5909955611921369, "term": "deception", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 0, "s": 0.809131261889664, "os": 0.058814176177520223, "bg": 1.4150939883083444e-05}, {"x": 0.043436905516804056, "y": 0.25998731769181993, "ox": 0.043436905516804056, "oy": 0.25998731769181993, "term": "mtl", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 1.6181186134880976e-05}, {"x": 0.04375396322130628, "y": 0.3243500317057704, "ox": 0.04375396322130628, "oy": 0.3243500317057704, "term": "binding", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 6.091146515494155e-07}, {"x": 0.044071020925808495, "y": 0.32466708941027267, "ox": 0.044071020925808495, "oy": 0.32466708941027267, "term": "similarity functions", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 0.0}, {"x": 0.04438807863031072, "y": 0.5009511731135067, "ox": 0.04438807863031072, "oy": 0.5009511731135067, "term": "pain", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 7.073187092046567e-07}, {"x": 0.044705136334812934, "y": 0.3249841471147749, "ox": 0.044705136334812934, "oy": 0.3249841471147749, "term": "are assumed", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 0.0}, {"x": 0.04502219403931516, "y": 0.2603043753963221, "ox": 0.04502219403931516, "oy": 0.2603043753963221, "term": "closer", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 1.162908941774829e-06}, {"x": 0.04533925174381737, "y": 0.3253012048192771, "ox": 0.04533925174381737, "oy": 0.3253012048192771, "term": "useful knowledge", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 0.0}, {"x": 0.045656309448319596, "y": 0.6712111604311984, "ox": 0.045656309448319596, "oy": 0.6712111604311984, "term": "fs", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 0, "s": 0.8547875713379836, "os": 0.07489134659552667, "bg": 4.706176258262403e-06}, {"x": 0.04597336715282181, "y": 0.5012682308180089, "ox": 0.04597336715282181, "oy": 0.5012682308180089, "term": "tsk fs", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 0.0}, {"x": 0.046290424857324035, "y": 0.6103360811667724, "ox": 0.046290424857324035, "oy": 0.6103360811667724, "term": "cpm", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 0, "s": 0.8195941661382372, "os": 0.06202961026112152, "bg": 2.0442352056142877e-05}, {"x": 0.04660748256182625, "y": 0.26062143310082436, "ox": 0.04660748256182625, "oy": 0.26062143310082436, "term": "bayesian networks", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.046924540266328474, "y": 0.5253646163601775, "ox": 0.046924540266328474, "oy": 0.5253646163601775, "term": "sample selection", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 0, "s": 0.7615726062143311, "os": 0.04916787392671637, "bg": 0.0}, {"x": 0.04724159797083069, "y": 0.4454660748256183, "ox": 0.04724159797083069, "oy": 0.4454660748256183, "term": "policy reuse", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 0.0}, {"x": 0.04755865567533291, "y": 0.5256816740646798, "ox": 0.04755865567533291, "oy": 0.5256816740646798, "term": "negative matrix", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 0, "s": 0.7615726062143311, "os": 0.04916787392671637, "bg": 0.0}, {"x": 0.04787571337983513, "y": 0.32561826252377934, "ox": 0.04787571337983513, "oy": 0.32561826252377934, "term": "nmf", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 6.571144134761024e-05}, {"x": 0.04819277108433735, "y": 0.4099556119213697, "ox": 0.04819277108433735, "oy": 0.4099556119213697, "term": "distribution mismatch", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 0.0}, {"x": 0.04850982878883957, "y": 0.47812301838934684, "ox": 0.04850982878883957, "oy": 0.47812301838934684, "term": "speech emotion", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.718135700697527, "os": 0.042737005759513794, "bg": 0.0}, {"x": 0.04882688649334179, "y": 0.36968928344958785, "ox": 0.04882688649334179, "oy": 0.36968928344958785, "term": "lda", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 1.8355436337923583e-05}, {"x": 0.04914394419784401, "y": 0.26093849080532655, "ox": 0.04914394419784401, "oy": 0.26093849080532655, "term": "latent factors", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.04946100190234623, "y": 0.4102726696258719, "ox": 0.04946100190234623, "oy": 0.4102726696258719, "term": "cca", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.6648700063411541, "os": 0.03630613759231122, "bg": 1.2043730786485731e-05}, {"x": 0.049778059606848446, "y": 0.2612555485098288, "ox": 0.049778059606848446, "oy": 0.2612555485098288, "term": "mec", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 1.622330477731261e-05}, {"x": 0.05009511731135067, "y": 0.4457831325301205, "ox": 0.05009511731135067, "oy": 0.4457831325301205, "term": "kt", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 5.531007573013023e-06}, {"x": 0.050412175015852885, "y": 0.4461001902346227, "ox": 0.050412175015852885, "oy": 0.4461001902346227, "term": "kt cm", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 0.0}, {"x": 0.05072923272035511, "y": 0.501585288522511, "ox": 0.05072923272035511, "oy": 0.501585288522511, "term": "ca", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 2.2978485718710275e-07}, {"x": 0.051046290424857324, "y": 0.32593532022828153, "ox": 0.051046290424857324, "oy": 0.32593532022828153, "term": "whispered", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 1.0286441390171408e-05}, {"x": 0.05136334812935954, "y": 0.261572606214331, "ox": 0.05136334812935954, "oy": 0.261572606214331, "term": "whispered speech", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.05168040583386176, "y": 0.37000634115409003, "ox": 0.05168040583386176, "oy": 0.37000634115409003, "term": "nonnegative matrix", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.05199746353836398, "y": 0.2618896639188332, "ox": 0.05199746353836398, "oy": 0.2618896639188332, "term": "dirichlet", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 3.80183163799804e-05}, {"x": 0.0523145212428662, "y": 0.5019023462270134, "ox": 0.0523145212428662, "oy": 0.5019023462270134, "term": "unlabeled samples", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 0.0}, {"x": 0.05263157894736842, "y": 0.3703233988585923, "ox": 0.05263157894736842, "oy": 0.3703233988585923, "term": "links", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 6.467593473203351e-08}, {"x": 0.05294863665187064, "y": 0.26220672162333547, "ox": 0.05294863665187064, "oy": 0.26220672162333547, "term": "2010", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.05326569435637286, "y": 0.4464172479391249, "ox": 0.05326569435637286, "oy": 0.4464172479391249, "term": "action models", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.6918199112238428, "os": 0.03952157167591251, "bg": 0.0}, {"x": 0.05358275206087508, "y": 0.26252377932783766, "ox": 0.05358275206087508, "oy": 0.26252377932783766, "term": "cue", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 4.454301097886236e-06}, {"x": 0.053899809765377296, "y": 0.2628408370323399, "ox": 0.053899809765377296, "oy": 0.2628408370323399, "term": "personalized ranking", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.05421686746987952, "y": 0.2631578947368421, "ox": 0.05421686746987952, "oy": 0.2631578947368421, "term": "deception detection", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.054533925174381735, "y": 0.3262523779327838, "ox": 0.054533925174381735, "oy": 0.3262523779327838, "term": "surgical", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 1.991477868756231e-06}, {"x": 0.05485098287888396, "y": 0.37064045656309447, "ox": 0.05485098287888396, "oy": 0.37064045656309447, "term": "transference", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 4.490794891108431e-05}, {"x": 0.055168040583386174, "y": 0.3709575142675967, "ox": 0.055168040583386174, "oy": 0.3709575142675967, "term": "consumer videos", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 0.0}, {"x": 0.0554850982878884, "y": 0.5022194039315155, "ox": 0.0554850982878884, "oy": 0.5022194039315155, "term": "image search", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7409638554216869, "os": 0.04595243984311508, "bg": 0.0}, {"x": 0.05580215599239061, "y": 0.26347495244134433, "ox": 0.05580215599239061, "oy": 0.26347495244134433, "term": "mstl", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 7.795886370625101e-05}, {"x": 0.056119213696892836, "y": 0.3712745719720989, "ox": 0.056119213696892836, "oy": 0.3712745719720989, "term": "ale", "cat25k": 2, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.62999365884591, "os": 0.03309070350870994, "bg": 5.851287921665085e-06}, {"x": 0.05643627140139505, "y": 0.2637920101458465, "ox": 0.05643627140139505, "oy": 0.2637920101458465, "term": "resolution face", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.056753329105897275, "y": 0.26410906785034877, "ox": 0.056753329105897275, "oy": 0.26410906785034877, "term": "selection bias", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 0.0}, {"x": 0.05707038681039949, "y": 0.264426125554851, "ox": 0.05707038681039949, "oy": 0.264426125554851, "term": "plsa", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 7.795886370625101e-05}, {"x": 0.057387444514901714, "y": 0.32656943563728597, "ox": 0.057387444514901714, "oy": 0.32656943563728597, "term": "nc", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.5948002536461636, "os": 0.029875269425108653, "bg": 4.788361599829916e-07}, {"x": 0.05770450221940393, "y": 0.2647431832593532, "ox": 0.05770450221940393, "oy": 0.2647431832593532, "term": "scl", "cat25k": 1, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.5529486366518707, "os": 0.026659835341507364, "bg": 2.2971753676437744e-05}], "docs": {"categories": ["FRONTIER", "NORMAL"], "labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "texts": ["Deep learning  Dictionary learning  Feature learning  Land-use classification  Sparse representation Land-use classification in very high spatial resolution images is critical in the remote sensing field. Consequently, remarkable efforts have been conducted towards developing increasingly accurate approaches for this task. In recent years, deep learning has emerged as a dominant paradigm for machine learning, and methodologies based on deep convolutional neural networks have received particular attention from the remote sensing community. These methods typically utilize transfer learning and/or data augmentation to accommodate a small number of labeled images in the publicly available datasets in this field. However, they typically require powerful computers and/or a long time for training. In this work, we propose a simple and novel method for land-use classification in very high spatial resolution images, which efficiently combines transfer learning with a sparse representation. Specifically, the proposed method performs the classification of land-use scenes using a modified version of the well-known sparse representation-based classification method. While this method directly uses the training images to form dictionaries, which are employed to classify test images, our method utilizes a pre-trained deep convolutional neural network and the Gaussian mixture model to generate more robust and compact dictionaries of deep features. The effectiveness of the proposed method was evaluated on two publicly available datasets: UC Merced and Brazilian Cerrado-Savana. The experimental results suggest that our method can potentially outperform state-of-the-art techniques for land-use classification in very high spatial resolution images. ", "MRI classification  Abnormal brain images  Deep transfer learning  CNN Magnetic resonance imaging (MRI) is the most common imaging technique used to detect abnormal brain tumors. Traditionally, MRI images are analyzed manually by radiologists to detect the abnormal conditions in the brain. Manual interpretation of huge volume of images is time consuming and difficult. Hence, computer-based detection helps in accurate and fast diagnosis. In this study, we proposed an approach that uses deep transfer learning to automatically classify normal and abnormal brain MR images. Convolutional neural network (CNN) based ResNet34 model is used as a deep learning model. We have used current deep learning techniques such as data augmentation, optimal learning rate finder and fine-tuning to train the model. The proposed model achieved 5-fold classification accuracy of 100% on 613 MR images. Our developed system is ready to test on huge database and can assist the radiologists in their daily screening of MR images. ", "Deep learning  Kernel methods  Deep kernel  Deep embedding kernel  Supervised learning In this paper, we propose a novel supervised learning method that is called Deep Embedding Kernel (DEK). DEK combines the advantages of deep learning and kernel methods in a unified framework. More specifically, DEK is a learnable kernel represented by a newly designed deep architecture. Compared with predefined kernels, this kernel can be explicitly trained to map data to an optimized high-level feature space where data may have favorable features toward the application. Compared with typical deep learning using SoftMax or logistic regression as the top layer, DEK is expected to be more generalizable to new data. Experimental results show that DEK has superior performance than typical machine learning methods in identity detection and classification, and transfer learning, on different types of data including images, sequences, and regularly structured data. (C) 2019 Elsevier B.V. All rights reserved.", "Scene classification  Transfer learning  ResNet  Data augmentation  CNN Scene classification is a significant aspect of computer vision. Convolutional neural networks (CNNs), a development of deep learning, are a well-understood tool for image classification. But training CNNs requires large-scale datasets. Transfer learning addresses this problem and produces a solution for smallscale datasets. Because scene image classification is more complex than common image classification. We propose a novel ResNet based transfer learning model utilizing multi-layer feature fusion, taking full advantage of interlayer discriminating features and fusing them for classification by softmax regression. In addition, a novel data augmentation method with a filter useful for small-scale datasets is presented. New image patches are generated by sliding block cropping of a raw image, which are then filtered to insure that the new images sufficiently represent the original categorization. Our new ResNet based transfer learning model with enhanced data augmentation is evaluated on six benchmark scene datasets (LF, OT, FP, LS, MIT67, SUN397). Extensive experimental results show that on the six datasets our method obtains better accuracy than other state-of-the-art models. (c) 2019 Elsevier B.V. All rights reserved.", "Transfer learning  Fault diagnosis  Dictionary learning  Sucker rod pumping systems Sucker rod pumping wells are systems that their operation states varies slowly. For a relatively new well, it is hard to collect all kinds of fault samples for training. Moreover, samples from different wells are not always have similar distributions, so directly using samples from other wells as the training data may hardly get good results. In this paper, a novel framework is proposed to solve the aforementioned problems. For the source data from one well and the target data from another well, a transform matrix is calculated to transfer these data into a common low dimensional subspace. In this subspace, the source data that contain all kinds of fault samples and the target data that lack some kinds of fault samples can be represented by a shared dictionary matrix. By introducing two idea regularization terms, the structure information of source data and target data are included into the dictionary learning process. So the obtained dictionary has discriminative ability. Extensive experiments are conducted to evaluate the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.", "Web page categorization  Metric learning  Transfer learning  Deep learning The growing amounts of online multimedia content challenge the current search, recommendation and information retrieval systems. Information in the form of visual elements is highly valuable in a range of web mining tasks. However, the mining of these resources is a difficult task due to the complexity and variability of images, and the cost of collecting big enough datasets to successfully train accurate deep learning models. This paper proposes a novel framework for the categorization of web pages on the basis of their visual content. This is achieved by exploring the joint application of a transfer learning strategy and metric learning techniques to build a Deep Convolutional Neural Network (DCNN) for feature extraction, even when training data is scarce. The obtained experimental results evidence that the proposed approach outperforms the state-of-the-art handcrafted image descriptors and achieves a high categorization accuracy. In addition, we address the problem of over-time learning, so the proposed framework can learn to identify new web page categories as new labeled images are provided at test time. As a result, prior knowledge of the complete set of possible web categories is not necessary in the initial training phase. (C) 2019 Elsevier B.V. All rights reserved.", "Convolutional Neural Networks  Soft tissue sarcoma  Multi-modal medical image fusion and classification  Type-2 fuzzy logic  Transfer learning Medical image analysis is motivated by deep learning emergence and computation power increase. Meanwhile, relevant deep features can significantly enhance learnable expert and intelligent systems performance and reduce diagnosis time and arduousness. This paper presents a deep learning-based radiomics framework for aided diagnosis of soft tissue sarcomas of the extremities. MR Images with histologically confirmed Liposarcoma (LPS) and Leiomyosarcomas (LMS) have been retrieved from the Cancer Imaging Archives database and pre-processed to recuperate ROls from MR scans with delineated tumors. This study investigates the significance and impact of medical image fusion on deep feature learning based on transfer learning from the natural domain to the medical domain. Towards this end, we propose to fuse T1 with T2FS or STIR modalities using type-2 fuzzy sets in the non-subsampled shearlet domain. Being decomposed, low-frequency sub-images were selected using local energy and type-2 fuzzy entropy, while high frequencies were selected according to the maximum of the absolute value. Experimental results indicated that the proposed fusion framework outperformed the state-of-the-art fuzzy logic-based fusion techniques in terms of entropy and mutual information. Accordingly, we fine-tuned the pre-trained AlexNet deep convolutional neural network (CNN) with stochastic gradient descent (SGD). First, with the pre-processed dataset, and second with the fused images. As a result, the average classification accuracy using the augmented training data by image rotation and flipping was 97.17% with the raw data and 98.28% with the fused images, which highlighted the usefulness of complementary information for deep feature learning. One crucial concern was to investigate the depth of knowledge transferability. We incrementally fine-tuned the pre-trained CNN to assess the required level that achieves performance improvements in STS classification. Through layer-wise fine-tuning, our study further confirms the potential of middle and deep layers in performance improvement. Moreover, the transferability was concluded better than random weights. With the encouragement of classification results, our aided diagnosis framework may be in the pipeline to assist radiologists in classifying LPS and LMS. ", "Texture classification  Texture scaling  Transfer learning  Partial least-square regression  Coupled dictionary learning  Local binary patterns Classification of texture patterns with large scale variations poses a great challenge for expert and intelligent systems. A pure learning approach addresses this issue by including texture patterns at all scales in the training dataset. This approach makes the construction of an expert system quite costly and unrealistic given the large variations in real-world texture scales and patterns. We propose a transfer learning approach where the full range of texture scales is available only for a small subset of the texture classes. Such a subset is used to learn the scaling map through partial least-square regression or coupled dictionary learning. Experimental results on classifiers equipped with the learned maps show promising reduction in training data scale variability with improved classification accuracy compared to the data-intensive pure learning approach. The proposed approach can be followed to build image-based expert systems of reasonable accuracy and limited data requirements. ", "Transfer learning  Domain adaptation  Low-rank representation  Structure preservation  Distribution alignment Domain adaptation (DA) is one of the most promising techniques for leveraging existing knowledge from a source domain and applying it to a related target domain. Most DA methods mainly focus on learning a common subspace for the two domains by exploiting either the statistical property or the geometric structure independently to reduce the domain distribution difference. However, these two properties are complementary to each other, and jointly exploring them could yield optimal results. Inspired by the theoretical results of DA, in this paper, we propose structure preservation and distribution alignment (SPDA) in discriminative transfer subspace learning, which embeds the source domain classification error and reduction and domain distribution alignment into a single framework for optimization. SPDA learns an appropriate projection matrix, by which (1) the source domain classification error can be reduced  and (2) the source and target domain data are projected into a common subspace, where the domain distributions are well aligned and each target datum can be linearly reconstructed using the data from the source domain. To reduce the source domain classification error, an epsilon-dragging technique that relaxes the strict binary label matrix is introduced to enlarge the distance between two data points from different classes. Further, the global subspace structure and the local geometric structure are preserved by imposing a low-rank constraint and a sparse constraint, respectively, on the reconstruction coefficient matrix. Moreover, the space relationship of the samples is preserved using a graph regularization method. In addition, marginal and conditional distributions between the domains are minimized to further reduce the domain shift statistically. We formulate source domain classifier design, geometric structure preservation, and distribution alignment as a rank-minimization problem, and we design an effective optimization algorithm based on the alternating direction method of multipliers (ADMM) to solve this problem. The functions and roles of each term in this framework are analyzed. The results of extensive experiments conducted on five datasets show that SPDA outperforms several state-of-the-art approaches and exhibits classification performance comparable with that of modern deep DA methods. (C) 2019 Elsevier B.V. All rights reserved.", "Transfer learning  multi-domain adaptation  constrained clustering  utility function Domain adaptation has been a primal approach to addressing the issues by lack of labels in many data mining tasks. Although considerable efforts have been devoted to domain adaptation with promising results, most existing work learns a classifier on a source domain and then predicts the labels for target data, where only the instances near the boundary determine the hyperplane and the whole structure information is ignored. Moreover, little work has been done regarding to multi-source domain adaptation. To that end, we develop a novel unsupervised domain adaptation framework, which ensures the whole structure of source domains is preserved to guide the target structure learning in a semi-supervised clustering fashion. To our knowledge, this is the first time when the domain adaptation problem is re-formulated as a semi-supervised clustering problem with target labels as missing values. Furthermore, by introducing an augmented matrix, a non-trivial solution is designed, which can be exactly mapped into a K-means-like optimization problem with modified distance function and update rule for centroids in an efficient way. Extensive experiments on several widely-used databases show the substantial improvements of our proposed approach over the state-of-the-art methods.", "Transfer learning  distance metric learning  heterogeneous domains  knowledge fragments  nonlinear The goal of transfer learning is to improve the performance of target learning task by leveraging information (or transferring knowledge) from other related tasks. In this paper, we examine the problem of transfer distance metric learning (DML), which usually aims to mitigate the label information deficiency issue in the target DML. Most of the current Transfer DML (TDML) methods are not applicable to the scenario where data are drawn from heterogeneous domains. Some existing heterogeneous transfer learning (HTL) approaches can learn target distance metric by usually transforming the samples of source and target domain into a common subspace. However, these approaches lack flexibility in real-world applications, and the learned transformations are often restricted to be linear. This motivates us to develop a general flexible heterogeneous TDML (HTDML) framework. In particular, any (linear/nonlinear) DML algorithms can be employed to learn the source metric beforehand. Then the pre-learned source metric is represented as a set of knowledge fragments to help target metric learning. We show how generalization error in the target domain could be reduced using the proposed transfer strategy, and develop novel algorithm to learn either linear or nonlinear target metric. Extensive experiments on various applications demonstrate the effectiveness of the proposed method.", " Rising crimes are likely to promote the need for effective security systems for baggage screening at airports. Therefore, technologies enabling public safety are of paramount importance. Out of all the technologies available, X-ray based baggage-screening plays a major role in threat detection. Originally the screening is done manually where a person scrutinizes the X-ray images on a screen to identify potential threat objects. Within this context, with limited dataset availability, we employ an imaging model for a generation of new X-ray images. In this article, an effort is made to perform threat object detection by using deep neural networks based framework. The framework is built upon Convolutional Neural Network (CNN) based techniques such as You Only Look Once (YOLO) and Faster Region based CNN (FRCNN) to perform threat object detection. Apart from this, to improve the model performance with limited original training data the transfer-learning paradigm is also tested out. The performance is studied on 4 classes of threat objects: 1) Gun  2) Shuriken  3) Razor-blade  4) Knife. As compared to traditional Machine Learning (ML) techniques, FRCNN uses region proposal in its first stage to produce better results. On using Faster RCNN with RESNET which was pre-trained on ImageNet dataset, 98.4% accuracy is achieved for 4class threat recognition requiring 0.16 sec per image. Comparative performance of these threat detection techniques for cluttered X-ray baggage imagery is also presented. We firmly believe that it is possible to fully automate this screening process by using these computer vision techniques (C) 2019 Elsevier B.V. All rights reserved.", "Multi-view learning  Transfer learning  Learning using privileged information  Support vector machine In this paper, we present a multi-view transfer learning model named Multi-view Transfer Discriminative Model (MTDM) for both image and text classification tasks. Transfer learning, which aims to learn a robust classifier for the target domain using data from a different distribution, has been proved to be effective in many real-world applications. However, most of the existing transfer learning methods map across domain data into a high-dimension space which the distance between domains is closed. This strategy always fails in the multi-view scenario. On the contrary, the multi-view learning methods are also difficult to extend in the transfer learning settings. One of our goals in this paper is to develop a model which can perform better in both multi-view and transfer learning settings. On the one hand, the problem of multi-view is implemented by the paradigm of learning using privileged information (LUPI), which could guarantee the principle of complementary and consensus. On the other hand, the model adequately utilizes the source domain data to build a robust classifier for the target domain. We evaluate our model on both image and text classification tasks and show the effectiveness compared with other baseline approaches. (C) 2019 Elsevier B.V. All rights reserved.", "Coral images classification  Deep learning  Convolutional neural networks  Inception  ResNet  DenseNet The recognition of coral species based on underwater texture images poses a significant difficulty for machine learning algorithms, due to the three following challenges embedded in the nature of this data: (1) datasets do not include information about the global structure of the coral  (2) several species of coral have very similar characteristics  and (3) defining the spatial borders between classes is difficult as many corals tend to appear together in groups. For this reasons, the classification of coral species has always required an aid from a domain expert. The objective of this paper is to develop an accurate classification model for coral texture images. Current datasets contain a large number of imbalanced classes, while the images are subject to inter-class variation. We have focused on the current small datasets and analyzed (1) several Convolutional Neural Network (CNN) architectures, (2) data augmentation techniques and (3) transfer learning approaches. We have achieved the state-of-the art accuracies using different variations of ResNet on the two small coral texture datasets, EILAT and RSMAS. ", "Segmentation recommender  Deep learning  Crowdsourcing  Transfer learning  VGG16  ResNet50 Deep learning is widely used in medical applications regarding the high performance it can achieve. In this paper, we propose a segmentation recommender based on crowdsourcing and transfer learning for skin lesion extraction. In fact, after collecting and pre-processing data from the ISIC2017 segmentation challenge, we tested two pre-trained architectures (VGG16 and ResNet50) to extract features from the convolutional parts. Then, a classifier with an output layer, composed of five nodes representing the segmentation methods' classes, was built. Thus, the proposed architecture is able to dynamically predict the most appropriate segmentation technique for the detection of skin lesions in any input image. Experimental results prove the capability of the proposed image-based method to improve the segmentation performance comparatively to the state of the art methods. ", "Covariate shift  Transfer learning  Density ratio  Structural risk minimization  Biconvex optimization In many real-world applications, the performance of machine learning models is often significantly degraded because of the covariate shift problem. Appropriately dealing with this problem remains an important challenge. A common way for covariate shift adaptation is to estimate the density ratio weight from unlabeled source and target data and then learn a final hypothesis by directly minimizing the weighted empirical loss. This approach may result in a poor final hypothesis for the target domain by overweighting a few training examples that carry large loss. The problem stems from separating the estimation of density ratio weight from the learning of the final hypothesis. In this paper, an Adaptively Weighted Structural Risk Minimization (AWSRM) framework is developed for addressing this problem. In the proposed framework, the raw weights are first estimated from any density ratio estimation technique, and then the tailored weights and the target model are obtained by simultaneously minimizing the weighted structural risk and the discrepancy between the tailored weights and the raw weights. Comprehensive experimental studies on both synthetic and real-world data sets demonstrate that the AWSRM framework outperforms existing state-of-the-art covariate shift adaptation methods in terms of prediction accuracy. ", "Affective brain-computer interface (aBCI)  cross dataset  domain adaptation  electroencephalography (EEG)  emotion recognition  transfer learning Affective brain-computer interface (aBCI) introduces personal affective factors to human-computer interaction. The state-of-the-art aBCI tailors its classifier to each individual user to achieve accurate emotion classification. A subject-independent classifier that is trained on pooled data from multiple subjects generally leads to inferior accuracy, due to the fact that electroencephalography patterns vary from subject to subject. Transfer learning or domain adaptation techniques have been leveraged to tackle this problem. Existing studies have reported successful applications of domain adaptation techniques on SEED dataset. However, little is known about the effectiveness of the domain adaptation techniques on other affective datasets or in a cross-dataset application. In this paper, we focus on a comparative study on several state-of-the-art domain adaptation techniques on two datasets: 1) DEAP and 2) SEED. We demonstrate that domain adaptation techniques can improve the classification accuracy on both datasets, but not so effective on DEAP as on SEED. Then, we explore the efficacy of domain adaptation in a cross-dataset setting when the data are collected under different environments using different devices and experimental protocols. Here, we propose to apply domain adaptation to reduce the intersubject variance as well as technical discrepancies between datasets, and then train a subject-independent classifier on one dataset and test on the other. Experiment results show that using domain adaptation technique in a transductive adaptation setting can improve the accuracy significantly by 7.25%-13.40% compared to the baseline accuracy where no domain adaptation technique is used.", " This study presents a transfer learning method for addressing the insufficient sample problem in hyperspectral image classification. In order to find common feature representation for both the source domain and target domain, we introduce a regularisation based on Bregman divergence into the objective function of the subspace learning algorithm, which can minimise the Bregman divergence between the distribution of training samples in the source domain and the test samples in the target domain. Hyperspectral image with biased sampling is used to evaluate the effectiveness of the proposed method. The results show that the proposed method can achieve a higher classification accuracy than traditional subspace learning methods under the condition of biased sampling.", " This study presents a novel algorithm which combines active learning (AL) and transfer learning for medical data classification. The main idea of the proposed algorithm is iteratively querying a small number of informative unlabelled target samples, and, at the same time, removing the source samples which do not fit with the posterior probability distributions in the target domain, so as to combine the basic idea of AL with transfer learning. The experimental results obtained in the classification of the datasets from the University of California Irvine (UCI) Machine Learning Repository and The Cancer Imaging Archive (TCIA) confirm the effectiveness of the proposed algorithm.", "Facial expression recognition  Deep neural network  Optical flow  Spatial-temporal feature fusion  Transfer learning Traditional methods of performing facial expression recognition commonly use hand-crafted spatial features. This paper proposes a multi-channel deep neural network that learns and fuses the spatial-temporal features for recognizing facial expressions in static images. The essential idea of this method is to extract optical flow from the changes between the peak expression face image (emotional-face) and the neutral face image (neutral-face) as the temporal information of a certain facial expression, and use the gray-level image of emotional-face as the spatial information. A Multi-channel Deep Spatial-Temporal feature Fusion neural Network (MDSTFN) is presented to perform the deep spatial-temporal feature extraction and fusion from static images. Each channel of the proposed method is fine-tuned from a pretrained deep convolutional neural networks (CNN) instead of training a new CNN from scratch. In addition, average-face is used as a substitute for neutral-face in real-world applications. Extensive experiments are conducted to evaluate the proposed method on benchmarks databases including CK+, MMI, and RaFD. The results show that the optical flow information from emotional-face and neutral-face is a useful complement to spatial feature and can effectively improve the performance of facial expression recognition from static images. Compared with state-of-the-art methods, the proposed method can achieve better recognition accuracy, with rates of 98.38% on the CK+ database, 99.17% on the RaFD database, and 99.59% on the MMI database, respectively. ", "Memetic evolutionary computing  Transfer learning  Co-evolution  Bi-level combinatorial optimization Bi-Level Optimization Problem (BLOP) is a class of challenging problems with two levels of optimization tasks. The main goal is to optimize the upper level problem, which has another optimization problem as a constraint. In this way, the evaluation of each upper level solution requires finding an optimal solution to the corresponding lower level problem, which is computationally so expensive. For this reason, most proposed bi-level resolution methods have been restricted to solve the simplest case (linear continuous BLOPs). This fact has attracted the evolutionary computation community to solve such complex problems. Besides, to enhance the search performance of Evolutionary Algorithms (EAs), reusing knowledge captured from past optimization experiences along the search process has been proposed in the literature, and was demonstrated much promise. Motivated by this observation, we propose in this paper, a memetic version of our proposed Co-evolutionary Decomposition-based Algorithm-II (CODBA-II), that we named M-CODBA-II, to solve combinatorial BLOPs. The main motivation of this paper is to incorporate transfer learning within our recently proposed CODBA-II scheme to make the search process more effective and more efficient. Our proposed hybrid algorithm is investigated on two bi-level production-distribution problems in supply chain management formulated to: (1) Bi-CVRP and (2) Bi-MDVRP. The experimental results reveal a potential advantage of memes incorporation in CODBA-II. Most notably, the results emphasize that transfer learning allows not only accelerating the convergence but also finding better solutions.", "Feature transfer learning  Heterogeneous domain adaptation  Random forests Transfer learning across heterogeneous feature spaces can, in general, be a very difficult problem in practice due to the heterogeneity of features and lack of correspondence between data points of different domains. In this paper, we present a novel supervised domain adaptation algorithm (SHDA-RF) that transfers knowledge from a data-rich source domain to a target domain with only few training instances. The proposed method makes use of random forests to identify pivot features that bridge the two domains. The key idea of the proposed feature transfer approach is that every path in a decision tree leading to a partition of the data is associated with a certain label distribution and the label distributions that appear both in the source and target random forest models can be used as pivots for bridging the two domains. This information is used to generate a sparse feature transformation matrix, which maps patterns from the source feature space to the target feature space. The target model is then retrained along with the projected source data. We conduct extensive experiments on diverse datasets of varying dimensions and sparsity to verify the superiority of the proposed approach over other baseline and state of the art transfer approaches. ", "Reinforcement learning  Deep Q-learning  Stock trading  Trading strategy  Transfer learning We study trading systems using reinforcement learning with three newly proposed methods to maximize total profits and reflect real financial market situations while overcoming the limitations of financial data. First, we propose a trading system that can predict the number of shares to trade. Specifically, we design an automated system that predicts the number of shares by adding a deep neural network (DNN) regressor to a deep Q-network, thereby combining reinforcement learning and a DNN. Second, we study various action strategies that use Q-values to analyze which action strategies are beneficial for profits in a confused market. Finally, we propose transfer learning approaches to prevent overfitting from insufficient financial data. We use four different stock indices-the S&P500, KOSPI, HSI, and EuroStoxx50-to experimentally verify our proposed methods and then conduct extensive research. The proposed automated trading system, which enables us to predict the number of shares with the DNN regressor, increases total profits by four times in S&P500, five times in KOSPI, 12 times in HSI, and six times in EuroStoxx50 compared with the fixed-number trading system. When the market situation is confused, delaying the decision to buy or sell increases total profits by 18% in S&P500, 24% in KOSPI, and 49% in EuroStoxx50. Further, transfer learning increases total profits by twofold in S&P500, 3 times in KOSPI, twofold in HSI, and 2.5 times in EuroStoxx50. The trading system with all three proposed methods increases total profits by 13 times in S&P500, 24 times in KOSPI, 30 times in HSI, and 18 times in EuroStoxx50, outperforming the market and the reinforcement learning model. ", "Multi-modality emotion recognition  Deep learning  Transfer learning The multi-modal emotion recognition lacks the explicit mapping relation between emotion state and audio and image features, so extracting the effective emotion information from the audio/visual data is always a challenging issue. In addition, the modeling of noise and data redundancy is not solved well, so that the emotion recognition model is often confronted with the problem of low efficiency. The deep neural network (DNN) performs excellently in the aspects of feature extraction and highly non-linear feature fusion, and the cross modal noise modeling has great potential in solving the data pollution and data redundancy. Inspired by these, our paper proposes a deep weighted fusion method for audio-visual emotion recognition. Firstly, we conduct the cross-modal noise modeling for the audio and video data, which eliminates most of the data pollution in the audio channel and the data redundancy in visual channel. The noise modeling is implemented by the voice activity detection(VAD), and the data redundancy in the visual data is solved through aligning the speech area both in audio and visual data. Then, we extract the audio emotion features and visual expression features via two feature extractors. The audio emotion feature extractor, audio-net, is a 2D CNN, which accepting the image based Mel-spectrograms as input data. On the other hand, the facial expression feature extractor, visual-net, is a 3D CNN to which facial expression image sequence is feeded. To train the two convolutional neural networks on the small data set efficiently, we adopt the strategy of transfer learning. Next, we employ the deep belief network (DBN) for highly non-linear fusion of multi-modal emotion features. We train the feature extractors and the fusion network synchronously. And finally the emotion classification is obtained by the support vector machine using the output of the fusion network. With consideration of cross-modal feature fusion, denoising and redundancy removing, our fusion method show excellent performance on the selected data set.", "Image processing  Target detection  Transfer learning  Target heat-map network, decoder network In this paper, we focus on approaching fast and accurate target detection in high-resolution remote sensing images (RSIs). Recently, machine-learning based target detection systems have drawn increasing attention and some excellent target detection frameworks have been proposed for RSIs. However, huge storage and time consumption are still key flaws of those methods in practical applications. Especially, the employment of transfer learning would produce tremendous growth in parameters. A new target detection framework named target heat-map network (THNet) is proposed in this paper to address these problems. This framework consists of three parts: shallow features-extracting network, decoder network, and the location method. Firstly, we introduce the transfer-compression learning to train a shallow network under the supervision of a deep pre-trained network. Secondly, a decoder network is constructed to predict heat-map layers. Finally, a location method based on thresholding is proposed to identify positions of target instances. Compared with existing state-of-the-art methods including Faster-R-CNN, YOLOv2 and SSD, THNet with transfer learning has better performance, and THNet with transfer-compression learning also has superior performance in quantitative evaluation as well as significantly reducing time and saving storage, which makes it a great choice for applications with critical requirements for storage cost and running time. ", "Federated learning  GDPR  transfer learning Today's artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.", "Age estimation  Deep CNNs  Transfer learning  Regression  Cross-database evaluation  Mean Absolute Error This paper was aimed to address the problem of image-based human age estimation. It has the following main contributions. First, we provide a comparison of three hand-crafted image features and five deep convolutional neural networks (DCNNs). Secondly, we show that the use of pre-trained DCNNs as feature extractors can transfer the knowledge of DCNNs to new datasets and domains that were not necessarily addressed in the training phase. This is achieved by only retraining a shallow regressor over the deep features. Thirdly, we provide a cross-database evaluation involving biological and apparent ages. The paper shows that transfer learning allows the use of pre-trained DCNNs regardless of the type of ages (apparent or biological) that is adopted in DCNN training. The experiments are carried out on three public databases: MORPH, PAL, and Chalearn2016.", "Reinforcement learning  Human-robot interaction  Robot gaze control  Neural networks  Transfer learning  Multimodal data fusion This paper introduces a novel neural network-based reinforcement learning approach for robot gaze control. Our approach enables a robot to learn and to adapt its gaze control strategy for human-robot interaction neither with the use of external sensors nor with human supervision. The robot learns to focus its attention onto groups of people from its own audio-visual experiences, independently of the number of people, of their positions and of their physical appearances. In particular, we use a recurrent neural network architecture in combination with Q-learning to find an optimal action-selection policy  we pre-train the network using a simulated environment that mimics realistic scenarios that involve speaking/silent participants, thus avoiding the need of tedious sessions of a robot interacting with people. Our experimental evaluation suggests that the proposed method is robust in terms of parameter configuration, i.e. the selection of the parameter values employed by the method do not have a decisive impact on the performance. The best results are obtained when both audio and visual information is jointly used. Experiments with the Nao robot indicate that our framework is a step forward towards the autonomous learning of a socially acceptable gaze behavior. (C) 2018 Published by Elsevier B.V.", "Dimensionality reduction  Machine learning  Data visualization  Human-computer interaction  Intelligent human interfaces Recent advances in machine learning allow us to analyze and describe the content of high-dimensional data like text, audio, images or other signals. In order to visualize that data in 2D or 3D, usually Dimensionality Reduction (DR) techniques are employed. Most of these techniques, e.g., PCA or t-SNE, produce static projections without taking into account corrections from humans or other data exploration scenarios. In this work, we propose the interactive Similarity Projection (iSP), a novel interactive DR framework based on similarity embeddings, where we form a differentiable objective based on the user interactions and perform learning using gradient descent, with an end-to-end trainable architecture. Two interaction scenarios are evaluated. First, a common methodology in multidimensional projection is to project a subset of data, arrange them in classes or clusters, and project the rest unseen dataset based on that manipulation, in a kind of semi-supervised interpolation. We report results that outperform competitive baselines in a wide range of metrics and datasets. Second, we explore the scenario of manipulating some classes, while enriching the optimization with high-dimensional neighbor information. Apart from improving classification precision and clustering on images and text documents, the new emerging structure of the projection unveils semantic manifolds. For example, on the Head Pose dataset, by just dragging the faces looking far left to the left and those looking far right to the right, all faces are re-arranged on a continuum even on the vertical axis (face up and down). This end-to-end framework can be used for fast, visual semi-supervised learning, manifold exploration, interactive domain adaptation of neural embeddings and transfer learning. ", "Transfer learning  Large-scale  Gaussian process  Aggregation models  Edge intelligence In transfer learning, we aim to improve the predictive modeling of a target output by using the knowledge from some related source outputs. In real-world applications, the data from the target domain is often precious and hard to obtain, while the data from source domains is plentiful. Thus, since the complexity of Gaussian process based multi-task/transfer learning approaches grows cubically with the total number of source+ target observations, the method becomes increasingly impractical for large (> 10(4)) source data inputs even with a small amount of target data. In order to scale known transfer Gaussian processes to large-scale source datasets, we propose an efficient aggregation model in this paper, which combines the predictions from distributed (small-scale) local experts in a principled manner. The proposed model inherits the advantages of single-task aggregation schemes, including efficient computation, analytically tractable inference, and straightforward parallelization during training and prediction. Further, a salient feature of the proposed method is the enhanced expressiveness in transfer learning - as a byproduct of flexible inter-task relationship modelings across different experts. When deploying such models in real-world applications, each local expert corresponds to a lightweight predictor that can be embedded in edge devices, thus catering to cases of online on-mote processing in fog computing settings. ", "Humanoid robotics  Robot vision  Visual object recognition  Machine learning  Deep learning  Transfer learning  Image dataset  Dataset collection  Representation invariance  iCub We report on an extensive study of the benefits and limitations of current deep learning approaches to object recognition in robot vision scenarios, introducing a novel dataset used for our investigation. To avoid the biases in currently available datasets, we consider a natural human-robot interaction setting to design a data-acquisition protocol for visual object recognition on the iCub humanoid robot. Analyzing the performance of off-the-shelf models trained off-line on large-scale image retrieval datasets, we show the necessity for knowledge transfer. We evaluate different ways in which this last step can be done, and identify the major bottlenecks affecting robotic scenarios. By studying both object categorization and identification problems, we highlight key differences between object recognition in robotics applications and in image retrieval tasks, for which the considered deep learning approaches have been originally designed. In a nutshell, our results confirm the remarkable improvements yield by deep learning in this setting, while pointing to specific open challenges that need be addressed for seamless deployment in robotics. ", "Domain adaptation  fuzzy rules  machine learning  regression  transfer learning Transfer learning is gaining considerable attention due to its ability to leverage previously acquired knowledge to assist in completing a prediction task in a related domain. Fuzzy transfer learning, which is based on fuzzy system (especially fuzzy rule-based models), has been developed because of its capability to deal with the uncertainty in transfer learning. However, two issues with fuzzy transfer learning have not yet been resolved: choosing an appropriate source domain and efficiently selecting labeled data for the target domain. This paper proposes an innovative method based on fuzzy rules that combines an infinite Gaussian mixture model (IGMM) with active learning to enhance the performance and generalizability of the constructed model. An IGMM is used to identify the data structures in the source and target domains providing a promising solution to the domain selection dilemma. Further, we exploit the interactive query strategy in active learning to correct imbalances in the knowledge to improve the generalizability of fuzzy learning models. Through experiments on synthetic datasets, we demonstrate the rationality of employing an IGMM and the effectiveness of applying an active learning technique. Additional experiments on real-world datasets further support the capabilities of the proposed method in practical situations.", "Domain adaptation  fuzzy rules  machine learning  regression  transfer learning Domain adaptation aims to leverage knowledge acquired from a related domain (called a source domain) to improve the efficiency of completing a prediction task (classification or regression) in the current domain (called the target domain), which has a different probability distribution fromthe source domain. Although domain adaptation has been widely studied, most existing research has focused on homogeneous domain adaptation, where both domains have identical feature spaces. Recently, a new challenge proposed in this area is heterogeneous domain adaptation where both the probability distributions and the feature spaces are different. Moreover, in both homogeneous and heterogeneous domain adaptation, the greatest efforts and major achievements have been made with classification tasks, while successful solutions for tackling regression problems are limited. This paper proposes two innovative fuzzy rule-based methods to deal with regression problems. The first method, called fuzzy homogeneous domain adaptation, handles homogeneous spaces while the second method, called fuzzy heterogeneous domain adaptation, handles heterogeneous spaces. Fuzzy rules are first generated from the source domain through a learning process  these rules, also known as knowledge, are then transferred to the target domain by establishing a latent feature space to minimize the gap between the feature spaces of the two domains. Through experiments on synthetic datasets, we demonstrate the effectiveness of both methods and discuss the impact of some of the significant parameters that affect performance. Experiments on real-world datasets also show that the proposed methods improve the performance of the target model over an existing source model or a model built using a small amount of target data.", "Domain-invariant Learning  multi-task learning  human action recognition Domain-invariant (view-invariant and modality-invariant) feature representation is essential for human action recognition. Moreover, given a discriminative visual representation, it is critical to discover the latent correlations among multiple actions in order to facilitate action modeling. To address these problems, we propose a multi-domain and multi-task learning (MDMTL) method to: 1) extract domain-invariant information for multi-view and multi-modal action representation and 2) explore the relatedness among multiple action categories. Specifically, we present a sparse transfer learning-based method to co-embed multi-domain (multi-view and multi-modality) data into a single common space for discriminative feature learning. Additionally, visual feature learning is incorporated into the multi-task learning framework, with the Frobenius-norm regularization term and the sparse constraint term, for joint task modeling and task relatedness-induced feature learning. To the best of our knowledge, MDMTL is the first supervised framework to jointly realize domain-invariant feature learning and task modeling for multi-domain action recognition. Experiments conducted on the INRIA Xmas Motion Acquisition Sequences data set, the MSR Daily Activity 3D (DailyActivity3D) data set, and the Multi-modal & Multi-view & Interactive data set, which is the most recent and largest multi-view and multi-model action recognition data set, demonstrate the superiority of MDMTL over the state-of-the-art approaches.", "Convolutional neural networks  steganalysis  transfer learning  gaussian high-pass filter  Inception-V3 Recently, a large number of studies have shown that Convolutional Neural Networks are effective for learning features automatically for steganalysis. This paper uses the transfer learning method to help the training of CNNs for steganalysis. First, a Gaussian high-pass filter is designed for pretreatment of the images, that can enhance the weak stego noise in the stegos. Then, the classical Inception-V3 model is improved, and the improved network is used for steganalysis through the method of transfer learning. In order to test the effectiveness of the developed model, two spatial domain content-adaptive steganographic algorithms WOW and S-UNIWARD are used. The results imply that the proposed CNN achieves a better performance at low embedding rates compared with the SRM with ensemble classifiers and the SPAM implemented with a Gaussian SVM on BOSSbase. Finally, a steganalysis system based on the trained model was designed. Through experiments, the generalization ability of the system was tested and discussed.", "Automatic ICD-9 coding  Deep learning  Transfer learning  MeSH ICD-9 codes have been widely used to describe a patient's diagnosis. Accurate automatic ICD-9 coding is important because manual coding is expensive, time-consuming. Inspired by the recent successes of deep transfer learning, in this study, we propose a deep transfer learning framework for automatic ICD-9 coding. Our proposed method makes use of transferring MeSH domain knowledge to improve automatic ICD-9 coding. We demonstrate its effectiveness by achieving state-of-the-art performance with a value of 0.420 for Micro-average F-measure on MIMIC-III dataset, which indicates that our method outperforms hierarchy-based SVM and flat-SVM. Furthermore, we analyze the deep neural network structure to discover the vital elements in the success of our proposed method. Our experimental results indicate that transfer learning is the key component to improve the performance of automatic ICD-9 coding and deep learning approach is the foundation in the success of our proposed model. In addition, to explore the best network architecture, we also compare the performance of multi-scale and sequential network architectures and find that using multi-scale network is better. Finally, we investigate the effects of transferring different percentage of samples on transfer learning and the results show that the best performance of target domain task can be obtained when 100% number samples are transferred. ", " Many image classification models have been introduced to help tackle the foremost issue of recognition accuracy. Image classification is one of the core problems in Computer Vision field with a large variety of practical applications. Examples include: object recognition for robotic manipulation, pedestrian or obstacle detection for autonomous vehicles, among others. A lot of attention has been associated with Machine Learning, specifically neural networks such as the Convolutional Neural Network (CNN) winning image classification competitions. This work proposes the study and investigation of such a CNN architecture model (i.e. Inception-v3) to establish whether it would work best in terms of accuracy and efficiency with new image datasets via Transfer Learning. The retrained model is evaluated, and the results are compared to some state-of-the-art approaches.", "Sketch-based local binary patterns  Siamese MLP  Transfer learning  Joint Bayesian  Shape retrieval With the rapid development of 3D technology, the demand to use and retrieve 3D models has become increasingly urgent. In this paper, we present a framework that consists of a sketch-based local binary pattern (SBLBP) feature extraction method, a learning algorithm for the best view of a shape based on multilayer perceptrons (MLPs) and a learning method for shape retrieval based on two Siamese MLP networks. The model is first projected into many multiview images. A transfer learning scheme based on graphic traversal to identify Harris key points is proposed to build relations between view images and sketches. In addition, an MLP classifier is used for classification to obtain the best views of each model. Moreover, we propose a new learning method for shape retrieval that simultaneously uses two Siamese MLP networks to learn SBLBP features. Furthermore, we build a joint Bayesian method to fuse the outputs of the views and sketches. Based on training with many samples, the MLP parameters are effectively fit to perform shape retrieval. Finally, an experiment is conducted to verify the feasibility of the approach, and the results show that the proposed framework is superior to other approaches. ", "Convolutional neural networks  Machine learning  Deep learning  TensorFlow  Inception V3  Inception ResNet V2 Classification of specific objects through Convolutional Neural Networks (CNN) has become an interesting research line in the area from information processing and machine learning, main idea is training a image dataset to perform the classifying a given pattern. In this work, a new dataset with 2504 images was introduced, the method used to train the networks was transfer learning to recognition of dolphin images. For this purpose, two models were used: Inception V3 and Inception ResNet V2 to train on TensorFlow platform with different images, corresponding to the four main classes: dolphin, dolphin_pod, open_sea, and seabirds. The paper ends with a critical discussion of the experimental results.", "Transductive transfer learning  Reconstruction matrix  Subspace learning  Discriminative capability of target domains In this paper, we investigate the unsupervised domain transfer learning in which there is no label in the target samples whereas the source samples are all labeled. We use the transformation matrix to transfer both target and source samples to a common subspace where they have the same distribution and each target sample in the transformed space is constructed of a linear combination of the source samples. To preserve the local and global structure of the samples in the transferred domain, the low-rank and sparse constraints are imposed on the reconstruction coefficient matrix. In this paper, in order to consider the discriminative ability of the target and source samples, the information content of the reconstruction coefficient matrix is utilized. To capture the discriminative ability of the target samples, it is assumed that the class labels of the source samples which are linearly incorporated in constructing a target sample should be the same. Based on this assumption, it is assured that the target samples are well distributed over the transferred domain. To handle this, we utilize the linear entropy to measure the discriminant power of the target domain. This term considers the discriminative ability of the target samples without using their hidden labels. Also, to assess the discriminative ability of source samples, we use max-margin classifier where the kernel matrix is defined by using the reconstruction coefficient matrix. To evaluate the proposed approach, it is applied on MSRC, VOC 2007, CMU PIE, Office, Caltech-256, Extended Yale B and two imbalanced datasets. The experimental results show that our proposed approach outperforms its competitors. ", "Collaborative filtering (CF)  data sparsity  recommender systems  social tags  transfer learning Traditional recommender systems suffer from the data sparsity problem. However, user knowledge acquired in one domain can be transferred and exploited in several other relevant domains. In this context, cross-domain recommender systems have been proposed to create a new and effective recommendation paradigm in which to exploit rich data from auxiliary domains to assist recommendations in a target domain. Before knowledge transfer takes place, building reliable and concrete domain correlation is the key ensuring that only relevant knowledge will be transferred. Social tags are used to explicitly link different domains, especially when neither users nor items overlap. However, existing models only exploit a subset of tags that are shared by heterogeneous domains. In this paper, we propose a complete tag-induced cross-domain recommendation (CTagCDR) model, which infers interdomain and intradomain correlations from tagging history and applies the learned structural constraints to regularize joint matrix factorization. Compared to similar models, CTagCDR is able to fully explore knowledge encoded in both shared and domain-specific tags. We demonstrate the performance of our proposed model on three public datasets and compare it with five state-of-the-art single and cross-domain recommendation approaches. The results show that CTagCDR works well in both rating prediction and item recommendation tasks, and can effectively improve recommendation performance.", "Transfer learning  Domain adaptation The difference in data distributions among related, but different domains is a long standing problem for knowledge adaptation. A new method to transform the source domain knowledge to fit the target domain is proposed in this work. The proposed method uses deep learning method and limited number of samples from target domain to transform the source domain dataset. It treats the limited samples of target domain as seeds for initiating the transfer of source knowledge. Comprehensive experiments are conducted using different computational intelligence models and different datasets. Obtained results reveal that prediction models trained using the proposed method demonstrate the best performance in comparison with the same models trained with only source knowledge or deep learned features. Experiments show that models trained using proposed method have outperformed the baseline methods by at least 50% in 14 experiments out of a total of 18. ", "Bayesian optimization  Transfer learning  Gaussian process Experimental optimization is prevalent in many areas of artificial intelligence including machine learning. Conventional methods like grid search and random search can be computationally demanding. Over the recent years, Bayesian optimization has emerged as an efficient technique for global optimization of black-box functions. However, a generic Bayesian optimization algorithm suffers from a cold start problem. It may struggle to find promising locations in the initial stages. We propose a novel transfer learning method for Bayesian optimization where we leverage the knowledge from an already completed source optimization task for the optimization of a target task. Assuming both the source and target functions lie in some proximity to each other, we model source data as noisy observations of the target function. The level of noise models the proximity or relatedness between the tasks. We provide a mechanism to compute the noise level from the data to automatically adjust for different relatedness between the source and target tasks. We then analyse the convergence properties of the proposed method using two popular acquisition functions. Our theoretical results show that the proposed method converges faster than a generic no-transfer Bayesian optimization. We demonstrate the effectiveness of our method empirically on the tasks of tuning the hyperparameters of three different machine learning algorithms. In all the experiments, our method outperforms state-of-the-art transfer learning and no-transfer Bayesian optimization methods. ", "Biomedical signal processing  Fetal monitoring  Deep convolutional neural network  Classification Electronic fetal monitoring (EFM) device which is used to record Fetal Heart Rate (FHR) and Uterine Contraction (UC) signals simultaneously is one of the significant tools in terms of the present obstetric clinical applications. In clinical practice, EFM traces are routinely evaluated with visual inspection by observers. For this reason, such a subjective interpretation has been caused various conflicts among observers to arise. Although the existing of international guidelines for ensuring more consistent assessment, the automated FHR analysis has been adopted as the most promising solution. In this study, an innovative approach based on deep convolutional neural network (DCNN) is proposed to classify FHR signals as normal and abnormal. The proposed method composes of three stages. FHR signals are passed through a set of preprocessing procedures in order to ensure more meaningful input images, firstly. Then, a visual representation of time-frequency information, spectrograms are obtained with the help of the Short Time Fourier Transform (STFT). Finally, DCNN method is utilized to classify FHR signals. To this end, the colored spectrograms images are used to train the network. In order to evaluate the proposed model, we conducted extensive experiments on the open CTU-UHB database considering the area under the receiver operating characteristic curve and other several performance metrics derived from the confusion matrix. Consequently, we achieved encouraging results.", "CNNs  Speech-music discrimination  Transfer learning  Audio analysis Speech music discrimination is a traditional task in audio analytics, useful for a wide range of applications, such as automatic speech recognition and radio broadcast monitoring, that focuses on segmenting audio streams and classifying each segment as either speech or music. In this paper we investigate the capabilities of Convolutional Neural Networks (CNNs) with regards to the speech - music discrimination task. Instead of representing the audio content using handcrafted audio features, as traditional methods do, we use deep structures to learn visual feature dependencies as they appear on the spectrogram domain (i.e. train a CNN using audio spectrograms as input images). The main contribution of our work focuses on the potentials of using pre-trained deep architectures along with transfer-learning to train robust audio classifiers for the particular task of speech music discrimination. We highlight the supremacy of the proposed methods, compared both to the typical audio-based and deep-learning methods that adopt handcrafted features, and we evaluate our system in terms of classification success and run-time execution. To our knowledge this is the first work that investigates CNNs for the task of speech music discrimination and the first that exploits transfer learning across very different domains for audio modeling using deep-learning in general. In particular, we fine-tune a deep architecture originally trained for the Imagenet classification task, using a relatively small amount of data (almost 80 min of training audio samples) along with data augmentation. We evaluate our system through extensive experimentation against three different datasets. Firstly we experiment on a real-world dataset of more than 10 h of uninterrupted radio broadcasts and secondly, for comparison purposes, we evaluate our best method on two publicly available datasets that were designed specifically for the task of speech-music discrimination. Our results indicate that CNNs can significantly outperform current state-of-the-art in terms of performance especially when transfer learning is applied, in all three test-datasets. All the discussed methods, along with the whole experimental setup and the respective datasets, are openly provided for reproduction and further experimentation. Published by Elsevier Ltd.", "Deep learning (DL)  Electroencephalogram (EEG)  Motor imagery (MI)  Convolutional neural networks (CNNs)  Brain computer interface (BCI)  Stroke rehabilitation Goal: To develop and implement a Deep Learning (DL) approach for an electroencephalogram (EEG) based Motor Imagery (MI) Brain-Computer Interface (BCI) system that could potentially be used to improve the current stroke rehabilitation strategies. Method: The DL model is using Convolutional Neural Network (CNN) layers for learning generalized features and dimension reduction, while a conventional Fully Connected (FC) layer is used for classification. Together they build a unified end-to-end model that can be applied to raw EEG signals. This previously proposed model was applied to a new set of data to validate its robustness against data variations. Furthermore, it was extended by subject-specific adaptation. Lastly, an analysis of the learned filters provides insights into how such a model derives a classification decision. Results: The selected global classifier reached 80.38%, 69.82%, and 58.58% mean accuracies for datasets with two, three, and four classes, respectively, validated using 5-fold crossvalidation. As a novel approach in this context, transfer learning was used to adapt the global classifier to single individuals improving the overall mean accuracy to 86.49%, 79.25%, and 68.51%, respectively. The global models were trained on 3s segments of EEG data from different subjects than they were tested on, which proved the generalization performance of the model. Conclusion: The results are comparable with the reported accuracy values in related studies and the presented model outperforms the results in the literature on the same underlying data. Given that the model can learn features from data without having to use specialized feature extraction methods, DL should be considered as an alternative to established EEG classification methods, if enough data is available. ", "Multi-instance learning  Transfer learning  Metric learning  Bag weights estimation  Consistent maximum likelihood estimation Multi-Instance learning (MIL) aims to predict labels of unlabeled bags by training a model with labeled bags. The usual assumption of existing MIL methods is that the underlying distribution of training data is the same as that of the testing data. However, this assumption may not be valid in practice, especially when training data from a source domain and testing data from a target domain are drawn from different distributions. In this paper, we put forward a novel algorithm Multi-Instance Transfer Metric Learning (MITML). Specially, MITML first attempts to bridge the distributions of different domains by using the bag weighting method. Then a consistent maximum likelihood estimation method is learned to construct an optimal distance metric and exploited to classify testing bags. Comprehensive experimental results on benchmark datasets have demonstrated that the learning performance of the proposed MITML algorithm is better than those of other state-of-the-art MIL algorithms. ", "neural nets  unsupervised learning  image recognition  image representation  iterative methods  pattern clustering  enhanced multidataset transfer learning method  unsupervised person re-identification  co-training strategy  progressive unsupervised co-learning  iterative training process  transferred models  multiple source datasets  discriminative person representations  single model  large-scale benchmark datasets  CNN models  labelled source datasets  multiple convolutional neural network models  soft labels  target dataset clustering This study proposes progressive unsupervised co-learning for unsupervised person re-identification by introducing a co-training strategy in an iterative training process. The authors' method adopts an iterative training process to improve transferred models by iterating among clustering, selection, exchange, and fine-tuning. To solve the problem of transferring representations learned from multiple source datasets, their method utilises multiple convolutional neural network (CNN) models trained on different labelled source datasets by feeding soft labels obtained by clustering on target dataset to each other. The enhanced model can learn more discriminative person representations than the single model trained on multiple datasets. Experimental results on two large-scale benchmark datasets (i.e. DukeMTMC-reID and Market-1501) demonstrate that their method can enhance transferred CNN models by using more source datasets and is competitive to the state-of-the-art methods.", "Domain adaptation  fuzzy relations  machine learning  transfer learning Unsupervised domain adaptation (UDA) aims to recognize newly emerged patterns in target domains, which may be unlabeled, by leveraging knowledge from patterns learnt from source domains. However, existing UDA models and algorithms still suffer from heterogeneous domains, known as the heterogeneous unsupervised domain adaptation (HeUDA) issue. To address this issue, this paper presents a novel HeUDA model via n-dimensional fuzzy geometry and fuzzy equivalence relations, called F-HeUDA. The n-dimensional fuzzy geometry is used to propose a metric to measure the similarity between features on one domain. Then, based on this metric, shared fuzzy equivalence relations (SFER) are proposed. The SFER can allow two domains to use the same a to get the same number of clustering categories. Through these clustering categories, knowledge from the heterogeneous source domain can be transferred to the unlabeled target domain. Different to existing HeUDA models, the proposed F-HeUDA model does not need that two domains must have the same number of instances. As a result, the proposed model has a better ability to handle the issue of small datasets. Experiments distributed across four real datasets were conducted to validate the proposed model. This testing regime demonstrates that the proposed model outperforms the state-of-the-art models, especially when the target domain has very few instances.", "Intention posts identification  Multi-instance learning  Multiple sources transfer learning  Classification in discussion forums This paper proposes a novel method for identifying intention posts in discussion forums. The main problem of identifying intention posts in discussion forums is that there exist a few intention sentences even in a post expressing an intention. That is, an intention post consists of a few intention sentences and a number of non-intention sentences, while non-intention posts have only non-intention sentences. Therefore, multi-instance learning which regards a post as a bag and the sentences in the post as instances of the bag is adopted as a solution to this problem. One distinct characteristic of the posts is that the ways of expressing an intention are similar across domains. Thus, we incorporate a multiple sources transfer learning into the multi-instance learning. As a result, the multi-instance learning is enhanced by leveraging knowledge of expressing intentions from multiple source domains. Through a set of experiments, it is proven that the proposed method is effective at identifying intention posts in discussion forums.", "covariate shift  distribution preservation  dithering  health care  k-member clustering  microdata release  privacy  reidentifcation risk  Rosenblatt's transformation  supervised learning  transfer learning Preserving the privacy of individuals by protecting their sensitive attributes is an important consideration during microdata release. However, it is equally important to preserve the quality or utility of the data for at least some targeted workloads. We propose a novel framework for privacy preservation based on the k-anonymity model that is ideally suited for workloads that require preserving the probability distribution of the quasi-identifier variables in the data. Our framework combines the principles of distribution-preserving quantization and k-member clustering, and we specialize it to 2 variants that respectively use intra-cluster and Gaussian dithering of cluster centers to achieve distribution preservation. We perform theoretical analysis of the proposed schemes in terms of distribution preservation, and describe their utility in workloads such as covariate shift and transfer learning where such a property is necessary. Using extensive experiments on real-world Medical Expenditure Panel Survey data, we demonstrate the merits of our algorithms over standard k-anonymization for a hallmark health care application where an insurance company wishes to understand the risk in entering a new market. Furthermore, by empirically quantifying the reidentification risk, we also show that the proposed approaches indeed maintain k-anonymity.", "Deep transfer hashing (DTH)  hashing  privileged information  transfer learning (TL) One major assumption used in most existing hashing approaches is that the domain of interest (i.e., the target domain) could provide sufficient training data, either labeled or unlabeled. However, this assumption may be violated in practice. To address this so-called data sparsity issue in hashing, a new framework termed transfer hashing with privileged information (THPI) is proposed, which marriages hashing and transfer learning (TL). To show the efficacy of THPI, we propose three variants of the well-known iterative quantization (ITQ) as a showcase. The proposed methods, ITQ+, LapITQ+, and deep transfer hashing (DTH), solve the aforementioned data sparsity issue from different aspects. Specifically, ITQ+ is a shallow model, which makes ITQ achieve hashing in a TL manner. ITQ+ learns a new slack function from the source domain to approximate the quantization error on the target domain given by ITQ. To further improve the performance of ITQ+, LapITQ+ is proposed by embedding the geometric relationship of the source domain into the target domain. Moreover, DTH is proposed to show the generality of our framework by utilizing the powerful representative capacity of deep learning. To the best of our knowledge, this could be one of the first DTH works. Extensive experiments on several popular data sets demonstrate the effectiveness of our shallow and DTH approaches comparing with several state-of-the-art hashing approaches.", "Domain adaptation  Image simulation  Machine vision  Roughness measurement  Transfer learning In conventional visual roughness measurement methods, constructing a relationship between an image feature index and surface roughness requires a large number of samples with a wide range of known roughness at uniform intervals as input for training or fitting. Considering these challenges, this paper has proposed a simulated data and transfer kernel learning-based visual roughness measurement method. In the proposed method, a virtual sample with specified roughness is first created via non-Gaussian surface digital simulation and three-dimensional entity modeling technology. After that step, a surface image of the virtual and processed samples is generated through image simulation and actual imaging experiments. Next, the image feature index distribution discrepancy between the simulation and actual domains is adapted by transfer kernel learning. A regression model is trained based on the simulated samples with known roughness, and is later generalized to the actual domain via a cross-domain kernel matrix to predict the roughness of the processed samples. To transfer the similar red and green mixing effects between the actual and simulation domains, a relative mixing degree index and a mixing region area index are designed based on the color information. By comparing these two indexes with the image pixel color difference index and image sharpness index, the feasibility and effectiveness of the proposed method are validated. The experiment results show that the proposed method can achieves an accuracy of over 90% based on the simulated data and transfer kernel learning. The proposed method provides a new improvement strategy for visual roughness measurement. ", "Collaborative filtering  Transfer learning  Multiple sources  Consensus regularization Collaborative filtering is one of the most successful approaches to build recommendation systems. Recently, transfer learning has been applied to recommendation systems for incorporating information from external sources. However, most existing transfer collaborative filtering algorithms tend to transfer knowledge from one single source domain. Rich information is available in many source domains, which can better complement the data in the target domain than that from a single source. However, it is common to get inconsistent information from different sources. To this end, we proposed a TRAnsfer collaborative filtering framework from multiple sources via ConsEnsus Regularization, called TRACER for short. The TRACER framework handles the information inconsistency with a consensus regularization, which enforces the outputs from multiple sources to converge. In addition, our algorithm is to learn and transfer knowledge at the same time while most of the traditional transfer learning algorithms are to learn knowledge first and then transfer it. Experiments conducted on two real-world data sets validate the effectiveness of the proposed algorithm. ", "Convolutional neural networks  transfer learning  multi-task learning  deep learning  visual recognition When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.", "Object detection  convolutional neural networks  semi-supervised learning  transfer learning  visual similarity  semantic similarity  weakly supervised object detection Deep CNN-based object detection systems have achieved remarkable success on several large-scale object detection benchmarks. However, training such detectors requires a large number of labeled bounding boxes, which are more difficult to obtain than image-level annotations. Previous work addresses this issue by transforming image-level classifiers into object detectors. This is done by modeling the differences between the two on categories with both image-level and bounding box annotations, and transferring this information to convert classifiers to detectors for categories without bounding box annotations. We improve this previous work by incorporating knowledge about object similarities from visual and semantic domains during the transfer process. The intuition behind our proposed method is that visually and semantically similar categories should exhibit more common transferable properties than dissimilar categories, e.g. a better detector would result by transforming the differences between a dog classifier and a dog detector onto the cat class, than would by transforming from the violin class. Experimental results on the challenging ILSVRC2013 detection dataset demonstrate that each of our proposed object similarity based knowledge transfer methods outperforms the baseline methods. We found strong evidence that visual similarity and semantic relatedness are complementary for the task, and when combined notably improve detection, achieving state-of-the-art detection performance in a semi-supervised setting.", "Convolutional neural network  Deep transfer learning  Feature visualization  Pedestrian recognition  Feature fusion The extensive deployment of surveillance cameras in public places, such as subway stations and shopping malls, necessitates automated visual-data processing approaches to match pedestrians across non-overlapping multiple cameras. However, due to the insufficient number of labeled training samples in real surveillance scene, it is difficult to train an effective deep neural network for cross-camera pedestrian recognition. Moreover, the cross-camera variation in viewpoint, illumination, and background makes the task even more challenging. To address these issues, in this paper we propose to transfer the parameters of a pre-trained network to our target network and then update the parameters adaptively using training samples from the target domain. More importantly, we develop new network structures that are specially tailored for cross-camera pedestrian recognition task, and implement a simple yet effective multi-level feature fusion method that yield more discriminative and robust features for pedestrian recognition. Specifically, rather than conventionally perform classification on the single-level feature of the last feature layer, we instead utilize multi-level feature by associating feature visualization with multi-level feature fusion. As another contribution, we have published our codes and extracted features to facilitate further research. Extensive experiments are conducted on WARD, PRID and MARS datasets, we show that the proposed method consistently outperforms state-of-the-arts. ", "Deep learning  Traffic sign detection  Convolutional neural network Traffic sign detection systems constitute a key component in trending real-world applications, such as autonomous driving, and driver safety and assistance. This paper analyses the state-of-the-art of several object-detection systems (Faster R-CNN, R-FCN, SSD, and YOLO V2) combined with various feature extractors (Resnet V1 50, Resnet V1 101, Inception V2, Inception Resnet V2, Mobilenet V1, and Darknet-19) previously developed by their corresponding authors. We aim to explore the properties of these object-detection models which are modified and specifically adapted to the traffic sign detection problem domain by means of transfer learning. In particular, various publicly available object-detection models that were pre-trained on the Microsoft COCO dataset are fine-tuned on the German Traffic Sign Detection Benchmark dataset. The evaluation and comparison of these models include key metrics, such as the mean average precision (mAP), memory allocation, running time, number of floating point operations, number of parameters of the model, and the effect of traffic sign image sizes. Our findings show that Faster R-CNN Inception Resnet V2 obtains the best mAP, while R-FCN Resnet 101 strikes the best trade-off between accuracy and execution time. YOLO V2 and SSD Mobilenet merit a special mention, in that the former achieves competitive accuracy results and is the second fastest detector, while the latter, is the fastest and the lightest model in terms of memory consumption, making it an optimal choice for deployment in mobile and embedded devices. ", "Transfer learning  attribute embedding  convolutional neural network  bankruptcy prediction In this paper, we study the problem of transfer learning with the attribute data. In the transfer learning problem, we want to leverage the data of the auxiliary and the target domains to build an effective model for the classification problem in the target domain. Meanwhile, the attributes are naturally stable cross different domains. This strongly motives us to learn effective domain transfer attribute representations. To this end, we proposed to embed the attributes of the data to a common space using the powerful convolutional neural network (CNN) model. The convolutional representations of the data points are mapped to the corresponding attributes so that they can be effective embedding of the attributes. We also represent the data of different domains by a domain-independent CNN, ant a domain-specific CNN and combine their outputs with the attribute embedding to build the classification model. An joint learning framework is constructed to minimise the classification errors, the attribute mapping error, the mismatching of the domain-independent representations cross different domains, and to encourage the neighbourhood smoothness of representations in the target domain. The minimisation problem is solved by an iterative algorithm based on gradient descent. Experiments over benchmark data-sets of person re-identification, bankruptcy prediction and spam email detection show the effectiveness of the proposed method.", "Cross-domain recommendation  cold start  partial least square regression  transfer learning Recommender systems are common in e-commerce platforms in recent years. Recommender systems are able to help users find preferential items among a large amount of products so that users' time is saved and sellers' profits are increased. Cross-domain recommender systems aim to recommend items based on users' different tastes across domains. While recommender systems usually suffer from the user cold-start problem that leads to unsatisfying recommendation performance, cross-domain recommendation can remedy such a problem. This article proposes a novel cross-domain recommendation model based on regression analysis, partial least squares regression (PLSR). The proposed recommendation models, PLSR-CrossRec and PLSR-Latent, are able to purely use source-domain ratings to predict the ratings for cold-start users who never rated items in the target domains. Experiments conducted on the Epinions dataset with ten various domains' rating records demonstrate that PLSR-Latent can outperform several matrix factorization-based competing methods under a variety of cross-domain settings. The time efficiency of PLSR-Latent is also satisfactory.", "Transfer learning  classification Learning from very few samples is a challenge for machine learning tasks, such as text and image classification. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm first learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm.", "Affective computing  Emotion recognition  Deep learning  Natural language processing  Text mining  Transfer learning Emotions widely affect human decision-making. This fact is taken into account by affective computing with the goal of tailoring decision support to the emotional states of individuals. However, the accurate recognition of emotions within narrative documents presents a challenging undertaking due to the complexity and ambiguity of language. Performance improvements can be achieved through deep learning  yet, as demonstrated in this paper, the specific nature of this task requires the customization of recurrent neural networks with regard to bidirectional processing, dropout layers as a means of regularization, and weighted loss functions. In addition, we propose sent2affect, a tailored form of transfer learning for affective computing: here the network is pre-trained for a different task (i.e. sentiment analysis), while the output layer is subsequently tuned to the task of emotion recognition. The resulting performance is evaluated in a holistic setting across 6 benchmark datasets, where we find that both recurrent neural networks and transfer learning consistently outperform traditional machine learning. Altogether, the findings have considerable implications for the use of affective computing.", "Face recognition  Biometrics  Face verification Face recognition performance evaluation has traditionally focused on one-to-one verification, popularized by the Labeled Faces in the Wild data set [1] for imagery and the YouTubeFaces data set [2] for videos. In contrast, the newly released IJB-A face recognition data set [3] unifies evaluation of one-to-many face identification with one-to-one face verification over templates, or sets of imagery and videos for a subject. In this paper, we study the problem of template adaptation, a form of transfer learning to the set of media in a template. Extensive performance evaluations on IJB-A show a surprising result, that perhaps the simplest method of template adaptation, combining deep convolutional network features with template specific linear SVMs, outperforms the state-of-the-art by a wide margin. We study the effects of template size, negative set construction and classifier fusion on performance, then compare template adaptation to convolutional networks with metric learning, 2D and 3D alignment. Our unexpected conclusion is that these other methods, when combined with template adaptation, all achieve nearly the same top performance on IJB-A for template-based face verification and identification. ", "Skin disease inference  transfer learning To benefit the skin care, this paper aims to design an automatic and effective visual analysis framework, with the expectation of recognizing the skin disease from a given image conveying the disease affected surface. This task is nontrivial, since it is hard to collect sufficient well-labeled samples. To address such problem, we present a novel transfer learning model, which is able to incorporate external knowledge obtained from the rich and relevant Web images contributed by grassroots. In particular, we first construct a target domain by crawling a small set of images from vertical and professional dermatological websites. We then construct a source domain by collecting a large set of skin disease related images from commercial search engines. To reinforce the learning performance in the target domain, we initially build a learning model in the target domain, and then seamlessly leverage the training samples in the source domain to enhance this learning model. The distribution gap between these two domains are bridged by a linear combination of Gaussian kernels. Instead of training models with low-level features, we resort to deep models to learn the succinct, invariant, and high-level image representations. Different from previous efforts that focus on a few types of skin diseases with a small and confidential set of images generated from hospitals, this paper targets at thousands of commonly seen skin diseases with publicly accessible Web images. Hence the proposed model is easily repeatable by other researchers and extendable to other disease types. Extensive experiments on a real-world dataset have demonstrated the superiority of our proposed method over the state-of-the-art competitors.", "Deep domain adaptation  Deep networks  Transfer learning  Computer vision applications Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted. ", "Micro-expression  Deep learning  Transferring learning  Convolutional neural network Micro-expression is one of important clues for detecting lies. Its most outstanding characteristics include short duration and low intensity of movement. Therefore, video clips of high spatial-temporal resolution are much more desired than still images to provide sufficient details. On the other hand, owing to the difficulties to collect and encode micro-expression data, it is small sample size. In this paper, we use only 560 micro-expression video clips to evaluate the proposed network model: Transferring Long-term Convolutional Neural Network (TLCNN). TLCNN uses Deep CNN to extract features from each frame of micro-expression video clips, then feeds them to Long Short Term Memory (LSTM) which learn the temporal sequence information of micro-expression. Due to the small sample size of micro-expression data, TLCNN uses two steps of transfer learning: (1) transferring from expression data and (2) transferring from single frame of micro-expression video clips, which can be regarded as big data. Evaluation on 560 micro-expression video clips collected from three spontaneous databases is performed. The results show that the proposed TLCNN is better than some state-of-the-art algorithms. ", "Deep learning  Transfer learning  VGG Periocular region  Video surveillance  UBIRIS.v2  MobBIO Usually, in the deep learning community, it is claimed that generalized representations that yielding outstanding performance / effectiveness require a huge amount of data for learning, which directly affect biometric applications. However, recent works combining transfer learning from other domains have surmounted such data application constraints designing interesting and promising deep learning approaches in diverse scenarios where data is not so abundant. In this direction, a biometric system for the periocular region based on deep learning approach is designed and applied on two non-cooperative ocular databases. Impressive representation discrimination is achieved with transfer learning from the facial domain (a deep convolutional network, called VGG) and fine tuning in the specific periocular region domain. With this design, our proposal surmounts previous state-of-the-art results on NICE (mean decidability of 3.47 against 2.57) and MobBio (equal error rate of 5.42% against 8.73%) competition databases. ", "Demand prediction  Demand-differentiation index  Life-cycle demand  New product development  Product differentiation  Transfer learning Predicting the demand for a new product at early stages is crucial in determining successful product designs. However, the lack of market and consumer related data during the early stages make demand prediction incredibly difficult and unreliable, often underestimating or overestimating the product's demand. With increasing global competition and shortening product life-cycle, almost all the new products have some amount of commonality (differentiation) in their design which presents an opportunity to learn from the abundant data available from the predecessor product. In this work, we developed a novel integrated approach for demand prediction, utilizing weighted product differentiation index between the new and the predecessor products and the prior knowledge of the historical demand for the predecessor. The proposed integrated framework employs advanced machine learning algorithms to first model the non-linear and non-stationary relationship between market demand and product differentiation (thus the product design), which we refer as demand differentiation index (DDI) and then utilize this relationship for predicting the initial demand of the new product in early stages. We further propose DDI modified exponential weighted moving average, DDI-EWMA for product life-cycle demand prediction. The efficacy of the model is demonstrated using real data from the automobile industry. ", "Transfer learning  Instance-feature knowledge  Co-clustering  Data reconstruction  Relativeness Transfer learning, which applies the knowledge from related but different source domains to improve the learning of the target domains, has attracted much attention in recent years. Lots of transfer learning methods have been proposed in literature, and many concentrate on selecting related source (-domain) instances or features for boosting learning. However, in each source instance, possibly only part of the features are related or helpful for transfer. That is, when a source instance is selected by relativeness, its unrelated feature knowledge can also be introduced. At the same time, some related feature knowledge may be discarded when an unrelated source instance is dropped. As a result, these methods ignore the partial related/unrelated knowledge in each source instance. In this paper, we attempt to discover such partial related instance-feature knowledge in transfer, and propose a new transfer learning method with partial related instance-feature knowledge (PRIF for short). Specifically, the partial instance-feature structure is first discovered by co-clustering over both instances and features, then the source instances are reconstructed considering both the related instance-feature knowledge and the related target (-domain) instances, so that the source instances can be more related to the target ones. Finally, transfer learning is performed with these newly-constructed source instances. Empirical results over several real-world datasets demonstrate the effectiveness of PRIF. ", "Performance gap  Sustainability  Building performance simulation  Transfer learning  Multi-task learning  LSTM Increasing sustainability requirements make evaluating different design options for identifying energy-efficient design ever more important. These requirements demand simulation models that are not only accurate but also fast. Machine Learning (ML) enables effective mimicry of Building Performance Simulation (BPS) while generating results much faster than BPS. Component-Based Machine Learning (CBML) enhances the capabilities of the monolithic ML model. Extending monolithic ML approach, the paper presents deep-learning architectures, component development methods and evaluates their suitability for space exploration in building design. Results indicate that deep learning increases the performance of models over simple artificial neural network models. Methods such as transfer learning and Multi-Task Learning make the component development process more efficient. Testing the deep-learning model on 201 new design cases indicates that its cooling energy prediction (R-2: 0.983) is similar to BPS, while errors for heating energy predictions (R-2: 0.848) are higher than BPS. Higher heating energy prediction error can be resolved by collecting heating data using better design space sampling methods that cover the heating demand distribution effectively. Given that the accuracy of the deep-learning model for heating predictions can be increased, the major advantage of deep-learning models over BPS is their high computation speed. BPS required 1145 s to simulate 201 design cases. Using the deep-learning model, similar results can be obtained in 0.9 s. High computation speed makes deep-learning models suitable for design space exploration.", "Satellite image  No-reference  Neural network  Assessment  Real time A method for deep satellite image quality assessment based on no-reference satellite images is proposed. We design suitable deep convolutional neural networks, which are named satellite image quality assessment of deep convolutional neural networks (SIQA-DCNN) and SIQA-DCNN++. These sophisticated methods can remove various distorted satellite images in real-time remote sensing. The novelty of this method lies in the objective assessment and restoration of the deep model which understands various distorted satellite images in high- and low-resolution problems. The activation function has a lower computational time and ensures the deactivation of noise by making the mean activators close to zero. Our methods are also effective for transfer learning, which can be used to adequately investigate satellite image classification in deep satellite image quality assessment. Using Spearman's rank order correlation coefficient (SROCC) and linear correlation coefficient (LCC) evaluations, we demonstrated that our methods show better performance than other algorithms, with more than 0.90 of SROCC and LCC values compared to the full-reference and no-reference satellite image in MODIS/Terra and USGS datasets. Regarding computational complexity, we obtained better performance in operational function times. As compared to other methods, SIQA-DCNN and SIQA-DCNN++ also reduced computational time by more than 40 and 56%, respectively, when applied to the USGS dataset, and by more than 46 and 60% respectively, when applied to the MODIS/Terra dataset.", "Cross domain  Opinion mining  Aspect extraction Aspect-Based Sentiment Analysis (ABSA) is a promising approach to analyze consumer reviews at a high level of detail, where the opinion about each feature of the product or service is considered. ABSA usually explores supervised inductive learning algorithms, which requires intense human effort for the labeling process. In this paper, we investigate Cross-Domain Transfer Learning approaches, in which aspects already labeled in some domains can be used to support the aspect extraction of another domain where there are no labeled aspects. Existing cross-domain transfer learning approaches learn classifiers from labeled aspects in the source domain and then apply these classifiers in the target domain, Le, two separate stages that may cause inconsistency due to different feature spaces. To overcome this drawback, we present an innovative approach called CD-ALPHN (Cross-Domain Aspect Label Propagation through Heterogeneous Networks). First, we propose a heterogeneous network-based representation that combines different features (labeled aspects, unlabeled aspects, and linguistic features) from source and target domain as nodes in a single network. Second, we propose a label propagation algorithm for aspect extraction from heterogeneous networks, where the linguistic features are used as a bridge for this propagation. Our algorithm is based on a transductive learning process, where we explore both labeled and unlabeled aspects during the label propagation. Experimental results show that the CD-ALPHN outperforms the state-of-the-art methods in scenarios where there is a high-level of inconsistency between the source and target domains the most common scenario in real-world applications.", "Iris sensor identification  Forensics  Iris sensor interoperability  Convolutional Neural Networks The aim of this paper is to propose an algorithm based on convolutional neural networks (CNN) for iris sensor model identification. This task is important in forensics applications as well as to face the problem of sensor interoperability in large scale systems. When different sensor models are involved in a recognition system, in fact, the overall performance can strongly decrease. A possible solution consists in first identifying the sensor model and then mapping the features extracted from the image from one sensor to the other. To keep low both complexity and memory requirements we propose a simple network architecture and the use of transfer learning to speed-up the training phase and tackle the problem of limited training set availability. Experiments are carried out on several public iris databases. First, we show that the proposed solution outperforms the state-of-the art approaches used for the model identification task. Then, we test the performance of a biometric recognition system and show that improving the sensor model identification step can benefit the iris sensor interoperability. ", "Image segmentation  Image classification  Deep learning  Transfer learning  Convolutional neural network  Fully convolutional network Deep learning has shown promising results in medical image analysis, however, the lack of very large annotated datasets confines its full potential. Although transfer learning with ImageNet pre-trained classification models can alleviate the problem, constrained image sizes and model complexities can lead to unnecessary increase in computational cost and decrease in performance. As many common morphological features are usually shared by different classification tasks of an organ, it is greatly beneficial if we can extract such features to improve classification with limited samples. Therefore, inspired by the idea of curriculum learning, we propose a strategy for building medical image classifiers using features from segmentation networks. By using a segmentation network pre-trained on similar data as the classification task, the machine can first learn the simpler shape and structural concepts before tackling the actual classification problem which usually involves more complicated concepts. Using our proposed framework on a 3D three-class brain tumor type classification problem, we achieved 82% accuracy on 191 testing samples with 91 training samples. When applying to a 2D nine-class cardiac semantic level classification problem, we achieved 86% accuracy on 263 testing samples with 108 training samples. Comparisons with ImageNet pre-trained classifiers and classifiers trained from scratch are presented. ", "Attribute  Annotation  Relationship  Active learning  Transfer learning Attributes are widely used in different vision tasks. However, existing attribute resources are quite limited and most of them are not in large scale. Current attribute annotation process is generally done by human, which is expensive and time-consuming. in this paper, we propose a novel framework to perform effective attribute annotations. Based on the common knowledge that attributes can be shared among different classes, we leverage the benefits of transfer learning and active learning together to transfer knowledge from some existing small attribute databases to large-scale target databases. In order to learn more robust attribute models, attribute relationships are incorporated to assist the learning process. Using the proposed framework, we conduct extensive experiments on two large-scale image databases, i.e. ImageNet and SUN Attribute, where high quality automatic attribute annotations are obtained. ", "Concept drift  data stream mining  ensemble learning  incremental learning  transfer learning Incremental learning with concept drift has often been tackled by ensemble methods, where models built in the past can be retrained to attain new models for the current data. Two design questions need to be addressed in developing ensemble methods for incremental learning with concept drift, i.e., which historical (i.e., previously trained) models should be preserved and how to utilize them. A novel ensemble learning method, namely, Diversity and Transfer-based Ensemble Learning (DTEL), is proposed in this paper. Given newly arrived data, DTEL uses each preserved historical model as an initial model and further trains it with the new data via transfer learning. Furthermore, DTEL preserves a diverse set of historical models, rather than a set of historical models that are merely accurate in terms of classification accuracy. Empirical studies on 15 synthetic data streams and 5 real-world data streams (all with concept drifts) demonstrate that DTEL can handle concept drift more effectively than 4 other state-of-the-art methods.", "Convolutional neural networks  Shearlet transform  Multimodal medical image fusion  Transfer learning  Similarity metric learning Recently, deep learning has been shown effectiveness in multimodal image fusion. In this paper, we propose a fusion method for CT and MR medical images based on convolutional neural network (CNN) in the shearlet domain. We initialize the Siamese fully convolutional neural network with a pre-trained architecture learned from natural data  then, we train it with medical images in a transfer learning fashion. Training dataset is made of positive and negative patch pair of shearlet coefficients. Examples are fed in two-stream deep CNN to extract features maps  then, a similarity metric learning based on cross-correlation is performed aiming to learn mapping between features. The minimization of the logistic loss objective function is applied with stochastic gradient descent. Consequently, the fusion process flow starts by decomposing source CT and MR images by the non-subsampled shearlet transform into several subimages. High-frequency subbands are fused based on weighted normalized cross-correlation between feature maps given by the extraction part of the CNN, while low-frequency coefficients are combined using local energy. Training and test datasets include pairs of pre-registered CT and MRI taken from the Harvard Medical School database. Visual analysis and objective assessment proved that the proposed deep architecture provides state-of-the-art performance in terms of subjective and objective assessment. The potential of the proposed CNN for multi-focus image fusion is exhibited in the experiments.", "Blast furnace (BF)  Molten iron quality (MIQ)  Multi-output LS-SVR (M-LS-SVR)  Data-driven predictive control  Inverse system identification Control of blast furnace (BF) ironmaking process has always been a hot and difficult issue in metallurgic engineering and automation. In this paper, a novel data-driven inverse system identification based predictive control method is proposed for multivariate molten iron quality (MIQ) indices in BF ironmaking process. First, since the widely used least-square support regression (LS-SVR) algorithm cannot cope with the multi-output problem directly, this paper uses multi-task transfer learning technology to construct a novel multi-output LS-SVR (M-LS-SVR) for multivariable nonlinear systems. Then, this M-LS-SVR is adopted to identify the inverse system model of the controlled BF ironmaking process with the help of the presented modeling performance comprehensive evaluation and NSGA II based multi-objective parameter optimization. In order to better perform the control of the MIQ indices, the identified inverse system model is used to compensate the controlled nonlinear BF system to a compounded pseudo linear system with linear transitive relation. Such an inverse system based data-driven predictive control can effectively improve the control performance of the conventional nonlinear predictive control. Data experiments using actual industrial data from a large BF show that the proposed methods are effective, advanced and practical, and provide a solution to the operational control and optimization of the BF ironmaking process. ", "Manufacturing analytics  Deep learning  Transfer learning  Pellet shape classification Manufacturing analytics is of paramount importance in many plants today, and its relevance increases in the current big data context of Industry 4.0. The fields of statistics, chemometrics, and machine learning are expected to provide tools that effectively handle many of the characteristics of industrial data. In this paper, the task of image-based product classification is considered. This is a supervised learning problem where the input is an image and the output is a unique label attributed to the image from a finite set of labels corresponding to the available product classes. This is a prevalent and highly relevant industrial challenge and recent developments in deep learning have proven to be successful in increasing the image classification accuracy, providing state-of-the-art results. Thus, in this work, we leverage deep neural networks' (DNN) ability to automatically learn features from images and test their performance in a real industrial context for predicting the pellet shape. In order to accelerate the training of DNN, transfer learning is employed and a network previously developed for one task is adapted to predict pellet shape. Furthermore, other less complex techniques such as partial least squares discriminant analysis (PLS-DA) and random forests (RF) are also explored in order to assess the benefits of adopting DNN as opposed to current classifiers. An industrial image classification case study was utilized to compare PLS-DA, RF, and DNN models. Compared to the in situ classification system currently in use, increasingly complex models (PLS-DA and RF) were able to better utilize the same pre-defined features and improve prediction accuracy significantly. DNN obtained the highest accuracy on the independent test set, with the advantages of not requiring the a priori computation of image features since they are directly extracted from the raw images. Moreover, by visualizing the output of some layers of the DNN, it is possible to verify that activations occurred in regions that are indeed meaningful for the classification tasks, further supporting that DNN were effectively modelling the relevant features of the pellet.", "Time series prediction  Transfer learning  Extreme learning machine (ELM)  Online learning  Ensemble learning Recently, many excellent algorithms for time series prediction issues have been proposed, most of which are developed based on the assumption that sufficient training data and testing data under the same distribution are available. However, in reality, time-series data usually exhibit some kind of time-varying characteristic, which may lead to a wide variability between old data and new data. Hence, how to transfer knowledge over a long time span, when addressing time series prediction issues, poses serious challenges. To solve this problem, in this paper, a hybrid algorithm based on transfer learning, Online Sequential Extreme Learning Machine with Kernels (OS-ELMK), and ensemble learning, abbreviated as TrEnOS-ELMK, is proposed, along with its precise mathematic derivation. It aims to make the most of, rather than discard, the adequate long-ago data, and constructs an algorithm framework for transfer learning in time series forecasting, which is groundbreaking. Inspired by the preferable performance of models ensemble, ensemble learning scheme is also incorporated into our proposed algorithm, where the weights of the constituent models are adaptively updated according to their performances on fresh samples. Compared to many existing time series prediction methods, the newly proposed algorithm takes long-ago data into consideration and can effectively leverage the latent knowledge implied in these data for current prediction. In addition, TrEnOS-ELMK naturally inherits merits of both OS-ELMK and ensemble learning due to its incorporation of the two techniques. Experimental results on three synthetic and six real world datasets demonstrate the effectiveness of the proposed algorithm.", "Transfer learning  test framework  domain adaptation A transfer learning environment is characterized by not having sufficient labeled training data from the domain of interest (target domain) to build a high-performing machine learner. Transfer learning algorithms use labeled data from an alternate domain (source domain), that is similar to the target domain, to build high-performing learners. The design of a transfer learning algorithm is typically comprised of a domain adaptation step following by a learning step. The domain adaptation step attempts to align the distribution differences between the source domain and the target domain. Then, the aligned data from the domain adaptation step is used in the learning step, which is typically implemented with a traditional machine learning algorithm. Our research studies the impact of the learning step on the performance of various transfer learning algorithms. In our experiment, we use five unique domain adaptation methods coupled with seven different traditional machine learning methods to create 35 different transfer learning algorithms. We perform comparative performance analyses of the 35 transfer learning algorithms, along with the seven stand-alone traditional machine learning methods. This research will aid machine learning practitioners in the algorithm selection process for a transfer learning environment in the absence of reliable validation techniques.", "Blast furnace (BF)  m-estimator  molten iron quality (MIQ)  multiobjective optimization  multioutput leastsquares support vector regression (LS-SVR)  multitask transfer learning (TL)  nonlinear autoregressive exogenous (NARX) model  robust modeling Optimal operation of an industrial blast furnace (BF) ironmaking process largely depends on a reliable measurement of molten iron quality (MIQ) indices, which are not feasible using the conventional sensors. This paper proposes a novel data-driven robust modeling method for the online estimation and control of MIQ indices. First, a nonlinear autoregressive exogenous (NARX) model is constructed for the MIQ indices to completely capture the nonlinear dynamics of the BF process. Then, considering that the standard least-squares support vector regression (LS-SVR) cannot directly cope with the multioutput problem, a multitask transfer learning is proposed to design a novel multioutput LS-SVR (M-LS-SVR) for the learning of the NARX model. Furthermore, a novel M-estimator is proposed to reduce the interference of outliers and improve the robustness of the M-LS-SVR model. Since the weights of different outlier data are properly given by the weight function, their corresponding contributions on modeling can properly be distinguished, thus a robust modeling result can be achieved. Finally, a novel multiobjective evaluation index on the modeling performance is developed by comprehensively considering the root-mean-square error of modeling and the correlation coefficient on trend fitting, based on which the nondominated sorting genetic algorithm II is used to globally optimize the model parameters. Both experiments using industrial data and industrial applications illustrate that the proposed method can eliminate the adverse effect caused by the fluctuation of data in BF process efficiently. This indicates its stronger robustness and higher accuracy. Moreover, control testing shows that the developed model can be well applied to realize data-driven control of the BF process.", "Distance metric learning (DML)  heterogeneous domain  high-order statistics  multitask  tensor Distance metric learning plays a crucial role in diverse machine learning algorithms and applications. When the labeled information in a target domain is limited, transfer metric learning (TML) helps to learn the metric by leveraging the sufficient information from other related domains. Multitask metric learning (MTML), which can be regarded as a special case of TML, performs transfer across all related domains. Current TML tools usually assume that the same feature representation is exploited for different domains. However, in real-world applications, data may be drawn from heterogeneous domains. Heterogeneous transfer learning approaches can be adopted to remedy this drawback by deriving a metric from the learned transformation across different domains. However, they are often limited in that only two domains can be handled. To appropriately handle multiple domains, we develop a novel heterogeneous MTML (HMTML) framework. In HMTML, the metrics of all different domains are learned together. The transformations derived from the metrics are utilized to induce a common subspace, and the high-order covariance among the predictive structures of these domains is maximized in this subspace. There do exist a few heterogeneous transfer learning approaches that deal with multiple domains, but the high-order statistics (correlation information), which can only be exploited by simultaneously examining all domains, is ignored in these approaches. Compared with them, the proposed HMTML can effectively explore such high-order information, thus obtaining more reliable feature transformations and metrics. Effectiveness of our method is validated by the extensive and intensive experiments on text categorization, scene classification, and social image annotation.", "Handwritten signature verification  Representation learning  Convolutional neural networks  Transfer learning  Domain adaptation Methods for learning feature representations for offline handwritten signature verification have been successfully proposed in recent literature, using deep convolutional neural networks to learn representations from signature pixels. Such methods reported large performance improvements compared to handcrafted feature extractors. However, they also introduced an important constraint: the inputs to the neural networks must have a fixed size, while signatures vary significantly in size between different users. In this paper, we propose addressing this issue by learning a fixed-sized representation from variable-sized signatures by modifying the network architecture, using spatial pyramid pooling. We also investigate the impact of the resolution of the images used for training and the impact of adapting (fine-tuning) the representations to new operating conditions (different acquisition protocols, such as writing instruments and scan resolution). On the GPDS dataset, we achieve results comparable with the state of the art, while removing the constraint of having a maximum size for the signatures to be processed. We also show that using higher resolutions (300 or 600 dpi) can improve performance when skilled forgeries from a subset of users are available for feature learning, but lower resolutions (around 100dpi) can be used if only genuine signatures are used. Lastly, we show that fine-tuning can improve performance when the operating conditions change.", "Foggy scene understanding  Semantic segmentation  Object detection  Depth denoising and completion  Dehazing  Transfer learning This work addresses the problem of semantic foggy scene understanding (SFSU). Although extensive research has been performed on image dehazing and on semantic scene understanding with clear-weather images, little attention has been paid to SFSU. Due to the difficulty of collecting and annotating foggy images, we choose to generate synthetic fog on real images that depict clear-weather outdoor scenes, and then leverage these partially synthetic data for SFSU by employing state-of-the-art convolutional neural networks (CNN). In particular, a complete pipeline to add synthetic fog to real, clear-weather images using incomplete depth information is developed. We apply our fog synthesis on the Cityscapes dataset and generate Foggy Cityscapes with 20,550 images. SFSU is tackled in two ways: (1) with typical supervised learning, and (2) with a novel type of semi-supervised learning, which combines (1) with an unsupervised supervision transfer from clear-weather images to their synthetic foggy counterparts. In addition, we carefully study the usefulness of image dehazing for SFSU. For evaluation, we present Foggy Driving, a dataset with 101 real-world images depicting foggy driving scenes, which come with ground truth annotations for semantic segmentation and object detection. Extensive experiments show that (1) supervised learning with our synthetic data significantly improves the performance of state-of-the-art CNN for SFSU on Foggy Driving  (2) our semi-supervised learning strategy further improves performance  and (3) image dehazing marginally advances SFSU with our learning strategy. The datasets, models and code are made publicly available.", "Transfer learning  Spectral clustering  Co-clustering  Multi-task learning Many real-world applications propose the request for sharing knowledge among different tasks or datasets. Transfer learning has been proposed to solve this kind of problems and it has been successfully applied in supervised learning and semi-supervised learning settings. However, its adoption in clustering, one of the most classical research problems in machine learning and data mining, is still scarce. Spectral clustering, as a major clustering algorithm with wide applications and better performance than k-means typically, has not been well incorporated with knowledge transfer. In this paper, we first consider the problem of learning from only one auxiliary unlabeled dataset for spectral clustering and propose a novel algorithm called transfer spectral clustering (TSC). Then, it is extended to the settings with multiple auxiliary tasks. TSC assumes the feature embeddings being shared with the auxiliary tasks and utilizes co-clustering to extract useful information from the auxiliary datasets to improve the clustering performance. TSC involves not only the data manifold information of individual task but also the feature manifold information shared between related tasks. An in-depth explanation of our algorithm together with a convergence analysis are provided. As demonstrated by the extensive experiments, TSC can effectively improve the clustering performance by using auxiliary unlabeled data when compared with other state-of-the-art clustering algorithms. ", "Convolutional Neural Networks  Transfer learning  Vehicle recognition  Noisy dataset  Isolation forest One of the main ingredients to learn a visual representation of an object using the Convolutional Neural Networks is a large and carefully annotated dataset. Acquiring a dataset in a demanded scale is not a straightforward task  therefore, the community attempts to solve this problem by creating noisy datasets gathered from web sources. In this paper, this issue is tackled by designing a vehicle recognition system using Convolutional Neural Networks and noisy web data. In the proposed system, the transfer learning technique is employed, and behavior of several deep architectures trained on a noisy dataset are studied. In addition, the external noise of the gathered dataset is reduced by exploiting an unsupervised method called Isolation Forest, and the new training results are examined. Based on the experiments, high recognition accuracies were achieved by training two states of the art networks on the noisy dataset, and the obtained results were slightly improved by using the proposed noise reduction framework. Finally, a demonstration application is provided to show the capability and the performance of the proposed approach. ", "Dimensionality reduction  domain adaption  dynamic multiobjective optimization  evolutionary algorithm (EA)  transfer learning One of the major distinguishing features of the dynamic multiobjective optimization problems (DMOPs) is that optimization objectives will change over time, thus tracking the varying Pareto-optimal front becomes a challenge. One of the promising solutions is reusing experiences to construct a prediction model via statistical machine learning approaches. However, most existing methods neglect the nonindependent and identically distributed nature of data to construct the prediction model. In this paper, we propose an algorithmic framework, called transfer learning-based dynamic multiobjective evolutionary algorithm (EA), which integrates transfer learning and population-based EAs to solve the DMOPs. This approach exploits the transfer learning technique as a tool to generate an effective initial population pool via reusing past experience to speed up the evolutionary process, and at the same time any population-based multiobjective algorithms can benefit from this integration without any extensive modifications. To verify this idea, we incorporate the proposed approach into the development of three well-known EAs, non-dominated sorting genetic algorithm II, multiobjective particle swarm optimization, and the regularity model-based multiobjective estimation of distribution algorithm. We employ 12 benchmark functions to test these algorithms as well as compare them with some chosen state-of-the-art designs. The experimental results confirm the effectiveness of the proposed design for DMOPs.", "Reinforcement learning  Transfer learning  Case-based reasoning  Robotics Reinforcement Learning (RL) is a well-known technique for learning the solutions of control problems from the interactions of an agent in its domain. However, RL is known to be inefficient in problems of the real-world where the state space and the set of actions grow up fast. Recently, heuristics, case-based reasoning (CBR) and transfer learning have been used as tools to accelerate the RL process. This paper investigates a class of algorithms called Transfer Learning Heuristically Accelerated Reinforcement Learning (TLHARL) that uses CBR as heuristics within a transfer learning setting to accelerate RL. The main contributions of this work are the proposal of a new TLHARL algorithm based on the traditional RL algorithm Q(lambda) and the application of TLHARL on two distinct real-robot domains: a robot soccer with small-scale robots and the humanoid-robot stability learning. Experimental results show that our proposed method led to a significant improvement of the learning rate in both domains.", "Bipartite graph learning  kernel methods  Kronecker product kernel  ridge regression  support vector machine (SVM)  transfer learning  zero-shot learning Kronecker product kernel provides the standard approach in the kernel methods' literature for learning from graph data, where edges are labeled and both start and end vertices have their own feature representations. The methods allow generalization to such new edges, whose start and end vertices do not appear in the training data, a setting known as zero-shot or zero-data learning. Such a setting occurs in numerous applications, including drug-target interaction prediction, collaborative filtering, and information retrieval. Efficient training algorithms based on the so-called vec trick that makes use of the special structure of the Kronecker product are known for the case where the training data are a complete bipartite graph. In this paper, we generalize these results to noncomplete training graphs. This allows us to derive a general framework for training Kronecker product kernel methods, as specific examples we implement Kronecker ridge regression and support vector machine algorithms. Experimental results demonstrate that the proposed approach leads to accurate models, while allowing order of magnitude improvements in training and prediction time.", "Transfer learning  Ensemble selection  Rank-based ensemble pruning  Rank-based Reduce Error (RankRE) ensemble selection approach  Transfer learning algorithm based on rankRE (RankRE-TL) A major assumption in traditional machine leaning is that the training and testing data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. In recent years, transfer learning has emerged as a new learning paradigm to cope with this considerable challenge. It focuses on exploiting previously learnt knowledge by leveraging information from an old source domain to help learning in a new target domain. In this work, we integrate the knowledge-leverage-based Transfer Learning mechanism with a Rank-based Reduce Error ensemble selection approach to fulfill the transfer learning task, called RankRE-TL. Ensemble selection is important for improving both efficiency and predictive accuracy of an ensemble system. It aims to select a proper subset of the whole ensemble, which usually outperforms the whole one. Therefore, we appropriately modify the Reduce Error (RE) pruning technique and design a new Rank-based Reduce Error ensemble selection method (RankRE) to deal with the transfer learning task. The design idea of RankRE is to find the candidate classifier which is expected to improve the classification performance of the extended subensemble the most. In the RankRE-TL algorithm, the initial Support Vector Machine (SVM) ensemble is learnt based upon dynamic training dataset regrouping. And simultaneously, a new construction method of validation set is designed for RankRE-TL, which differs from the method used in conventional ensemble selection paradigm.", "Genital injury  Forensics  Digital colposcopy  Deep learning  Neural networks  Segmentation  Classification  Transfer learning  Image processing Despite the existence of patterns able to discriminate between consensual and non-consensual intercourse, the relevance of genital lesions in the corroboration of a legal rape complaint is currently under debate in many countries. The testimony of the physicians when assessing these lesions has been questioned in court due to several factors (e.g., a lack of comprehensive knowledge of lesions, wide spectrum of background area, among others). Therefore, it is relevant to provide automated tools to support the decision process in an objective manner. In this work, we evaluate the performance of state-of-the-art deep learning architectures for the forensic assessment of sexual assault. We propose a deep architecture and learning strategy to tackle the class imbalance on deep learning using ranking. The proposed methodologies achieved the best results when compared with handcrafted feature engineering and with other deep architectures .", "Domain adaptation  Image classification  Fusion methods  Domain adaptation frameworks Automatic annotation of images is one of the fundamental problems in computer vision applications. With the increasing amount of freely available images, it is quite possible that the training data used to learn a classifier has different distribution from the data which is used for testing. This results in degradation of the classifier performance and highlights the problem known as domain adaptation. Framework for domain adaptation typically requires a classification model which can utilize several classifiers by combining their results to get the desired accuracy. This work proposes depth-based and iterative depth-based fusion methods which are basically rank-based fusion methods and utilize rank of the predicted labels from different classifiers. Two frameworks are also proposed for domain adaptation. The first framework uses traditional machine learning algorithms, while the other works with metric learning as well as transfer learning algorithm. Motivated from ImageCLEF's 2014 domain adaptation task, these frameworks with the proposed fusion methods are validated and verified by conducting experiments on the images from five domains having varied distributions. Bing, Caltech, ImageNet, and PASCAL are used as source domains and the target domain is SUN. Twelve object categories are chosen from these domains. The experimental results show the performance improvement not only over the baseline system, but also over the winner of the ImageCLEF's 2014 domain adaptation challenge.", "Image restoration  discriminative learning  proximal optimization Recently, several discriminative learning approaches have been proposed for effective image restoration, achieving convincing tradeoff between image quality and computational efficiency. However, these methods require separate training for each restoration task (e.g., denoising, deblurring, and demosaicing) and problem condition (e.g., noise level of input images). This makes it time-consuming and difficult to encompass all tasks and conditions during training. In this paper, we propose a discriminative transfer learning method that incorporates formal proximal optimization and discriminative learning for general image restoration. The method requires a single-pass discriminative training and allows for reuse across various problems and conditions while achieving an efficiency comparable to previous discriminative approaches. Furthermore, after being trained, our model can be easily transferred to new likelihood terms to solve untrained tasks, or be combined with existing priors to further improve image restoration quality.", "Transfer learning  Facial attribute classification  Multi-label learning  Deep learning  Convolutional neural networks Deep Neural Network (DNN) has recently achieved outstanding performance in a variety of computer vision tasks, including facial attribute classification. The great success of classifying facial attributes with DNN often relies on a massive amount of labelled data. However, in real-world applications, labelled data are only provided for some commonly used attributes (such as age, gender)  whereas, unlabelled data are available for other attributes (such as attraction, hairline). To address the above problem, we propose a novel deep transfer neural network method based on multi-label learning for facial attribute classification, termed FMTNet, which consists of three sub-networks: the Face detection Network (FNet), the Multi-label learning Network (MNet) and the Transfer learning Network (TNet). Firstly, based on the Faster Region-based Convolutional Neural Network (Faster R-CNN), FNet is fine-tuned for face detection. Then, MNet is fine-tuned by FNet to predict multiple attributes with labelled data, where an effective loss weight scheme is developed to explicitly exploit the correlation between facial attributes based on attribute grouping. Finally, based on MNet, TNet is trained by taking advantage of unsupervised domain adaptation for unlabelled facial attribute classification. The three sub-networks are tightly coupled to perform effective facial attribute classification. A distinguishing characteristic of the proposed FMTNet method is that the three sub-networks (FNet, MNet and TNet) are constructed in a similar network structure. Extensive experimental results on challenging face datasets demonstrate the effectiveness of our proposed method compared with several state-of-the-art methods. ", "Deep Convolutional Neural Network (DCNN)  Handwritten Arabic Character Recognition (HACR)  OIHACDB  AHCD  Transfer learning Handwritten character recognition is a system widely used in the modern world and it is still an important challenge. Traditional machine-learning techniques require careful engineering and considerable domain expertise to transform raw data into a feature vector from which the classifier could classify the input pattern. To cope with this problem, the popular Deep Convolutional Neural Networks (DCNN), introduced recently, have effectively replaced the hand-crafted descriptors with network features and have been shown to provide significantly better results than traditional methods. It is one of the fastest growing areas in machine learning, promising to reshape the future of artificial intelligence. However, the problem with deep learning is that it requires large datasets for training because of the huge number of parameters needed to be tuned by a learning algorithm. CNN model can be used in three different ways: (i) training the CNN from scratch  (ii) using the transfer learning strategy to leverage features from a pre-trained model on a larger dataset  and (iii) keeping the transfer learning strategy and fine-tune the weights of CNN architecture. In this work, we investigate the applicability of DCNN using transfer learning strategies on two datasets  a new expanded version of our recently proposed database for off-line isolated handwritten Arabic character, referred to as OIHACDB and AHCD. Our results showed satisfactory recognition accuracies and outperform all other prominent exiting methods in the field of Handwritten Arabic Character Recognition (HACR). ", "Stacked RICA  Transfer learning  Logistic regression model  KL-Divergence Significant improvements to transfer learning have emerged in recent years, because deep learning has been proposed to learn more higher level and robust features. However, most of existing deep learning approaches are based on the framework of auto-encoder or sparse auto-encoder, which pose challenges for independent component analysis and fail to measure similarities between data spaces. Therefore, in this paper, we propose a new strategy to achieve a better feature representation performance for transfer learning. There are several advantages in our method as follows: 1) The model of Stacked Reconstruction Independent Component Analysis (SRICA) is used to pursuit an optimal feature representation  2) The label information is used by Logistic Regression Model to optimize representation features and the distance of distributions between domains is minimized by the method of KL-Divergence. Extensive experiments conducted on several image datasets demonstrate the superiority of our proposed method compared with all competing state-of-the-art methods. ", "Transfer learning  Gaussian mixture models  Learning vector quantization  Expectation maximization  Bionic prostheses Machine learning models in practical settings are typically confronted with changes to the distribution of the incoming data. Such changes can severely affect the model performance, leading for example to misclassifications of data. This is particularly apparent in the domain of bionic hand prostheses, where machine learning models promise faster and more intuitive user interfaces, but are hindered by their lack of robustness to everyday disturbances, such as electrode shifts. One way to address changes in the data distribution is transfer learning, that is, to transfer the disturbed data to a space where the original model is applicable again. In this contribution, we propose a novel expectation maximization algorithm to learn linear transformations that maximize the likelihood of disturbed data according to the undisturbed model. We also show that this approach generalizes to discriminative models, in particular learning vector quantization models. In our evaluation on data from the bionic prostheses domain we demonstrate that our approach can learn a transformation which improves classification accuracy significantly and outperforms all tested baselines, if few data or few classes are available in the target domain. ", "Video anomaly detection  CNN  Transfer learning  Real-time processing The detection of abnormal behaviour in crowded scenes has to deal with many challenges. This paper presents an efficient method for detection and localization of anomalies in videos. Using fully convolutional neural networks (FCNs) and temporal data, a pre-trained supervised FCN is transferred into an unsupervised FCN ensuring the detection of (global) anomalies in scenes. High performance in terms of speed and accuracy is achieved by investigating the cascaded detection as a result of reducing computation complexities. This FCN-based architecture addresses two main tasks, feature representation and cascaded outlier detection. Experimental results on two benchmarks suggest that the proposed method outperforms existing methods in terms of accuracy regarding detection and localization.", "Computer vision  sparse representation  transfer learning (TL)  visual concept recognition Transfer learning (TL) aims at solving the problem of learning an effective classification model for a target category, which has few training samples, by leveraging knowledge from source categories with far more training data. We propose a new discriminative TL (DTL) method, combining a series of hypotheses made by both the model learned with target training samples and the additional models learned with source category samples. Specifically, we use the sparse reconstruction residual as a basic discriminant and enhance its discriminative power by comparing two residuals from a positive and a negative dictionary. On this basis, we make use of similarities and dissimilarities by choosing both positively correlated and negatively correlated source categories to form additional dictionaries. A new Wilcoxon-Mann-Whitney statistic-based cost function is proposed to choose the additional dictionaries with unbalanced training data. Also, two parallel boosting processes are applied to both the positive and negative data distributions to further improve classifier performance. On two different image classification databases, the proposed DTL consistently outperforms other state-of-the-art TL methods while at the same time maintaining very efficient runtime.", "Co-occurrence data  hedge weighting  heterogeneous transfer learning (HTL)  online learning In this paper, we study the online heterogeneous transfer (OHT) learning problem, where the target data of interest arrive in an online manner, while the source data and auxiliary co-occurrence data are from offline sources and can be easily annotated. OHT is very challenging, since the feature spaces of the source and target domains are different. To address this, we propose a novel technique called OHT by hedge ensemble by exploiting both offline knowledge and online knowledge of different domains. To this end, we build an offline decision function based on a heterogeneous similarity that is constructed using labeled source data and unlabeled auxiliary co-occurrence data. After that, an online decision function is learned from the target data. Last, we employ a hedge weighting strategy to combine the offline and online decision functions to exploit knowledge from the source and target domains of different feature spaces. We also provide a theoretical analysis regarding the mistake bounds of the proposed approach. Comprehensive experiments on three real-world data sets demonstrate the effectiveness of the proposed technique.", "active learning  activity recognition  data mining  deep learning  machine learning  transfer learning  wearable sensors There has been an upsurge recently in investigating machine learning techniques for activity recognition (AR) problems as they have been very effective in extracting and learning knowledge from the activity datasets. The technique ranges from heuristically derived hand-crafted feature-based traditional machine learning algorithms to the recently developed hierarchically self-evolving feature-based deep learning algorithms. AR continues to remain a challenging problem in uncontrolled smart environments despite the amount of work contributed by the researcher in this field. The complex, volatile, and chaotic nature of the activity data presents numerous challenges that influence the performance of the AR systems in the wild. In this article, we present a comprehensive overview of recent machine learning and data mining techniques generally employed for AR and the underpinning problems and challenges associated with the existing systems. We also articulate the recent advances and state-of-the-art techniques in this domain in an attempt to identify the possible directions for future AR research. This article is categorized under: Application Areas > Science and Technology Algorithmic Development > Spatial and Temporal Data Mining Technologies > Machine Learning Fundamental Concepts of Data and Knowledge > Motivation and Emergence of Data Mining", "Attribute learning  dictionary learning  multi-task learning  zero-shot learning  person re-identification  transfer learning A number of vision problems such as zero-shot learning and person re-identification can be considered as cross-class transfer learning problems. As mid-level semantic properties shared cross different object classes, attributes have been studied extensively for knowledge transfer across classes. Most previous attribute learning methods focus only on human-defined/nameable semantic attributes, whilst ignoring the fact there also exist undefined/latent shareable visual properties, or latent attributes. These latent attributes can be either discriminative or non-discriminative parts depending on whether they can contribute to an object recognition task. In this work, we argue that learning the latent attributes jointly with user-defined semantic attributes not only leads to better representation but also helps semantic attribute prediction. A novel dictionary learning model is proposed which decomposes the dictionary space into three parts corresponding to semantic, latent discriminative and latent background attributes respectively. Such a joint attribute learning model is then extended by following a multi-task transfer learning framework to address a more challenging unsupervised domain adaptation problem, where annotations are only available on an auxiliary dataset and the target dataset is completely unlabelled. Extensive experiments show that the proposed models, though being linear and thus extremely efficient to compute, produce state-of-the-art results on both zero-shot learning and person re-identification.", "HEp-2 cell classification  Residual network  Deeply supervised ResNet  Cross-modal transfer learning Accurate Human Epithelial-2 (HEp-2) cell image classification plays an important role in the diagnosis of many autoimmune diseases and subsequent treatment. One of the key challenges is huge intra-class variations caused by inhomogeneous illumination. To address it, we propose a framework based on very deep supervised residual network (DSRN) to classify HEp-2 cell images. Specifically, we adopt a residual network of 50 layers (ResNet-50) that is substantially deep to extract rich and discriminative features. The deep supervision is imposed on the ResNet-based framework to further boost the classification performance by directly guiding the training of the lower and upper levels of the network. The proposed method is evaluated using two publicly available datasets (i.e., International Conference on Pattern Recognition (ICPR) 2012 and ICPR2016-Task1 cell classification contest datasets). Different from the previous deep learning models learned from scratch, a cross-modal transfer learning strategy is developed. Namely, we pretrain ICPR2012 dataset to fine-tune ICPR2016 dataset based on our DSRN model since both datasets are similar. Extensive experiments show that the proposed method delivers state-of-the-art performance and outperforms the traditional methods based on deep convolutional neural network (DCNN). ", "Content-based image retrieval  CBIR  Medical imaging  Deep learning  Radon Closing the semantic gap in medical image analysis is critical. Access to large-scale datasets might help to narrow the gap. However, large and balanced datasets may not always be a bailable. On the other side, retrieving similar images from an archive is a valuable task to facilitate better diagnosis. In this work, we concentrate on forming a search space, consisting of the most similar images for a given query, to be used for a similarity-based search technique in a retrieval system. We propose a two-step hierarchical shrinking search space when local binary patterns are used. Transfer learning via convolutional neural networks is utilized for the first stage of search space shrinking, followed by creating a selection pool using Radon transform for further reduction. The difference between two orthogonal Radon projections is considered in the selection pool to extract more information. The IRMA dataset, from ImageCLEF initiative, containing 14,400 X-ray images, is used to validate the proposed scheme. We report a total IRMA error of 168.05 (or 90.30% accuracy) which is the best result compared with existing methods in the literature for this dataset when real-time processing is considered. ", "Generative adversarial networks  Image-to-image translation  Semantic invariance Recently, thanks to the state-of-the-art techniques in Generative Adversarial Networks, a lot of work achieves remarkable performance on learning the mapping between an input image and an output image without any paired relation. However, traditional methods on image-to-image translation merely consider the visual appearance properties, they fail to maintain the true semantics of an image during the transfer learning procedure from source to target domain. We propose a new approach that utilizes GAN to translate unpaired images between domains and remain high level semantic abstraction aligned. Our model controls the hierarchical semantics of images by processing semantic information on label level and spatial level respectively by constructing label and attention consistent losses. The experimental results on several benchmark datasets show that generated samples are both visually similar with target images and semantically consistent with their source counterparts. Furthermore, the experiment also suggests that our method can effectively improve the classification performance in unsupervised domain adaptation problem. ", "Object recognition  Image datasets  Convolutional neural networks  Transfer learning  Multimodality  Human computer interaction MirBot is a collaborative application for smartphones that allows users to perform object recognition. This app can be used to take a photograph of an object, select the region of interest and obtain the most likely class (dog, chair, etc.) by means of similarity search using features extracted from a convolutional neural network (CNN). The answers provided by the system can be validated by the user so as to improve the results for future queries. All the images are stored together with a series of metadata, thus enabling a multimodal incremental dataset labeled with synset identifiers from the WordNet ontology. This dataset grows continuously thanks to the users' feedback, and is publicly available for research. This work details the MirBot object recognition system, analyzes the statistics gathered after more than four years of usage, describes the image classification methodology, and performs an exhaustive evaluation using handcrafted features, neural codes, different transfer learning techniques, PCA compression and metadata, which can be used to improve the image classifier results. The app is freely available at the Apple and Google Play stores. ", "Egocentric vision  Transfer learning  Action recognition In this work we address the task of relating action information across two drastically different visual domains, namely, first-person (egocentric) and third-person (exocentric). We investigate two different yet highly interconnected problems including cross-view action classification and action based video retrieval. First, we perform action classification in one domain using the knowledge transferred from the other domain. Second, given a video in one view, we retrieve videos from the same action class in the other view. In order to evaluate our models, we collect a new cross-domain dataset of egocentric-exocentric action videos containing 14 action classes and 3569 videos (1676 collected egocentric videos and 1893 exocentric videos borrowed from the UCF 101 dataset). Our results demonstrate the possibility of transferring action information across the two domains and suggest new directions in relating first and third person vision for other tasks.", "medical image processing  feature extraction  neural nets  support vector machines  image classification  image fusion  learning (artificial intelligence)  biomedical compound figure detection  deep learning model  compound figure detection  CFD  pre-trained convolutional neural networks  CNN  transfer learning  image classification  support vector machine classifier  score-based fusion Images contain significant amounts of information but present different challenges relative to textual information. One such challenge is compound figures or images made up of two or more subfigures. A deep learning model is proposed for compound figure detection (CFD) in the biomedical article domain. First, pre-trained convolutional neural networks (CNNs) are selected for transfer learning to take advantage of the image classification performance of CNNs and to overcome the limited dataset of the CFD problem. Next, the pre-trained CNNs are fine-tuned on the training data with early-stopping to avoid overfitting. Alternatively, layer activations of the pre-trained CNNs are extracted and used as input features to a support vector machine classifier. Finally, individual model outputs are combined with score-based fusion. The proposed combined model obtained a best test accuracy of 90.03 and 96.93% outperforming traditional hand-crafted and other deep learning representations on the ImageCLEF 2015 and 2016 CFD subtask datasets, respectively, by using AlexNet, VGG-16, VGG-19 pre-trained CNNs fine-tuned until best validation accuracy stops increasing combined with the combPROD score-based fusion operator.", "Medical images  Leukemia diagnosis  Convolutional neural networks  Transfer learning Leukemia is a pathology that affects young people and adults, causing premature death and several other symptoms. Computer-aided systems can be used to reduce the possibility of prescribing inappropriate treatments and assist specialists in the diagnosis of this disease. There is a growing use of Convolutional Neural Networks (CNNs) in the classification and diagnosis of medical image problems. However, the training of CNNs requires a large set of images. To overcome this problem, we use transfer learning to extract images features for further classification. We tested three state-of-the-art CNN architectures and the features were selected according to their gain ratios and used as input to the Support Vector Machine classifier. The proposed methodology aims to correctly classify images with different characteristics derived from different image databases and does not require a segmentation process. We built a new database from the union of three distinct databases presented in the literature to validate the proposed methodology. The proposed methodology achieved hit rates above 99% and outperformed nine methods found in the literature.", "learning (artificial intelligence)  pattern classification  negative back-dropout transfer learning  action recognition  dataset annotation  vision tasks  classification performance  category bias  NB-TL  UCF 101 dataset Transfer learning aims at adapting a model learned from source dataset to target dataset. It is a beneficial approach especially when annotating on the target dataset is expensive or infeasible. Transfer learning has demonstrated its powerful learning capabilities in various vision tasks. Despite transfer learning being a promising approach, it is still an open question how to adapt the model learned from the source dataset to the target dataset. One big challenge is to prevent the impact of category bias on classification performance. Dataset bias exists when two images from the same category, but from different datasets, are not classified as the same. To address this problem, a transfer learning algorithm has been proposed, called negative back-dropout transfer learning (NB-TL), which utilizes images that have been misclassified and further performs back-dropout strategy on them to penalize errors. Experimental results demonstrate the effectiveness of the proposed algorithm. In particular, the authors evaluate the performance of the proposed NB-TL algorithm on UCF 101 action recognition dataset, achieving 88.9% recognition rate.", "Actor learning  Atari2600 game  double deep Q network (DQN)  multisource transfer Deep reinforcement learning (RL) comprehensively uses the psychological mechanisms of trial and error and reward and punishment in RL as well as powerful feature expression and nonlinear mapping in deep learning. Currently, it plays an essential role in the fields of artificial intelligence and machine learning. Since an RL agent needs to constantly interact with its surroundings, the deep Q network (DQN) is inevitably faced with the need to learn numerous network parameters, which results in low learning efficiency. In this paper, a multisource transfer double DQN (MTDDQN) based on actor learning is proposed. The transfer learning technique is integrated with deep RL to make the RL agent collect, summarize, and transfer action knowledge, including policy mimic and feature regression, to the training of related tasks. There exists action overestimation in DQN, i.e., the lower probability limit of action corresponding to the maximum Q value is nonzero. Therefore, the transfer network is trained by using double DQN to eliminate the error accumulation caused by action overestimation. In addition, to avoid negative transfer, i.e., to ensure strong correlations between source and target tasks, a multisource transfer learning mechanism is applied. The Atari2600 game is tested on the arcade learning environment platform to evaluate the feasibility and performance of MTDDQN by comparing it with some mainstream approaches, such as DQN and double DQN. Experiments prove that MTDDQN achieves not only human-like actor learning transfer capability, but also the desired learning efficiency and testing accuracy on target task.", "Egocentric  navigation  network routing  reinforcement learning (RL)  transfer learning The reinforcement learning (RL) paradigm allows agents to solve tasks through trial-and-error learning. To be capable of efficient, long-term learning, RL agents should be able to apply knowledge gained in the past to new tasks they may encounter in the future. The ability to predict actions' consequences may facilitate such knowledge transfer. We consider here domains where an RL agent has access to two kinds of information: agent-centric information with constant semantics across tasks, and environment-centric information, which is necessary to solve the task, but with semantics that differ between tasks. For example, in robot navigation, environment-centric information may include the robot's geographic location, while agent-centric information may include sensor readings of various nearby obstacles. We propose that these situations provide an opportunity for a very natural style of knowledge transfer, in which the agent learns to predict actions' environmental consequences using agent-centric information. These predictions contain important information about the affordances and dangers present in a novel environment, and can effectively transfer knowledge from agent-centric to environment-centric learning systems. Using several example problems including spatial navigation and network routing, we show that our knowledge transfer approach can allow faster and lower cost learning than existing alternatives.", "Data-sensitive granularity  hidden Markov model (HMM)  relative entropy (RE)  sequence transfer learning  substructural regularization Sequence transfer learning is of interest in both academia and industry with the emergence of numerous new text domains from Twitter and other social media tools. In this paper, we put forward the data-sensitive granularity for transfer learning, and then, a novel substructural regularization transfer learning model (STLM) is proposed to preserve target domain features at substructural granularity in the light of the condition of labeled data set size. Our model is underpinned by hidden Markov model and regularization theory, where the substructural representation can be integrated as a penalty after measuring the dissimilarity of substructures between target domain and STLM with relative entropy. STLM can achieve the competing goals of preserving the target domain substructure and utilizing the observations from both the target and source domains simultaneously. The estimation of STLM is very efficient since an analytical solution can be derived as a necessary and sufficient condition. The relative usability of substructures to act as regularization parameters and the time complexity of STLM are also analyzed and discussed. Comprehensive experiments of part-of-speech tagging with both Brown and Twitter corpora fully justify that our model can make improvements on all the combinations of source and target domains.", "Automated planning  case-base reasoning  robotics  control systems  planning and execution Many robotic control architectures perform a continuous cycle of sensing, reasoning and acting, where that reasoning can be carried out in a reactive or deliberative form. Reactive methods are fast and provide the robot with high interaction and response capabilities. Deliberative reasoning is particularly suitable in robotic systems because it employs some form of forward projection (reasoning in depth about goals, pre-conditions, resources and timing constraints) and provides the robot reasonable responses in situations unforeseen by the designer. However, this reasoning, typically conducted using Artificial Intelligence techniques like Automated Planning (AP), is not effective for controlling autonomous agents which operate in complex and dynamic environments. Deliberative planning, although feasible in stable situations, takes too long in unexpected or changing situations which require re-planning. Therefore, planning cannot be done on-line in many complex robotic problems, where quick responses are frequently required. In this paper, we propose an alternative approach based on case-based policy learning which integrates deliberative reasoning through AP and reactive response time through reactive planning policies. The method is based on learning planning knowledge from actual experiences to obtain a case-based policy. The contribution of this paper is two fold. First, it is shown that the learned case-based policy produces reasonable and timely responses in complex environments. Second, it is also shown how one case-based policy that solves a particular problem can be reused to solve a similar but more complex problem in a transfer learning scope.", "Transfer learning  Recurrent neural network  LSTM network  Relation classification A lack of sufficient labeled data often limits the applicability of advanced machine learning algorithms to real life problems. However, the efficient use of transfer learning (TL) has been shown to be very useful across domains. TL make use of valuable knowledge learned in one task (source task), where sufficient data is available, in order to improve performance on the task of interest (target task). In the biomedical and clinical domain, a lack of sufficient training data means that machine learning models cannot be fully exploited. In this work, we present two unified recurrent neural models leading to three transfer learning frameworks for relation classification tasks. We systematically investigate the effectiveness of the proposed frameworks in transferring knowledge from a source task to a target task when the characteristics of the source data vary, such as similarity or relatedness between the source and target tasks, and the size of training data for the source task. Our empirical results show that the proposed frameworks, in general, improve the model performance. However, these improvements do depend on characteristics of source and target tasks. This dependence then finally determine the choice of a particular TL framework. ", "Cross-company defect prediction  Transfer learning  SSDBSCAN  Multi-source TrAdaBoost Cross-company defect prediction (CCDP) is a practical way that trains a prediction model by exploiting one or multiple projects of a source company and then applies the model to a target company. Unfortunately, larger irrelevant cross-company (CC) data usually make it difficult to build a prediction model with high performance. On the other hand, brute force leveraging of CC data poorly related to within-company data may decrease the prediction model performance. To address such issues, we aim to provide an effective solution for CCDP. First, we propose a novel semi-supervised clustering-based data filtering method (i.e., SSDBSCAN filter) to filter out irrelevant CC data. Second, based on the filtered CC data, we for the first time introduce multi-source TrAdaBoost algorithm, an effective transfer learning method, into CCDP to import knowledge not from one but from multiple sources to avoid negative transfer. Experiments on 15 public datasets indicate that: (1) our proposed SSDBSCAN filter achieves better overall performance than compared data filtering methods  (2) our proposed CCDP approach achieves the best overall performance among all tested CCDP approaches  and (3) our proposed CCDP approach performs significantly better than with-company defect prediction models.", "Low rank  multiview data  transfer learning Multiview data are of great abundance in real-world applications, since various viewpoints and multiple sensors desire to represent the data in a better way. Conventional multiview learning methods aimed to learn multiple view-specific transformations meanwhile assumed the view knowledge of training, and test data were available in advance. However, they would fail when we do not have any prior knowledge for the probe data's view information, since the correct view-specific projections cannot be utilized to extract effective feature representations. In this paper, we develop a collective low-rank subspace (CLRS) algorithm to deal with this problem in multiview data analysis. CLRS attempts to reduce the semantic gap across multiple views through seeking a view-free low-rank projection shared by multiple view-specific transformations. Moreover, we exploit low-rank reconstruction to build a bridge between the view-specific features and those view-free ones transformed with the CLRS. Furthermore, a supervised cross-view regularizer is developed to couple the within-class data across different views to make the learned collective subspace more discriminative. Our CLRS makes our algorithm more flexible when addressing the challenging issue without any prior knowledge of the probe data's view information. To that end, two different settings of experiments on several multiview benchmarks are designed to evaluate the proposed approach. Experimental results have verified the effective performance of our proposed method by comparing with the state-of-the-art algorithms.", "neural nets  ear  feature extraction  learning (artificial intelligence)  image classification  shallow classifier  deep learning-based averaging ensemble  DNNs  feature extractor  transfer learning  deep neural networks  combined AWE plus CVLE dataset  unconstrained ear recognition datasets  feature-extraction models  training dataset The authors perform unconstrained ear recognition using transfer learning with deep neural networks (DNNs). First, they show how existing DNNs can be used as a feature extractor. The extracted features are used by a shallow classifier to perform ear recognition. Performance can be improved by augmenting the training dataset with small image transformations. Next, they compare the performance of the feature-extraction models with fine-tuned networks. However, because the datasets are limited in size, a fine-tuned network tends to over-fit. They propose a deep learning-based averaging ensemble to reduce the effect of over-fitting. Performance results are provided on unconstrained ear recognition datasets, the AWE and CVLE datasets as well as a combined AWE + CVLE dataset. They show that their ensemble results in the best recognition performance on these datasets as compared to DNN feature-extraction based models and single fine-tuned models.", "Transfer learning  sharable information  convolutional sparse coding  deep learning  biomedical application  brain tumors  low dose ionizing radiation (LDIR)  mouse model  breast cancer subtypes The capabilities of (I) learning transferable knowledge across domains  and (II) fine-tuning the pre-learned base knowledge towards tasks with considerably smaller data scale are extremely important. Many of the existing transfer learning techniques are supervised approaches, among which deep learning has the demonstrated power of learning domain transferrable knowledge with large scale network trained on massive amounts of labeled data. However, in many biomedical tasks, both the data and the corresponding label can be very limited, where the unsupervised transfer learning capability is urgently needed. In this paper, we proposed a novel multi-scale convolutional sparse coding (MSCSC) method, that (I) automatically learns filter banks at different scales in a joint fashion with enforced scale-specificity of learned patterns  and (II) provides an unsupervised solution for learning transferable base knowledge and fine-tuning it towards target tasks. Extensive experimental evaluation of MSCSC demonstrates the effectiveness of the proposed MSCSC in both regular and transfer learning tasks in various biomedical domains.", "Transfer learning  Sparse coding  Image representation Transfer learning can transfer knowledge from a source domain to a target domain, promoting the performance of the model learned from the source data. Sparse coding can make the representation of a model more succinct and easy to manipulate. Existing transfer sparse coding methods assume the data from the source and the target domains are accurate, which can provide useful information. However, in many real applications, the data in the source and target domains may contain noise and useless information, which could severely degrade the performance of the learned model. In this paper, we propose a transfer robust sparse coding based on graph and joint distribution adaption for image representation. The noise matrix model is utilized to handle noise and useless information in the transfer sparse coding. Moreover, the differences of marginal distribution and conditional distribution are simultaneously reduced in the transfer robust sparse coding. Extensive experiments on six benchmark datasets show the proposed method can effectively deal with the noise and useless information and therefore outperforms several state-of-the-art transfer learning methods on cross-distribution domains. (c) 2018Elsevier B.V. All rights reserved.", "Cross-database micro-expression recognition  micro-expression recognition  domain adaptation  transfer learning Recently, micro-expression recognition has attracted lots of researchers' attention due to its potential value in many practical applications, e.g., lie detection. In this paper, we investigate an interesting and challenging problem in micro-expression recognition, i.e., cross-database micro-expression recognition, in which the training and testing samples come from different micro-expression databases. Under this problem setting, the consistent feature distribution between the training and testing samples originally existing in conventional micro-expression recognition would be seriously broken, and hence, the performance of most current well-performing micro-expression recognition methods may sharply drop. In order to overcome it, we propose a simple yet effective framework called domain regeneration (DR) in this paper. The DR framework aims at learning a domain regenerator to regenerate the micro-expression samples from source and target databases, respectively, such that they can abide by the same or similar feature distributions. Thus, we are able to use the classifier learned based on the labeled source micro-expression samples to predict the label information of the unlabeled target micro-expression samples. To evaluate the proposed DR framework, we conduct extensive cross-database micro-expression recognition experiments designed based on the Spontaneous Micro-Expression Database and Chinese Academy of Sciences Micro-Expression II Database. Experimental results show that compared with the recent state-of-the-art cross-database emotion recognition methods, the proposed DR framework has more promising performance.", "Manifold  Transfer learning  Alternating direction method of multipliers  Object tracking  Augmented lagrange multiplier  Image tracking  Image recognition  Object categorization In this paper, we leverage the manifold structure of visual data in order to improve performance in general optimization problems subject to linear constraints. As the main theoretical result, we show that manifold constraints can be transferred from the data to the optimized variables if these are linearly correlated. We also show that the resulting optimization problem can be solved with an efficient alternating direction method of multipliers that can consistently integrate the manifold constraints during the optimization process. This leads to a simplification of the approach, which instead of directly optimizing on the manifold, we can iteratively recast the problem as the projection over the manifold via an embedding method. The proposed method is extremely versatile since it can be applied to different problems including Kernel Ridge Regression (KRR) and sparse coding which have numerous applications in machine learning and computer vision. In particular, we apply the methods to different problems such as tracking, object recognition and categorization showing a consistent increase of performance with respect to the state of the art. (C) 2017 Elsevier Ltd. All rights reserved.", "Video emotion recognition  transfer learning  zero-shot learning  summarization Emotion is a key element in user-generated video. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames expressing emotion. In this paper, for the first time, we propose a technique for transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in understanding video emotion: emotion recognition, emotion attribution and emotionoriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpora for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework.", "Fuzzy rules  granular computing (GrC)  machine learning  regression  transfer learning In classical data-driven machine learning methods, massive amounts of labeled data are required to build a high-performance prediction model. However, the amount of labeled data in many real-world applications is insufficient, so establishing a prediction model is impossible. Transfer learning has recently emerged as a solution to this problem. It exploits the knowledge accumulated in auxiliary domains to help construct prediction models in a target domain with inadequate training data. Most existing transfer learning methods solve classification tasks  only a few are devoted to regression problems. In addition, the current methods ignore the inherent phenomenon of information granularity in transfer learning. In this study, granular computing techniques are applied to transfer learning. Three granular fuzzy regression domain adaptation methods to determine the estimated values for a regression target are proposed to address three challenging cases in domain adaptation. The proposed granular fuzzy regression domain adaptation methods change the input and/or output space of the source domain's model using space transformation, so that the fuzzy rules are more compatible with the target data. Experiments on synthetic and real-world datasets validate the effectiveness of the proposed methods.", "Wind power prediction  Transfer learning  Cluster based data  Distribution  Domain adaptation Historical wind power production figures are not available when a new wind farm goes into power production. It is thus difficult to forecast power productions of such wind farms that is required for demand management. Wind power is a function of weather variables and it is likely that weather patterns of the new station is similar to some existing operational wind farms. It will thus be interesting to investigate how the forecast/prediction models of the existing wind farms can be adapted to generate a prediction model for new stations. On this regard, we explore a particular branch of machine learning called Multi Source Domain Adaptation (MSDA). MSDA approaches identify a weighing mechanism to fuse the predictions from the source models (i.e. existing stations) to produce a prediction for the target (i.e. new station). The weights are computed based on similarity of data distributions between source and target. Conventional MSDA approaches utilise an instance based weighting scheme and we identified that fails to capture the data distribution of wind data sets appropriately. We thus propose a novel cluster based MSDA approach that captures wind data distribution in terms of natural groups that exist within data and compute distribution similarity (and source weight) in terms of cluster distributions. Experimental results demonstrate that cluster based MSDA approach can reduce regression error by 20.63% over instance based MSDA approach. ", "Semi-supervised discriminant analysis  Transfer learning  Cross-domain mean constraint  Maximum mean discrepancy In this paper, a novel semi-supervised feature extraction algorithm, i.e., semi-supervised transfer discriminant analysis (STDA) with knowledge transfer capability is proposed, based on the traditional algorithm that cannot get adapted in the change of the learning environment. By using both the pseudo label information from target domain samples and the actual label information from source domain samples in the label iterative refinement process, not only the between-class scatter is maximized while that within-class scatter is minimized, but also the original space structure is maintained via Laplacian matrix, and the distribution difference is reduced by using maximum mean discrepancy as well. Moreover, semi-supervised transfer discriminant analysis based on cross-domain mean constraint (STDA-CMC) is proposed. In this algorithm, the cross-domain mean constraint term is incorporated into STDA, such that knowledge transfer between domains is facilitated by making source and target samples after being projected are located more closely in the low-dimensional feature subspace. The proposed algorithm is proved efficient and feasible from experiments on several datasets.", "Event recognition  Deep learning  Transfer learning  Multitask learning This paper addresses the problem of image-based event recognition by transferring deep representations learned from object and scene datasets. First we empirically investigate the correlation of the concepts of object, scene, and event, thus motivating our representation transfer methods. Based on this empirical study, we propose an iterative selection method to identify a subset of object and scene classes deemed most relevant for representation transfer. Afterwards, we develop three transfer techniques: (1) initialization-based transfer, (2) knowledge-based transfer, and (3) data-based transfer. These newly designed transfer techniques exploit multitask learning frameworks to incorporate extra knowledge from other networks or additional datasets into the fine-tuning procedure of event CNNs. These multitask learning frameworks turn out to be effective in reducing the effect of over-fitting and improving the generalization ability of the learned CNNs. We perform experiments on four event recognition benchmarks: the ChaLearn LAP Cultural Event Recognition dataset, the Web Image Dataset for Event Recognition, the UIUC Sports Event dataset, and the Photo Event Collection dataset. The experimental results show that our proposed algorithm successfully transfers object and scene representations towards the event dataset and achieves the current state-of-the-art performance on all considered datasets.", "Feature transferring  Data augmentation  Convolutional neural network  Feature representation  Parameter fine-tuning  Bayesian optimization Since Convolutional Neural Network (CNN) won the image classification competition 202 (ILSVRC12), a lot of attention has been paid to deep layer CNN study. The success of CNN is attributed to its superior multi-scale high-level image representations as opposed to hand-engineering low-level features. However, estimating millions of parameters of a deep CNN requires a large number of annotated samples, which currently prevents many superior deep CNNs (such as AlexNet, VGG, ResNet) being applied to problems with limited training data. To address this problem, a novel two-phase method combining CNN transfer learning and web data augmentation is proposed. With our method, the useful feature presentation of pre-trained network can be efficiently transferred to target task, and the original dataset can be augmented with the most valuable Internet images for classification. Our method not only greatly reduces the requirement of a large training data, but also effectively expand the training dataset. Both of method features contribute to the considerable over-fitting reduction of deep CNNs on small dataset. In addition, we successfully apply Bayesian optimization to solve the tuff problem, hyper-parameter tuning, in network fine-tuning. Our solution is applied to six public small datasets. Extensive experiments show that, comparing to traditional methods, our solution can assist the popular deep CNNs to achieve better performance. Particularly, ResNet can outperform all the state-of-the-art models on six small datasets. The experiment results prove that the proposed solution will be the great tool for dealing with practice problems which are related to use deep CNNs on small dataset. (C) 2017 Elsevier Ltd. All rights reserved.", "Computer vision  Convolutional neural networks (CNNs)  Zernike moments  Geodesic correction  Omni-directional image  Fisheye camera calibration  Pose classification  Transfer learning Convolutional neural networks (CNNs) are used frequently in several computer vision applications. In this work, we present a methodology for pose classification of binary human silhouettes using CNNs, enhanced with image features based on Zernike moments, which are modified for fisheye images. The training set consists of synthetic images that are generated from three-dimensional (3D) human models, using the calibration model of an omni-directional camera (fisheye). Testing is performed using real images, also acquired by omni-directional cameras. Here, we employ our previously proposed geodesically corrected Zernike moments (GZMI) and confirm their merit as stand-alone descriptors of calibrated fisheye images. Subsequently, we explore the efficiency of transfer learning from the previously trained model with synthetically generated silhouettes, to the problem of real pose classification, by continuing the training of the already trained network, using a few frames of annotated real silhouettes. Furthermore, we propose an enhanced architecture that combines the calculated GZMI features of each image with the features generated at CNNs' last convolutional layer, both feeding the first hidden layer of the traditional neural network that exists at the end of the CNN. Testing is performed using synthetically generated silhouettes as well as real ones. Results show that the proposed enhancement of CNN architecture, combined with transfer learning improves pose classification accuracy for both the synthetic and the real silhouette images. ", " Recent academic studies have demonstrated the possibility of inferring user actions performed in mobile apps by analyzing the resulting encrypted network traffic. Due to the multitude of app versions, mobile operating systems, and device models (collectively referred to in this paper as configurations), previous approaches are not applicable to real life settings. In this work, we extend the ability of these approaches to generalize across different configurations. We treat the different configurations as a case for transfer learning and adapt the co-training method to support the transfer learning process. Our approach leverages a small number of labeled instances of encrypted traffic from a source configuration, in order to construct a classifier capable of identifying a user's actions in a different (target) configuration which is completely unlabeled. Experiments on real datasets collected from different applications on Android devices show that the proposed method achieves F1 measures over 0.8 for most of the considered user actions.", "Generation of representation during development  robots with development and learning skills  transfer learning Reinforcement learning (RL) problems are hard to solve in a robotics context as classical algorithms rely on discrete representations of actions and states, but in robotics both are continuous. A discrete set of actions and states can be defined, but it requires an expertise that may not be available, in particular in open environments. It is proposed to define a process to make a robot build its own representation for an RL algorithm. The principle is to first use a direct policy search in the sensorimotor space, i.e., with no predefined discrete sets of states nor actions, and then extract from the corresponding learning traces discrete actions and identify the relevant dimensions of the state to estimate the value function. Once this is done, the robot can apply RL: 1) to be more robust to new domains and, if required and 2) to learn faster than a direct policy search. This approach allows to take the best of both worlds: first learning in a continuous space to avoid the need of a specific representation, but at a price of a long learning process and a poor generalization, and then learning with an adapted representation to be faster and more robust.", "Transfer learning  Subspace learning  Distribution matching  Weighted mean  Travel destination review Transfer learning is a problem defined over two domains. These two domains share the same feature space and class label space, but have significantly different distributions. One domain has sufficient labels, named as source domain, and the other domain has few labels, named as target domain. The problem is to learn an effective classifier for the target domain. In this paper, we propose a novel transfer learning method for this problem by learning a partially shared classifier for the target domain, and weighting the source domain data points. We learn some shared subspaces for both the data points of the two domains, and a shared classifier in the shared subspaces. We hope that in the shared subspaces, the distributions of two domain can match each other well, and to match the distributions, we weight the source domain data points with different weighting factors. Moreover, we adapt the shared classifier to each domain by learning different adaptation functions. To learn the subspace transformation matrices, the classifier parameters, and the adaptation parameters, we build an objective function with weighted classification errors, parameter regularization, local reconstruction regularization, and distribution matching. This objective function is minimized by an iterative algorithm. Experiments show its effectiveness over benchmark data sets, including travel destination review data set, face expression data set, spam email data set.", "Domain adaptation  Remote sensing  Artificial neural networks  Transfer learning  Auto-encoding In this article, a domain adaptation (DA) technique using artificial neural networks based classifiers has been proposed using two-level cluster mapping technique by integrating the common data transformation and transfer learning approaches in a single framework. Here, after applying self-organizing feature mapping based clustering technique, a semi-automatic threshold selection mechanism is used to separate out most-confidently paired source-target clusters (i.e. the most similar ones) and alien target clusters (i.e. non-similar ones). Moreover, this strategy makes the proposed technique eligible to apply the transfer learning mechanism. Thereafter, the samples from the most confidently paired target clusters are transformed in terms of the corresponding source clusters using an auto-encoder. Here, the labelled samples are collected from the corresponding source clusters for the paired target clusters  whereas the transfer learning technique is used to select labelled samples from the alien target clusters. To assess the effectiveness of the proposed DA approach, experiments are conducted on the three source-target datasets and the results are compared with other state-of-the-art techniques. Results are also found to be encouraging for the proposed technique. ", "Data clustering  image classification  low-rank coding  self-taught learning (STL)  transfer learning The lack of labeled data presents a common challenge in many computer vision and machine learning tasks. Semisupervised learning and transfer learning methods have been developed to tackle this challenge by utilizing auxiliary samples from the same domain or from a different domain, respectively. Self-taught learning, which is a special type of transfer learning, has fewer restrictions on the choice of auxiliary data. It has shown promising performance in visual learning. However, existing self-taught learning methods usually ignore the structure information in data. In this paper, we focus on building a self-taught coding framework, which can effectively utilize the rich low-level pattern information abstracted from the auxiliary domain, in order to characterize the high-level structural information in the target domain. By leveraging a high quality dictionary learned across auxiliary and target domains, the proposed approach learns expressive codings for the samples in the target domain. Since many types of visual data have been proven to contain subspace structures, a low-rank constraint is introduced into the coding objective to better characterize the structure of the given target set. The proposed representation learning framework is called self-taught low-rank (S-Low) coding, which can be formulated as a nonconvex rank-minimization and dictionary learning problem. We devise an efficient majorization-minimization augmented Lagrange multiplier algorithm to solve it. Based on the proposed S-Low coding mechanism, both unsupervised and supervised visual learning algorithms are derived. Extensive experiments on five benchmark data sets demonstrate the effectiveness of our approach.", "Multi-object tracking  tracking-by-detection  tracklet confidence  confidence-based data association  deep appearance learning  online transfer learning  surveillance system Online multi-object tracking aims at estimating the tracks of multiple objects instantly with each incoming frame and the information provided up to the moment. It still remains a difficult problem in complex scenes, because of the large ambiguity in associating multiple objects in consecutive frames and the low discriminability between objects appearances. In this paper, we propose a robust online multi-object tracking method that can handle these difficulties effectively. We first define the tracklet confidence using the detectability and continuity of a tracklet, and decompose a multi-object tracking problem into small subproblems based on the tracklet confidence. We then solve the online multi-object tracking problem by associating tracklets and detections in different ways according to their confidence values. Based on this strategy, tracklets sequentially grow with online-provided detections, and fragmented tracklets are linked up with others without any iterative and expensive association steps. For more reliable association between tracklets and detections, we also propose a deep appearance learning method to learn a discriminative appearance model from large training datasets, since the conventional appearance learning methods do not provide rich representation that can distinguish multiple objects with large appearance variations. In addition, we combine online transfer learning for improving appearance discriminability by adapting the pre-trained deep model during online tracking. Experiments with challenging public datasets show distinct performance improvement over other state-of-the-arts batch and online tracking methods, and prove the effect and usefulness of the proposed methods for online multi-object tracking.", "Similarity function learning  Local metric learning  Nearest neighbors classification  Face verification We study the problem of learning a similarity function from a set of binary labeled data pairs. A common approach is to learn a similarly function which is a bilinear form associated to the pair of data points. We argue that this class may be too restrictive when handling heterogeneous datasets. To overcome this limitation local metric learning techniques have been advocated in the literature. However, they are subject to certain constraints preventing their usage in many applications. For example, they require knowledge of the class label of the training points. In this paper, we present a local metric learning method, which overcomes these limitations. The method first initializes a Gaussian mixture model on the training data. Then it estimates a set of local metrics and simultaneously refines the mixture's parameters. Finally, a similarity function is obtained by aggregating the local metrics. We also introduce a novel regularization term, which works well in a transfer learning setting. Our experiments show that the proposed method achieves state-of-the-art results on several real datasets. (C) 2017 Elsevier Ltd. All rights reserved.", "Gaze estimation  implicit modeling  data validation  gaze-interaction alignment  gaze transfer learning Understanding human visual attention is essential for understanding human cognition, which in turn benefits human-computer interaction. Recent work has demonstrated a Personalized, Auto-Calibrating Eye-tracking (PACE) system, which makes it possible to achieve accurate gaze estimation using only an off-the-shelf webcam by identifying and collecting data implicitly from user interaction events. However, this method is constrained by the need for large amounts of well-annotated data. We thus present fast-PACE, an adaptation to PACE that exploits knowledge from existing data from different users to accelerate the learning speed of the personalized model. The result is an adaptive, data-driven approach that continuously learns its user and recalibrates, adapts, and improves with additional usage by a user. Experimental evaluations of fast-PACE demonstrate its competitive accuracy in iris localization, validity of alignment identification between gaze and interactions, and effectiveness of gaze transfer. In general, fast-PACE achieves an initial visual error of 3.98 degrees and then steadily improves to 2.52 degrees given incremental interaction-informed data. Our performance is comparable to state-of-the-art, but without the need for explicit training or calibration. Our technique addresses the data quality and quantity problems. It therefore has the potential to enable comprehensive gaze-aware applications in the wild.", "Cross domain collaborative filtering  Transfer learning  Bi-orthogonal tri-factorization  Classification problem Recently, Cross Domain Collaborate Filtering (CDCF) is a new way to alleviate the sparsity problem in the recommender systems. CDCF solves the sparsity problem by transferring rating knowledge from auxiliary domains. Most of previous work only uses one-side (user-side or item-side) auxiliary domain information to help the recommendation in the target domain. In this paper, we propose a two-side Cross Domain Collaborate Filtering model. We assume that there exist two auxiliary domains, i.e., user-side domain and item-side domain, where the user-side auxiliary domain shares the same aligned users with the target domain, and the item-side shares the same aligned items. Also both the two auxiliary domains contain dense rating data. In this scenario, we first employ the bi-orthogonal tri-factorization model to infer the intrinsic user and item features from the user-side and item-side auxiliary domain respectively. The inferred intrinsic features are independent on domains. Then we convert the recommendation problem into a classification problem. In detail, we use the inferred user and item features to compose the feature vector, and use the corresponding rating as the class label. Thus the user-item interactions can be represented as training samples. Finally, we employ SVMs model to solve the converted classification problem. The major advantage of our model is that it can make use of both user-side and item-side shared information. Furthermore, it can infer the domain independent user and item features. Thus it can transfer' knowledge from auxiliary domains more effectively. We conduct extensive experiments to show that the proposed model performs significantly better than many state-of-the-art single domain and cross domain CF methods. ", "Data Mining  Transfer Learning Multiple instance learning (MIL) is a generalization of supervised learning which attempts to learn a distinctive classifier from bags of instances. This paper addresses the problem of the transfer learning-based multiple instance method for text categorization problem. To provide a safe transfer of knowledge from a source task to a target task, this paper proposes a new approach, called selective multiple instance transfer learning (SMITL), which selects the case that the multiple instance transfer learning will work in step one, and then builds a multiple instance transfer learning classifier in step two. Specifically, in the first step, we measure whether the source task and the target task are related or not by investigating the similarity of the positive features of both tasks. In the second step, we construct a transfer learning-based multiple instance method to transfer knowledge from a source task to a target task if both tasks are found to be related in the first step. Our proposed approach explicitly addresses the problem of safe transfer of knowledge for Multiple instance learning on the text classification problem. Extensive experiments have shown that SMITL can determine whether the two tasks are related for most data sets, and outperforms classic multiple instance learning methods. ", "Transfer learning  Negative transfer  Class noise detection Transfer learning method has been widely used in machine learning when training data is limited. However, class noise accumulated during learning iterations can lead to negative transfer which can adversely affect performance when more training data is used. In this paper, we propose a novel method to identify noise samples for noise reduction. More importantly, the method can detect the point where negative transfer happens such that transfer learning can terminate at the near top performance point. In this method, we use the sum of the Rademacher distribution to estimate the class noise rate of transferred data. Transferred data having high probability of being labeled wrongly is removed to reduce noise accumulation. This negative sample reduction process can be repeated several times during transfer learning until we find the point where negative transfer occurs. As we can detect the point where negative transfer occurs, our method not only has the ability to delay the point where negative transfer happens, but also the ability to stop transfer learning algorithms at the right place for top performance gain. Evaluation based on cross-lingual/domain opinion analysis evaluation data set shows that our algorithm achieves the state-of-the-art result. Furthermore, our system shows a monotonic increase trend in performance improvement when more training data are used beating the performance degradation curse of most transfer learning methods when training data reaches certain size.", "Cross domain/source  incomplete multisource  transfer learning Transfer learning is generally exploited to adapt well-established source knowledge for learning tasks in weakly labeled or unlabeled target domain. Nowadays, it is common to see multiple sources available for knowledge transfer, each of which, however, may not include complete classes information of the target domain. Naively merging multiple sources together would lead to inferior results due to the large divergence among multiple sources. In this paper, we attempt to utilize incomplete multiple sources for effective knowledge transfer to facilitate the learning task in target domain. To this end, we propose an incomplete multisource transfer learning through two directional knowledge transfer, i.e., cross-domain transfer from each source to target, and cross-source transfer. In particular, in cross-domain direction, we deploy latent low-rank transfer learning guided by iterative structure learning to transfer knowledge from each single source to target domain. This practice reinforces to compensate for any missing data in each source by the complete target data. While in cross-source direction, unsupervised manifold regularizer and effective multisource alignment are explored to jointly compensate for missing data from one portion of source to another. In this way, both marginal and conditional distribution discrepancy in two directions would be mitigated. Experimental results on standard cross-domain benchmarks and synthetic data sets demonstrate the effectiveness of our proposed model in knowledge transfer from incomplete multiple sources.", "Natural language structuring  WordNet  WordNet2Vec  Vectorization  Network transformation  Sentiment analysis  Transfer learning  Big data  Complex networks The complex nature of big data resources requires new structuring methods, especially for textual content. WordNet is a good knowledge source for the comprehensive abstraction of natural language as it offers good implementation for many languages. Since WordNet embeds natural language in the form of a complex network, a transformation mechanism, WordNet2Vec, is proposed in this paper. This creates vectors for each word from WordNet. These vectors encapsulate a general position - the role of a given word related to all other words in the given natural language. Any list or set of such vectors contains knowledge about the context of its components within the whole language. This type of word representation can be easily applied to many analytic tasks such as classification or clustering. The usefulness of the WordNet2Vec method is demonstrated in sentiment analysis including the classification of an Amazon opinion text dataset with transfer learning. ", "Domain adaptation  Subspace learning  Transfer learning This paper presents a novel unsupervised multi-source domain adaptation approach, named as coupled local-global adaptation (CLGA). At the global level, in order to maximize the adaptation ability, CLGA regards multiple domains as a unity, and jointly mitigates the gaps of both marginal and conditional distributions between source and target dataset. At the local level, with the intention of maximizing the discriminative ability, CLGA investigates the relationship among distinctive domains, and exploits both class and domain manifold structures embedded in data samples. We formulate both local and global adaptation in a concise optimization problem, and further derive an analytic solution for the objective function. Extensive evaluations verify that CLGA performs better than several existing methods not only in multi-source adaptation tasks but also in single source scenarios. ", "Deep learning  Transfer learning  Computer vision  Object detection  Underwater video analysis In recent years, underwater video technologies allow us to explore the ocean in scientific and noninvasive ways, such as environmental monitoring, marine ecology studies, and fisheries management. However the low-light and high-noise scenarios pose great challenges for the underwater image and video analysis. We here propose a CNN knowledge transfer framework for underwater object recognition and tackle the problem of extracting discriminative features from relatively low contrast images. Even with the insufficient training set, the transfer framework can well learn a recognition model for the special underwater object recognition task together with the help of data augmentation. For better identifying objects from an underwater video, a weighted probabilities decision mechanism is introduced to identify the object from a series of frames. The proposed framework can be implemented for real-time underwater object recognition on autonomous underwater vehicles and video monitoring systems. To verify the effectiveness of our method, experiments on a public dataset are carried out. The results show that the proposed method achieves promising results for underwater object recognition on both test image datasets and underwater videos. ", "Textual sentiment analysis  Word embedding  Lexicon embedding  Attention mechanism  Cross-modality consistent regression Word embeddings and CNN (convolutional neural networks) architecture are crucial ingredients of sentiment analysis. However, sentiment and lexicon embeddings are rarely used and CNN is incompetent to capture global features of sentence. To this end, semantic embeddings, sentiment embeddings and lexicon embeddings are applied for texts encoding, and three different attentions including attention vector, LSTM (long short term memory) attention and attentive pooling are integrated with CNN model in this paper. Additionally, a word and its context are explored to disambiguate the meaning of the word for rich input representation. To improve the performance of three different attention CNN models, CCR (cross-modality consistent regression) and transfer learning are presented. It is worth noticing that CCR and transfer learning are used in textual sentiment analysis for the first time. Finally, some experiments on two different datasets demonstrate that the proposed attention CNN models achieve the best or the next-best results against the existing state-of-the-art models. ", "Large datasets  Classification  Support vector machine  Transfer learning  Group probability A novel transfer support vector machine called TSVM-GP with group probabilities is proposed for the scenarios where plenty of labeled data in the source domain and the group probabilities of unlabeled data in the target domain are available. TSVM-GP integrates a transfer term and group probabilities into the support vector machine (SVM) to improve the classification accuracy. In order to reduce the high computational complexity of TSVM-GP, the scalable version of TSVM-GP called scalable transfer support vector machine with group probabilities (STSVM-GP) is further developed by selecting the representative set of the training samples as the training data in the source domain. Experimental results on synthetic datasets as well as several real-world datasets show the effectiveness of the proposed classifiers, and especially STSVM-GP is very feasible for large scale transfer datasets. ", "Automatic Speech Recognition  Bangla Speech Recognition  Short Speech Commands  MFCC  Transfer learning  Convolutional neural network Despite being one of the most widely spoken languages of the world, no significant efforts have been made in Bangla speech recognition. Speech recognition is a difficult task, particularly if the demand is to do so in noisy real-life conditions. In this study, Bangla short speech commands data set has been reported, where all the samples are taken in the real-life setting. Three different convolutional neural network (CNN) architectures have been designed to recognize those short speech commands. Mel-frequency cepstral coefficients (MFCC) features have been extracted from the audio files in one approach whereas only the raw audio files have been used in another CNN architecture. Lastly, a pre-trained model which is trained on a large English short speech commands data set has been fine-tuned by retraining on Bangla data set. Experimental results reveal that the MFCC model shows better accuracy in recognizing Bangla short speech commands where, surprisingly, the model predicting on raw audio data is very competitive. The models have shown proficiency in identifying single syllable words but encounter difficulties in recognizing multi-syllable commands.", "bengali digit classification  deep learning  convolution neural networks  transfer learning  data augmentation  keras  Numtadb In this modern age, natural language processing (NLP) is evolving due to advances in the field of deep learning and its access to huge amount of data and computation power. Recently a lot of attention has been given to OCR for Bangla, the 5th most widely spoken language in the world. This paper reports on certain rather unconventional transfer learning approaches used to attain 6th place in the Kaggle Numta competition, where the challenge was to classify images of isolated Bangla numerals. The best result reported in this paper is an accuracy of 97.09% on the NumtaDB Bengali handwritten digit datasets test set, which was obtained by freezing intermediate layers. The unconventional approach used in this paper produces better results than conventional transfer learning while taking less epochs and having almost half the number of trainable parameters.", "Sence text detection  Multi-language  Transfer learning  Fully convolution networks In the paper, we propose a method based on transfer learning to detect multi-lingual text in natural scenes. First, a semantic segmentation map of the input image is obtained through a fully convolution network (FCN). In this map, each pixel is classified to text or none-text. And then, the candidate boxes of text regions are computed based on the map. In this procedure, VGG network is trained to obtain a basic character classifier of single language. Based on this VGG model, FCN has the ability to classify each pixel to text or none-text for multi-lingual with doing transfer learning. Finally, the bounding boxes of text are carry out by filtering the unsatisfied candidates with some rules. The experimental results show that our method achieves good performance on the task of multi-lingual text detection. And compared with other advanced method, the time cost of our method is shortest.", " Segmentation of anatomical structures in Chest Posterior-Anterior Radiographs is a classical task on biomedical image analysis. Deep Learning has been widely used for detection and diagnosis of illnesses in several medical image modalities over the last years, but the portability of deep methods is still limited, hampering the reusability of pre-trained models in new data. We address this problem by proposing a novel method for Cross-Dataset Transfer Learning in Chest X-Ray images based on Unsupervised Image Translation architectures. Our Transfer Learning approach achieved Jaccard values of 88.20% on lung field segmentation in the Montgomery Set by using a pre-trained model on the JSRT dataset and no labeled data from the target dataset. Several experiments in unsupervised and semi-supervised transfer were performed and our method consistently outperformed simple fine-tuning when a limited amount of labels is used. Qualitative analysis on the tasks of clavicle and heart segmentation are also performed on Montgomery samples and pre-trained models from JSRT dataset. Our secondary contributions encompass several experiments in anatomical structure segmentation on JSRT, achieving state-of-the-art results in lung field (96.02%), heart (89.64%) and clavicle segmentation (87.30%).", " The use of iris as a biometric trait is widely used because of its high level of distinction and uniqueness. Nowadays, one of the major research challenges relies on the recognition of iris images obtained in visible spectrum under unconstrained environments. In this scenario, the acquired iris are affected by capture distance, rotation, blur, motion blur, low contrast and specular reflection, creating noises that disturb the iris recognition systems. Besides delineating the iris region, usually preprocessing techniques such as normalization and segmentation of noisy iris images are employed to minimize these problems. But these techniques inevitably run into some errors. In this context, we propose the use of deep representations, more specifically, architectures based on VGG and ResNet-50 networks, for dealing with the images using (and not) iris segmentation and normalization. We use transfer learning from the face domain and also propose a specific data augmentation technique for iris images. Our results show that the approach using non-normalized and only circle-delimited iris images reaches a new state of the art in the official protocol of the NICE.II competition, a subset of the UBIRIS database, one of the most challenging databases on unconstrained environments, reporting an average Equal Error Rate (EER) of 13.98% which represents an absolute reduction of about 5%.", " Transfer learning is seen as one of the most promising areas of machine learning. Lately, features from pre-trained models have been used to achieve state-of-the-art results in several machine vision problems. Those models are usually employed when the problem of interest does not have enough supervised examples to support the network training from scratch. Most applications use networks pre-trained on noise-free RGB image datasets, what is observed even when the target domain counts on grayscale images or when data is degraded by noise. In this paper, we evaluate the use of Convolutional Neural Networks (CNNs) on such transfer learning scenarios and the impact of using RGB trained networks on grayscale image tasks. Our results confirm that the use of networks trained using colored images on grayscale tasks hinders the overall performance when compared to a similar network trained on a quantized version of the original dataset. Results also show that higher quantization levels (resulting in less colors) increase the robustness of CNN features in the presence of noise.", " It is well known that image classification problems can be effectively solved by Convolutional Neural Networks (CNNs). However, the number of supervised training examples from all categories must be high enough to avoid model over-fitting. In this case, two key alternatives are usually presented (a) the generation of artificial examples, known as data augmentation, and (b) reusing a CNN previously trained over a large supervised training set from another image classification problem - a strategy known as transfer learning. Deep learning approaches have rarely exploited the superior ability of humans for cognitive tasks during the machine learning loop. We advocate that the expert intervention through visual analytics can improve machine learning. In this work, we demonstrate this claim by proposing a data augmentation framework based on Encoder-Decoder Neural Networks (EDNNs) and visual analytics for the design of more effective CNN-based image classifiers. An EDNN is initially trained such that its encoder extracts a feature vector from each training image. These samples are projected from the encoder feature space on to a 2D coordinate space. The expert includes points to the projection space and the feature vectors of the new samples are obtained on the original feature space by interpolation. The decoder generates artificial images from the feature vectors of the new samples and the augmented training set is used to improve the CNN-based classifier. We evaluate methods for the proposed framework and demonstrate its advantages using data from a real problem as case study - the diagnosis of helminth eggs in humans. We also show that transfer learning and data augmentation by affine transformations can further improve the results.", " In dentistry, radiological examinations help specialists by showing structure of the tooth bones with the goal of screening embedded teeth, bone abnormalities, cysts, tumors, infections, fractures, problems in the temporomandibular regions, just to cite a few. Sometimes, relying solely in the specialist's opinion can bring differences in the diagnoses, which can ultimately hinder the treatment. Although tools for complete automatic diagnosis are no yet expected, image pattern recognition has evolved towards decision support, mainly starting with the detection of teeth and their components in X-ray images. Tooth detection has been object of research during at least the last two decades, mainly relying in threshold and region-based methods. Following a different direction, this paper proposes to explore a deep learning method for instance segmentation of the teeth. To the best of our knowledge, it is the first system that detects and segment each tooth in panoramic X-ray images. It is noteworthy that this image type is the most challenging one to isolate teeth, since it shows other parts of patient's body (e.g., chin, spine and jaws). We propose a segmentation system based on mask regionbased convolutional neural network to accomplish an instance segmentation. Performance was thoroughly assessed from a 1500 challenging image data set, with high variation and containing 10 categories of different types of buccal image. By training the proposed system with only 193 images of mouth containing 32 teeth in average, using transfer learning strategies, we achieved 98% of accuracy, 88% of Fl-score, 94% of precision, 84% of recall and 99% of specificity over 1224 unseen images, results very superior than other 10 unsupervised methods.", " Face recognition made tremendous leaps in the last five years with a myriad of systems proposing novel techniques substantially backed by deep convolutional neural networks (DCNN). Although face recognition performance sky-rocketed using deep-learning in classic datasets like LFW, leading to the belief that this technique reached human performance, it still remains an open problem in unconstrained environments as demonstrated by the newly released IJB datasets. This survey aims to summarize the main advances in deep face recognition and, more in general, in learning face representations for verification and identification. The survey provides a clear, structured presentation of the principal, state-of-the-art (SOTA) face recognition techniques appearing within the past five years in top computer vision venues. The survey is broken down into multiple parts that follow a standard face recognition pipeline: (a) how SOTA systems are trained and which public data sets have they used  (b) face preprocessing part (detection, alignment, etc.)  (c) architecture and loss functions used for transfer learning (d) face recognition for verification and identification. The survey concludes with an overview of the SOTA results at a glance along with some open issues currently overlooked by the community.", " Visual tracking plays an important role in unmanned systems. In many cases, the system needs to keep track of targets it has never seen before, and the only training sample available is the specified object in the initial frame. In this paper, we propose a deep architecture called adversarial transfer networks (ATNet), which aims to make well use of offline video training data and solve the problem of lacking training samples in visual tracking. Different from most existing trackers which neglect significant differences between videos and gulp the training data all together, our method utilizes the special nature of tracking problem and concentrates on transferring domain-specific information across similar tracking tasks. We first propose an efficient way to select a training video that is most similar to online tracking task and regard it as source domain. With the labeled data in the selected source domain, we apply adversarial transfer learning to make the feature distribution of source-domain samples and target-domain samples as similar as possible. Therefore, the transferred sourcedomain samples can provide various possible appearance of tracked target for training and boost the tracking performance. Experimental results on three OTB tracking benchmarks show that our method outperforms the state-of-the-art trackers in both accuracy and robustness.", " Robots hold promise in many scenarios involving outdoor use, such as search-and-rescue, wildlife management, and collecting data to improve environment, climate, and weather forecasting. However, autonomous navigation of outdoor trails remains a challenging problem. Recent work has sought to address this issue using deep learning. Although this approach has achieved state-of-the-art results, the deep learning paradigm may be limited due to a reliance on large amounts of annotated training data. Collecting and curating training datasets may not be feasible or practical in many situations, especially as trail conditions may change due to seasonal weather variations, storms, and natural erosion. In this paper, we explore an approach to address this issue through virtual-to-real-world transfer learning using a variety of deep learning models trained to classify the direction of a trail in an image. Our approach utilizes synthetic data gathered from virtual environments for model training, bypassing the need to collect a large amount of real images of the outdoors. We validate our approach in three main ways. First, we demonstrate that our models achieve classification accuracies upwards of 95% on our synthetic data set. Next, we utilize our classification models in the control system of a simulated robot to demonstrate feasibility. Finally, we evaluate our models on real-world trail data and demonstrate the potential of virtual-to-real-world transfer learning.", " One desirable capability of autonomous cars is to accurately predict the pedestrian motion near intersections for safe and efficient trajectory planning. We are interested in developing transfer learning algorithms that can be trained on the pedestrian trajectories collected at one intersection and yet still provide accurate predictions of the trajectories at another, previously unseen intersection. We first discussed the feature selection for transferable pedestrian motion models in general. Following this discussion, we developed one transferable pedestrian motion prediction algorithm based on Inverse Reinforcement Learning (IRL) that infers pedestrian intentions and predicts future trajectories based on observed trajectory. We evaluated our algorithm at three intersections. We used the accuracy of augmented semi- nonnegative sparse coding (ASNSC), trained and tested at the same intersection as a baseline. The result shows that the proposed algorithm improves the baseline accuracy by a statistically significant percentage in both non- transfer task and transfer task.", " Learning from demonstration (LfD) has enabled robots to rapidly gain new skills and capabilities by lever-aging examples provided by novice human operators. While effective, this training mechanism presents the potential for sub-optimal demonstrations to negatively impact performance due to unintentional operator error. In this work we introduce Concept Constrained Learning from Demonstration (CC-LfD), a novel algorithm for robust skill learning and skill repair that incorporates annotations of conceptually-grounded constraints (in the form of planning predicates) during live demonstrations into the LfD process. Through our evaluation, we show that CC-LfD can be used to quickly repair skills with as little as a single annotated demonstration without the need to identify and remove low-quality demonstrations. We also provide evidence for potential applications to transfer learning, whereby constraints can be used to adapt demonstrations from a related task to achieve proficiency with few new demonstrations required.", " Human detection and tracking is an essential task for service robots, where the combined use of multiple sensors has potential advantages that are yet to be fully exploited. In this paper, we introduce a framework allowing a robot to learn a new 3D LiDAR-based human classifier from other sensors over time, taking advantage of a multisensor tracking system. The main innovation is the use of different detectors for existing sensors (i.e. RGB-D camera, 2D LiDAR) to train, online, a new 3D LiDAR-based human classifier based on a new trajectory probability. Our framework uses this probability to check whether new detection belongs to a human trajectory, estimated by different sensors and/or detectors, and to learn a human classifier in a semi-supervised fashion. The framework has been implemented and tested on a real-world dataset collected by a mobile robot. We present experiments illustrating that our system is able to effectively learn from different sensors and from the environment, and that the performance of the 3D LiDAR-based human classification improves with the number of sensors/detectors used.", "business closure prediction  sentiment aligned topic model  NLP  lexicon generation  apsect-wise ratings  yelp  review analysis  deep learning  hybrid neural network  CNN Business closure is a very good indicator for success or failure of a business. This will help investors and banks as to whether to invest or lend to a particular business for future growth and benefits. Traditional machine learning techniques require extensive manual feature engineering and still do not perform satisfactorily due to significant class imbalance problem and little difference in the attributes for open and closed businesses. We have used historical data besides taking care of the class imbalance problem. Transfer learning also has been used to tackle the issue of having small categorical dalasets. A hybrid deep learning model has been proposed to predict whether a business would be shut down within a specific period of time. Sentiment Aligned Topic Model (SATM) is used to extract aspect-wise sentiment scores from user reviews. Our results show a marked improvement over traditional machine learning techniques. It also shows how the aspect-wise sentiment scores corresponding to each business, computed using SATM, help to give better results.", " Yawning is an important indicator of drivers' drowsiness or fatigue. Techniques for automatic detection of driver's yawning have been developed for use as a component of driver fatigue monitoring system. However, detecting driver's yawning event accurately in real-time is still a challenging task, in particular in applications such as driver fatigue detection, illumination conditions vary in a broad range, driver facial features vary in size, shape, texture and degrees of distortion. In this paper, we present a deep neural network model built using transfer learning and sequential learning from yawning video clips as well as augmented images for yawning detection. As a result, unlike many other methods that follow a sequence of processes such as face ROI detection, eye/nose/mouth positioning and mouth open/dose determination, the proposed yawning detection system detect yawning events directly from video images without requiring any facial part positions. The system is robust to variations in object scales, positions and subject view angles. The system has been evaluated on publicly available yawning data sets, YawDD and NTHU-DDD, as well as a data set containing challenging yawning videos. The experimental results show that the proposed yawning detection system has the capability of detecting yawning events in high precision even when face turns away from camera up to 70 degrees, while exhibiting capability of being scale- and spatial-invariant. In addition, the model demonstrates the capability of discriminating yawning events very well from the actions involving mouth opening-closing motions such as talking and laughing.", "Dark Knowledge  Deep Learning  Model Compression  Neural Network  Transfer Learning This paper focuses on a simple technique to extract the dark knowledge of a Deep Multi-Column Deep Learning Network, and its compression into a shallow neural network, causing not only the improvement of the train and test performance of the latter but a cheap way to approximate the former results but with fewer parameters. First, we built a Multi-Column Deep Learning Network, i.e., a Committee Machine, using simple techniques to improve its training accuracy. Finally, we transfer its knowledge to a shallow neural network, compressing its learned information and demonstrating that dark knowledge techniques still have a huge impact on Deep Multi-Layer Perceptron's studies. This paper validates the performance of the proposed model in the MNIST database, comparing it with popular neural nets used before, where we were able to achieve better scores.", "video summarization  transfer learning  cricket  deep convolutional networks  image classification  inceptionv3  vgg19 In recent years, there has been increased interest in video summarization and automatic sports highlights generation. In this work, we introduce a new dataset, called SNOW, for umpire pose detection in the game of cricket. The proposed dataset is evaluated as a preliminary aid for developing systems to automatically generate cricket highlights. In cricket, the umpire has the authority to make important decisions about events on the field. The umpire signals important events using unique hand signals and gestures. We identify four such events for classification namely SIX, NO BALL, OFT and WIDE based on detecting the pose of the umpire from the frames of a cricket video. Pre-trained convolutional neural networks such as Inception V3 and VGG19 networks arc selected as primary candidates for feature extraction. The results are obtained using a linear SVM classifier. The highest classification performance was achieved for the SVM trained on features extracted from the VGG19 network. The preliminary results suggest that the proposed system is an effective solution for the application of cricket highlights generation.", "switching reinforcement learning  decision making  autonomous system  uncertain environment In this paper, finite horizon intelligent decision making problem has been investigated for autonomous systems especially under uncertain environment. According to latest studies, the uncertainty of environment will seriously affect the effectiveness of decision making especially for autonomous systems. To handle this issues, transfer learning and deep reinforcement learning has been presented recently. However, those existing Learning algorithms commonly needs a large set of state space which cause the algorithm to be time consuming and not suitable for real-lime application. Therefore, in this paper, a library of polices trained using Deep Q-Learning under similar environments are built firstly. Then, a neural network is designed to estimate the environment. Using the learned environment, a novel of switching policy will be developed and integrated with the designed deep reinforcement learning which can efficiently stop learning according to the practical error tolerance. Meanwhile, through the novel policy evaluation method based on the environment estimator, the autonomous agent will select the best policy to follow in an online manner. Eventually, simulation results are provided to demonstrate the effectiveness of the designed algorithm.", "Deep learning  convolution neural networks  transfer learning  image classification The artificially supervised classification of real world entities have gained a phenomenal significance in recent year of computational advancements. An intelligent classification model focuses on rendering accurate outcomes vide the implicated paradigms with respect to the subjected data employed to train the classifier. This paper proposes a novel deep learning approach to classify the various parts of any operational engine such as crank shafts, rock-arms, distributer, air duct, assecorybelt etc. deployed in automobiles. The proposed architecture distinctively utilizes convolution neural networks for this typical classification problem and altogether constructs a robust transfer learning paradigm to render the correct class label against the validation and test images as the conclusive result of the classification. The proposed methodology poses in such a way that it can qualitatively classify and henceforth give the corresponding class label of the machinery/engine part under consideration. This computationally intelligent architecture requires the user to feed the image of the engine part to the model in order to achieve the requisite responses of classification. The main contribution of the proposed method is the development of a robust algorithm that can exhibit pronounced results without training the entire ConvNet architecture from scratch, thereby enabling the proposed paradigm to be deployable in application instances wherein limited labeled training data is available.", "computer vision  deep learning  transfer learning  distracted driver detection Studies show that the volume of traffic deaths per year is high and that a large part of these accidents are caused by distractions whose risk is aggravated by the use of cell phones while driving. This work presents results of a comparative study of three transfer learning approaches applied to classification of driver images in moments of concentration or distraction. For this study, four architectures of deep convolutional neural networks were evaluated: VGG19, Inception v3, Resnet152 and Densenet161. Results suggested that for the studied database, end-to-end transfer learning outperformed fine tuning only fully connected layers and also outperformed shallow classifiers trained with features extracted by the same deep convolutional networks.", "Pedestrian detection  Deep learning  Convolutional neural networks  Region-based neural networks (RCNN) Pedestrian detection continues to hold a significant role in the concept, analysis and function of computer vision. Deep learning techniques in pedestrian detection have demonstrated powerful results in recent experiments and research. In this paper a powerful deep learning technique of R-CNN is evaluated for Pedestrian detection on two different pedestrian detection datasets. The experiment involves the use of a deep learning feature extraction model along with the R-CNN detector. The deep learning feature extraction used is the Alexnet. Transfer learning is performed on the feature extraction model to adjust the weights of the convolutional neural networks to favour classification on the selected datasets. The R-CNN detector is then trained on the deep learning feature extraction model for pedestrian detection. The results of the experiments as evidently demonstrated, indicate some important truths about the performance of R-CNN detector on varying datasets.", "Recommender Systems  Deep Learning  Transfer Learning This industry talk covers the deep learning architecture developed at Realtor.com to recommend real estate listings to our userbase. The recommendation of homes is a different problem than most other domains both in the sense that listings are unique and that there are additional geographic and time constraints that increase the sparsity of interactions and make recommendation of individual listings more challenging. In particular time on market in a hot area can be limited to weeks or even days, and listing cold-start is critical to providing up to date market information. Thankfully the structured feature data for listings is incredibly rich and provides a framework from which to map listings into a meaningful vector space. User first impressions are also incredibly important in this highly competitive field, and offline recommendation or models that don't adapt during the users session are less desirable. In order to solve this recommendation problem we have developed a model based off of session based recommendation [1]. The architecture utilizes state of the art techniques from Natural Language Processing, including the AWD-LSTM language model developed by Salesforce [2]. To solve for coldstart of listings a structured data based denoising autoencoder was adapted from the methodology described in the winning entry of the Puerto Segurno Safe Driver Kaggle Competition [3]. This model is not used in the common way of generating fixed feature vectors, but rather the entire head of the autoencoder model, from the feature inputs to the middle layer commonly used as the vector output, is first trained to encode listing features, and then becomes the input to the AWD-LSTM architecture. This style of transfer learning is common in Computer Vision, and has recently been utilized in NLP to achieve state of the art results for text classification [4]. By including the head we are able to further optimize the listing encoder network and embeddings to take user interactions into account. As in traditional session based recommendation users are represented as the sequence of listings that they view, however those listings are fed into the model as the sequence of features. The final system consists of several components. The first attempts to calculate and maintain the users' feature vector and model hidden weights in near realtime, providing a representation for the user within the system. This representation is used by several downstream components, most notably the search rerank and recommendation modules which calculate users' interest in listings both in the context of the output of more traditional elasticsearch queries via cosine similarity of user/listing vectors and through approximate nearest neighbor vector space searches for relevant listings which form the input set for a pointwise scoring model trained on time on listing as done by YouTube [5].", "cross-domain  knowledge transfer  recommender system Having data from multiple sources, cross-domain and context-aware recommender systems, with the help of transfer learning approaches, aim to integrate such data to improve recommendation quality and alleviate issues such as cold-start problem. With the advantages of these techniques, we host the second international workshop on intelligent recommender systems by knowledge transfer and learning (RecSysKTL) to provide such a forum for both academia and industry researchers as well as application developers from around the world to present their work and discuss exciting research ideas or outcomes. The workshop is held in conjunction with the ACM Conference on Recommender Systems 2018 on October 6th in Vancouver, Canada.", "microblog  profile estimation  transfer learning In this paper, we propose a method of retrieving posts on social networking services (SNSs) by specifying a pair of queries: a topic query and an entity query. A topic query specifies the topic of the posts to retrieve (e.g., iPhone) and an entity query specifies the type of users who posted them (e.g., students). In the existing search systems for SNS posts, we can specify topics of posts by keywords, but we cannot specify types of users. Even if we include keywords specifying types of users in a query, such keywords are not usually included in tweets or user profile data. In our method, we estimate types of users by learning vocabulary whose appearance is correlated with specific types of users. We learn it from the datasets obtained through Web search. We retrieve Web documents through the search with a keyword specifying the type of users (e.g., student), and we also retrieve Web documents by using a keyword specifying its opposite (e.g., adult). We regard the documents retrieved by these queries as positive and negative examples of documents describing the target type, and we train a model for recognizing users of the given type. We recognize users of the target type by inputting their posts and their profile data into the model. We use Web documents instead of SNS posts for training the model because the Web has more documents describing types of people.", " Companies aim to promote their products under competitions and try to gain more profit than other companies. This problem is formulated as a Competitive Influence Maximization (CIM). Recently, a reinforcement learning has been used to solve the CIM problem, that is, to find an optimal strategy against competitor in order to maximize the commutative reward under the competition from other agents. However, reinforcement learning agents require huge training time to find an optimal strategy whenever the settings of the agents or the networks change. To tackle this issue, we propose a transfer learning method in reinforcement learning to reduce the training time and utilize the knowledge gained on source network to target network. Our method relies on two ideas, the first one is the state representation of the source and target networks in order to efficiently utilize the knowledge gained on source network to target network. The second idea is to transfer the final Q-solution of source network while learning on the target network. We validate our transfer learning method in similar or different settings of source and target networks while competing against the competitor's known strategies. Experimental results show that our proposed transfer learning method achieves similar or better performance as a baseline model while significantly reducing training time in all settings.", "CNN  AlexNet  Transfer Learning  Computer Vision  MSER  OCR Prohibition signs are commonly used for safety purposes in order to prevent and protect individuals from dangerous situations. These signs are placed in or around areas whereby they are clearly visible to the public. However, the visually impaired cannot visualize such signs. To help them, this paper proposes a system that combines Convolutional Neural Network (CNN) model and Computer Vision (CV) algorithms to detect and recognize prohibition signs in real scenes. The system uses pre-trained AlexNet model, fine-tuned using Prohibition Signage Boards (PSB) dataset and combined with Maximally Stable Extremal Regions (MSER) and Optical Character Recognition (OCR) techniques for text extraction and classification, to enhance the system performance. The experiments indicate that high recognition accuracies are achieved from a variety of prohibition images and prohibition texts.", "Gastro intestinal tract  Deep learning  Texture  Ensemble  Transfer learning An endoscopy is a strategy in which a specialist utilizes specific instruments to see and work on the inward vessels and organs of the body. This paper expects to predict the abnormalities and diseases in the Gastro-Intestinal Tract, utilizing multimedia data acquired from endoscopy. Deep Analysis of GI tract pictures can foresee diseases and abnormalities, in its early stages and accordingly spare human lives. In this paper, a novel ensemble method is presented, where texture and deep learning features are integrated to improve the prediction of the abnormalities in the GI tract e.g. Peptic ulcer disease. Multimedia content analysis (to extricate data from the visual information) and machine learning (for classification) have been explored. Deep learning has additionally been joined by means of Transfer learning. Medieval Benchmarking Initiative for Multimedia Evaluation provided the dataset, which includes 8000 pictures. The data is gathered from conventional colonoscopy process. Using logistic regression and ensemble of different extracted features, 83% accuracy and a F1 score of 0.821 is achieved on testing sample. The proposed approach is compared with several state-of-the-art methods and results have indicated significant performance gains when compared with other approaches.", "Face recognition  Face spoof detection  Deep learning  CNN  Feature extraction Face recognition is now widely being used to verify the identity of the person in various applications ranging from border crossing to mobile authentication. However, most face recognition systems are vulnerable to spoofing or presentation attacks, where a photo, a video, or a 3D mask of a genuine user's face may be utilized to fool the biometric system. Although many face spoof detection techniques have been proposed, the issue is still unsolved. Recently deep learning based models have achieved impressive results in various challenging image and video classification tasks. Consequently, very few works have applied convolutional neural networks (CNNs) for face liveness detection. Nonetheless, it is still unclear how different CNN features and methods compare with each other for face spoof detection, since prior CNN based face liveness detection approaches employ different fine-tuning procedures and/or datasets for training. Thus, in this paper, an approach based on transfer learning using some well-known and well-adopted pre-trained CNNs architectures is presented. This study explores different deep features and compares them on a common ground for face liveness detection in videos. Experimental analysis on two publicly available databases, NUAA and CASIA-FASD, shows that the proposed method is able to attain satisfactory and comparable results to the state-of-the-art methods.", "Transfer Learning  Activity Recognition  Deep Learning  Domain Adaptation Human activity recognition plays an important role in people's daily life. However, it is often expensive and time-consuming to acquire sufficient labeled activity data. To solve this problem, transfer learning leverages the labeled samples from the source domain to annotate the target domain which has few or none labels. Unfortunately, when there are several source domains available, it is difficult to select the right source domains for transfer. The right source domain means that it has the most similar properties with the target domain, thus their similarity is higher, which can facilitate transfer learning. Choosing the right source domain helps the algorithm perform well and prevents the negative transfer. In this paper, we propose an effective Unsupervised Source Selection algorithm for Activity Recognition (USSAR). USSAR is able to select the most similar K source domains from a list of available domains. After this, we propose an effective Transfer Neural Network to perform knowledge transfer for Activity Recognition (TNNAR). TNNAR could capture both the time and spatial relationship between activities while transferring knowledge. Experiments on three public activity recognition datasets demonstrate that: 1) The USSAR algorithm is effective in selecting the best source domains. 2) The TNNAR method can reach high accuracy when performing activity knowledge transfer.", " With the steady rise of advanced driver assistance systems (ADAS), more and more aspects of the driving task are transferred from the human driver to the vehicle's control system. In order to handle many of these responsibilities, vehicles need to understand their environment and adjust their behavior according to it. An important aspect of the vehicle environment is the layout of the road segment right ahead of the vehicle, such as the presence and type of an intersection, as it defines the scenario, provides context information and constrains the future motion of traffic participants. The knowledge of upcoming intersections can help to improve various aspects in the context of driver assistance systems and automated driving, such as the prediction of traffic participants or the adjustment of a system with respect to the current scenario. The contribution of this paper is threefold: First, it introduces a model for intersection identification and classification ahead of a vehicle solely from on-board sensor data via deep learning. Second, it proposes a transfer-learning technique allowing to train with fewer samples and showing that intermediate features from path prediction are also beneficial for intersection classification tasks. Third, it allows to reduce necessary computational power since feature extraction is partially shared between the path prediction and the intersection classification model.", "Network traffic  convolutional neural networks  deep learning  transfer learning Large-scale network traffic analysis is crucial for many transport applications, ranging from estimation and prediction to control and planning. One of the key issues is how to integrate spatial and temporal analyses efficiently. Deep Learning is gaining momentum as a go-to approach for artificial vision, and transfer learning approaches allow to exploit pretrained models and apply them to new domains. In this paper, we encode traffic states as images and use a pretrained deep convolutional neural network as a feature extractor. Experimental results show how the extracted feature vectors cluster naturally into meaningful network traffic states and illustrate how these network states can be used for traffic state prediction.", " This article demonstrates the ability for modelfree reinforcement learning (RL) techniques to generate traffic control strategies for connected and automated vehicles (CAVs) in various network geometries. This method is demonstrated to achieve near complete wave dissipation in a straight open road network with only 10% CAV penetration, while penetration rates as low as 2.5% are revealed to contribute greatly to reductions in the frequency and magnitude of formed waves. Moreover, a study of controllers generated in closed network scenarios exhibiting otherwise similar densities and perturbing behaviors confirms that closed network policies generalize to open network tasks, and presents the potential role of transfer learning in fine-tuning the parameters of these policies. Videos of the results are available at: https://sites.google.com/view/itsc-dissipating-waves.", " Vision-based global localization without a prior location estimate is a fundamental task for safe and efficient vehicle navigation in GPS-denied environments. Cross-season localization, in which query and database images involve different seasons is one of the most challenging task scenarios, owing to appearance variations among seasons. Because of recent advances in deep convolutional neural networks (DCNs) and transfer learning techniques, the task can be solved accurately by training and fine-tuning a DCN-based visual place classifier. However, the direct implementation of this would require collecting and storing a large amount of visual experiences (i.e., training data) for every new season, which is impractical. The goal of our study is to suppress the space cost for long-term memory and to develop a constant cost framework for long-term global localization. Moreover, we formulate and consider the task of experience compression as a scheduling problem of how to choose the part of the previous season's experience that is to be replaced with the current season's experience, to achieve an optimal tradeoff between localization accuracy and training efficiency. Experimental results using the publicly available North Campus Long-Term autonomy dataset validate the efficacy of our proposed approach.", "Vehicle classification  Convolutional neural network  Resnet152  Support vector machines  Transfer learning Classification of vehicles is one of the most important tasks in intelligent transportation systems (ITS). While there are various types of sensors for measuring vehicle characteristics, this paper is focused on an image-based vehicle classification system. Most traditional approaches for image-based vehicle classification are computationally extensive and typically require a large amount of data for model training. This paper investigates whether it is possible to transfer the learning of a highly accurate pre-trained model for classifying truck images based on body type. Results show that using a pre-trained model to extract lowlevel features of images increases the accuracy of the model significantly, even with a relatively small size of training data. Furthermore, a convolutional neural network (CNN) is shown to outperform other types of models to classify trucks based on the extracted features.", " Road scene segmentation is of great significance in intelligent transportation system for different applications such as autonomous driving and semantic map building. Despite great progress in this field with the deep learning methods, there are still many difficulties such as robust segmentation of small objects and same type of objects with different sizes in different scenes. In this paper, we propose a new pyramid architecture for scene segmentation, which is a top-down architecture with lateral connections for multi-scale semantic feature maps building, and sufficiently incorporate the momentous global scenery prior. Besides, we also propose a novel training method, which combines the re-sampling, pixel-wise cost learning and transfer learning together, to deal with the imbalance problem. Experimental results on KITTI and Cityscapes dataset demonstrate effectiveness of the proposed method.", " In human learning, it is common to use multiple sources of information jointly. However, most existing feature learning approaches learn from only a single task. In this paper, we propose a novel multi-task deep network to learn generalizable high-level visual representations. Since multitask learning requires annotations for multiple properties of the same training instance, we look to synthetic images to train our network. To overcome the domain difference between real and synthetic data, we employ an unsupervised feature space domain adaptation method based on adversarial learning. Given an input synthetic RGB image, our network simultaneously predicts its surface normal, depth, and instance contour, while also minimizing the feature space domain differences between real and synthetic data. Through extensive experiments, we demonstrate that our network learns more transferable representations compared to single-task baselines. Our learned representation produces state-of-the-art transfer learning results on PASCAL VOC 2007 classification and 2012 detection.", " We propose to revisit knowledge transfer for training object detectors on target classes from weakly supervised training images, helped by a set of source classes with bounding-box annotations. We present a unified knowledge transfer framework based on training a single neural network multi-class object detector over all source classes, organized in a semantic hierarchy. This generates proposals with scores at multiple levels in the hierarchy, which we use to explore knowledge transfer over a broad range of generality, ranging from class-specific (bycicle to motorbike) to class-generic (objectness to any class). Experiments on the 200 object classes in the ILSVRC 2013 detection dataset show that our technique (1) leads to much better performance on the target classes (70.3% CorLoc, 36.9% mAP) than a weakly supervised baseline which uses manually engineered objectness [11] (50.5% CorLoc, 25.4% mAP). (2) delivers target object detectors reaching 80% of the mAP of their fully supervised counterparts. (3) outperforms the best reported transfer learning results on this dataset (+41% CorLoc and + 3% mAP over [18, 46], +16.2% mAP over [32]). Moreover, we also carry out several acrossdataset knowledge transfer experiments [27, 24, 35] and find that (4) our technique outperforms the weakly supervised baseline in all dataset pairs by 1.5 x -1.9x, establishing its general applicability.", " Fully supervised methods for semantic segmentation require pixel-level class masks to train, the creation of which is expensive in terms of manual labour and time. In this work, we focus on weak supervision, developing a method for training a high-quality pixel-level classifier for semantic segmentation, using only image-level class labels as the provided ground-truth. Our method is formulated as a two-stage approach in which we first aim to create accurate pixel-level masks for the training images via a bootstrapping process, and then use these now-accurately segmented images as a proxy ground-truth in a more standard supervised setting. The key driver for our work is that in the target dataset we typically have reliable ground-truth image-level labels, while data crawled from the web may have unreliable labels, but can be filtered to comprise only easy images to segment, therefore having reliable boundaries. These two forms of information are complementary and we use this observation to build a novel bi-directional transfer learning framework. This framework transfers knowledge between two domains, target domain and web domain, bootstrapping the performance of weakly supervised semantic segmentation. Conducting experiments on the popular benchmark dataset PASCAL VOC 2012 based on both a VGG16 network and on ResNet50, we reach state-of-the-art performance with scores of 60.2% IoU and 63.9% IoU respectively(1).", " Paucity of large curated hand-labeled training data forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label, given a set of weak labeling functions. We validated our method on the MNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many state-of-the-art models. We conducted extensive experiments to study its usefulness, as well as showed how the proposed ADP framework can be used for transfer learning as well as multi-task learning, where data from two domains are generated simultaneously using the framework along with the label information. Our future work will involve understanding the theoretical implications of this new framework from a game-theoretic perspective, as well as explore the performance of the method on more complex datasets.", " Adversarial learning has been successfully embedded into deep networks to learn transferable features, which reduce distribution discrepancy between the source and target domains. Existing domain adversarial networks assume fully shared label space across domains. In the presence of big data, there is strong motivation of transferring both classification and representation models from existing large-scale domains to unknown small-scale domains. This paper introduces partial transfer learning, which relaxes the shared label space assumption to that the target label space is only a subspace of the source label space. Previous methods typically match the whole source domain to the target domain, which are prone to negative transfer for the partial transfer problem. We present Selective Adversarial Network (SAN), which simultaneously circumvents negative transfer by selecting out the outlier source classes and promotes positive transfer by maximally matching the data distributions in the shared label space. Experiments demonstrate that our models exceed state-of-the-art results for partial transfer learning tasks on several benchmark datasets.", " This work addresses the novel problem of one-shot one class classification. The goal is to estimate a classification decision boundary for a novel class based on a single image example. Our method exploits transfer learning to model the transformation from a representation of the input, extracted by a Convolutional Neural Network, to a classification decision boundary. We use a deep neural network to learn this transformation from a large labelled dataset of images and their associated class decision boundaries generated from ImageNet, and then apply the learned decision boundary to classify subsequent query images. We tested our approach on several benchmark datasets and significantly outperformed the baseline methods.", " Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values  it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. We provide a set of tools for computing and probing this taxonomical structure including a solver users can employ to find supervision policies for their use cases.", " Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via fine-tuning offers an effective solution for domain-specific fine-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car make & model). In such scenarios, data annotation often calls for specialized domain knowledge and thus is difficult to scale. In this work, we first tackle a problem in large scale FGVC. Our method won first place in iNaturalist 2017 large scale species classification challenge. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via fine-tuning from large scale datasets to small scale, domain specific FGVC datasets. We propose a measure to estimate domain similarity via Earth Mover's Distance and demonstrate that transfer learning benefits from pre-training on a source domain that is similar to the target domain by this measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on multiple commonly used FGVC datasets.", " In transfer learning, one seeks to transfer related information from source tasks with sufficient data to help with the learning of target task with only limited data. In this paper, we propose a novel Coupled End-to-end Transfer Learning (CETL) framework, which mainly consists of two convolutional neural networks (source and target) that connect to a shared decoder. A novel loss function, the coupled loss, is used for CETL training. From a theoretical perspective, we demonstrate the rationale of the coupled loss by establishing a learning bound for CETL. Moreover, we introduce the generalized Fisher information to improve multi-task optimization in CETL. From a practical aspect, CETL provides a unified and highly flexible solution for various learning tasks such as domain adaption and knowledge distillation. Empirical result shows the superior performance of CETL on cross-domain and cross-task image classification.", " Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle's steering angle. To make the best out of this sensor-algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset (approximate to 1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.", " We propose a novel probabilistic model for visual question answering (Visual QA). The key idea is to infer two sets of embeddings: one for the image and the question jointly and the other for the answers. The learning objective is to learn the best parameterization of those embeddings such that the correct answer has higher likelihood among all possible answers. In contrast to several existing approaches of treating Visual QA as multi-way classification, the proposed approach takes the semantic relationships (as characterized by the embeddings) among answers into consideration, instead of viewing them as independent ordinal numbers. Thus, the learned embedded function can be used to embed unseen answers (in the training dataset). These properties make the approach particularly appealing for transfer learning for open-ended Visual QA, where the source dataset on which the model is learned has limited overlapping with the target dataset in the space of answers. We have also developed large-scale optimization techniques for applying the model to datasets with a large number of answers, where the challenge is to properly normalize the proposed probabilistic models. We validate our approach on several Visual QA datasets and investigate its utility for transferring models across datasets. The empirical results have shown that the approach performs well not only on in-domain learning but also on transfer learning.", " In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task. Source code and dataset will be available at kuanghuei.github.io/CleanNetProject.", " In this paper we present the design and evaluation of an end-to-end trainable, deep neural network with a visual attention mechanism for memorability estimation in still images. We analyze the suitability of transfer learning of deep models from image classification to the memorability task. Further on we study the impact of the attention mechanism on the memorability estimation and evaluate our network on the SUN Memorability and the LaMem datasets. Our network outperforms the existing state of the art models on both datasets in terms of the Spearman's rank correlation as well as the mean squared error, closely matching human consistency.", " Most of the proposed person re-identification algorithms conduct supervised training and testing on single labeled datasets with small size, so directly deploying these trained models to a large-scale real-world camera network may lead to poor performance due to underfitting. It is challenging to incrementally optimize the models by using the abundant unlabeled data collected from the target domain. To address this challenge, we propose an unsupervised incremental learning algorithm, TFusion, which is aided by the transfer learning of the pedestrians' spatio-temporal patterns in the target domain. Specifically, the algorithm firstly transfers the visual classifier trained from small labeled source dataset to the unlabeled target dataset so as to learn the pedestrians' spatial-temporal patterns. Secondly, a Bayesian fusion model is proposed to combine the learned spatio-temporal patterns with visual features to achieve a significantly improved classifier. Finally, we propose a learning-to-rank based mutual promotion procedure to incrementally optimize the classifiers based on the unlabeled data in the target domain. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm gains significant improvement compared with the state-of-art cross-dataset unsupervised person re identification algorithms.", " A practical limitation of deep neural networks is their high degree of specialization to a single task and visual domain. Recently, inspired by the successes of transfer learning, several authors have proposed to learn instead universal feature extractors that, used as the first stage of any deep network, work well for several tasks and domains simultaneously. Nevertheless, such universal features are still somewhat inferior to specialized networks. To overcome this limitation, in this paper we propose to consider instead universal parametric families of neural networks, which still contain specialized problem-specific models, but differing only by a small number of parameters. We study different designs for such parametrization, including series and parallel residual adapters, joint adapter compression, and parameter allocations, and empirically identify the ones that yield the highest compression. We show that, in order to maximize performance, it is necessary to adapt both shallow and deep layers of a deep network, but the required changes are very small. We also show that these universal parametrization are very effective for transfer learning, where they outperform traditional fine-tuning techniques.", " The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder/ decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category-and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one/few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.", " We study the problem of learning representations for single cells in microscopy images to discover biological relationships between their experimental conditions. Many new applications in drug discovery and functional genomics require capturing the morphology of individual cells as comprehensively as possible. Deep convolutional neural networks (CNNs) can learn powerful visual representations, but require ground truth for training  this is rarely available in biomedical profiling experiments. While we do not know which experimental treatments produce cells that look alike, we do know that cells exposed to the same experimental treatment should generally look similar. Thus, we explore training CNNs using a weakly supervised approach that uses this information for feature learning. In addition, the training stage is regularized to control for unwanted variations using mixup or RNNs. We conduct experiments on two different datasets  the proposed approach yields single-cell embeddings that are more accurate than the widely adopted classical features, and are competitive with previously proposed transfer learning approaches.", " We develop a set of methods to improve on the results of self-supervised learning using context. We start with a baseline of patch based arrangement context learning and go from there. Our methods address some overt problems such as chromatic aberration as well as other potential problems such as spatial skew and mid-level feature neglect. We prevent problems with testing generalization on common self-supervised benchmark tests by using different datasets during our development. The results of our methods combined yield top scores on all standard self-supervised benchmarks, including classification and detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and linear tests on the ImageNet and CSAIL Places datasets. We obtain an improvement over our baseline method of between 4.0 to 7.1 percentage points on transfer learning classification tests. We also show results on different standard network architectures to demonstrate generalization as well as portability. All data, models and programs are available at: https://gdo-datasci.llnl.gov/selfsupervised/.", "Deep Convolutional Neural Networks (DCNN)  Indoor navigation  Semantic segmentation  Siamese network The vast majority of indoor navigation algorithms either rely on manual scene augmentation and labelling or exploit multi-sensor fusion techniques in achieving simultaneous localization and mapping (SLAM), leading to high computational costs, hardware complexities and robustness deficiencies. This paper proposes an efficient and robust deep learning-based indoor navigation framework for robots. Firstly, we put forward an end-to-end trainable siamese deep convolutional neural network (DCNN) which decomposes navigation into orientation and localization in one branch, while achieving semantic scene mapping in another. In mitigating the computational costs associated with DCNNs, the proposed model design shares a significant amount of convolutional operations between the two branches, streamlining the model and optimizing for efficiency in terms of memory and inference latency. Secondly, a transfer learning regime is explored in demonstrating how such siamese DCNNs can be efficiently trained for high convergence rates without extensive manual dataset labelling. The resulting siamese framework combines semantic scene understanding with orientation estimation towards predicting collision-free and optimal navigation paths. Experimental results demonstrate that the proposed framework achieves accurate and efficient navigation and outperforms existing navigation-by-classification variants.", "Big data  multi-view  transfer learning  data Insufficiency  subspace learning  Low rank Big data has brought many new challenges for machine learning research. In many learning tasks, we have to deal with diverse data from different domains, different representations, different distributions, scale, and density in order to achieve a good performance. With the recent advances in data storage and internet technology, data become more prominent, noisier and more complex which bring new opportunities and challen ges into Transfer learning. In Urban computing when inferring knowledge for new or less developed cities we often need to deal with large-scale, multi-view, noisy and incomplete data. This calls for advanced techniques that can make practical use of massive, sparse and noisy data to efficiently transfer knowledge of multiple and diverse datasets (views) from a source domain to a target domain. Such a problem becomes much more challenging in an unsupervised learning setting where we do not dispose any label in the target domain which is not uncommon in my real-world scenarios. To tackle this challenge, in this paper we propose novel unsupervised multi-view transfer with missing data by learning a shared subspace across views from different domains through a latent low-rank transfer. Before performing knowledge transfer our approach learns an enriched representation of the source domain via a novel joint multi-view dictionary learning based on low-rank tensor. We also propose a multi-view co-classifier to predict the label in the target domain. Tailored for big data applications with EM-ADMM based optimization algorithm our method can efficiently perform knowledge transfer from a multi-view source domain to an unlabeled multi-view target domain with a high rate of missing values and noise.", "computer vision  convolutional neural network  object detection  out of stock  tensorflow Out of stock (OOS) is a problem all stores are facing and it reduces their profit. Standard procedures for solving OOS are mostly manual and not scalable. This paper analyzes and proposes an automated and scalable solution for solving OOS problem inside commercial refrigerators. Small, low resolution cameras are placed inside refrigerators. Images taken with those cameras are analyzed with Faster R-CNN and Single Shot Multibox (SSD) models for object detection. Models were trained using transfer learning and their performances were analyzed and compared. After object detection, K-mean clustering algorithm is used to group objects on same shelves. Distance between objects on the same shelf determines if and where the OOS problem is present.", "Text games  reinforcement learning  neural networks The ability to learn optimal control policies in systems where action space is defined by sentences in natural language would allow many interesting real-world applications such as automatic optimisation of dialogue systems. Text-based games with multiple endings and rewards are a promising platform for this task, since their feedback allows us to employ reinforcement learning techniques to jointly learn text representations and control policies. We argue that the key property of AI agents, especially in the text-games context, is their ability to generalise to previously unseen games. We present a minimalistic text-game playing agent, testing its generalisation and transfer learning performance and showing its ability to play multiple games at once. We also present pyfiction, an open-source library for universal access to different text games that could, together with our agent that implements its interface, serve as a baseline for future research.", "transfer learning  online learning  online transfer learning  homogeneous transfer Transfer learning has made great achievements in many fields and many excellent algorithms have been proposed. In recent years, many scholars have focused on a new research area called online transfer learning, which is different from general transfer learning. Online transfer learning concentrates on how to build a good classifier on the target domain when the training data arrive in an online/sequential manner. This paper focuses on online transfer learning problem based on a single source domain under homogeneous space. The existing algorithms HomOTL-I and HomOTL-II simply ensemble the classifiers on the source and target domains directly. When the distribution difference between the source domain and the target domain is large, it will not result in a good transfer effect. We are inspired by the idea of the boosting algorithm, that is we could form a strong classification model by a combination of multiple weak classifications. We train multiple classifiers on the source domain in an offfine manner using AdaBoost algorithm, combine these classifiers on source domain with the classifier trained in an online manner on the target domain to form multiple weak combination in an ensemble manner. Based on the above ideas, we propose two algorithms AB-HomOTL-I and AB-HomOTLII, which have different ways to adjust the weights. We tested our algorithms on sentiment analysis dataset and 20newsgroup dataset. The results show that our algorithms are superior to other baseline algorithms.", "online transfer learning  heterogeneous transfer learning  ensemble learning Transfer learning is an important topic in machine learning and has been broadly studied for many years. However, most existing transfer learning methods assume the training sets are prepared in advance, which is often not the case in practice. Fortunately, online transfer learning (OTL), which addresses the transfer learning tasks in an online fashion, has been proposed to solve the problem. This paper mainly focuses on the heterogeneous OTL, which is in general very challenging because the feature space of target domain is different from that of the source domain. In order to enhance the learning performance, we design the algorithm called Heterogeneous Ensembled Online Transfer Learning (HetEOTL) using ensemble learning strategy. Finally, we evaluate our algorithm on some benchmark datasets, and the experimental results show that HetEOTL has better performance than some other existing online learning and transfer learning algorithms, which proves the effectiveness of HetEOTL.", " The performance of current automatic face recognition algorithms is hindered by different covariates such as facial aging, disguises, and pose variations. Specifically, disguises are employed for intentional or unintentional modifications in the facial appearance for hiding one's own identity or impersonating someone else's identity. In this paper, we utilize deep learning based transfer learning approach for face verification with disguise variations. We employ Residual Inception network framework with center loss for learning inherent face representations. The training for the Inception-ResNet model is performed using a large-scale face database which is followed by inductive transfer learning to mitigate the impact of facial disguises. To evaluate the performance of the proposed Deep Disguise Recognizer (DDR) framework, Disguised Faces in the Wild and IIIT-Delhi Disguise Version 1 face databases are used. Experimental evaluation reveals that for the two databases, the proposed DDR framework yields 90.36% and 66.9% face verification accuracy at the false accept rate of 10%.", " The most common approaches to instance segmentation are complex and use two-stage networks with object proposals, conditional random-fields, template matching or recurrent neural networks. In this work we present Ternaus-NetV2 - a simple fully convolutional network that allows extracting objects from a high-resolution satellite imagery on an instance level. The network has popular encoder-decoder type of architecture with skip connections but has a few essential modifications that allows using for semantic as well as for instance segmentation tasks. This approach is universal and allows to extend any network that has been successfully applied for semantic segmentation to perform instance segmentation task. In addition, we generalize network encoder that was pre-trained for RGB images to use additional input channels. It makes possible to use transfer learning from visual to a wider spectral range. For DeepGlobe-CVPR 2018 building detection sub-challenge, based on public leaderboard score, our approach shows superior performance in comparison to other methods.", " Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training.", " Scene parsing aims to assign a class (semantic) label for each pixel in an image. It is a comprehensive analysis of an image. Given the rise of autonomous driving, pixel-accurate environmental perception is expected to be a key enabling technical piece. However, providing a large scale dataset for the design and evaluation of scene parsing algorithms, in particular for outdoor scenes, has been difficult. The per-pixel labelling process is prohibitively expensive, limiting the scale of existing ones. In this paper, we present a large-scale open dataset, ApolloScape, that consists of RGB videos and corresponding dense 3D point clouds. Comparing with existing datasets, our dataset has the following unique properties. The first is its scale, our initial release contains over 140K images - each with its per-pixel semantic mask, up to 1M is scheduled. The second is its complexity. Captured in various traffic conditions, the number of moving objects averages from tens to over one hundred (Figure 1). And the third is the 3D attribute, each image is tagged with high-accuracy pose information at cm accuracy and the static background point cloud has mm relative accuracy. We are able to label these many images by an interactive and efficient labelling pipeline that utilizes the high-quality 3D point cloud. Moreover, our dataset also contains different lane markings based on the lane colors and styles. We expect our new dataset can deeply benefit various autonomous driving related applications that include but not limited to 2D/3D scene understanding, localization, transfer learning, and driving simulation.", " Monitoring of the marine environment requires large amounts of data, simply due to its vast size. Therefore, underwater autonomous vehicles and drones are increasingly deployed to acquire numerous photographs. However, ecological conclusions from them are lagging as the data requires expert annotation and thus realistically cannot be manually processed. This calls for developing automatic classification algorithms dedicated for this type of data. Current out-of-the-box solutions struggle to provide optimal results in these scenarios as the marine data is very different from everyday data. Images taken under water display low contrast levels and reduced visibility range thus making objects harder to localize and classify. Scale varies dramatically because of the complex 3 dimensionality of the scenes. In addition, the scarcity of labeled marine data prevents training these dedicated networks from scratch. In this work, we demonstrate how transfer learning can be utilized to achieve high quality results for both detection and classification in the marine environment. We also demonstrate tracking in videos that enables counting and measuring the organisms. We demonstrate the suggested method on two very different marine datasets, an aerial dataset and an underwater one.", " In this paper, efforts have been made to analyze the impact of training strategies, transfer learning and domain knowledge on two biometric-based problems namely: three class oculus classification and fingerprint sensor classification. For analyzing these problems we have considered deep-learning based architecture and evaluated our results on benchmark contact-lens datasets like IIIT-D, ND, IIT-K ( our model is publicly available) and on fingerprint datasets like FVC-2002, FVC-2004, FVC-2006, IIITD-MOLF, IIT-K. In-depth feature analysis of various proposed deep-learning models has been done in order to infer that indeed training in different ways along with transfer learning and domain knowledge plays a vital role in deciding the learning ability of any network.", " We present a novel context-aware attention-based deep architecture for image caption generation. Our architecture employs a Bidirectional Grid LSTM, which takes visual features of an image as input and learns complex spatial patterns based on two-dimensional context, by selecting or ignoring its input. The Grid LSTM has not been applied to image caption generation task before. Another novel aspect is that we leverage a set of local region-grounded texts obtained by transfer learning. The region-grounded texts often describe the properties of the objects and their relationships in an image. To generate a global caption for the image, we integrate the spatial features from the Grid LSTM with the local region-grounded texts, using a two-layer Bidirectional LSTM. The first layer models the global scene context such as object presence. The second layer utilizes a novel dynamic spatial attention mechanism, based on another Grid LSTM, to generate the global caption word-by-word, while considering the caption context around a word in both directions. Unlike recent models that use a soft attention mechanism, our dynamic spatial attention mechanism considers the spatial context of the image regions. Experimental results on MS-COCO dataset show that our architecture outperforms the state-of-the-art.", " Unconstrained remote gaze tracking using off-the-shelf cameras is a challenging problem. Recently, promising algorithms for appearance-based gaze estimation using convolutional neural networks (CNN) have been proposed. Improving their robustness to various confounding factors including variable head pose, subject identity, illumination and image quality remain open problems. In this work, we study the effect of variable head pose on machine learning regressors trained to estimate gaze direction. We propose a novel branched CNN architecture that improves the robustness of gaze classifiers to variable head pose, without increasing computational cost. We also present various procedures to effectively train our gaze network including transfer learning from the more closely related task of object viewpoint estimation and from a large high-fidelity synthetic gaze dataset, which enable our ten times faster gaze network to achieve competitive accuracy to its current state-of-the-art direct competitor.", " In this paper, we study deep transfer learning as a way of overcoming object recognition challenges encountered in the field of digital pathology. Through several experiments, we investigate various uses of pre-trained neural network architectures and different combination schemes with random forests for feature selection. Our experiments on eight classification datasets show that densely connected and residual networks consistently yield best performances across strategies. It also appears that network fine-tuning and using inner layers features are the best performing strategies, with the former yielding slightly superior results.", "reinforcement learning  transfer learning  object-oriented options Reinforcement Learning is a successful yet slow technique to train autonomous agents. Option-based solutions can be used to accelerate learning and to transfer learned behaviors across tasks by encapsulating a partial policy. However, commonly these options are specific for a single task, do not take in account similar features between tasks and may not correspond exactly to an optimal behavior when transferred to another task. Therefore, unprincipled transfer might provide bad options to the agent, hampering the learning process. We here propose a way to discover and reuse learned object-oriented options in a probabilistic way in order to enable better actuation choices to the agent in multiple different tasks. Our experimental evaluation show that our proposal is able to learn and successfully reuse options across different tasks.", "Software defect prediction  Cross-project  Transfer learning Various software metrics and statistical models have been developed to help companies to predict software defects. Traditional software defect prediction approaches use historical data about previous bugs on a project in order to build predictive machine learning models. However, in many cases the historical testing data available in a project is scarce, i.e., very few or even no labeled training instances are available, which will result on a low quality defect prediction model. In order to overcome this limitation, Cross-Project Defect Prediction (CPDP) can be adopted to learn a defect prediction model for a project of interest (i.e., a target project) by reusing (transferring) data collected from several previous projects (i.e., source projects). In this paper, we focused on neighborhood-based instance selection techniques for CPDP which select labeled instances in the source projects that are similar to the unlabeled instances available in the target project. Despite its simplicity, these techniques have limitations which were addressed in our work. First, although they can select representative source instances, the quality of the selected instances is usually not addressed. Additionally, bug prediction datasets are normally unbalanced (i.e., there are more nondefect instances than defect ones), which can harm learning performance. In this paper, we proposed a new transfer learning approach for CPDP, in which instances selected by a neighborhood-based technique are filtered by the FuzzyRough Instance Selection (FRIS) technique in order to remove noisy instances in the training set. Following, in order to solve class balancing problems, the Synthetic Minority Oversampling Technique (SMOTE) technique is adopted to oversample the minority (defect-prone) class, thus increasing the chance of finding bugs correctly. Experiments were performed on a benchmark set of Java projects, achieving promising results.", " Collaborative filtering techniques are a common approach for building recommendations, and have been widely applied in real recommender systems. However, collaborative filtering usually suffers from limited performance due to the sparsity of user-item interaction. To address this issue, auxiliary information is usually used to improve the performance. Transfer learning provides the key idea of using knowledge from auxiliary domains. An assumption of transfer learning in collaborative filtering is that the source domain is a full rating matrix, which may not hold in many real-world applications. In this paper, we investigate how to leverage rating patterns from multiple incomplete source domains to improve the quality of recommender systems. First, by exploiting the transferred learning, we compress the knowledge from the source domain into a cluster-level rating matrix. The rating patterns in the low-level matrix can be transferred to the target domain. Specifically, we design a knowledge extraction method to enrich rating patterns by relaxing the full rating restriction on the source domain. Finally, we propose a robust multiple-rating-pattern transfer learning model for cross-domain collaborative filtering, which is called MINDTL, to accurately predict missing values in the target domain. Extensive experiments on real-world datasets demonstrate that our proposed approach is effective and outperforms several alternative methods.", " Nowadays, it is a heated topic for many industries to build automatic question-answering (QA) systems. A key solution to these QA systems is to retrieve from a QA knowledge base the most similar question of a given question, which can be reformulated as a paraphrase identification (PI) or a natural language inference (NLI) problem. However, most existing models for PI and NLI have at least two problems: They rely on a large amount of labeled data, which is not always available in real scenarios, and they may not be efficient for industrial applications. In this paper, we study transfer learning for the PI and NLI problems, aiming to propose a general framework, which can effectively and efficiently adapt the shared knowledge learned from a resource-rich source domain to a resource-poor target domain. Specifically, since most existing transfer learning methods only focus on learning a shared feature space across domains while ignoring the relationship between the source and target domains, we propose to simultaneously learn shared representations and domain relationships in a unified framework. Furthermore, we propose an efficient and effective hybrid model by combining a sentence encoding-based method and a sentence interaction-based method as our base model. Extensive experiments on both paraphrase identification and natural language inference demonstrate that our base model is efficient and has promising performance compared to the competing models, and our transfer learning method can help to significantly boost the performance. Further analysis shows that the inter-domain and intra-domain relationship captured by our model are insightful. Last but not least, we deploy our transfer learning model for PI into our online chatbot system, which can bring in significant improvements over our existing system.", " Networks are natural analytic tools in modeling adversarial activities (e.g., human trafficking, illicit drug production, terrorist financial transaction) using different intelligence data sources. However, such activities are often covert and embedded across multiple domains and contexts. They are generally not detectable and recognizable from the perspective of an isolated network, and only become apparent when multiple networks are analyzed in a joint manner. Thus, one of the main research topics in modeling adversarial activities is to develop effective techniques to align and fuse information from different networks into a unified representation for global analysis. Based on the combined network representation, an equally important research topic is on detecting and matching indicating patterns to recognize the underlining adversarial activities in the integrated network. Two key challenge problems involved in the modeling process include: Network alignment and merging: develop accurate and scalable methods for mapping of nodes across heterogeneous networks based on different associational and causal dependencies. Subgraph detection and matching: develop robust and efficient algorithms for richly attributed networks to support recognition of complex query patterns for networks. The focus of this workshop is to gather together the researchers from all relevant fields to share their experience and opinions on graph mining techniques in the era of big data, with emphasis on two fundamental problems - Connecting the dots and finding a needle in a haystack, in the context of graph-based adversarial activity analytics. A best paper will be selected and announced in our workshop based on the collective feedback from our reviewers. This workshop (co-located with the 11th ACM Conference on Web Search and Data Mining) aims to bring together a cross-disciplinary audience of researchers from both academia and industry to share experience with techniques, resources and best practices, and to exchange perspectives and future directions. We expect the workshop to develop a community of interested researchers and facilitate their future collaborations. Topics of Interest Data integration and alignment from multiple heterogeneous networks Novel algorithms for subgraph detection and matching in large networks Graph construction and modeling for different domains (e.g., financial fraud, human trafficking,DDoS attack) Complex anomaly (e.g., group anomaly) detection and interpretation Atypical behavior and rare event detection Limits of detectability and identifiability Evolution analysis and forecasting Game theoretic approach on anticipating opponent intents and actions Identification of novel datasets and/or evaluation metrics Multilayer and multiplex network analytics Clustering and ranking methods for composite networks Large-scaled link prediction and recommendation algorithms Community detection in big networks Information diffusion and influence maximization Interactive visualization for big graphs New methods and frontiers in spectral graph theory Analysis of network topologies (e.g., centrality and network motif analysis) Semi-supervised learning, Transductive inference, Active learning, and Transfer learning", " Unknown landscape identification is the problem of identifying an unknown landscape from a set of already provided landscape images that are considered to be known. The aim of this work is to extract the intrinsic semantic of landscape images in order to automatically generalize concepts like a stadium, roads, a parking lot etc., and use this concept to identify unknown landscapes. This problem can be easily extended to many security applications. We propose two effective semi-supervised novelty detection approaches for the unknown landscape identification problem using Convolutional Neural Network (CNN) Transfer Learning. This is based on the use of pre-trained CNNs (i.e. already trained on large datasets) already containing general image knowledge that we transfer to our domain. Our best values of AUROC and Average Precision scores for the identification problem are 0.96 and 0.94, respectively. In addition, we statistically prove that our semi-supervised methods outperform the baseline.", "Object Detection  Computer Vision  Deep Learning  Convolutional Neural Networks  Region based Convolutional Neural Network  Inception  You Only Look Once  Single Shot Detection Object detection plays a vital role in many real-world computer vision applications such as self-driving cars, human-less stores and general purpose robotic systems. Convolutional Neural Network(CNN) based Deep Learning has evolved to become the backbone of most computer vision algorithms, including object detection. Most of the research has focused on detecting objects that differ significantly e.g. a car, a person, and a bird. Achieving fine-grained object detection to detect different types within one class of objects can be crucial in tasks like automated retail checkout. This research has developed deep learning models to detect 200 types of similar birds. The models were trained and tested on CUB-200-2011 dataset. To the best of our knowledge, by attaining a mean Average Precision (mAP) of 71.5% we achieved an improvement of 5 percentage points over the previous best mAP of 66.2%.", " We apply transfer learning techniques to create topically and/or stylistically biased natural language models from small data samples, given generic long short-term memory (LSTM) language models trained on larger data sets. Although LSTM language models are powerful tools with wide-ranging applications, they require enormous amounts of data and time to train. Thus, we build general purpose language models that take advantage of large standing corpora and computational resources proactively, allowing us to build more specialized analytical tools from smaller data sets on demand. We show that it is possible to construct a language model from a small, focused corpus by first training an LSTM language model on a large corpus (e.g., the text from English Wikipedia) and then retraining only the internal transition model parameters on the smaller corpus. We also show that a single general language model can be reused through transfer learning to create many distinct special purpose language models quickly with modest amounts of data.", " A significant problem of using deep learning techniques is the limited amount of data available for training. There are some datasets available for the popular problems like item recognition and classification or self-driving cars, however, it is very limited for the industrial robotics field. In previous work, we have trained a multi-objective Convolutional Neural Network (CNN) to identify the robot body in the image and estimate 3D positions of the joints by using just a 2D image, but it was limited to a range of robots produced by Universal Robots (UR). In this work, we extend our method to work with a new robot arm - Kuka LBR iiwa, which has a significantly different appearance and an additional joint. However, instead of collecting large datasets once again, we collect a number of smaller datasets containing a few hundred frames each and use transfer learning techniques on the CNN trained on UR robots to adapt it to a new robot having different shapes and visual features. We have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects.", "Face authentication  Machine Learning  Transfer Learning The aim of this paper is to highlight differences between classical machine learning and transfer learning applied to low cost real-time face authentication. Furthermore, in an access control context, the size of biometric data should be minimized so it can be stored on a remote personal media. These constraints have led us to compare only lightest versions of these algorithms. Transfer learning applied on Mobilenet vl raises to 85% of accuracy, for a 457Ko model, with 3680s and 1.43s for training and prediction tasks. In comparison, the fastest integrated method (Random Forest) shows accuracy up to 90% for a 7,9Ko model, with a fifth of a second to be trained and a hundred of microseconds for the prediction, enabling embedded real-time face authentication at 10 fps.", " Waste management and recycling is the fundamental part a sustainable economy. For more efficient and safe recycling, it is necessary to use intelligent systems instead of employing humans as workers in the dump-yards. This is one of the early works demonstrating the efficiency of latest intelligent approaches. In order to provide the most efficient approach, we experimented on well-known deep convolutional neural network architectures. For training without any pre-trained weights, Inception-Resnet, Inception-v4 outperformed all others with 90% test accuracy. For transfer learning and fine-tuning of weight parameters using ImageNet, DenseNet121 gave the best result with 95% test accuracy. One disadvantage of these networks, however, is that they are slightly slower in prediction time. To enhance the prediction performance of the models we altered the connection patterns of the skip connections inside dense blocks. Our model RecycleNet is carefully optimized deep convolutional neural network architecture for classification of selected recyclable object classes. This novel model reduced the number of parameters in a 121 layered network from 7 million to about 3 million.", "deep learning  convolutional network  quality control system  production spoilage detection The paper presents a method of the furniture dowel quality control based on the convolutional neural network. We applied AlexNet, testing the ability of transfer learning. The paper presents details of the method and experiments performed to justify our solution on images datasets collected from real dowels production. The outcomes are compared with the results of baseline vision method. The experiments show that both methods have their strengths and weaknesses, but they complement each other.", "Bioinformatics  Cheminformatics  Computer Vision  Natural Language Processing  Transfer Learning  Weak Supervised Learning With access to large datasets, deep neural networks (DNN) have achieved human-level accuracy in image and speech recognition tasks. However, in chemistry data is inherently small and fragmented. In this work, we develop an approach of using rule-based knowledge for training ChemNet, a transferable and generalizable deep neural network for chemical property prediction that learns in a weak-supervised manner from large unlabeled chemical databases. When coupled with transfer learning approaches to predict other smaller datasets for chemical properties that it was not originally trained on, we show that ChemNet's accuracy outperforms contemporary DNN models that were trained using conventional supervised learning. Furthermore, we demonstrate that the ChemNet pre-training approach is equally effective on both CNN (Chemception) and RNN (SMILES2vec) models, indicating that this approach is network architecture agnostic and is effective across multiple data modalities. Our results indicate a pre-trained ChemNet that incorporates chemistry domain knowledge and enables the development of generalizable neural networks for more accurate prediction of novel chemical properties.", "Deep learning  sentiment analysis  visual analysis  transfer learning  natural language processing We propose a novel approach to multimodal sentiment analysis using deep neural networks combining visual analysis and natural language processing. Our goal is different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment  instead, we aim to infer the latent emotional state of the user. Thus, we focus on predicting the emotion word tags attached by users to their Tumblr posts, treating these as self-reported emotions. We demonstrate that our multi-modal model combining both text and image features outperforms separate models based solely on either images or text. Our model's results are interpretable, automatically yielding sensible word lists associated with emotions. We explore the structure of emotions implied by our model and compare it to what has been posited in the psychology literature, and validate our model on a set of images that have been used in psychology studies. Finally, our work also provides a useful tool for the growing academic study of images-both photographs and memes-on social networks.", "Dockless shared bikes  hotspots detection  urban computing  transfer learning Dockless shared bikes, which aim at providing a more flexible and convenient solution to the first-and-last mile connection, come into China and expand to other countries at a very impressing speed. The expansion of shared bike business in new cities brings many challenges among which, the most critical one is the parking chaos caused by too many bikes yet insufficient demands. To allow possible actions to be taken in advance, this paper studies the problem of detecting parking hotspots in a new city where no dockless shared bike has been deployed. We propose to measure road hotness by bike density with the help of the Kernal Density Estimation. We extract useful features from multi-source urban data and introduce a novel domain adaption network for transferring hotspots knowledge learned from one city with shared bikes to a new city. The extensive experimental results demonstrate the effectiveness of our proposed approach compared with various baselines.", "Medical image analysis  Computer-assisted detection  Machine learning  Transfer learning  Negative transfer The reading workload for radiologists is increasing because the numbers of examinations and images per examination are increasing due to the technical progress on imaging modalities such as computed tomography and magnetic resonance imaging. A computer-assisted detection (CAD) system based on machine learning is expected to assist radiologists. The preliminary results of a multi-institutional study indicate that the performance of the CAD system for each institution improved using training data of other institutions. This indicates that transfer learning may be useful for developing the CAD systems among multiple institutions. In this paper, we focus on transfer learning without sharing training data due to the need to protect personal information in each institution. Moreover, we raise a problem of negative transfer in CAD system and propose an algorithm for inhibiting negative transfer. Our algorithm provides a theoretical guarantee for managing CAD software in terms of transfer learning and exhibits experimentally better performance compared to that of the current algorithm in cerebral aneurysm detection.", "atrial fibrillation  convolutional neural network  recurrent neural network  deep learning  transfer learning Detection of atrial fibrillation (AF), a type of cardiac arrhythmia, is difficult since many cases of AF are usually clinically silent and undiagnosed. In particular paroxysmal AF is a form of AF that occurs occasionally, and has a higher probability of being undetected. In this work, we present an attention based deep learning framework for detection of paroxysmal AF episodes from a sequence of windows. Time-frequency representation of 30 seconds recording windows, over a 10 minute data segment, are fed sequentially into a deep convolutional neural network for image-based feature extraction, which are then presented to a bidirectional recurrent neural network with an attention layer for AF detection. To demonstrate the effectiveness of the proposed framework for transient AF detection, we use a database of 24 hour Holter Electrocardiogram (ECG) recordings acquired from 2850 patients at the University of Virginia heart station. The algorithm achieves an AUC of 0.94 on the testing set, which exceeds the performance of baseline models. We also demonstrate the cross-domain generalizablity of the approach by adapting the learned model parameters from one recording modality (ECG) to another (photoplethysmogram) with improved AF detection performance. The proposed high accuracy, low false alarm algorithm for detecting paroxysmal AF has potential applications in long-term monitoring using wearable sensors.", "Transfer learning  cross-lingual  subgraph isomorphism Transfer learning has gained increasing attention due to the inferior performance of machine learning algorithms with insufficient training data. Most of the previous homogeneous or heterogeneous transfer learning works aim to learn a mapping function between feature spaces based on the inherent correspondence across the source and target domains or labeled instances. However, in many real world applications, existing methods may not be robust when the correspondence across domains is noisy or labeled instances are not representative. In this paper, we develop a novel transfer learning framework called Transfer Learning via Feature Isomorphism Discovery (abbreviated to TLFid), which owns high tolerance for noisy correspondence between domains as well as scarce or non-existing labeled instances. More specifically, we propose a feature isomorphism approach to discovering common substructures across feature spaces and learning a feature mapping function from the target domain to the source domain. We evaluate the performance of TLFid on the cross-lingual sentiment classification tasks. The results show that our method achieves significant improvement in terms of accuracy compared with the state-of-the-art methods.", "Transfer learning  Semi-supervised learning  Concept drift We propose a method for learning the dynamics of the decision boundary to maintain classification performance without additional labeled data. In various applications, such as spam-mail classification, the decision boundary dynamically changes over time. Accordingly, the performance of classifiers deteriorates quickly unless the classifiers are retrained using additional labeled data. However, continuously preparing such data is quite expensive or impossible. The proposed method alleviates this deterioration in performance by using newly obtained unlabeled data, which are easy to prepare, as well as labeled data collected beforehand. With the proposed method, the dynamics of the decision boundary is modeled by Gaussian processes. To exploit information on the decision boundaries from unlabeled data, the low-density separation criterion, i.e., the decision boundary should not cross high-density regions, but instead lie in low-density regions, is assumed with the proposed method. We incorporate this criterion into our framework in a principled manner by introducing the entropy posterior regularization to the posterior of the classifier parameters on the basis of the generic regularized Bayesian framework. We developed an efficient inference algorithm for the model based on variational Bayesian inference. The effectiveness of the proposed method was demonstrated through experiments using two synthetic and four real-world data sets.", "equivalence structure  transfer learning  imitation learning Equivalence structure (ES) extraction can allow for finding correspondence relations between different sequential datasets. A K-dimensional ES is a set of K-tuples to specify K-dimensional sequences that are considered equivalent. Whether or not two K-dimensional sequences are equivalent is decided based on comparisons of all of their subsequences. ES extraction can be used for preprocessing for transfer learning or imitation learning, as well as an analysis of multidimensional sequences. A recently proposed method called incremental search (IS) was much faster than brute-force search. However, IS can still take a long time to obtain ESs, because ESs obtained by IS can be subsets of other ESs and such subsets must be removed in the process. In this paper, we propose a new fast method called pairwise incremental search (PIS). In the process of PIS, the aforementioned problem about subsets of ESs does not exist, because the elements of ESs are searched pairwise. As shown by results of two experiments we conducted, PIS was 48 times faster than IS in an experiment using synthetic datasets and 171 times faster in an experiment using motion capture datasets.", "multi-source clustering  Laplace operator  spectral learning  multi-view spectral embedding The research and analysis on multi-source data is one of important tasks in information science. Compared with traditional single-source data learning algorithms, multi-source data learning ones can describe objects more real and complete. Meanwhile, the learning process of multi-source data is more in line with the cognitive mechanism of human brain. So far, the research on multi-source data learning algorithms includes three classes, multi-source data transfer learning, multi-source data collaborative learning and multi-source multi-view learning. The traditional multi-source multi-view learning algorithms lack the ability of handling with the data missing issue, which means that these algorithms require the multi-source data to be complete. This paper proposes a multi-source clustering algorithm. Based on the spectral properties of Laplace operator, we first obtain the complete representation of multi-source data. Then, we utilize the multi-view spectral embedding (MVSE) to construct the fusion model. Experimental results show that our proposed method can improve the ability of clustering efficiently in the case of data missing.", " Deep architectures can now be well trained on massive labeled data. However, there exist many application scenarios where labeled data are sparse or absent. Domain adaptation and multi-task transfer learning provide attractive options when related labeled data or tasks are abundant from different domains. In this paper, a new graphical modeling approach to multi-layer factorization based domain adaptation is explored to address the scenarios that insufficient labeled data are available for supervised learning. A deep convolutional factorization based transfer learning (DCFTL) algorithm is proposed to facilitate layer-wise transfer learning between domains. Completely based on graphical model representation, the proposed framework can seamlessly merge inference and learning, and has clear interpretability of conditional independence. The empirical performances on image classification tasks in both supervised and semi-supervised adaptation settings illustrate the effectiveness and generalization of the proposed deep layered knowledge transfer framework.", " In this work we present a method to improve the pruning step of the current state-of-the-art methodology to compress neural networks. The novelty of the proposed pruning technique is in its differentiability, which allows pruning to be performed during the backpropagation phase of the network training. This enables an end-to-end learning and strongly reduces the training time. The technique is based on a family of differentiable pruning functions and a new regularizer specifically designed to enforce pruning. The experimental results show that the joint optimization of both the thresholds and the network weights permits to reach a higher compression rate, reducing the number of weights of the pruned network by a further 14% to 33% compared to the current state-of-the-art. Furthermore, we believe that this is the first study where the generalization capabilities in transfer learning tasks of the features extracted by a pruned network are analyzed. To achieve this goal, we show that the representations learned using the proposed pruning methodology maintain the same effectiveness and generality of those learned by the corresponding non-compressed network on a set of different recognition tasks.", "remote sensing  superpixel segmentation  convolutional neural network  transfer learning  feature selection  semi-supervised learning In this paper, we present a novel terrain classification framework for large-scale remote sensing images. A well-performing multi-scale superpixel tessellation based segmentation approach is employed to generate homogeneous and irregularly shaped regions, and a transfer learning technique is sequentially deployed to derive representative deep features by utilizing successful pre-trained convolutional neural network (CNN) models. This design is aimed to overcome the big problem of lacking available ground-truth data and to increase the generalization power of the multi-pixel descriptor. In the subsequent classification step, we train a fast and robust support vector machine (SVM) to assign the pixel-level labels. Its maximum-margin property can be easily combined with a graph Laplacian propagation approach. Moreover, we analyze the advantages of applying a feature selection technique to the deep CNN features which are extracted by transfer learning. In the experiments, we evaluate the whole framework based on different geographical types. Compared with other region-based classification methods, the results show that our framework can obtain state-of-the-art performance w.r.t. both classification accuracy and computational efficiency.", "domain adaptation  multi-source transfer learning  common subspace learning  face recognition For transfer learning, many research works have demonstrated that effective use of information from multi-source domains will improve classification performance. In this paper, we propose a method of Targetize Multi-source Domain Bridged by Common Subspace (TMSD) for face recognition, which transfers rich supervision knowledge from more than one labeled source domains to the unlabeled target domain. Specifically, a common subspace is learnt for several domains by keeping the maximum total correlation. In this way, the discrepancy of each domain is reduced, and the structures of both the source and target domains are well preserved for classification. In the common subspace, each sample projected from the source domains is sparsely represented as a linear combination of several samples projected from the target domain, such that the samples projected from different domains can be well interlaced. Then, in the original image space, each source domain image can be represented as a linear combination of neighbors in the target domain. Finally, the discriminant subspace can be obtained by targetized multi-source domain images using supervised learning algorithm. The experimental results illustrate the superiority of TMSD over those competitive ones.", " A major impediment to the application of deep learning to real-world problems is the scarcity of labeled data. Small training sets are in fact of no use to deep networks as, due to the large number of trainable parameters, they will very likely be subject to overfitting phenomena. On the other hand, the increment of the training set size through further manual or semi-automatic labellings can be costly, if not possible at times. Thus, the standard techniques to address this issue are transfer learning and data augmentation, which consists of applying some sort of transformation to existing labeled instances to let the training set grow in size. Although this approach works well in applications such as image classification, where it is relatively simple to design suitable transformation operators, it is not obvious how to apply it in more structured scenarios. Motivated by the observation that in virtually all application domains it is easy to obtain unlabeled data, in this paper we take a different perspective and propose a label augmentation approach. We start from a small, curated labeled dataset and let the labels propagate through a larger set of unlabeled data using graph transduction techniques. This allows us to naturally use (second-order) similarity information which resides in the data, a source of information which is typically neglected by standard augmentation techniques. In particular, we show that by using known game theoretic transductive processes we can create larger and accurate enough labeled datasets which use results in better trained neural networks. Preliminary experiments are reported which demonstrate a consistent improvement over standard image classification datasets.", " Existing techniques for satellite-based tropical cyclone (TC) intensity estimation involve an explicit feature extraction step to model TC intensity on a set of relevant TC features or patterns such as eye formation and cloud organization. However, crafting such a feature set is often time-consuming and requires expert knowledge. In this paper, a convolutional neural network (CNN) approach, which eliminates explicit feature extraction, for estimating the intensity of tropical cyclones is proposed. Utilizing a Visual Geometry Group 19-layer CNN (VGG19) model pre-trained on ImageNet, transfer learning experiments were performed using grayscale IR images of TCs obtained from various geostationary satellites in the Western North Pacific region (1996 - 2016) to estimate TC intensity. The model re-trained on TC images achieved a root-mean-square error (RMSE) of 13.23 knots - a performance comparable to existing feature-based approaches (RMSE ranging from 12 to 20 knots). Moreover, the model was able to learn generic TC features that were previously identified in feature-based approaches as important indicators of TC intensity.", "document structure learning  deep convolutional neural network  document recognition  deep learning  transfer learning  intra-domain  neural network In this article, a region-based Deep Convolutional Neural Network framework is presented for document structure learning. The contribution of this work involves efficient training of region based classifiers and effective ensembling for document image classification. A primary level of 'inter-domain' transfer learning is used by exporting weights from a pre-trained VGG16 architecture on the ImageNet dataset to train a document classifier on whole document images. Exploiting the nature of region based influence modelling, a secondary level of 'intra-domain' transfer learning is used for rapid training of deep learning models for image segments. Finally, a stacked generalization based ensembling is utilized for combining the predictions of the base deep neural network models. The proposed method achieves state-of-the-art accuracy of 92.21% on the popular RVL-CDIP document image dataset, exceeding the benchmarks set by the existing algorithms.", " The use of deep learning techniques for automatic facial expression recognition has recently attracted great interest but developed models are still unable to generalize well due to the lack of large emotion datasets for deep learning. To overcome this problem, in this paper, we propose utilizing a novel transfer learning approach relying on PathNet and investigate how knowledge can be accumulated within a given dataset and how the knowledge captured from one emotion dataset can be transferred into another in order to improve the overall performance. To evaluate the robustness of our system, we have conducted various sets of experiments on two emotion datasets: SAVEE and eNTERFACE. The experimental results demonstrate that our proposed system leads to improvement in performance of emotion recognition and performs significantly better than the recent state-of-the-art schemes adopting fine-tuning/pre-trained approaches.", " Remote photoplethysmography (rPPG) based non-contact heart rate (HR) measurement from a face video has drawn increasing attention recently because of its potential applications in many scenarios such as training aid, health monitoring, and nursing care. Although a number of methods have been proposed, most of them are designed under certain assumptions and could fail when such assumptions do not hold. At the same time, while deep learning based methods have been reported to achieve promising results in many computer vision tasks, their use in rPPG-based heart rate estimation has been limited due to the very limited data available in public domain. To overcome this limitation and leverage the strong modeling ability of deep neural networks, in this paper, we propose a novel spatial-temporal representation for the HR signal and design a general-to-specific transfer learning strategy to train a deep heart rate estimator from a large volume of synthetic rhythm signals and a limited number of available face video data. Experiment results on the public-domain databases show the effectiveness of the proposed approach.", "Gaussian Process  Domain Adaptation  Transfer Learning  Homogeneous Traditional machining learning method aims at using the labeled data or unlabeled data to train a mathematic model then it can be used to predict the unlabeled data for Data mining problem, but it requires that the data which be trained should have same distribution with the predicting data. For the real world datasets, it is hard to get enough training datasets which has the same distribution. Thus, how to train a good mathematic model by using different distribution data is crucial problem, and the researchers using the probability view to solve transfer classification problem is relative less. In this paper, we propose a transfer classification algorithm based on the Gaussian Process model, which can be used to solve the homogeneous transfer classification problem. We use the probability theory to propose a novel classification transfer learning model based on the Gaussian Process (GP) model. We experiment on the synthetic and real-world datasets and compare to other method, the result has verified the effectiveness of our approach.", "Forward learning  Convolutional neural network  Transfer learning  Extreme learning machine A conventional convolutional neural network (CNN) is trained by back-propagation (BP) from output layer to input layer through the entire network. In this paper, we propose a novel training approach such that CNN can be trained in forward way unit by unit. For example, we separate a CNN network with three convolutional layers into three units. Each unit contains one convolutional layer and will be trained one by one in sequence. Experiments shows that training can be restricted in local unit and processed one by one from input to output. In most cases, our novel feed forward approach has equal or better performance compared to the traditional approach. In the worst case, our novel feed forward approach is inferior to the traditional approach less than 5% accuracy. Our training approach also obtains benefits from transfer learning by setting different targets for middle units. As the full network back propagation is unnecessary, BP learning becomes more efficiently and least square method can be applied to speed learning. Our novel approach gives out a new focus on training methods of convolutional neural network.", "Handwriting Recognition  Language Model  Transfer Learning  Bootstrap  Historical Document Analysis Not all languages and domains of handwriting have large labeled datasets available for training handwriting recognition (HWR) models. One way to address this problem is to leverage high resource languages to help train models for low resource languages. In this work, we adapt HWR models trained on a source language to a target language that uses the same writing script. We do so using only labeled data in the source language, unlabeled data in the target language, and a language model in the target language. The language model is used to produce target transcriptions to allow regular example based training. Using this approach we demonstrate improved transferability among French, English, and Spanish languages using both historical and modern handwriting datasets.", "Handwriting recognition  knowledge transfer  Optical model  Language model Lack of data can be an issue when beginning a new study on historical handwritten documents. To deal with this, we propose a deep-learning based recognizer which separates the optical and the language models in order to train them separately using different resources. In this work, we present the optical encoder part of a multilingual transductive transfer learning applied to historical handwriting recognition. The optical encoder transforms the input word image into a non-latent space that depends only on the letter-n-grams: it enables it to be independent of the language. This transformation avoids embedding a language model and operating the transfer learning across languages using the same alphabet. The language decoder creates from a vector of letter-n-grams a word as a sequence of characters. Experiments show that separating optical and language model can be a solution for multilingual transfer learning.", "Air-writing  human-computer interaction  gesture recognition  handwritten character recognition  convolutional neural networks Air-writing refers to virtually writing linguistic characters through hand gestures in three dimensional space with six degrees of freedom. In this paper a generic video camera dependent convolutional neural network (CNN) based air-writing framework has been proposed. Gestures are performed using a marker of fixed color in front of a generic video camera followed by color based segmentation to identify the marker and track the trajectory of marker tip. A pre-trained CNN is then used to classify the gesture. The recognition accuracy is further improved using transfer learning with the newly acquired data. The performance of the system varies greatly on the illumination condition due to color based segmentation. In a less fluctuating illumination condition the system is able to recognize isolated unistroke numerals of multiple languages. The proposed framework achieved 97.7%, 95.4% and 93.7% recognition rate in person independent evaluation over English, Bengali and Devanagari numerals, respectively.", " In this paper we deal with the offline handwriting text recognition (HTR) problem with reduced training data sets. Recent HTR solutions based on artificial neural networks exhibit remarkable solutions in referenced databases. These deep learning neural networks are composed of both convolutional (CNN) and long short-term memory recurrent units (LSTM). In addition, connectionist temporal classification (CTC) is the key to avoid segmentation at character level, greatly facilitating the labeling task. One of the main drawbacks of the CNN-LSTM-CTC (CRNN) solutions is that they need a considerable part of the text to be transcribed for every type of calligraphy, typically in the order of a few thousands of lines. Furthermore, in some scenarios the text to transcribe is not that long, e.g. in the Washington database. The CRNN typically overfits for this reduced number of training samples. Our proposal is based on the transfer learning (TL) from the parameters learned with a bigger database. We first investigate, for a reduced and fixed number of training samples, 350 lines, how the learning from a large database, the IAM, can be transferred to the learning of the CRNN of a reduced database, Washington. We focus on which layers of the network could not be re-trained. We conclude that the best solution is to re-train the whole CRNN parameters initialized to the values obtained after the training of the CRNN from the larger database. We also investigate results when the training size is further reduced. For the sake of comparison, we study the character error rate (CER) with no dictionary or any language modeling technique. The differences in the CER are more remarkable when training with just 350 lines, a CER of 3.3% is achieved with TL while we have a CER of 18.2% when training from scratch. As a byproduct, the learning times are quite reduced. Similar good results are obtained from the Parzival database when trained with this reduced number of lines and this new approach.", "transfer learning  metric learning  marginalized denoising  low-rank constraint Most commonly used metric learning procedures suppose that the input feature space and domain of the training and test data are identical. In such cases these algorithms cannot improve target learning problems. This paper presents a robust distance metric for domain adaptation in two stages. At first stage both source and target features are transferred to a newly found latent feature space, which minimizes the difference between domains as well as the data properties are preserved. Then in the second stage, the desired metric is learned with a marginalized denoising strategy and the low-rank constraint. To show the superiority and power of the proposed method it is tested on distinct kinds of cross-domain image categorization datasets and the results prove that our approach remarkably exceeds other existing domain adaptation algorithms in the classification tasks.", "machine learning  intrusion detection  transfer learning  training samples In the past decades, machine learning based intrusion detection systems have been developed. This paper discloses a new aspect of machine learning based intrusion detection systems. The proposed method detects normal and anomaly behaviors in the desired network where there are not any labeled samples as training dataset. That is while a plenty of labeled samples may exist in another network that is different from the desired network. Because of the difference between two networks, their samples produce in different manners. So, direct utilizing of labeled samples of a different network as training samples does not provide acceptable accuracy to detect anomaly behaviors in the desired network. In this paper, we propose a transfer learning based intrusion detection method which transfers knowledge between the networks and eliminates the problem of providing training samples that is a costly procedure. Comparing the experimental results with the results of a basic machine learning method (SVM) and also baseline method(DAMA) shows the effectiveness of the proposed method for transferring knowledge for intrusion detection systems.", "Transfer Learning  Artificial Neural Networks  Face Verification  Metric Learning Features extracted with deep learning have now achieved state-of-the-art results in many tasks. However, to reuse a learned deep model, transfer learning with fine-tuning needs to be employed, which requires to re-train the whole model or part of it to extract useful features in the new domain. This step is burdensome and requires heavy computing power. Therefore, this work investigates alternatives in transfer-learning that do not involve performing fine-tuning for a model with the new domain. Namely, we explore the correlation of depth and scale in deep models, and look for the layer/scale that yields the best results for the new domain, we also explore metrics for the verification task, using locally connected convolutions to learn distance metrics. Our experiments use a model pre-trained in face identification and adapt it to the face verification task with different data, but still on the face domain. We achieve 96.65% mean accuracy on the Labeled Faces in the Wild dataset and 93.12% mean accuracy on the Youtube Faces dataset comparable to the state-of-the-art.", "FER  GAN  Identity-adaptive  CNN Subject variation is a challenging issue for facial expression recognition, especially when handling unseen subjects with small-scale lableled facial expression databases. Although transfer learning has been widely used to tackle the problem, the performance degrades on new data. In this paper, we present a novel approach (so-called LA-gen) to alleviate the issue of subject variations by regenerating expressions from any input facial images. First of all, we train conditional generative models to generate six prototypic facial expressions from any given query face image while keeping the identity related information unchanged. Generative Adversarial Networks are employed to train the conditional generative models, and each of them is designed to generate one of the prototypic facial expression images. Second, a regular CNN (FER-Net) is fine-tuned for expression classification. After the corresponding prototypic facial expressions are regenerated from each facial image, we output the last EC layer of FER-Net as features for both the input image and the generated images. Based on the minimum distance between the input image and the generated expression images in the feature space, the input image is classified as one of the prototypic expressions consequently. Our proposed method can not only alleviate the influence of inter-subject variations, but will also be flexible enough to integrate with any other FER CNNs for person-independent facial expression recognition. Our method has been evaluated on CK+, Oulu-CASIA, BU-3DFE and BU-4DFE databases, and the results demonstrate the effectiveness of our proposed method.", " Face recognition is challenge task which involves determining the identity of facial images. With availability of a massive amount of labeled facial images gathered from Internet, deep convolution neural networks(DCNNs) have achieved great success in face recognition tasks. Those images are gathered from unconstrain environment, which contain people with different ethnicity, age, gender and so on. However, in the actual application scenario, the target face database may be gathered under different conditions compered with source training dataset, e.g. different ethnicity, different age distribution, disparate shooting environment. These factors increase domain discrepancy between source training database and target application database and make the learnt model degenerate in target database. Meanwhile, for the target database where labeled data are lacking or unavailable, directly using target data to fine-tune pre-learnt model becomes intractable and impractical. In this paper, we adopt unsupervised transfer learning methods to address this issue. To alleviate the discrepancy between source and target face database and ensure the generalization ability of the model, we constrain the maximum mean discrepancy (MMD) between source database and target database and utilize the massive amount of labeled facial images of source database to training the deep neural network at the same time. We evaluate our method on two face recognition benchmarks and significantly enhance the performance without utilizing the target label.", "micro expression recognition  deep learning  trasnfer learning This paper presents the methods used in our submission to 2018 Facial Micro-Expression Grand Challenge (MEGC). The object of the challenge is to recognize micro-expression in two provided databases, including holdout-database recognition and composite database recognition. Considering the small size of the databases, we follow a rout of transfer learning to implement convolutional neural network to recognize the micro expression. ResNetlO pre-trained on ImageNet dataset was fine-tuned on macro-expression datasets with large size and then on the provided micro-expression datasets. Experimental results show that the method can achieve weighted average recall (WAR) of 0.561 and unweighted average recall (UAR) of 0.389 in Holdout-database Evaluation Task, and F1 Score of 0.64 in Composite Database Evaluation Task, which are much higher than what baseline methods (LBP-TOP, HOOF, HOG3D) can achieve.", "smile detection  CNN  deep learning  transfer learning Smile detection from unconstrained facial images is a specialized and challenging problem. As one of the most informative expressions, smiles convey basic underlying emotions, such as happiness and satisfaction, and leads to multiple applications, such as human behavior analysis and interactive controlling. Compared to the size of databases for face recognition, far less labeled data is available for training smile detection systems. This paper proposes an efficient transfer learning-based smile detection approach to leverage the large amount of labeled data from face recognition datasets and to alleviate overfitting on smile detection. A well-trained deep face recognition model is explored and fine-tuned for smile detection in the wild, unlike previous works which use either hand-engineered features or train deep convolutional networks from scratch. Three different models are built as a result of fine-tuning the face recognition model with different inputs, including aligned, unaligned and grayscale images generated from the GENKI-4K dataset. Experiments show that the proposed approach achieves improved state-of-the-art performance. Robustness of the model to noise and blur artifacts is also evaluated in this paper.", "incipient fault diagnosis  DNN  transfer learning Diagnosis of incipient fault is critical for safe operation of the system because it can prevent disastrous accidents from happening by diagnosing the early fault before deterioration. Deep learning is efficient in feature extraction but it requires a large number of samples to train traditional deep neural network (DNN). It is thus inevitable that the efficiency of DNN will be affected when it is applied to incipient fault diagnosis for there are usually a very limited number of incipient fault samples. Furthermore, a large amount of information involved in significant fault samples was not adequately used for incipient fault diagnosis. To solve this problem, this paper proposes an incipient fault diagnosis model with DNN-based transfer learning. The model can extract fault feature involved in a large number of significant fault samples and apply it to extract insignificant fault feature with a small number of incipient fault samples. In this way, the proposed transfer learning method can efficiently diagnose incipient fault in the case when only a limited number of incipient fault data is available. The efficiency of the proposed model is demonstrated by utilizing the Case Western Reserve University bearing data set.", "end-to-end  expressive speech  multi-speaker speech synthesis  transfer learning  emphatic speech End-to-end text-to-speech (E2E TTS) synthesis has achieved great success. This work investigates the emphatic speech synthesis and control mechanisms in the E2E framework and proposes an E2E-based method for transferring emphasis characteristic between speakers. Characteristic differences between emphatic and neutral speech are learned from a small-scale corpus containing parallel neutral and emphasis speech utterances recorded by one speaker and further transferred to another speaker so that we can generate emphatic speech with latter speakers voice. Emphasis embedding is injected to the encoder of the extended E2E TTS model to capture the aforementioned differences  while the decoder and attention module are used to decode those differences into synthetic neutral / emphatic speech. Speaker codes linked to the decoder and attention module provide the E2E model the ability for characteristic transferring between speakers. To control the emphatic strength, an encoder memory manipulation mechanism is proposed. Experimental results indicate the effectiveness of our proposed model.", "reinforcement learning  policy gradient financial trading  transfer learning  strategy cloning Investment decision making is considered as a series of complicated processes, which are difficult to be analyzed and imitated. Given large amounts of trading records with rich expert knowledge in financial domain, extracting its original decision logics and cloning the trading strategies are also quite challenging. In this paper, an agent-based reinforcement learning (RL) system is proposed to mimic professional trading strategies. The concept of continuous Markov decision process (MDP) in RL is similar to the trading decision making in financial time series data. With the specific-designed RL components, including states, actions, and rewards for financial applications, policy gradient method can successfully imitate the expert's strategies. In order to improve the convergence of RL agent in such highly dynamic environment, a pre-trained model based on supervised learning is transferred to the deep policy networks. The experimental results show that the proposed system can reproduce around eighty percent trading decisions both in training and testing stages. With the discussion of the tradeoff between explorations and model updating, this paper tried to fine-tuning the system parameters to get reasonable results. Finally, an advanced strategy is proposed to dynamically adjust the number of explorations in each episode to achieve better results.", "Autonomous Driving  Deep Reinforcement Learning  Distributed Machine Learning  Cloud Computing  Simulation This paper proposes an architecture for leveraging cloud computing technology to reduce training time for deep reinforcement learning models for autonomous driving by distributing the training process across a pool of virtual machines. By parallelizing the training process, careful design of the reward function and use of techniques like transfer learning, we demonstrate a decrease in training time for our example autonomous driving problem from 140 hours to less than 1 hour. We go over our network architecture, job distribution paradigm, reward function design and report results from experiments on small sized cluster (1-6 training nodes) of machines. We also discuss the limitations of our approach when trying to scale up to massive clusters.", " The gold standard for malaria diagnosis still remains to be microscopy. However, cases from remote areas needing immediate diagnosis and treatment can benefit from a faster diagnostic process. Several intelligent systems for malaria diagnosis have been proposed using different computer vision techniques. In this research, models using convolutional neural networks, and a model using extracted shape features are implemented and compared. The CNN models, one trained from scratch and the other utilizing transfer learning, with accuracies of 92.4% and 93.60%, both outperform the shape feature model in malaria parasite recognition.", "Times Roman  image area  acronyms  references Fast obstacle detection is essential for autonomous driving. In this research, we have developed an obstacle detection model using Single Shot Multi Box Detector. SSD is a regression-based object detecting convolutional neural network that takes images as an input to compute localization and classification at once. By using SSD, processing time is dramatically reduced compare to multi shot detector. SSD object detection model was trained using APIs provided by Google in different patterns of number of classes and availability of transfer learning. Increase of the number of classes tended to decrease the detection rate. Training with transfer learning increased the average precision in general. The effectiveness of transfer learning in image recognition can be confirmed. Also there is a difference in average precision depending on the class.", "Minimiax  Imbalanced Classification  Intersection of K Hyperplanes  Transfer Learning In this work we consider non-linear classifiers that comprise intersections of hyperplanes. We learn these classifiers by minimizing the minimax bound over the negative training examples and the hinge type loss of the positive training examples. These classifiers fit typical real-life datasets that consist of a small number of positive data points and a large number of negative data points. Such an approach is computationally appealing since the majority of training examples (belonging to the negative class) are represented by the statistics of their distribution, which is used in a single constraint on the empirical risk, as opposed to SVM, in which the number of variables is equal to the size of the training set. We first focus on intersection of K hyperplanes, for which we provide empirical risk bounds. We show that these bounds are dimensionally independent and decay as K/root m for m samples. We then extend the K-hyperplane mixed risk to the latent mixed risk for training a union of C K-hyperplane models, which can form an arbitrary complex, piecewise linear boundaries. We propose efficient algorithms for training the proposed models. Finally, we show how to combine hinge-minimax training with deep architectures and extend it to multi-class settings using transfer learning. The empirical evaluation of the proposed models shows their advantage over the existing methods in a small training labeled data regime.", "Multi-class problems  face recognition  object recognition  transfer learning  nonparametric models The difficulty of multi-class classification generally increases with the number of classes. Using data for a small set of the classes, can we predict how well the classifier scales as the number of classes increases ? We propose a framework for studying this question, assuming that classes in both sets are sampled from the same population and that the classifier is based on independently learned scoring functions. Under this framework, we can express the classification accuracy on a set of k classes as the (k-1)st moment of a discriminability function  the discriminability function itself does not depend on k. We leverage this result to develop a non-parametric regression estimator for the discriminability function, which can extrapolate accuracy results to larger unobserved sets. We also formalize an alternative approach that extrapolates accuracy separately for each class, and identify tradeoffs between the two methods. We show that both methods can accurately predict classifier performance on label sets up to ten times the size of the original set, both in simulations as well as in realistic face recognition or character recognition tasks.", " There are several biometric-based systems which rely on a single biometric modality, most of them focus on face, iris or fingerprint. Despite the good accuracies obtained with single modalities, these systems are more susceptible to attacks, i.e, spoofing attacks, and noises of all kinds, especially in non-cooperative (in-the-wild) environments. Since noncooperative environments are becoming more and more common, new approaches involving multi-modal biometrics have received more attention. One challenge in multimodal biometric systems is how to integrate the data from different modalities. Initially, we propose a deep transfer learning optimized from a model trained for face recognition achieving outstanding representation for only iris modality. Our feature level fusion by means of features selection targets the use of the Particle Swarm Optimization (PSO) for such aims. In our pool, we have the proposed iris fine-tuned representation and a periocular one from previous work of us. We compare this approach for fusion in feature level against three basic function rules for matching at score level: sum, multi, and min. Results are reported for iris and periocular region (NICE. II competition database) and also in an open-world scenario. The experiments in the NICE. II competition databases showed that our transfer learning representation for iris modality achieved a new state-of-the-art, i.e., decidability of 2.22 and 14.56% of EER. We also yielded a new state-of-the-art result when the fusion at feature level by PSO is done on periocular and iris modalities, i.e., decidability of 3.45 and 5.55% of EER.", "transfer learning  domain adaptation  fuzzy features  machine learning Domain adaptation can transfer knowledge from the source domain to improve pattern recognition accuracy in the target domain. However, it is rarely discussed when the target domain is unlabeled and heterogeneous with the source domain, which is a very challenging problem in the domain adaptation field. This paper presents a new feature reconstruction method: unconstrained fuzzy feature fusion. Through the reconstructed features of a source and a target domain, a geodesic flow kernel is applied to transfer knowledge between them. Furthermore, the original information of the target domain is also preserved when reconstructing the features of the two domains. Compared to the previous work, this work has two advantages: 1) the sum of the memberships of the original features to fuzzy features no longer must be one, and 2) the original information of the target domain is persevered. As a result of these advantages, this work delivers a better performance than previous studies using two public datasets.", "Bayesian transfer learning  fully probabilistic design  incomplete modelling  Kalman filtering Transfer learning is a framework that includes-among other topics-the design of knowledge transfer mechanisms between Bayesian filters. Transfer learning strategies in this context typically rely on a complete stochastic dependence structure being specified between the participating learning procedures (filters). This paper proposes a method that does not require such a restrictive assumption. The solution in this incomplete modelling case is based on the fully probabilistic design of an unknown probability distribution which conditions on knowledge in the form of an externally supplied distribution. We are specifically interested in the situation where the external distribution accumulates knowledge dynamically via Kalman filtering. Simulations illustrate that the proposed algorithm outperforms alternative methods for transferring this dynamic knowledge from the external Kalman filter.", "Image classification  Convolutional neural network  Transfer learning  Animal behavior analysis  Hummingbird detection The analysis of natural images has been the topic of research in uncountable articles in computer vision and pattern recognition (e.g., natural images has been used as benchmarks for object recognition and image retrieval). However, despite the research progress in such field, there is a gap in the analysis of certain type of natural images, for instance, those in the context of animal behavior. In fact, biologists perform the analysis of natural images manually without the aid of techniques that were supposedly developed for this purpose. In this context, this paper presents a study on automated methods for the analysis of natural images of hummingbirds with the goal to assist biologists in the study of animal behavior. The automated analysis of hummingbird behavior is challenging mainly because of (1) the speed at which these birds move and interact  (2) the unpredictability of their trajectories  and (3) its camouflage skills. We report a comparative study of two deep learning approaches for the detection of hummingbirds in their nest. Two variants of transfer learning from convolutional neural networks (CNNs) are evaluated in real imagery for hummingbird behavior analysis. Transfer learning is adopted because not enough images are available for training a CNN from scratch, besides, transfer learning is less time consuming. Experimental results are encouraging, as acceptable classification performance is achieved with CNN-based features. Interestingly, a pretrained CNN without fine tunning and a standard classifier performed better in the considered data set.", " Domain adaptation, where no labeled target data is available, is a challenging task. To solve this problem, we first propose a new SVM based approach with a supplementary Maximum Mean Discrepancy (MMD)-like constraint. With this heuristic, source and target data are projected onto a common subspace of a Reproducing Kernel Hilbert Space (RKHS) where both data distributions are expected to become similar. Therefore, a classifier trained on source data might perform well on target data, if the conditional probabilities of labels are similar for source and target data, which is the main assumption of this paper. We demonstrate that adding this constraint does not change the quadratic nature of the optimization problem, so we can use common quadratic optimization tools. Secondly, using the same idea that rendering source and target data similar might ensure efficient transfer learning, and with the same assumption, a Kernel Principal Component Analysis (KPCA) based transfer learning method is proposed. Different from the first heuristic, this second method ensures other higher order moments to be aligned in the RKHS, which leads to better performances. Here again, we select MMD as the similarity measure. Then, a linear transformation is also applied to further improve the alignment between source and target data. We finally compare both methods with other transfer learning methods from the literature to show their efficiency on synthetic and real datasets.", "Deep autoencoders  Knowledge transfer  Hierarchical dataset  Corrupted dataset Deep Transfer Learning or DTS has proven successful with deep neural networks and deep belief networks. However, there has been limited research on to using deep autoencoder (DAE)-based network to implement DTS. This paper for the first time attempts to identify transferable features in the form of learning and transfer them to another network implementing a simple DTS mechanism. In this paper, a transfer of knowledge process is proposed where in knowledge is transferred from one Deep autoencoder network to another. This knowledge transfer has helped to improve the classification accuracy of the receiving autoencoder, particularly when experimented using corrupted dataset. The experiments are carried out on a texa based hierarchical dataset. Firstly, a DAE is trained with regular undamaged dataset to achieve maximum accuracy. Then, a distorted dataset was used to train second DAEN for classification with which only 56.7% of the data is correctly classified. Then a set of weights are transferred from from first DAEN to the second DAEN which resulted in an an improvement of classification accuracy by about 22%. The key contribution of this paper is highlighting importance of knowledge transfer between two deep autoencoder networks which is proposed for the first time.", "component  deep learning  synthetic aperture radar  automatic target recognition In this paper we propose a new approach for Synthetic Aperture Radar (SAR) automatic target recognition (ATR). One of the main obstacles in SAR ATR is the limited availability of datasets that are used for training. In this paper, a deep learning approach is employed for ATR. The proposed scheme is based on employing a pre-trained convolutional neural network (CNNs) as transfer learning. A pre-trained CNN namely AlexNet is utilized as a feature extractor whereas the output features are used to train a multiclass support vector machine (SVM) classifier. The effectiveness of the proposed framework is verified on a public database where the final result using three target classes attain an accuracy of 99.4%.", "Passenger Flow  Prediction Model  Knowledge Transfer Predicting anomaly in travel demand is a crucial finding from smart card data analytics. The output of these predictions is a significant contribution to planning sustainable public transport system and generating possible knowledge for transportation learning models. This paper investigates the anomaly effects of the surge in bus passengers demand and compare it with an increase in taxi demand. Indeed, both short-term and long-term demands reveal different patterns of passengers in uncertain situations. In pursuit of our goal, we estimated the similarity in stations by both selected and latent features where pre trained knowledge are combined as an ensemble with different weights. We present Surge Prediction and Knowledge Transfer (SPKT) model that uses Seq2Seq method combined with Multi-source Transfer Learning method on travel patterns extracted from smart card data to classify source stations and target station. To illustrate the demands blueprint, we considered multiple source stations as input to the predictor, to develop a mechanism that bridges the knowledge transfer learning with the targeted stations. To exemplify our method, we use a case study of an event with passenger surge. From experiments, we found that transferring knowledge can make the surge prediction better compared to only limited training data for the target stations. The results have proved the effectiveness of surge predictions and knowledge transfer for learning models.", "Activity recognition  Activity classification  Assistive robotics Assisted living homes aim to deploy tools to promote better living of elderly population. One of such tools is assistive robotics to perform tasks a human carer would normally be required to perform. For assistive robots to perform activities without explicit programming, a major requirement is learning and classifying activities while it observes a human carry out the activities. This work proposes a human activity learning and classification system from features obtained using 3D RGB-D data. Different classifiers are explored in this approach and the system is evaluated on a publicly available data set, showing promising results which is capable of improving assistive robots performance in living environments.", "convolutional neural network  transfer learning  AlexNet  micro-doppler  radar classification  automatic target recognition In this work, we propose a transfer learning approach with Convolutional Neural Networks (CNNs) for radar Automatic Target Recognition (ATR). Radar echo signals of moving targets introduce micro-Doppler signatures that are widely used in classifying moving targets. Spectrograms have the advantage of expressing the distinctive micro-Doppler signatures of different targets, and thus fed as 2D images to a CNN model. A pre-trained CNN model namely AlexNet is employed as a feature extractor in which feature maps can be extracted from any of the layers to train a classical classifier. SoftMax classifier have been used in this approach. The efficiency of the presented framework is demonstrated on the public RadEch database of 8 ground moving target classes, in which the experimental results indicate that our methodology significantly outperforms other competitive state-of-the-art methods with an accuracy of 99.9%.", "deep learning  synthetic aperture radar  transfer learning  automatic target recognition Synthetic aperture radar (SAR) are high resolution imaging radar systems. In many SAR applications classifying objects that are detected within the SAR image is important. In this paper an approach is proposed to tackle the Synthetic SAR Automatic Target Recognition (ATR) problem. The proposed scheme is based on a transfer leaning approach where three different pre-trained Convolutional Neural Networks (CNNs) are used as feature extractors in combination with a Support Vector Machine classifier (SVM). The CNNs used in this paper are AlexNet, VGG16 and GoogLeNet. The performance of these three CNNs is compared in regards to the SAR-ATR problem  where it is observed that AlexNet gives the best performance accuracy of 99.27%.", "Transfer Learning  Multi-task Learning  Outliers Detection  One Class Classification In this paper, we use the multi-task learning idea to solve a problem of detection with one class SVM when new sensors are added to the system. The main idea is to adapt the detection system to the upgraded sensor system. To solve that problem, the kernel matrix of multi-task learning model can be divided into two parts, one part is based on the former features and the other part is based on the new features. Typical estimation methods can be used to fill the corresponding new features in the old detection system, and a variable kernel is used for the new features in order to balance the importance of the new features with the number of observed samples. Experimental results show that it can keep the false alarm rate relatively stable and decrease the miss alarm rate rapidly as the number of samples increases in the target task.", "Semantic Segmentation  Transfer Learning  Deep Learning  CNN  Plant Segmentation We discuss the applicability of a fully convolutional network (FCN), which provides promising performance in semantic segmentation tasks, to plant segmentation tasks. The challenge lies in training the network with a small dataset because there are not many samples in plant image datasets, as compared to object image datasets such as ImageNet and PASCAL VOC datasets. The proposed method is inspired by transfer learning, but involves a two-step adaptation. In the first step, we apply transfer learning from a source domain that contains many objects with a large amount of labeled data to a major category in the plant domain. Then, in the second step, category adaptation is performed from the major category to a minor category with a few samples within the plant domain. With leaf segmentation challenge (LSC) dataset, the experimental results confirm the effectiveness of the proposed method such that F-measure criterion was, for instance, 0.953 for the A2 dataset, which was 0.355 higher than that of direct adaptation, and 0.527 higher than that of non-adaptation.", "Handwritting Recognition  Image Generation  Digit Detection  Deep Neural Networks  Knowledge Transfer Despite recent achievements in handwritten text recognition due to major advances in deep neural networks, historical handwritten documents analysis is still a challenging problem because of the requirement of large annotated training database. In this context, knowledge transfer of neural networks pre-trained on already available labeled data could allow us to process new collections of documents. In this study, we focus on localization of structures at the word-level, distinguishing words from numbers, in unlabeled handwritten documents. We based our approach on a transductive transfer learning paradigm using a deep convolutional neural network pre-trained on artificial labeled images randomly generated with strokes, word and number patches. We designed our model to predict a mask of the structures positions at the pixel-level, directly from the pixel values. The model has been trained using 100,000 generated images. The classification performances of our model were assessed by using randomly generated images coming from a different set of images of words and digits. At the pixel level, the averaged accuracy of the proposed structures detection system reach 96.1%. We evaluated the transfer capability of our model on two datasets of real handwritten documents unseen during the training. Results show that our model is able to distinguish most digits structures from word structures while avoiding other various structures present in the documents, showing the good transferability of the system to real documents.", "Handwriting Recognition  Historical Document  Transfer Learning  Deep Neural Network  Unlabeled Data In this work, we investigate handwriting recognition on new historical handwritten documents using transfer learning. Establishing a manual ground-truth of a new collection of handwritten documents is time consuming but needed to train and to test recognition systems. We want to implement a recognition system without performing this annotation step. Our research deals with transfer learning from heterogeneous datasets with a ground-truth and sharing common properties with a new dataset that has no ground-truth. The main difficulties of transfer learning lie in changes in the writing style, the vocabulary, and the named entities over centuries and datasets. In our experiment, we show how a CNN-BLSTM-CTC neural network behaves, for the task of transcribing handwritten titles of plays of the Italian Comedy, when trained on combinations of various datasets such as RIMES, Georges Washington, and Los Esposalles. We show that the choice of the training datasets and the merging methods are determinant to the results of the transfer learning task.", "Analog Neural Network  Process Variation  Low-Precision  In-Situ Transfer Learning Process Variation (PV) may cause accuracy loss of the analog neural network (ANN) processors, and make it hard to be scaled down, as well as feasibility degrading. This paper first analyses the impact of PV on the performance of ANN dhips. Then proposes an in-situ transfer learning method at system level to reduce PV's influence with low-precision back-propagation. Simulation results show the proposed method could increase 50% tolerance of operating point drift and 70% 100% tolerance of mismatch with less than I% accuracy loss of benchmarks. It also reduces 66.7% memories and has about 50x energy-efficiency improvement of multiplication in the learning stage, compared with the conventional full-precision (32bit float) training system.", "Transfer learning  Multi-task learning  Causality  Domain adaptation  Domain generalization Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization  we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.", " The cyberbullying is becoming a significant social issue, in proportion to the proliferation of Social Network Service (SNS). The cyberbullying commentaries can be categorized into syntactic and semantic subsets. In this paper, we propose an ensemble method of the two deep learning models: One is character-level CNN which captures low-level syntactic information from the sequence of characters and is robust to noise using the transfer learning. The other is word-level LRCN which captures high-level semantic information from the sequence of words, complementing the CNN model. Empirical results show that the performance of the ensemble method is significantly enhanced, outperforming the state-of-the-art methods for detecting cyberbullying comment. The model is analyzed by t-SNE algorithm to investigate the mutually cooperative relations between syntactic and semantic models.", " In the application of neural interface, the neural activity of neurons and neuronal groups is not fixed even under the same task conditions. Meanwhile, the recording conditions of neural signals are also very unstable, with a high degree of within-and across-day variability. This results in a very unstable firing pattern for the recorded neural spike signals. In order to get better performance, the decoder often requires a lot of online calibration samples. This brings a heavy training burden to neural interface users. To solve this problem, this paper proposes to apply transfer learning (TL) to online calibration of intracortical neural interface to reduce the dependence of decoder on a large number of online calibration samples. Experimental results show that through transferring from a large amount of historical data, decoder can achieve satisfactory classification accuracy with only a small amount of online data.", "Deep learning  Temporal ConvNets  Transfer learning Text classification  Sentiment analysis Temporal (one-dimensional) Convolutional Neural Network (Temporal CNN, ConvNet) is an emergent technology for text understanding. The input for the ConvNets could be either a sequence of words or a sequence of characters. In the latter case there are no needs for natural language processing. Past studies showed that the character-level ConvNets worked well for text classification in English and romanized Chinese corpus. In this article we apply the character-level ConvNets to Japanese corpus. We confirmed that meaningful representations are extracted by the ConvNets in English corpus and Japanese corpus. We attempt to reuse the meaningful representations that are learned in the ConvNets from a large-scale dataset in the form of transfer learning. As for the application to the news categorization and the sentiment analysis tasks in Japanese corpus, the ConvNets outperformed N-gram-based classifiers. In addition, our ConvNets transfer learning frameworks worked well for a task which is similar to one used for pre-training.", " Computer-Aided Detection/Diagnosis (CAD) tools were created to assist the detection and diagnosis of early stage cancers, decreasing false negative rate and improving radiologists' efficiency. Convolutional Neural Networks (CNNs) are one example of deep learning algorithms that proved to be successful in image classification. In this paper we aim to study the application of CNNs to the classification of lesions in mammograms. One major problem in the training of CNNs for medical applications is the large dataset of images that is often required but seldom available. To solve this problem, we use a transfer learning approach, which is based on three different networks that were pre-trained on the Imagenet dataset. We then investigate the performance of these pretrained CNNs and two types of image normalization to classify lesions in mammograms. The best results were obtained using the Caffe reference model for the CNN with no image normalization.", "Deep learning  Preprocessing  Transfer learning  Deep convolutional network Several preprocessing methods are applied to the automatic classification of interstitial lung disease (ILD). The proposed methods are used for the inputs to an established convolutional neural network in order to investigate the effect of those preprocessing techniques to slice-level classification accuracy. Experimental results demonstrate that the proposed preprocessing methods and a deep learning approach outperformed the case of the original images input to deep learning without preprocessing.", "Classification  Random forest  Decision tree  Transfer learning  Quadratic optimization A Transfer Learning Deep Forest (TLDF) is proposed in the paper. It is based on the Deep Forest or gcForest proposed by Zhou and Feng and can be viewed as a gcForest modification whose aim is to implement the transductive transfer learning. The transfer learning is based on introducing weights of trees in forests which impact on the forest class probability distributions. The weights can be regarded as training parameters of the deep forest and are determined in order to maximize the agreement on target and source domains. The convex quadratic optimization problem with linear constraints is obtained to compute optimal weights for every forest taking into account the consensus principle. The numerical experiments illustrate the proposed distance metric method.", "Rough sets  deep learning  transfer learning  English to Arabic Translation  rough neural networks Machine translation is one of the parts of language processing within linguistic computing for automatic translation from one language to another. The paper introduces one of the most critical areas of soft computing and natural language processing, i.e. machine translation technique that is based on deep model structure of rough sets with capability to transfer learning. A deep rough set learning is developed to support machine translation to recognize and translate tens of thousands of words/sentences automatically. To our knowledge, this is the first attempt aiming to use rough sets in machine translation rather than Arabic language translation. A deep information table is learned by assigning the morphemes-similar objects with similar learning complexities into same class and it can identify the inter-related learning tasks automatically. To account for the differences among source-languages domains, we proposed a partial transfer learning scheme in which only part of source information is transferred. The experiments have demonstrated that the proposed model can achieve competitive results and significantly outperformed other methods for translation on both accuracy rates and the efficiency for machine translation.", "Tactile sensors  Rescue robotics  Machine learning  Deep learning  Transfer learning  Object recognition This paper presents the application of machine learning to tactile sensing for rescue robotics. Disaster situations often exhibit low-visibility scenarios where haptic feedback provides a valuable information for the search of potential victims. To extract haptic information from the environment, a tactile sensor attached to a lightweight robotic arm is used. Then, methods based on the SURF descriptor, support vector machines (SVM), Deep Convolutional Neural Networks (DCNN) and transfer learning are implemented to classify the data. Besides, experiments have been carried out, to compare those procedures, using different contact elements, such as human parts and objects that could be found in catastrophe scenarios. The best achieved accuracy of 92.22%, results from the application of the transfer learning procedure using a pre-trained DCNN and fine-tuning the classification layer of the network.", " In this paper, we explore methods of complicating self-supervised tasks for representation learning. That is, we do severe damage to data and encourage a network to recover them. First, we complicate each of three powerful self-supervised task candidates: jigsaw puzzle, inpainting, and colorization. In addition, we introduce a novel complicated self-supervised task called Completing damaged jigsaw puzzles which is puzzles with one piece missing and the other pieces without color. We train a convolutional neural network not only to solve the puzzles, but also generate the missing content and colorize the puzzles. The recovery of the aforementioned damage pushes the network to obtain robust and general-purpose representations. We demonstrate that complicating the self-supervised tasks improves their original versions and that our final task learns more robust and transferable representations compared to the previous methods, as well as the simple combination of our candidate tasks. Our approach achieves state-of-the-art performance in transfer learning on PASCAL classification and semantic segmentation.", " Depth sensors used in autonomous driving and gaming systems often report back 3D point clouds. The lack of structure from these sensors does not allow these systems to take advantage of recent advances in convolutional neural networks which are dependent upon traditional filtering and pooling operations. Analogous to image based convolutional architectures, recently introduced graph based architectures afford similar filtering and pooling operations on arbitrary graphs. We adopt these graph based methods to 3D point clouds to introduce a generic vector representation of 3D graphs, we call graph 3D (G3D). We believe we are the first to use large scale transfer learning on 3D point cloud data and demonstrate the discriminant power of our salient latent representation of 3D point clouds on unforeseen test sets. By using our G3D network (G3DNet) as a feature extractor, and then pairing G3D feature vectors with a standard classifier, we achieve the best accuracy on ModelNet10 (93.1%) and ModelNet 40 (91.7%) for a graph network, and comparable performance on the Sydney Urban Objects dataset to other methods. This general-purpose feature extractor can be used as an off-the-shelf component in other 3D scene understanding or object tracking works.", "Soccer video scene and event classification  Deep transfer learning  CNN Soccer video scene and event classification are two essential tasks for the soccer video semantic analysis and have attracted many interests of researchers because of their importance and practicability. However most proposed methods solve these two tasks separately. In order to solve two tasks at the same time and improve the efficiency of video processing, we treat them as one end-to-end classification task. We introduce a new Soccer Video Scene and Event Dataset (SVSED) with six categories from the scenes and events, which contains 600 video clips. Then, we show that frame features extracted from pretrained CNN model of different categories are separable in 3-D space. Finally, we construct a CNN model for the classification task and deep transfer learning method is used for optimizing classification task result considering relative small training datasets. We fine-tuned several state-of-art CNN models and achieves accuracy above 89% within several minutes training.", "Deep learning  Inception  ResNet  transfer learning  fine-tuning  white blood cell classification This works gives an account of evaluation of white blood cell differential counts via computer aided diagnosis (CAD) system and hematology rules. Leukocytes, also called white blood cells (WBCs) play main role of the immune system. Leukocyte is responsible for phagocytosis and immunity and therefore in defense against infection involving the fatal diseases incidence and mortality related issues. Admittedly, microscopic examination of blood samples is a time consuming, expensive and error-prone task. A manual diagnosis would search for specific Leukocytes and number abnormalities in the blood slides while complete blood count (CBC) examination is performed. Complications may arise from the large number of varying samples including different types of Leukocytes, related sub-types and concentration in blood, which makes the analysis prone to human error. This process can be automated by computerized techniques which are more reliable and economical. In essence, we seek to determine a fast, accurate mechanism for classification and gather information about distribution of white blood evidences which may help to diagnose the degree of any abnormalities during CBC test. In this work, we consider the problem of pre-processing and supervised classification of white blood cells into their four primary types including Neutrophils, Eosinophils, Lymphocytes, and Monocytes using a consecutive proposed deep learning framework. For first step, this research proposes three consecutive pre-processing calculations namely are color distortion  bounding box distortion (crop) and image flipping mirroring. In second phase, white blood cell recognition performed with hierarchy topological feature extraction using Inception and ResNet architectures. Finally, the results obtained from the preliminary analysis of cell classification with (11200) training samples and 1244 white blood cells evaluation data set are presented in confusion matrices and interpreted using accuracy rate, and false positive with the classification framework being validated with experiments conducted on poor quality blood images sized 320 x 240 pixels. The deferential outcomes in the challenging cell detection task, as shown in result section, indicate that there is a significant achievement in using Inception and ResNet architecture with proposed settings. Our framework detects on average 100% of the four main white blood cell types using ResNet V1 50 while also alternative promising result with 99.84% and 99.46% accuracy rate obtained with ResNet V1 152 and ResNet 101, respectively with 3000 epochs and fine-tuning all layers. Further statistical confusion matrix tests revealed that this work achieved 1, 0.9979, 0.9989 sensitivity values when area under the curve (AUC) scores above 1, 0.9992, 0.9833 on three proposed techniques. In addition, current work shows negligible and small false negative 0, 2, 1 and substantial false positive with 0, 0, 5 values in Leukocytes detection.", "Convolutional neural network  computer vision  transfer learning Presently, rice type is identified manually by humans, which is time consuming and error prone. Therefore, there is a need to do this by machine which makes it faster with greater accuracy. This paper proposes a deep learning based method for classification of rice types. We propose two methods to classify the rice types. In the first method, we train a deep convolutional neural network (CNN) using the given segmented rice images. In the second method, we train a combination of a pretrained VGG16 network and the proposed method, while using transfer learning in which the weights of a pretrained network are used to achieve better accuracy. Our approach can also be used for classification of rice grain as broken or fine. We train a 5-class model for classifying rice types using 4000 training images and another 2- class model for the classification of broken and normal rice using 1600 training images. We observe that despite having distinct rice images, our architecture, pretrained on ImageNet data boosts classification accuracy significantly.", " Deep neural networks are representation learning techniques. During training, a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. Extracting the descriptive language coded within a trained CNN model (in the case of image data), and reusing it for other purposes is a field of interest, as it provides access to the visual descriptors previously learnt by the CNN after processing millions of images, without requiring an expensive training phase. Contributions to this field (commonly known as feature representation transfer or transfer learning) have been purely empirical so far, extracting all CNN features from a single layer close to the output and testing their performance by feeding them to a classifier. This approach has provided consistent results, although its relevance is limited to classification tasks. In a completely different approach, in this paper we statistically measure the discriminative power of every single feature found within a deep CNN, when used for characterizing every class of 11 datasets. We seek to provide new insights into the behavior of CNN features, particularly the ones from convolutional layers, as this can be relevant for their application to knowledge representation and reasoning. Our results confirm that low and middle level features may behave differently to high level features, but only under certain conditions. We find that all CNN features can be used for knowledge representation purposes both by their presence or by their absence, doubling the information a single CNN feature may provide. We also study how much noise these features may include, and propose a thresholding approach to discard most of it. All these insights have a direct application to the generation of CNN embedding spaces.", "Extreme learning machine  Transfer learning (TL)  Multiple kernel learning In this paper, a novel transfer extreme learning machine (TELM) algorithm based on multi-kernel (MK) framework has been proposed for classification. In this case, the problem is transformed into a semi-supervised learning problem, which allows multi-kernel extreme learning machine (MK-TELM) classifiers to be trained for the data categorization. Compared with many popular algorithms, the proposed method, named as MK-TELM, shows its satisfactorily experimental results on the variety of data sets, which highlights the robustness and effectiveness for classification applications.", "Domain adaption  transductive transfer learning  cross-domain color facial expression recognition  color scale-invariant feature transform Facial expression recognition across domains, e.g., training and testing facial images come from different facial poses, is very challenging due to the different marginal distributions between training and testing facial feature vectors. To deal with such challenging cross-domain facial expression recognition problem, a novel transductive transfer subspace learning method is proposed in this paper. In this method, a labelled facial image set from source domain is combined with an unlabelled auxiliary facial image set from target domain to jointly learn a discriminative subspace and make the class labels prediction of the unlabelled facial images, where a transductive transfer regularized least-squares regression (TTRLSR) model is proposed to this end. Then, based on the auxiliary facial image set, we train a SVM classifier for classifying the expressions of other facial images in the target domain. Moreover, we also investigate the use of color facial features to evaluate the recognition performance of the proposed facial expression recognition method, where color scale invariant feature transform (CSIFT) features associated with 49 landmark facial points are extracted to describe each color facial image. Finally, extensive experiments on BU-3DFE and Multi-PIE multiview color facial expression databases are conducted to evaluate the cross-database & cross-view facial expression recognition performance of the proposed method. Comparisons with state-of-the-art domain adaption methods are also included in the experiments. The experimental results demonstrate that the proposed method achieves much better recognition performance compared with the state-of-the-art methods.", "Web mining  deep learning  transfer learning The growth in the amount of multimedia content available online supposes a challenge for search and recommender systems. This information in the form of visual elements is of great value to a variety of web mining tasks  however, the mining of these resources is a difficult task due to the complexity and variability of the images. In this paper, we propose applying a deep learning model to the problem of web categorization. In addition, we make use of a technique known as transfer or inductive learning to drastically reduce the computational cost of the training phase. Finally, we report experimental results on the effectiveness of the proposed method using different classification methods and features from various depths of the deep model.", "Crowdsensing  task allocation  data quality Data quality and budget are two primary concerns in urban-scale mobile crowdsensing. Traditional research on mobile crowdsensing mainly takes sensing coverage ratio as the data quality metric rather than the overall sensed data error in the target-sensing area. In this article, we propose to leverage spatiotemporal correlations among the sensed data in the target-sensing area to significantly reduce the number of sensing task assignments. In particular, we exploit both intradata correlations within the same type of sensed data and interdata correlations among different types of sensed data in the sensing task. We propose a novel crowdsensing task allocation framework called SPACE-TA (SPArse Cost-Effective Task Allocation), combining compressive sensing, statistical analysis, active learning, and transfer learning, to dynamically select a small set of subareas for sensing in each timeslot (cycle), while inferring the data of unsensed subareas under a probabilistic data quality guarantee. Evaluations on real-life temperature, humidity, air quality, and traffic monitoring datasets verify the effectiveness of SPACE-TA. In the temperature-monitoring task leveraging intradata correlations, SPACE-TA requires data from only 15.5% of the subareas while keeping the inference error below 0.25 degrees C in 95% of the cycles, reducing the number of sensed subareas by 18.0% to 26.5% compared to baselines. When multiple tasks run simultaneously, for example, for temperature and humidity monitoring, SPACE-TA can further reduce similar to 10% of the sensed subareas by exploiting interdata correlations.", "Double encoding-layer autoencoder  representation learning  distribution difference measure Transfer learning has gained a lot of attention and interest in the past decade. One crucial research issue in transfer learning is how to find a good representation for instances of different domains such that the divergence between domains can be reduced with the new representation. Recently, deep learning has been proposed to learn more robust or higher-level features for transfer learning. In this article, we adapt the autoencoder technique to transfer learning and propose a supervised representation learning method based on double encoding-layer autoencoder. The proposed framework consists of two encoding layers: one for embedding and the other one for label encoding. In the embedding layer, the distribution distance of the embedded instances between the source and target domains is minimized in terms of KL-Divergence. In the label encoding layer, label information of the source domain is encoded using a softmax regression model. Moreover, to empirically explore why the proposed framework can work well for transfer learning, we propose a new effective measure based on autoencoder to compute the distribution distance between different domains. Experimental results show that the proposed new measure can better reflect the degree of transfer difficulty and has stronger correlation with the performance from supervised learning algorithms (e.g., Logistic Regression), compared with previous ones, such as KL-Divergence and Maximum Mean Discrepancy. Therefore, in our model, we have incorporated two distribution distance measures to minimize the difference between source and target domains in the embedding representations. Extensive experiments conducted on three real-world image datasets and one text data demonstrate the effectiveness of our proposed method compared with several state-of-the-art baseline methods.", "Meta-learning  Ensemble learning  Evolutionary algorithms  Evolutionary programming  Combining classifiers  Regression models  Model blending  Automatic algorithm selection  Map reduce Recent meta-learning approaches are oriented towards algorithm selection, optimization or recommendation of existing algorithms. In this article we show how data-tailored algorithms can be constructed from building blocks on small data sub-samples. Building blocks, typically weak learners, are optimized and evolved into data-tailored hierarchical ensembles. Good-performing algorithms discovered by evolutionary algorithm can be reused on data sets of comparable complexity. Furthermore, these algorithms can be scaled up to model large data sets. We demonstrate how one particular template (simple ensemble of fast sigmoidal regression models) outperforms state-of-the-art approaches on the Airline data set. Evolved hierarchical ensembles can therefore be beneficial as algorithmic building blocks in meta-learning, including meta-learning at scale.", "Dimensionality reduction  domain adaptation  drift correction  Hilbert-Schmidt independence criterion (HSIC)  machine olfaction  transfer learning Domain adaptation algorithms are useful when the distributions of the training and the test data are different. In this paper, we focus on the problem of instrumental variation and time-varying drift in the field of sensors and measurement, which can be viewed as discrete and continuous distributional change in the feature space. We propose maximum independence domain adaptation (MIDA) and semi-supervised MIDA to address this problem. Domain features are first defined to describe the background information of a sample, such as the device label and acquisition time. Then, MIDA learns a subspace which has maximum independence with the domain features, so as to reduce the interdomain discrepancy in distributions. A feature augmentation strategy is also designed to project samples according to their backgrounds so as to improve the adaptation. The proposed algorithms are flexible and fast. Their effectiveness is verified by experiments on synthetic datasets and four real-world ones on sensors, measurement, and computer vision. They can greatly enhance the practicability of sensor systems, as well as extend the application scope of existing domain adaptation algorithms by uniformly handling different kinds of distributional change.", "Transfer learning  CNN selection  Deep learning Transfer learning, or inductive transfer, refers to the transfer of knowledge from a source task to a target task. In the context of convolutional neural networks (CNNs), transfer learning can be implemented by transplanting the learned feature layers from one CNN (derived from the source task) to initialize another (for the target task). Previous research has shown that the choice of the source CNN impacts the performance of the target task. In the current literature, there is no principled way for selecting a source CNN for a given target task despite the increasing availability of pre-trained source CNNs. In this paper we investigate the possibility of automatically ranking source CNNs prior to utilizing them for a target task. In particular, we present an information theoretic framework to understand the source-target relationship and use this as a basis to derive an approach to automatically rank source CNNs in an efficient, zero shot manner. The practical utility of the approach is thoroughly evaluated using the Places-MIT dataset, MNIST dataset and a real-world MRI database. Experimental results demonstrate the efficacy of the proposed ranking method for transfer learning. (C) 2017 Elsevier Ltd. All rights reserved.", "Transfer learning  Extreme learning machine Extreme learning machine (ELM) has been increasingly popular in the field of transfer learning (TL) due to its simplicity, training speed and ease of use in online sequential learning process. This paper critically examines transfer learning algorithms formulated with ELM technique and provides state of the art knowledge to expedite the learning process ELM based TL algorithms. As this article discusses available ELM based TL algorithm in detail, it provides a holistic overview of current literature, serves as a starting point for new researchers in ELM based TL algorithms and facilitates identification of future research direction in concise manner. ", "Decision support  Deep learning  Transfer learning  Text mining  Financial news  Machine learning Company disclosures greatly aid in the process of financial decision-making  therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives. ", "Recommender systems  Cross-domain recommender system  Knowledge transfer  Collaborative filtering Recommender systems provide users with personalized online product and service recommendations and are a ubiquitous part of today's online entertainment smorgasbord. However, many suffer from cold-start problems due to a lack of sufficient preference data, and this is hindering their development. Cross-domain recommender systems have been proposed as one possible solution. These systems transfer knowledge from one domain that has adequate preference information to another domain that does not. The outlook for cross-domain recommendation is promising, but existing methods cannot ensure the knowledge extracted from the source domain is consistent with the target domain, which may impact the accuracy of the recommendations. To address this challenging issue, we propose a cross-domain recommender system with consistent information transfer (CIT). Knowledge consistency is based on user and item latent groups, and domain adaptation techniques are used to map and adjust these groups in both domains to maintain consistency during the transfer learning process. Experiments were conducted on five real-world datasets in three categories: movies, books, and music. The results for nine cross-domain recommendation tasks show that CIT outperforms five benchmarks and increases the accuracy of recommendations in the target domain, especially with sparse data. Practically, our proposed method is applied into a telecom product recommender system and a business partner recommender system (Smart BizSeeker) to enhance personalized decision making for both businesses and individual customers. ", "Affordance  bootstrapping  discriminative learning  diversity maximization  feature selection  intrinsic motivation (IM)  prediction hierarchies  reuse of concepts  structure learning  transfer learning This paper studies mechanisms that produce hierarchical structuring of affordance learning tasks of different levels of complexity. Guided by intrinsic motivation, our system detects easy tasks first, and learns them in selected environments which are maximally different from the previously encountered ones. Easy tasks are learned from observed low-level attributes of the environment, and provide abstractions over these attributes. As learning progresses, the system shifts its focus and starts learning harder tasks not only from low-level attributes but also from previously-learned abstract concepts. Therefore, hard tasks are autonomously placed higher in the hierarchy if the easy task concepts are identified as distinctive input attributes of hard tasks. Use of abstract concepts allows hard tasks to be learned faster than learning them from scratch, i.e., from lowlevel perception only. We tested our system with the tasks of learning effect predictions for poke and stack actions using a dataset that includes 83 real-world objects. On the basis of a large number of runs of the method, our analysis shows that the hierarchical task structure emerged as expected, along with a consistent learning order. Furthermore, a significant bootstrapping effect in learning speed of the stack action was observed with the discovered hierarchy, albeit only when fully-learned poke actions were used from the beginning.", "Transfer learning  Fault detection  Distribution change  Kernel adaptation  One class classification Data based one class classification rules are widely used in system monitoring. Due to maintenance for example, we may come across a change of data distribution with respect to training data. While lacking of representative samples for the new data set, one can try to adapt the former learned detection rule to the new data set instead of retraining a new rule which implies to gather a significant amount of data. Based on the above, a multi-task learning detection rule approach is proposed to deal with the training of the updated system as some new data are available. The key feature of the new approach is the introduction of a parameter to control how much we rely on the former model. This parameter has to be set and changed as the amount of new data coming from the system increases. We define the new detection model as a classical one class SVM with a specific kernel matrix which depends on the parameter we introduced. A kernel adaptation method for C-one class SVM is developed in order to get the path solution along that parameter and a criteria is established to select a good value. Experiments conducted on toy data and real data set show that the proposed method could adapt to data change, and it gives a good transition from the old detection rule to the new one which is just obtained using the new data set only when the number of samples gathered from that new one is large enough. ", "Brain-computer interface  domain adaptation (DA)  EEG  ensemble learning  fuzzy sets (FSs)  transfer learning (TL) One big challenge that hinders the transition of brain-computer interfaces (BCIs) from laboratory settings to real-life applications is the availability of high-performance and robust learning algorithms that can effectively handle individual differences, i.e., algorithms that can be applied to a new subject with zero or very little subject-specific calibration data. Transfer learning and domain adaptation have been extensively used for this purpose. However, most previous works focused on classification problems. This paper considers an important regression problem in BCI, namely, online driver drowsiness estimation fromEEG signals. By integrating fuzzy sets with domain adaptation, we propose a novel online weighted adaptation regularization for regression (OwARR) algorithm to reduce the amount of subject-specific calibration data, and also a source domain selection (SDS) approach to save about half of the computational cost of OwARR. Using a simulated driving dataset with 15 subjects, we show that OwARR and OwARR-SDS can achieve significantly smaller estimation errors than several other approaches. We also provide comprehensive analyses on the robustness of OwARR and OwARR-SDS.", "Fuzzy model  fuzzy rules  machine learning  regression  transfer learning Data science is a research field concerned with processes and systems that extract knowledge from massive amounts of data. In some situations, however, data shortage renders existing data-driven methods difficult or even impossible to apply. Transfer learning has recently emerged as a way of exploiting previously acquired knowledge to solve new yet similar problems much more quickly and effectively. In contrast to classical data-driven machine learning methods, transfer learning methods exploit the knowledge accumulated from data in auxiliary domains to facilitate predictive modeling in the current domain. A significant number of transfer learning methods that address classification tasks have been proposed, but studies on transfer learning in the case of regression problems are still scarce. This study focuses on using transfer learning techniques to handle regression problems in a domain that has insufficient training data. We propose an original fuzzy regression transfer learning method, based on fuzzy rules, to address the problem of estimating the value of the target for regression. A Takagi-Sugeno fuzzy regression model is developed to transfer knowledge from a source domain to a target domain. Experimental results using synthetic data and real-world datasets demonstrate that the proposed fuzzy regression transfer learning method significantly improves the performance of existing models when tackling regression problems in the target domain.", "Multi-instance learning  Domain transfer learning  Classifier adaptation  Gradient descent In this paper, we invest the domain transfer learning problem with multi-instance data. We assume we already have a well-trained multi-instance dictionary and its corresponding classifier from the source domain, which can be used to represent and classify the bags. But it cannot be directly used to the target domain. Thus we propose to adapt them to the target domain by adding an adaptive term to the source domain classifier. The adaptive function is a linear function based on a domain transfer multi-instance dictionary. Given a target domain bag, we first map it to a bag-level feature space using the domain transfer dictionary and then apply a linear adaptive function to its baglevel feature vector. To learn the domain transfer dictionary and the adaptive function parameter, we simultaneously minimize the average classification error of the target domain classifier over the target domain training set, and the complexities of both the adaptive function parameter and the domain transfer dictionary. The minimization problem is solved by an iterative algorithm which updates the dictionary and the function parameter alternately. Experiments over several benchmark data sets show the advantage of the proposed method over existing state-of-the-art domain transfer multi-instance learning methods.", "Domain generalization  Domain adaptation  Transfer learning  Transfer component analysis This paper presents the domain generalization methods Multi-Domain Transfer Component Analysis (Multi-TCA) and Multi-Domain Semi-Supervised Transfer Component Analysis (Multi-SSTCA) which are extensions of the domain adaptation method Transfer Component Analysis to multiple domains. Multi-TCA learns a shared subspace by minimizing the dissimilarities across domains, while maximally preserving the data variance. The proposed methods are compared to other state-of-the-art methods on three public datasets and on a real-world case study on climate control in residential buildings. Experimental results demonstrate that Multi-TCA and Multi-SSTCA can improve predictive performance on previously unseen domains. We perform sensitivity analysis on model parameters and evaluate different kernel distances, which facilitate further improvements in predictive performance.", "Reinforcement learning  Lifelong learning  Distributed optimization  Transfer learning Lifelong reinforcement learning provides a successful framework for agents to learn multiple consecutive tasks sequentially. Current methods, however, suffer from scalability issues when the agent has to solve a large number of tasks. In this paper, we remedy the above drawbacks and propose a novel scalable technique for lifelong reinforcement learning. We derive an algorithm which assumes the availability of multiple processing units and computes shared repositories and local policies using only local information exchange. We then show an improvement to reach a linear convergence rate compared to current lifelong policy search methods. Finally, we evaluate our technique on a set of benchmark dynamical systems and demonstrate learning speed-ups and reduced running times. (C) 2017 Elsevier Ltd. All rights reserved.", "Transfer learning  Deep learning  Specialization  Faster R-CNN  Sequential monte carlo filter  Traffic object detection Generally, the performance of a generic detector decreases significantly when it is tested on a specific scene due to the large variation between the source training dataset and the samples from the target scene. To solve this problem, we propose a new formalism of transfer learning based on the theory of a Sequential Monte Carlo (SMC) filter to automatically specialize a scene-specific Faster R-CNN detector. The suggested framework uses different strategies based on the SMC filter steps to approximate iteratively the target distribution as a set of samples in order to specialize the Faster R-CNN detector towards a target scene. Moreover, we put forward a likelihood function that combines spatio-temporal information extracted from the target video sequence and the confidence-score given by the output layer of the Faster R-CNN, to favor the selection of target samples associated with the right label. The effectiveness of the suggested framework is demonstrated through experiments on several public traffic datasets. Compared with the state-of-the-art specialization frameworks, the proposed framework presents encouraging results for both single and multi-traffic object detections. (C) 2017 Elsevier Inc. All rights reserved.", "Semantic segmentation  Model compression  Transfer learning  Real-time application Deep Learning (DL) has been proven as a powerful recognition method as evidenced by its success in recent computer vision competitions. The most accurate results have been obtained by ensembles of DL models that pool their results. However, such ensembles are computationally costly, making them inapplicable to real-time applications. In this paper, we apply model compression techniques to the problem of semantic segmentation, which is one of the most challenging problems in computer vision. Our results suggest that compressed models can approach the accuracy of full ensembles on this task, combining the diverse strengths of networks of very different architectures, while maintaining real-time performance. (C) 2017 Published by Elsevier Inc.", " The modern aircraft has evolved to become an important part of our society. Its design is multidisciplinary in nature and is characterized by complex analyses of mutually interdependent disciplines and large search spaces. Machine learning has, historically, played a significant role in aircraft design, primarily by approximating expensive physics-based numerical simulations. In this work, we summarize the current role of machine learning in this application domain, and highlight the opportunity of incorporating recent advances in the field to further its impact. Specifically, regression models (or surrogate models) that represent a major portion of the current efforts are generally built from scratch assuming a zero prior knowledge state, only relying on data from the ongoing target problem of interest. However, due to the incremental nature of design processes, there likely exists relevant knowledge from various related sources that can potentially be leveraged. As such, we present three relatively advanced machine learning technologies that facilitate automatic knowledge transfer in order to improve design performance. Subsequently, we demonstrate the efficacy of one of these technologies, i.e. transfer learning, on two use cases of aircraft engine design yielding noteworthy results. Our aim is to unveil this new application as a well-suited arena for the salient features of knowledge transfer in machine learning to come to the fore, thereby encouraging future research efforts.", "Activity recognition  Machine learning  Transfer learning  Pervasive computing Activity recognition algorithms have matured and become more ubiquitous in recent years. However, these algorithms are typically customized for a particular sensor platform. In this paper, we introduce PECO, a Personalized activity ECOsystem, that transfers learned activity information seamlessly between sensor platforms in real time so that any available sensor can continue to track activities without requiring its own extensive labeled training data. We introduce a multi-view transfer learning algorithm that facilitates this information handoff between sensor platforms and provide theoretical performance bounds for the algorithm. In addition, we empirically evaluate PECO using datasets that utilize heterogeneous sensor platforms to perform activity recognition. These results indicate that not only can activity recognition algorithms transfer important information to new sensor platforms, but any number of platforms can work together as colleagues to boost performance.", "Deep learning  Transfer learning  Non-handcrafted features  Texture descriptors  Texture classification  Ensemble of descriptors This work presents a generic computer vision system designed for exploiting trained deep Convolutional Neural Networks (CNN) as a generic feature extractor and mixing these features with more traditional hand-crafted features. Such a system is a single structure that can be used for synthesizing a large number of different image classification tasks. Three substructures are proposed for creating the generic computer vision system starting from handcrafted and non-handcrafter features: i) one that remaps the output layer of a trained CNN to classify a different problem using an SVM  ii) a second for exploiting the output of the penultimate layer of a trained CNN as a feature vector to feed an SVM  and iii) a third for merging the output of some deep layers, applying a dimensionality reduction method, and using these features as the input to an SVM. The application of feature transform techniques to reduce the dimensionality of feature sets coming from the deep layers represents one of the main contributions of this paper. Three approaches are used for the non-handcrafted features: deep transfer learning features based on convolutional neural networks (CNN), principal component analysis network (PCAN), and the compact binary descriptor (CBD). For the handcrafted features, a wide variety of state-of-the-art algorithms are considered: Local Ternary Patterns, Local Phase Quantization, Rotation Invariant Co-occurrence Local Binary Patterns, Completed Local Binary Patterns, Rotated local binary pattern image, Globally Rotation Invariant Multi-scale Co-occurrence Local Binary Pattern, and several others. The computer vision system based on the proposed approach was tested on many different datasets, demonstrating the generalizability of the proposed approach thanks to the strong performance recorded. The Wilcoxon signed rank test is used to compare the different methods  moreover, the independence of the different methods is studied using the Q-statistic. To facilitate replication of our experiments, the MATLAB source code will be available at (https://www.dropbox.com/s/bguw035yrqz0pwp/ElencoCode.docx?dl=0). (C) 2017 Elsevier Ltd. All rights reserved.", "Pre-training  Transfer learning  Spoken language understanding  Sequence labeling  Conditional random fiends  Multi-sense clustering  Word embedding  Hidden unit conditional random fields  LSTMs In this paper, we introduce a simple unsupervised framework for pre-training hidden-unit conditional random fields (HUCRFs), i.e., learning initial parameter estimates for HUCRFs prior to supervised training.Our framework exploits the model structure of HUCRFs to make effective use of unlabeled data from the same domain or labeled data from a different domain. The key idea is to use the separation of HUCRF parameters between observations and labels: this allows us to pre-train observation parameters independently of label parameters. Pre-training is achieved by creating pseudo-labels from such resources. In the case of unlabeled data, we cluster observations and use the resulting clusters as pseudo-labels. Observation parameters can be trained on these resources and then transferred to initialize the supervised training process on the target labeled data. Experiments on various sequence labeling tasks demonstrate that the proposed pre-training method consistently yields significant improvement in performance. The core idea could be extended to other learning techniques including deep learning. We applied the proposed technique to recurrent neural networks (RNN) with long short term memory (LSTM) architecture and obtained similar gains. (C) 2017 Elsevier Ltd. All rights reserved.", "Instance-transfer learning  Source-subset selection  Conformal test Instance transfer aims at improving prediction models for a target domain by transferring data from related source domains. The effectiveness of instance transfer depends on the relevance of source data to the target domain. When the relevance of source data is limited, the only option is to select a subset of source data of which the relevance is acceptable. In this paper, we introduce three algorithms that perform source-subset selection prior to model training. The algorithms employ a conformity-based test that estimates the source-subset relevance based on individual instances or on subsets as a whole. Experiments conducted on four real-world data sets demonstrated the effectiveness of the proposed algorithms. Especially, it was shown that pre-training subset-selection based on set relevance is capable of outperforming the existing instance-transfer techniques. ", "Social emotion classification  hybrid neural network  sparse encoding  transfer learning Social emotion classification aims to predict the aggregation of emotional responses embedded in online comments contributed by various users. Such a task is inherently challenging because extracting relevant semantics from free texts is a classical research problem. Moreover, online comments are typically characterized by a sparse feature space, which makes the corresponding emotion classification task very difficult. On the other hand, though deep neural networks have been shown to be effective for speech recognition and image analysis tasks because of their capabilities of transforming sparse low-level features to dense high-level features, their effectiveness on emotion classification requires further investigation. The main contribution of our work reported in this paper is the development of a novel model of semantically rich hybrid neural network (HNN) which leverages unsupervised teaching models to incorporate semantic domain knowledge into the neural network to bootstrap its inference power and interpretability. To our best knowledge, this is the first successful work of incorporating semantics into neural networks to enhance social emotion classification and network interpretability. Through empirical studies based on three real-world social media datasets, our experimental results confirm that the proposed hybrid neural networks outperform other state-of-the-art emotion classification methods.", "Cast shadows  Human postures classification  CNN  Video surveillance  Transfer learning  Infrared This paper presents a system for human posture recognition using a camera and two infrared light sources. It uses as input the combination of the body silhouette and its (invisible to the eye) cast shadows. Conventional video-surveillance methods based on a single camera can fail to infer the correct posture since different postures can look similar under perspective projection. Fortunately, cast body shadows, generated by infrared lights, offer additional posture information that cannot be directly captured by a single camera. Each shadow can be projected on different surfaces (e.g. floor, walls, and furniture) generating complex body projections that represent various shapes within the same posture class. These shadow images are very challenging and difficult to describe with traditional handcrafted features that need to be somewhat invariant to these within-class changes. However, a deep convolution neural network (CNN) is able to learn a better data representation from a large-scale dataset. In the absence of a big real dataset, we propose to use synthetic data for training the CNN classifier. Learning from synthetic data is a challenging task due to the gap between synthetic and real feature distributions. Thus, we propose a normalization technique to bridge the gap and help the classifier to better generalize with real data. We evaluated the proposed system on a new real dataset captured in our laboratory and a simulated dataset generated with computer graphics tools. Experimental results validated the efficiency of the CNN model against other conventional methods. Furthermore, the combination of cast shadows and body silhouette had better performance than using only the body silhouette as expected. ", "Unsupervised learning  Semi-supervised learning  Hierarchical parametric models  Latent variable estimation Hierarchical probabilistic models, such as mixture models, are used for cluster analysis. These models have two types of variables: observable and latent. In cluster analysis, the latent variable is estimated, and it is expected that additional information will improve the accuracy of the estimation of the latent variable. Many proposed learning methods are able to use additional data  these include semi-supervised learning and transfer learning. However, from a statistical point of view, a complex probabilistic model that encompasses both the initial and additional data might be less accurate due to having a higher-dimensional parameter. The present paper presents a theoretical analysis of the accuracy of such a model and clarifies which factor has the greatest effect on its accuracy, the advantages of obtaining additional data, and the disadvantages of increasing the complexity. (C) 2017 Elsevier Ltd. All rights reserved.", "Convolutional Neural Networks  Object Recognition  Deep Learning  Few-Shot Learning  Transfer Learning Training a deep convolution neural network (CNN) to succeed in visual object classification usually requires a great number of examples. Here, starting from such a pre-learned CNN, we study the task of extending the network to classify additional categories on the basis of only few examples (fewshot learning''). We find that a simple and fast prototype-based learning procedure in the global feature layers (Global Prototype Learning'', GPL) leads to some remarkably good classification results for a large portion of the new classes. It requires only up to ten examples for the new classes to reach a plateau in performance. To understand this few-shot learning performance resulting from GPL as well as the performance of the original network, we use the t-SNE method (Maaten and Hinton, 2008) to visualize clusters of object category examples. This reveals the strong connection between classification performance and data distribution and explains why some new categories only need few examples for learning while others resist good classification results even when trained with many more examples. (C) 2017 Elsevier Ltd. All rights reserved.", "Transfer learning  Varying-coefficient models  Housing-price prediction  Seismic-hazard models We study prediction problems in which the conditional distribution of the output given the input varies as a function of task variables which, in our applications, represent space and time. In varying-coefficient models, the coefficients of this conditional are allowed to change smoothly in space and time  the strength of the correlations between neighboring points is determined by the data. This is achieved by placing a Gaussian process (GP) prior on the coefficients. Bayesian inference in varying-coefficient models is generally intractable. We show that with an isotropic GP prior, inference in varying-coefficient models resolves to standard inference for a GP that can be solved efficiently. MAP inference in this model resolves to multitask learning using task and instance kernels. We clarify the relationship between varying-coefficient models and the hierarchical Bayesian multitask model and show that inference for hierarchical Bayesian multitask models can be carried out efficiently using graph-Laplacian kernels. We explore the model empirically for the problems of predicting rent and real-estate prices, and predicting the ground motion during seismic events. We find that varying-coefficient models with GP priors excel at predicting rents and real-estate prices. The ground-motion model predicts seismic hazards in the State of California more accurately than the previous state of the art.", "Bayesian methods  Experimental design  Human-to-machine transfer learning  Interactive machine learning  Statistics in high dimensions Prediction in a small-sized sample with a large number of covariates, the small n, large p problem, is challenging. This setting is encountered in multiple applications, such as in precision medicine, where obtaining additional data can be extremely costly or even impossible, and extensive research effort has recently been dedicated to finding principled solutions for accurate prediction. However, a valuable source of additional information, domain experts, has not yet been efficiently exploited. We formulate knowledge elicitation generally as a probabilistic inference process, where expert knowledge is sequentially queried to improve predictions. In the specific case of sparse linear regression, where we assume the expert has knowledge about the relevance of the covariates, or of values of the regression coefficients, we propose an algorithm and computational approximation for fast and efficient interaction, which sequentially identifies the most informative features on which to query expert knowledge. Evaluations of the proposed method in experiments with simulated and real users show improved prediction accuracy already with a small effort from the expert.", "Domain adaptation (DA)  instance weighting  transfer learning In real-world applications, the assumption of independent and identical distribution is no longer consistent. To alleviate the significant mismatch between source and target domains, importance weighting import vector machine, which is an adaptive classifier, is proposed. This adaptive probabilistic classification method, which is sparse and computationally efficient, can be used for unsupervised domain adaptation (DA). The effectiveness of the proposed approach is demonstrated via a toy problem, and a real-world cross-domain object recognition task. Even though the sparseness, the proposed method outperforms the state-of-the-art in both unsupervised and semisupervised DA scenarios. We also introduce a reliable importance weighted cross validation (RIWCV), which is an improvement of importance weighted cross validation, for parameter and model selection. The RIWCV avoid falling down in local minimum, by selecting a more reliable combination of the parameters instead of the best parameters.", "Image classification  Texture  Image processing  Ensemble of descriptors  Person re-identification This paper presents an improved version of a recent state-of-the-art texture descriptor called Gaussians of Local Descriptors (GOLD), which is based on a multivariate Gaussian that models the local feature distribution that describes the original image. The full rank covariance matrix, which lies on a Riemannian manifold, is projected on the tangent Euclidean space and concatenated to the mean vector for representing a given image. In this paper, we test the following features for describing the original image: scale-invariant feature transform (SIFT), histogram of gradients (HOG), and weber's law descriptor (WLD). To improve the baseline version of GOLD, we describe the covariance matrix using a set of visual features that are fed into a set of Support Vector Machines (SVMs). The SVMs are combined by sum rule. The scores obtained by an SVM trained using the original GOLD approach and the SVMs trained with visual features are then combined by sum rule. Experiments show that our proposed variant outperforms the original GOLD approach. The superior performance of the proposed system is validated across a large set of datasets. Particularly interesting is the performance obtained in two widely used person re-identification datasets, CAVIAR4REID and IAS, where the proposed GOLD variant is coupled with a state-of-the-art ensemble to obtain an improvement of performance on these two datasets. Moreover, we performed further tests that combine GOLD with non-binary features (local ternary/quinary patterns) and deep transfer learning. The fusion among SVMs trained with deep features and the SVMs trained using the ternary/quinary coding ensemble is demonstrated to obtain a very high performance across datasets. The MATLAB code for the ensemble of classifiers and for the extraction of the features will be publicly available(1) to other researchers for future comparisons. (C) 2017 Elsevier Ltd. All rights reserved.", "Transfer learning  model transfer  random forest  decision tree We propose novel model transfer-learning methods that refine a decision forest model M learned within a source domain using a training set sampled from a target domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems.", "EmotiW  Emotion recognition in the wild  Multimodal fusion  Convolutional neural networks  Kernel extreme learning machine  Partial least squares Multimodal recognition of affective states is a difficult problem, unless the recording conditions are carefully controlled. For recognition in the wild, large variances in face pose and illumination, cluttered backgrounds, occlusions, audio and video noise, as well as issues with subtle cues of expression are some of the issues to target. In this paper, we describe a multimodal approach for video-based emotion recognition in the wild. We propose using summarizing functionals of complementary visual descriptors for video modeling. These features include deep convolutional neural network (CNN) based features obtained via transfer learning, for which we illustrate the importance of flexible registration and fine-tuning. Our approach combines audio and visual features with least squares regression based classifiers and weighted score level fusion. We report state-of-the-art results on the EmotiW Challenge for in the wild facial expression recognition. Our approach scales to other problems, and ranked top in the ChaLearn-LAP First Impressions Challenge 2016 from video clips collected in the wild. ", "Transfer to rank  behavior ranking  preference learning  collaborative recommendation Intelligent recommendation has been well recognized as one of the major approaches to address the information overload problem in the big data era. A typical intelligent recommendation engine usually consists of three major components, that is, data as the main input, algorithms for preference learning, and system for user interaction and high-performance computation. We observe that the data (e.g., users' behavior) are usually in different forms, such as examinations (e.g., browse and collection) and ratings, where the former are often much more abundant than the latter. Although the data are in different representations, they are both related to users' true preferences and are also deemed complementary to each other for preference learning. However, very few ranking or recommendation algorithms have been developed to exploit such two types of user behavior. In this article, we focus on jointly modeling the examination behavior and rating behavior and develop a novel and efficient ranking-oriented recommendation algorithm accordingly. First, we formally define a new recommendation problem termed behavior ranking, which aims to build a ranking-oriented model by exploiting both the examination behavior and rating behavior. Second, we develop a simple and generic transfer to rank (ToR) algorithm for behavior ranking, which transfers knowledge of candidate items from a global preference learning task to a local preference learning task. Compared with the previous work on integrating heterogeneous user behavior, our ToR algorithm is the first ranking-oriented solution, which can effectively generate recommendations in a more direct manner than those regression-oriented methods. Extensive empirical studies show that our ToR algorithm performs significantly more accurately than the state-of-the-art methods in most cases. Furthermore, our ToR algorithm is very efficient in terms of the time complexity, which is similar to those for homogeneous user behavior alone.", "Transfer learning  Deep neural networks  Source-target-source  Optimization  Cross-sensor biometrics Deep transfer learning emerged as a new paradigm in machine learning in which a deep model is trained on a source task and the knowledge acquired is then totally or partially transferred to help in solving a target task. In this paper, we apply the source-target-source methodology, both in its original form and an extended multi-source version, to the problem of cross-sensor biometric recognition. We tested the proposed methodology on the publicly available CSIP image database, achieving state-of-the-art results in a wide variety of cross-sensor scenarios.", "Customer identification  Feature selection  Class imbalance  Absolute rarity  Transfer learning  Group method of data handling Class imbalance brings great challenges to feature selection in customer identification, and most of the current feature selection approaches cannot produce good prediction on the minority class. A number of studies have attempted to solve this issue by using resampling techniques. However, resampling techniques only use the in-domain information and they cannot achieve good performance when the imbalance is caused by the absolute rarity of the minority class. In this paper, we focus on the issue of feature selection with class imbalance caused by absolute rarity. By introducing the idea of transfer learning, we develop a transferred feature selection method based on the group method of data handling neural networks. The proposed ensemble neural network extracts information of similar customers from related domains to deal with the information scarcity of the minority class in the target domain. Experiments are done on a real-world application using data from a cigarette company. The results indicate that the new method gives better predictive performance than other benchmark feature selection methods, especially in terms of the predictive accuracy of the minority high-value customers. At the same time, the new algorithm can help to identify important features that distinguish high-value customers from low-value ones.", "Transfer learning  Online learning  Online transfer learning  multiple source domains Transfer learning aims to enhance performance in a target domain by exploiting useful information from auxiliary or source domains when the labeled data in the target domain are insufficient or difficult to acquire. In some real-world applications, the data of source domain are provided in advance, but the data of target domain may arrive in a stream fashion. This kind of problem is known as online transfer learning. In practice, there can be several source domains that are related to the target domain. The performance of online transfer learning is highly associated with selected source domains, and simply combining the source domains may lead to unsatisfactory performance. In this paper, we seek to promote classification performance in a target domain by leveraging labeled data from multiple source domains in online setting. To achieve this, we propose a new online transfer learning algorithm that merges and leverages the classifiers of the source and target domain with an ensemble method. The mistake bound of the proposed algorithm is analyzed, and the comprehensive experiments on three real-world data sets illustrate that our algorithm outperforms the compared baseline algorithms.", "Unsupervised domain adaptation  optimal transport  transfer learning  visual adaptation  classification Domain adaptation is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, finding domain-invariant representations has shown excellent properties, in particular since it allows to train a unique classifier effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi-supervised case where few labeled samples are available in the target domain.", "Wind power prediction  Sparse denoising auto-encoders  Meta-regressor  Transfer learning  Meteorological properties An innovative short term wind power prediction system is proposed which exploits the learning ability of deep neural network based ensemble technique and the concept of transfer learning. In the proposed DNN-MRT scheme, deep auto-encoders act as base-regressors, whereas Deep Belief Network is used as a meta-regressor. Employing the concept of ensemble learning facilitates robust and collective decision on test data, whereas deep base and meta-regressors ultimately enhance the performance of the proposed DNN-MRT approach. The concept of transfer learning not only saves time required during training of a base-regressor on each individual wind farm dataset from scratch but also stipulates good weight initialization points for each of the wind farm for training. The effectiveness of the proposed, DNN-MRT technique is expressed by comparing statistical performance measures in terms of root mean squared error (RMSE), mean absolute error (MAE), and standard deviation error (SDE) with other existing techniques. (C) 2017 Published by Elsevier B.V.", "Human activity recognition  Extreme learning machine  Inertial sensors  Mixed kernel  Machine learning Fixed placements of inertial sensors have been utilized by previous human activity recognition algorithms to train the classifier. However, the distribution of sensor data is seriously affected by the sensor placement. The performance will be degraded when the model trained on one placement is used in others. In order to tackle this problem, a fast and robust human activity recognition model called TransM-RKELM (Transfer learning mixed and reduced kernel Extreme Learning Machine) is proposed in this paper  It uses a kernel fusion method to reduce the influence by the'choice of kernel function and the reduced kernel is utilized to reduce the computational cost. After realizing initial activity recognition model by mixed and reduced kernel extreme learning model (M-RKELM), in the online phase M-RKELM is utilized to classify the activity and adapt the model to new locations based on high confident recognition results in real time. Experimental results show that the proposed model can adapt the classifier to new sensor locations quickly and obtain good recognition performance. ", "Fuzzy C-means (FCM)  Transfer learning  Knowledge transfer  Image segmentation  Data heterogeneity We study a novel fuzzy clustering method to improve the segmentation performance on the target texture image by leveraging the knowledge from a prior texture image. Two knowledge transfer mechanisms, i.e. knowledge-leveraged prototype transfer (KL-PT) and knowledge-leveraged prototype matching (KL-PM) are first introduced as the bases. Applying them, the knowledge-leveraged transfer fuzzy C-means (KL-TFCM) method and its three-stage-interlinked framework, including knowledge extraction, knowledge matching, and knowledge utilization, are developed. There are two specific versions: KL-TFCM-c and KL-TFCM-f, i.e. the so-called crisp and flexible forms, which use the strategies of maximum matching degree and weighted sum, respectively. The significance of our work is fourfold: 1) Owing to the adjustability of referable degree between the source and target domains, KL-PT is capable of appropriately learning the insightful knowledge, i.e. the cluster prototypes, from the source domain  2) KL-PM is able to self adaptively determine the reasonable pairwise relationships of cluster prototypes between the source and target domains, even if the numbers of clusters differ in the two domains  3) The joint action of KL-PM and KL-PT can effectively resolve the data inconsistency and heterogeneity between the source and target domains, e.g. the data distribution diversity and cluster number difference. Thus, using the three stage-based knowledge transfer, the beneficial knowledge from the source domain can be extensively, self-adaptively leveraged in the target domain. As evidence of this, both KL-TFCM-c and KL-TFCM-f surpass many existing clustering methods in texture image segmentation  and 4) In the case of different cluster numbers between the source and target domains, KL-TFCM-f proves higher clustering effectiveness and segmentation performance than does KL-TFCM-c. ", "Building blocks  code fragments  genetic programming (GP)  image classification  knowledge extraction Genetic programming (GP) is a well-known evolutionary computation technique, which has been successfully used to solve various problems, such as optimization, image analysis, and classification. Transfer learning is a type of machine learning approach that can be used to solve complex tasks. Transfer learning has been introduced to GP to solve complex Boolean and symbolic regression problems with some promise. However, the use of transfer learning with GP has not been investigated to address complex image classification tasks with noise and rotations, where GP cannot achieve satisfactory performance, but GP with transfer learning may improve the performance. In this paper, we propose a novel approach based on transfer learning and GP to solve complex image classification problems by extracting and reusing blocks of knowledge/information, which are automatically discovered from similar as well as different image classification tasks during the evolutionary process. The proposed approach is evaluated on three texture data sets and three office data sets of image classification benchmarks, and achieves better classification performance than the state-of-the-art image classification algorithm. Further analysis on the evolved solutions/trees shows that the proposed approach with transfer learning can successfully discover and reuse knowledge/information extracted from similar or different problems to improve its performance on complex image classification problems.", "Memetic automaton  multiagent systems (MASs)  natural selection  reinforcement learning (RL)  transfer learning (TL) In this paper, we present an evolutionary transfer reinforcement learning framework (eTL) for developing intelligent agents capable of adapting to the dynamic environment of multiagent systems (MASs). Specifically, we take inspiration from Darwin's theory of natural selection and Universal Darwinism as the principal driving forces that govern the evolutionary knowledge transfer process. The essential backbone of our proposed eTL comprises several meme-inspired evolutionary mechanisms, namely meme representation, meme expression, meme assimilation, meme internal evolution, and meme external evolution. Our proposed approach constructs social selection mechanisms that are modeled after the principles of human learning to identify appropriate interacting partners. eTL also models the intrinsic parallelism of natural evolution and errors that are introduced due to the physiological limits of the agents' ability to perceive differences, so as to generate growth and variation of knowledge that agents have of the world, thus exhibiting higher adaptivity capabilities on solving complex problems. To verify the efficacy of the proposed paradigm, comprehensive investigations of the proposed eTL against existing state-of-the-art TL methods in MAS, are conducted on the minefield navigation tasks platform and the Unreal Tournament 2004 first person shooter computer game, in which homogeneous and heterogeneous learning machines are considered.", "Brain-computer interface (BCI)  domain adaptation (DA)  EEG  event-related potential (ERP)  transfer learning Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals. However, because different subjects have different neural responses to even the same stimulus, it is very difficult to build a generic ERP classifier whose parameters fit all subjects. The classifier needs to be calibrated for each individual subject, using some labeled subject-specific data. This paper proposes both online and offline weighted adaptation regularization (wAR) algorithms to reduce this calibration effort, i.e., to minimize the amount of labeled subject-specific EEG data required in BCI calibration, and hence to increase the utility of the BCI system. We demonstrate using a visually evoked potential oddball task and three different EEG headsets that both online and offline wAR algorithms significantly outperform several other algorithms. Moreover, through source domain selection, we can reduce their computational cost by about 50%, making them more suitable for real-time applications.", "Online transfer learning  multiple source domains  heterogeneous transfer Transfer learning techniques have been broadly applied in applications where labeled data in a target domain are difficult to obtain while a lot of labeled data are available in related source domains. In practice, there can be multiple source domains that are related to the target domain, and how to combine them is still an open problem. In this paper, we seek to leverage labeled data from multiple source domains to enhance classification performance in a target domain where the target data are received in an online fashion. This problem is known as the online transfer learning problem. To achieve this, we propose novel online transfer learning paradigms in which the source and target domains are leveraged adaptively. We consider two different problem settings: homogeneous transfer learning and heterogeneous transfer learning. The proposed methods work in an online manner, where the weights of the source domains are adjusted dynamically. We provide the mistake bounds of the proposed methods and perform comprehensive experiments on real-world data sets to demonstrate the effectiveness of the proposed algorithms.", "Zero-shot action recognition  Zero-shot learning  Semantic embedding  Semi-supervised learning  Transfer learning  Action recognition The number of categories for action recognition is growing rapidly and it has become increasingly hard to label sufficient training data for learning conventional models for all categories. Instead of collecting ever more data and labelling them exhaustively for all categories, an attractive alternative approach is zero-shot learning (ZSL). To that end, in this study we construct a mapping between visual features and a semantic descriptor of each action category, allowing new categories to be recognised in the absence of any visual training data. Existing ZSL studies focus primarily on still images, and attribute-based semantic representations. In this work, we explore word-vectors as the shared semantic space to embed videos and category labels for ZSL action recognition. This is a more challenging problem than existing ZSL of still images and/or attributes, because the mapping between video space-time features of actions and the semantic space is more complex and harder to learn for the purpose of generalising over any cross-category domain shift. To solve this generalisation problem in ZSL action recognition, we investigate a series of synergistic strategies to improve upon the standard ZSL pipeline. Most of these strategies are transductive in nature which means access to testing data in the training phase. First, we enhance significantly the semantic space mapping by proposing manifold-regularized regression and data augmentation strategies. Second, we evaluate two existing post processing strategies (transductive self-training and hubness correction), and show that they are complementary. We evaluate extensively our model on a wide range of human action datasets including HMDB51, UCF101, Olympic Sports and event datasets including CCV and TRECVID MED 13. The results demonstrate that our approach achieves the state-of-the-art zero-shot action recognition performance with a simple and efficient pipeline, and without supervised annotation of attributes. Finally, we present in-depth analysis into why and when zero-shot works, including demonstrating the ability to predict cross-category transferability in advance.", "Activity recognition  hidden Markov model (HMM)  maximum a posteriori (MAP) adaptation  soft-assignment  transfer learning This paper presents a novel activity class representation using a single sequence for training. The contribution of this representation lays on the ability to train an one-shot learning recognition system, useful in new scenarios where capturing and labeling sequences is expensive or impractical. The method uses a universal background model of local descriptors obtained from source databases available on-line and adapts it to a new sequence in the target scenario through a maximum a posteriori adaptation. Each activity sample is encoded in a sequence of normalized bag of features and modeled by a new hidden Markov model formulation, where the expectation-maximization algorithm for training is modified to deal with observations consisting in vectors in a unit simplex. Extensive experiments in recognition have been performed using one-shot learning over the public datasets Weizmann, KTH, and IXMAS. These experiments demonstrate the discriminative properties of the representation and the validity of application in recognition systems, achieving state-of-the-art results.", "Zero-shot learning  transfer learning  robust support vector machine (SVM)  inductive learning  transductive learning  experiment By transferring knowledge from the abundant labeled samples of known source classes, zero-shot learning (ZSL) makes it possible to train recognition models for novel target classes that have no labeled samples. Conventional ZSL approaches usually adopt a two-step recognition strategy, in which the test sample is projected into an intermediary space in the first step, and then the recognition is carried out by considering the similarity between the sample and target classes in the intermediary space. Due to this redundant intermediate transformation, information loss is unavoidable, thus degrading the performance of overall system. Rather than adopting this two-step strategy, in this paper, we propose a novel one-step recognition framework that is able to perform recognition in the original feature space by using directly trained classifiers. To address the lack of labeled samples for training supervised classifiers for the target classes, we propose to transfer samples from source classes with pseudo labels assigned, in which the transferred samples are selected based on their transferability and diversity. Moreover, to account for the unreliability of pseudo labels of transferred samples, we modify the standard support vector machine formulation such that the unreliable positive samples can be recognized and suppressed in the training phase. The entire framework is fairly general with the possibility of further extensions to several common ZSL settings. Extensive experiments on four benchmark data sets demonstrate the superiority of the proposed framework, compared with the state-of-the-art approaches, in various settings.", "Activity recognition (AR)  feature-based knowledge transfer framework  smart home  transfer learning Building contextual models for new smart environments is not considered cost effective if data for model training must be collected from scratch. It is more practical to transfer as much learned knowledge as possible from an existing environment to the new target environment in order to reduce the data collection effort. In order to reuse learned knowledge from an original environment, this study proposed a feature-based knowledge transfer framework. The framework makes use of transfer learning, which relaxes the constraint requiring model training and testing datasets to be highly similar in distribution. Experimental results show that this framework can successfully help extract and transfer knowledge between two different smart-home environments. Models trained via the proposed framework can even outperform nontransfer-learning models by up to 8% in accuracy. Finally, the flexibility of the proposed framework enables used as a test bed for evaluating different methods and models in order to improve the service quality of human-centric context-aware applications.", "Deep Learning  Convolutional neural network  Clothing image recognition In the Big Data era, there is a need for powerful visual-based analytics tools when pictures have replaced texts and become main contents on the Internet. Hence, in this study, we explore convolutional neural networks with a goal of resolving clothing style classification and retrieval tasks. To reduce training complexity, low-level and mid-level features were learned in the deep models on large-scale datasets and then transfer learning is incorporated by fine-tuning pre-trained models using the clothing dataset. However, a large amount of collected data needs huge computations for tuning parameters. Therefore, one architecture inspired from Adaboost is designed to use multiple deep nets that are trained with a sub-dataset. Thus, the training time can be accelerated if each net is computed in one client node in a distributed computing environment. Moreover, to increase system flexibility, two architectures with multiple deep nets with two outputs are proposed for binary-class classification. Therefore, when new classes are added, no additional computation is needed for all training data. In order to integrate output responses from multiple nets, classification rules are proposed as well. Experiments are performed to compare existing systems with hand-crafted features. According to the results, the proposed system can provide significant improvements on three public clothing datasets for style classifications, particularly on the large dataset with 80,000 images where an improvement of 18% in accuracy was recognized.", "Model-based reinforcement learning  Transfer learning  Online feature selection Reinforcement learning is a plausible theoretical basis for developing self-learning, autonomous agents or robots that can effectively represent the world dynamics and efficiently learn the problem features to perform different tasks in different environments. The computational costs and complexities involved, however, are often prohibitive for real world applications. This study introduces a scalable methodology to learn and transfer knowledge of the transition (and reward) models for model-based reinforcement learning in a complex world. We propose a variant formulation of Markov decision processes that supports efficient online-learning of the relevant problem features to approximate the world dynamics. We apply the new feature selection and dynamics approximation techniques in heterogeneous transfer learning, where the agent automatically maintains and adapts multiple representations of the world to cope with the different environments it encounters during its lifetime. We prove regret bounds for our approach, and empirically demonstrate its capability to quickly converge to a near optimal policy in both real and simulated environments. ", "Reinforcement learning  Multi-objective optimization  Robotic tasks  Policy search Recently reinforcement learning has been widely applied to robotic tasks. However, most of these tasks hide more than one objective. In these cases, the construction of a reward function is a key and difficult issue. A typical solution is combining the multiple objectives into one single-objective reward function. However, quite often this formulation is far from being intuitive, and the learning process might converge to a behaviour far from what we need. Another alternative to face these multi-objective tasks is to use what is called transfer learning. In this case, the idea is to reuse the experience gained after the learning of an objective to learn a new one. Nevertheless, the transfer affects only to the learned policy, leaving out other gained information that might be relevant. In this paper, we propose a different approach to learn problems with more than one objective. In particular, we describe a two-stage approach. During the first stage, our algorithm will learn a policy compatible with a main goal at the same time that it gathers relevant information for a subsequent search process. Once this is done, a second stage will start, which consists of a cyclical process of small perturbations and stabilizations, and which tries to avoid degrading the performance of the system while it searches for a new valid policy but that also optimizes a sub-objective. We have applied our proposal for the learning of the biped walking. We have tested it on a humanoid robot, both on simulation and on a real robot.", "RGB-D image  Visual place recognition  Object categorization  Multi-modal deep learning Recognizing semantic category of objects and scenes captured using vision-based sensors is a challenging yet essential capability for mobile robots and UAVs to perform high-level tasks such as long-term autonomous navigation. However, extracting discriminative features from multi-modal inputs, such as RGB-D images, in a unified manner is non-trivial given the heterogeneous nature of the modalities. We propose a deep network which seeks to construct a joint and shared multi-modal representation through bilinearly combining the convolutional neural network (CNN) streams of the RGB and depth channels. This technique motivates bilateral transfer learning between the modalities by taking the outer product of each feature extractor output. Furthermore, we devise a technique for multi-scale feature abstraction using deeply supervised branches which are connected to all convolutional layers of the multi-stream CNN. We show that end-to-end learning of the network is feasible even with a limited amount of training data and the trained network generalizes across different datasets and applications. Experimental evaluations on benchmark RGB-D object and scene categorization datasets show that the proposed technique consistently outperforms state-of-the-art algorithms. ", "Transfer learning  metric learning  density ratio reweighting  Mahalanobis distance  learning framework Transfer learning has been proven to be effective for the problems where training data from a source domain and test data from a target domain are drawn from different distributions. To reduce the distribution divergence between the source domain and the target domain, many previous studies have been focused on designing and optimizing objective functions with the Euclidean distance to measure dissimilarity between instances. However, in some real-world applications, the Euclidean distance may be inappropriate to capture the intrinsic similarity or dissimilarity between instances. To deal with this issue, in this paper, we propose a metric transfer learning framework (MTLF) to encode metric learning in transfer learning. In MTLF, instance weights are learned and exploited to bridge the distributions of different domains, while Mahalanobis distance is learned simultaneously to maximize the intra-class distances and minimize the inter-class distances for the target domain. Unlike previous work where instance weights and Mahalanobis distance are trained in a pipelined framework that potentially leads to error propagation across different components, MTLF attempts to learn instance weights and a Mahalanobis distance in a parallel framework to make knowledge transfer across domains more effective. Furthermore, we develop general solutions to both classification and regression problems on top of MTLF, respectively. We conduct extensive experiments on several real-world datasets on object recognition, handwriting recognition, and WiFi location to verify the effectiveness of MTLF compared with a number of state-of-the-art methods.", "Transfer learning  Manipulation  Prediction  Robot Learning The ability to predict how objects behave during manipulation is an important problem. Models informed by mechanics are powerful, but are hard to tune. An alternative is to learn a model of the object's motion from data, to learn to predict. We study this for push manipulation. The paper starts by formulating a quasi-static prediction problem. We then pose the problem of learning to predict in two different frameworks: (i) regression and (ii) density estimation. Our architecture is modular: many simple, object specific, and context specific predictors are learned. We show empirically that such predictors outperform a rigid body dynamics engine tuned on the same data. We then extend the density estimation approach using a product of experts. This allows transfer of learned motion models to objects of novel shape, and to novel actions. With the right representation and learning method, these transferred models can match the prediction performance of a rigid body dynamics engine for novel objects or actions.", "Machine vision  universal feature extraction  information theory  transfer learning  extreme learning In many real-world image based pattern recognition tasks, the extraction and usage of task-relevant features are the most crucial part of the diagnosis. In the standard approach, either the features are given by common sense like edges or corners in image analysis, or they are directly determined by expertise. They mostly remain task-specific, although human may learn the life time, and use different features too, although same features do help in recognition. It seems that a universal feature set exists, but it is not yet systematically found. In our contribution, we try to find such a universal image feature set that is valuable for most image related tasks. We trained a shallow neural network for recognition of natural and non-natural object images before different backgrounds, including pure texture and handwritten digits, using a Shannon information-based algorithm and learning constraints. In this context, the goal was to extract those features that give the most valuable information for classification of the visual objects, hand-written digits and texture datasets by a one layer network and then classify them by a second layer. This will give a good start and performance for all other image learning tasks, implementing a transfer learning approach. As result, in our case we found that we could indeed extract unique features which are valid in all three different kinds of tasks. They give classification results that are about as good as the results reported by the corresponding literature for the specialized systems, or even better ones.", "Zero-shot learning  Object recognition  Semantic correlation  Manifold structure In practical object recognition tasks, one often encounters a problem to recognize some unseen objects, which are some new categories without labeled images at training stage. For solving the challenging problem, zero-shot learning has been studied widely which can be seen as a special case of transfer learning. Thus, zero-shot learning deals with the problem of predicting labels of target images based on source images and their common semantic knowledge. Most existing zero-shot learning methods focus on how to project the images into the semantic space. However, the projection function learnt by source images and attributes has a shift for the prediction of target attributes. In this paper, we proposed a zero-shot classification method by transferring knowledge from source domain and preserving target data structure (TKDS). Particularly, we directly learn the target classification model by the semantic correlation with source classification model. Different from existing similarity based zero-shot learning methods, we also utilize the data properties of target data themselves. We simultaneously consider transferring knowledge from source domain to explicit the source images and utilizing the manifold structure of target data to rectify domain shift problem, thus, boosting the prediction performance. Extensive experiments on the widely used datasets show that our model outperforms significantly the state-of-the-art methods. ", "Fully probabilistic design  Parameter prior  External predictive distribution  Bayesian transfer learning  Kullback-Leibler divergence This paper exploits knowledge made available by an external source in the form of a predictive distribution in order to elicit a parameter prior. It uses the terminology of Bayesian transfer learning, one of many domains dealing with reasoning as coherent knowledge processing. An empirical solution of the addressed problem was provided in [19], based on an interpretation of the external predictor as an empirical distribution constructed from fictitious data. In this paper, two main contributions are provided. First, the problem is solved using formal hierarchical Bayesian modeling [25], and the knowledge transfer is achieved optimally, i.e. in the minimum-KLD sense. Second, this hierarchical setting yields a distribution on the set of possible priors, with the choice [19] acting as the base distribution. This allows randomized choices of the prior to be generated, avoiding costly and/or intractable estimation of this prior. It also provides measures of uncertainty in the prior choice, allowing subsequent learning tasks to be assessed for robustness to this prior choice. The instantiation of the method in already published applications in knowledge elicitation, recursive learning and flat cooperation of adaptive controllers is recalled, and prospective application domains are also mentioned. (C) 2017 Elsevier Inc. All rights reserved.", "Tool-body assimilation  Motor babbling  Deep neural network  Recurrent neural network  Transfer learning We propose a tool-body assimilation model that considers grasping during motor babbling for using tools. A robot with tool-use skills can be useful in human robot symbiosis because this allows the robot to expand its task performing abilities. Past studies that included tool-body assimilation approaches were mainly focused on obtaining the functions of the tools, and demonstrated the robot starting its motions with a tool pre-attached to the robot. This implies that the robot would not be able to decide whether and where to grasp the tool. In real life environments, robots would need to consider the possibilities of tool grasping positions, and then grasp the tool. To address these issues, the robot performs motor babbling by grasping and nongrasping the tools to learn the robot's body model and tool functions. In addition, the robot grasps various parts of the tools to learn different tool functions from different grasping positions. The motion experiences are learned using deep learning. In model evaluation, the robot manipulates an object task without tools, and with several tools of different shapes. The robot generates motions after being shown the initial state and a target image, by deciding whether and where to grasp the tool. Therefore, the robot is capable of generating the correct motion and grasping decision when the initial state and a target image are provided to the robot. (C) 2017 The Authors. Published by Elsevier B.V.", "Convolutional neural networks  Deep learning  Plant identification  Transfer learning  Inverse rank score We use deep convolutional neural networks to identify the plant species captured in a photograph and evaluate different factors affecting the performance of these networks. Three powerful and popular deep learning architectures, namely GoogLeNet, AlexNet, and VGGNet, are used for this purpose. Transfer learning is used to fine-tune the pre-trained models, using the plant task datasets of LifeCLEF 2015. To decrease the chance of overfitting, data augmentation techniques are applied based on image transforms such as rotation, translation, reflection, and scaling. Furthermore, the networks' parameters are adjusted and different classifiers are fused to improve overall performance. Our best combined system has achieved an overall accuracy of 80% on the validation set and an overall inverse rank score of 0.752 on the official test set. A comparison of our results against the results of the LifeCLEF 2015 plant identification campaign shows that we have improved the overall validation accuracy of the top system by 15% points and its overall inverse rank score on the test set by 0.1 while outperforming the top three competition participants in all categories. The system recently obtained a very close second place in the P1antCLEF 2016.", "Sum-Product Networks  Transfer learning To learn an effective Sum-Product Network (SPN) for probabilistic inference, one needs to have a substantial amount of data. In the case when the training dataset is small, SPN performance can be degraded. In this paper, we investigate how transfer learning can improve a SPN when the number of training examples is limited. In particular, we consider a structural transfer setting where (i) one does not have a source dataset but a source SPN, and (ii) there is some kind of similarity between the source SPN and the target domain. We propose a transfer learning approach called TopTrSPN, utilizing the information of the first layer clusters in the source SPN to learn the first layer of the target SPN. Our approach is motivated by transfer learning characteristics of Convolution Neural Network (CNN) as SPN can be viewed as a probabilistic, general-purpose convolution network. Moreover, since the source SPN may have some distribution differences from the target domain, we perform matching between the two by filtering out inconsistent variables. Empirical results on twenty benchmark datasets show the feasibility of our proposed transfer learning approach for SPN structure learning a target domain. Moreover, our proposed transfer learning approach shows encouraging performance when it is applied to text datasets with different set of variables. ", "Transfer learning  Discriminant analysis  Sparsity regularization  Single-sample face recognition Discriminant analysis is an important technique for face recognition because it can extract discriminative features to classify different persons. However, most existing discriminant analysis methods fail to work for single-sample face recognition (SSFR) because there is only a single training sample per person such that the within-class variation of this person cannot be estimated in such scenario. In this paper, we present a new discriminative transfer learning (DTL) approach for SSFR, where discriminant analysis is performed on a multiple-sample generic training set and then transferred into the single-sample gallery set. Specifically, our DTL learns a feature projection to minimize the intra-class variation and maximize the inter-class variation of samples in the training set, and minimize the difference between the generic training set and the gallery set, simultaneously. To make the DTL be robust to outliers and noise, we employ a sparsity regularizer to regularize the DTL and further propose a novel discriminative transfer learning with sparsity regularization (DTLSR) method. Experimental results on three face datasets including the FERET, CAS-PEAL-R1, and real world LFW datasets are presented to show the efficacy of the proposed methods. ", "Mammograms  Masses  Detection  Segmentation  Classification  Deep learning  Bayesian optimisation  Transfer learning  Structured output learning We present an integrated methodology for detecting, segmenting and classifying breast masses from mammograms with minimal user intervention. This is a long standing problem due to low signal-tonoise ratio in the visualisation of breast masses, combined with their large variability in terms of shape, size, appearance and location. We break the problem down into three stages: mass detection, mass segmentation, and mass classification. For the detection, we propose a cascade of deep learning methods to select hypotheses that are refined based on Bayesian optimisation. For the segmentation, we propose the use of deep structured output learning that is subsequently refined by a level set method. Finally, for the classification, we propose the use of a deep learning classifier, which is pre-trained with a regression to hand-crafted feature values and fine-tuned based on the annotations of the breast mass classification dataset. We test our proposed system on the publicly available INbreast dataset and compare the results with the current state-of-the-art methodologies. This evaluation shows that our system detects 90% of masses at 1 false positive per image, has a segmentation accuracy of around 0.85 (Dice index) on the correctly detected masses, and overall classifies masses as malignant or benign with sensitivity (Se) of 0.98 and specificity (Sp) of 0.7. ", "Transfer learning  Active learning  Recommender systems In the past decade, artificial intelligence (AI) techniques have been successfully applied to recommender systems employed in many e-commerce companies, such as Amazon, eBay, Netflix, etc., which aim to provide personalized recommendations on products or services. Among various AI-based recommendation techniques, collaborative filtering has proven to be one of the most promising methods. However, most collaborative-filtering-based recommender systems, especially the newly launched ones, have trouble making accurate recommendations for users. This is caused by the data sparsity issue in recommender systems, where little existing rating information is available. To address this issue, one of the most effective practices is applying transfer learning techniques by leveraging relatively rich collaborative data knowledge from related systems, which have been well running. Previous transfer learning models for recommender systems often assume that a sufficient set of entity correspondences (either user or item) across the target and auxiliary systems (a.k.a. source systems) is given in advance. This assumption does not hold in many real world scenarios where entity correspondences across systems are usually unknown, and the cost of identifying them can be expensive. In this paper, we propose a new transfer learning framework for recommender systems, which relaxes the above assumption to facilitate flexible knowledge transfer across different systems with low cost by using an active learning principle to construct entity correspondences across systems. Specifically, for the purpose of maximizing knowledge transfer, we first iteratively select entities in the target system based on some criterion to query their correspondences in the source system. We then plug the actively constructed entity correspondences into a general transferred collaborative-filtering model to improve recommendation quality. Based on the framework, we propose three solutions by specifying three state-of-the-art collaborative filtering methods, namely Maximum-Margin Matrix Factorization, Regularized Low-rank Matrix Factorization, and Probabilistic Matrix Factorization. We perform extensive experiments on two real-world datasets to verify the effectiveness of our proposed framework and the three specified solutions for cross-system recommendation. ", "Computer vision  convolutional nets  deep learning  transfer learning Models based on deep convolutional networks have dominated recent image interpretation tasks  we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a fixed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are doubly deep in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics  yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined or optimized.", "Semantic Segmentation  Convolutional Networks  Deep Learning  Transfer Learning Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build fully convolutional networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.", "Instance-transfer learning  Conformity prediction framework  Exchangeability test This paper introduces a non-parametric test to decide whether to transfer data from a source domain to a target domain to improve the generalization performance of predictive models on the target domain. The test is based on the conformal prediction framework: it statistically tests whether the target and source data are generated from the same distribution under the exchangeability assumption. The experiments show that the test is capable of outperforming existing methods when it decides on instance transfer. ", "Transfer learning  Domain adaptation  Visual object detection  Greedy algorithms  Feature selection In this paper we consider the binary transfer learning problem, focusing on how to select and combine sources from a large pool to yield a good performance on a target task. Constraining our scenario to real world, we do not assume the direct access to the source data, but rather we employ the source hypotheses trained from them. We propose an efficient algorithm that selects relevant source hypotheses and feature dimensions simultaneously, building on the literature on the best subset selection problem. Our algorithm achieves state-of-the-art results on three computer vision datasets, substantially outperforming both transfer learning and popular feature selection baselines in a small-sample setting. We also present a randomized variant that achieves the same results with the computational cost independent from the number of source hypotheses and feature dimensions. Also, we theoretically prove that, under reasonable assumptions on the source hypotheses, our algorithm can learn effectively from few examples. (C) 2016 Elsevier Inc. All rights reserved.", "Standard-free calibration transfer  Transfer learning  Transfer component analysis  Artificial transfer standards  Transfer orthogonal signal correction  Piecewise direct standardization The combination of spectroscopic measurements and multivariate calibration techniques (chemometrics) has become a state-of-the-art technology for process analytical chemistry. Changes, intended or unintended, in the environmental conditions, the measurement setup or of the measured substance itself can result in a calibration model that is no longer adequate for the intended purpose. In such a situation, either a new model needs to be developed or (calibration) transfer methods, can be applied to transfer models from the original (main, master) to the new (remote, slave) setting. In this contribution, we introduce, discuss and evaluate a wide-ranging subset of transfer approaches available in chemometrics and the field of machine learning, where we focus on techniques applicable in situations where transfer standards, i.e. a set of samples measured under the original as well as the new setting, cannot be provided and only few reference measurements are available for the new setting. The introduced techniques are evaluated on a public data set as well as a new industrial data set displaying three forms of transfer problems. The efficiency of the proposed transfer approaches in terms of the number of required reference measurements compared to full model recalibration can bee confirmed. Average rank maps are presented to provide guidance on a proper choice among evaluated techniques.", "Transfer learning  cross-domain metric  marginalized denoising Metric learning has attracted increasing attention due to its critical role in image analysis and classification. Conventional metric learning always assumes that the training and test data are sampled from the same or similar distribution. However, to build an effective distance metric, we need abundant supervised knowledge (i.e., side/label information), which is generally inaccessible in practice, because of the expensive labeling cost. In this paper, we develop a robust transfer metric learning (RTML) framework to effectively assist the unlabeled target learning by transferring the knowledge from the well-labeled source domain. Specifically, RTML exploits knowledge transfer to mitigate the domain shift in two directions, i.e., sample space and feature space. In the sample space, domain-wise and class-wise adaption schemes are adopted to bridge the gap of marginal and conditional distribution disparities across two domains. In the feature space, our metric is built in a marginalized denoising fashion and low-rank constraint, which make it more robust to tackle noisy data in reality. Furthermore, we design an explicit rank constraint regularizer to replace the rank minimization NP-hard problem to guide the low-rank metric learning. Experimental results on several standard benchmarks demonstrate the effectiveness of our proposed RTML by comparing it with the state-of-the-art transfer learning and metric learning algorithms.", "Reinforcement learning  Skill acquisition  Transfer learning  Graph learning Since reinforcement learning algorithms suffer from the curse of dimensionality in continuous domains, generalization is the most challenging issue in this area. Both skill acquisition and transfer learning are successful techniques to overcome such problem that result in big improvements in agent learning performance. In this paper, we propose a novel graph based skill acquisition method, named GSL, and a skill based transfer learning framework, named STL. GSL discovers skills as high-level knowledge using community detection from connectivity graph, a model to capture not only the agent's experience but also the environment's dynamics. STL incorporates skills previously learned from source task to speed up learning on a new target task. The experimental results indicate the effectiveness of the proposed methods in dealing with continuous reinforcement learning problems. ", "Context-aware computing  egocentric dataset  egocentric vision  first person vision  personal location recognition Contextual awareness in wearable computing allows for construction of intelligent systems, which are able to interact with the user in a more natural way. In this paper, we study how personal locations arising from the user's daily activities can be recognized from egocentric videos. We assume that few training samples are available for learning purposes. Considering the diversity of the devices available on the market, we introduce a benchmark dataset containing egocentric videos of eight personal locations acquired by a user with four different wearable cameras. To make our analysis useful in real-world scenarios, we propose a method to reject negative locations, i.e., those not belonging to any of the categories of interest for the end-user. We assess the performances of the main state-of-the-art representations for scene and object classification on the considered task, as well as the influence of device-specific factors such as the field of view and the wearing modality. Concerning the different device-specific factors, experiments revealed that the best results are obtained using a head-mounted wide-angular device. Our analysis shows the effectiveness of using representations based on convolutional neural networks, employing basic transfer learning techniques and an entropy-based rejection algorithm.", "Facial expression analysis  personalization  domain adaptation  transfer learning  support vector machine (SVM) Automatic facial action unit (AU) and expression detection from videos is a long-standing problem. The problem is challenging in part because classifiers must generalize to previously unknown subjects that differ markedly in behavior and facial morphology (e. g., heavy versus delicate brows, smooth versus deeply etched wrinkles) from those on which the classifiers are trained. While some progress has been achieved through improvements in choices of features and classifiers, the challenge occasioned by individual differences among people remains. Person-specific classifiers would be a possible solution but for a paucity of training data. Sufficient training data for person-specific classifiers typically is unavailable. This paper addresses the problem of how to personalize a generic classifier without additional labels from the test subject. We propose a transductive learning method, which we refer to as a Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific mismatches. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. We compared STM to both generic classifiers and cross-domain learning methods on four benchmarks: CK+ [44], GEMEP-FERA [67], RUFACS [4] and GFT [57]. STM outperformed generic classifiers in all.", "Fast-rate generalization bounds  Transfer learning  Domain adaptation  Rademacher complexity  Smooth loss functions  Strongly-convex regularizers In this work we consider the learning setting where, in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks. We focus on a broad class of ERM-based linear algorithms that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer. We establish generalization and excess risk bounds, showing that, if the algorithm is fed with a good combination of source hypotheses, generalization happens at the fast rate O(1/m) instead of the usual O(1/root m). On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works.", "Transfer learning  Unsupervised domain adaptation  Domain invariant clustering  Domain shift  Invariant feature representation One of the serious challenges in computer vision and image classification is learning an accurate classifier for a new unlabeled image dataset, considering that there is no available labeled training data. Transfer learning and domain adaptation are two outstanding solutions that tackle this challenge by employing available datasets, even with significant difference in distribution and properties, and transfer the knowledge from a related domain to the target domain. The main difference between these two solutions is their primary assumption about change in marginal and conditional distributions where transfer learning emphasizes on problems with same marginal distribution and different conditional distribution, and domain adaptation deals with opposite conditions. Most prior works have exploited these two learning strategies separately for domain shift problem where training and test sets are drawn from different distributions. In this paper, we exploit joint transfer learning and domain adaptation to cope with domain shift problem in which the distribution difference is significantly large, particularly vision datasets. We therefore put forward a novel transfer learning and domain adaptation approach, referred to as visual domain adaptation (VDA). Specifically, VDA reduces the joint marginal and conditional distributions across domains in an unsupervised manner where no label is available in test set. Moreover, VDA constructs condensed domain invariant clusters in the embedding representation to separate various classes alongside the domain transfer. In this work, we employ pseudo target labels refinement to iteratively converge to final solution. Employing an iterative procedure along with a novel optimization problem creates a robust and effective representation for adaptation across domains. Extensive experiments on 16 real vision datasets with different difficulties verify that VDA can significantly outperform state-of-the-art methods in image classification problem.", "Histopathological image analysis  Deep contour-aware network  Deep learning  Transfer learning  Object detection  Instance segmentation In histopathological image analysis, the morphology of histological structures, such as glands and nuclei, has been routinely adopted by pathologists to assess the malignancy degree of adenocarcinomas. Accurate detection and segmentation of these objects of interest from histology images is an essential prerequisite to obtain reliable morphological statistics for quantitative diagnosis. While manual annotation is error-prone, time-consuming and operator-dependant, automated detection and segmentation of objects of interest from histology images can be very challenging due to the large appearance variation, existence of strong mimics, and serious degeneration of histological structures. In order to meet these challenges, we propose a novel deep contour-aware network (DCAN) under a unified multi-task learning framework for more accurate detection and segmentation. In the proposed network, multi-level contextual features are explored based on an end-to-end fully convolutional network (FCN) to deal with the large appearance variation. We further propose to employ an auxiliary supervision mechanism to overcome the problem of vanishing gradients when training such a deep network. More importantly, our network can not only output accurate probability maps of histological objects, but also depict clear contours simultaneouily for separating clustered object instances, which further boosts the segmentation performance. Our method ranked the first in two histological object segmentation challenges, including 2015 MICCAI Gland Segmentation Challenge and 2015 MICCAI Nuclei Segmentation Challenge. Extensive experiments on these two challenging datasets demonstrate the superior performance of our method, surpassing all the other methods by a significant margin. ", " Most recently proposed methods for Neural Program Induction work under the assumption of having a large set of input/output (I/O) examples for learning any underlying input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a k-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language [17]. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance.", " We consider the Hypothesis Transfer Learning (HTL) problem where one incorporates a hypothesis trained on the source domain into the learning procedure of the target domain. Existing theoretical analysis either only studies specific algorithms or only presents upper bounds on the generalization error but not on the excess risk. In this paper, we propose a unified algorithm-dependent framework for HTL through a novel notion of transformation function, which characterizes the relation between the source and the target domains. We conduct a general risk analysis of this framework and in particular, we show for the first time, if two domains are related, HTL enjoys faster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge Regression than those of the classical non-transfer learning settings. Experiments on real world data demonstrate the effectiveness of our framework.", " We introduce a new formulation of the Hidden Parameter Markov Decision Process (HiP-MDP), a framework for modeling families of related tasks using low-dimensional latent embeddings. Our new framework correctly models the joint uncertainty in the latent parameters and the state space. We also replace the original Gaussian Process-based model with a Bayesian Neural Network, enabling more scalable inference. Thus, we expand the scope of the HiP-MDP to applications with higher dimensions and more complex dynamics.", " Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSDBirds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.", " We propose a framework that learns a representation transferable across different domains and tasks in a label efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.", " Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a distilled policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust to hyperparameter settings and more stable-attributes that are critical in deep reinforcement learning.", " We describe an approach to learning from long-tailed, imbalanced datasets that are prevalent in real-world settings. Here, the challenge is to learn accurate few-shot models for classes in the tail of the class distribution, for which little data is available. We cast this problem as transfer learning, where knowledge from the data-rich classes in the head of the distribution is transferred to the data-poor classes in the tail. Our key insights are as follows. First, we propose to transfer meta-knowledge about learning-to-learn from the head classes. This knowledge is encoded with a meta-network that operates on the space of model parameters, that is trained to predict many-shot model parameters from few-shot model parameters. Second, we transfer this meta-knowledge in a progressive manner, from classes in the head to the body, and from the body to the tail. That is, we transfer knowledge in a gradual fashion, regularizing meta-networks for few-shot regression with those trained with more training data. This allows our final network to capture a notion of model dynamics, that predicts how model parameters are likely to change as more training data is gradually added. We demonstrate results on image classification datasets (SUN, Places, and ImageNet) tuned for the long-tailed setting, that significantly outperform common heuristics, such as data resampling or reweighting.", " Synthesizing realistic profile faces is promising for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss  (ii) an identity perception loss  (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our submissions to NIST IJB-A 2017 face recognition competitions, where we won the 1st places on the tracks of verification and identification.", " There are many open issues and challenges in the multi-agent reward-based learning field. Theoretical convergence guarantees are lost, and the complexity of the action-space is also exponential to the amount of agents calculating their optimal joint-action. Function approximators, such as deep neural networks, have successfully been used in singleagent environments with high dimensional state-spaces. We propose the Multi-agent Double Deep Q-Networks algorithm, an extension of Deep Q-Networks to the multi-agent paradigm. Two common techniques of multi-agent Q-learning are used to formally describe our proposal, and are tested in a Foraging Task and a Pursuit Game. We also demonstrate how they can generalize to similar tasks and to larger teams, due to the strength of deep-learning techniques, and their viability for transfer learning approaches. With only a small fraction of the initial task's training, we adapt to longer tasks, and we accelerate the task completion by increasing the team size, thus empirically demonstrating a solution to the complexity issues of the multi-agent field.", "Query classification  Active learning  Transfer learning  Search engine  Query log In this paper, we address the problem of recognizing domain-specific queries from general search engine's query log. Unlike most previous work in query classification relying on external resources or annotated training queries, we take query log as the only resource for recognizing domain-specific queries. In the proposed approach, we represent query log as a heterogeneous graph and then formulate the task of domain-specific query recognition as graph-based transductive learning. In order to reduce the impact of noisy and insufficient of initial annotated queries, we further introduce an active learning strategy into the learning process such that the manual annotations needed are reduced and the recognition results can be continuously refined through interactive human supervision. Experimental results demonstrate that the proposed approach is capable of recognizing a certain amount of high-quality domain-specific queries with only a small number of manually annotated queries.", " Recommender systems that make use of collaborative filtering tend to suffer from data sparsity as the number of items rated by the users are very small as compared to the very large item space. In order to alleviate it, recently transfer learning (TL) methods have seen a growing interest wherein data is considered from multiple domains so that ratings from the first (source) domain can be used to improve the prediction accuracy in the second (target) domain. In this paper, we propose a model for transfer learning in collaborative filtering wherein the latent factor model for the source domain is obtained through Matrix Factorization (MF). User and Item matrices are combined in a novel way to generate cluster level rating pattern and a Code Book Transfer (CBT) is used for transfer of information from source to the target domain. Results from experiments using benchmark datasets show that our model approximates the target matrix well.", " The dynamic scene in a video comprises of a specific spatio-temporal pattern. A mask can learn the features efficiently compared to a sliding kernel approach as in a convolutional neural network that shrinks many parameters with respect to non-sliding or fully connected neural networks. In this paper, 3DPyraNet-F a discriminative approach of spatio-temporal feature learning is proposed for dynamic scene recognition. It performs transfer learning by considering the highest layer of the learned network structure and combines it with a linear-SVM classifier, in a way that enhances dynamic scenes in videos. Encouraging results are achieved despite the lower computational cost, fewer parameters, and camera-induced motion. It outperforms the state-of-the-art for MaryLand-in-the-wild and shows a comparable result for YUPPEN dataset.", "Scene understanding  food related scene classification  fine-tuning  convolutional neural networks In human smart nutrition systems, environment based food classification has become popular to help analyzing the food intake based on the nutrition related activity. In this paper, we address the problem of food related environments, which refer to different eating places such as, bars, restaurants, coffee shops, etc. using state-of-the-art convolutional neural networks (CNNs). We collected a new dataset on different food related environments by integrating three publicly available datasets: Places365, ImageNet and SUN397. We have named it FoodPlaces and it contains 35 different types of classes. In order to achieve satisfactory results on the food related environment recognition, we fine-tuned several state-of-the-art CNNs, such as VGG16, RsNet50 and InceptionV3 using different transfer learning approaches. The results show that the fully fine-tunned InceptionV3 yields 75.22% classification accuracy among the discussed state-of-the-art CNNs.", "Transfer learning  Deep convolutional neural networks  EEG signals Transfer learning (TL) has gained significant interests recently in brain computer interface (BCI) as a key approach to design robust predictors for cross-subject and cross-experiment prediction of the brain activities in response to cognitive events. We carried out in this.aper the first comprehensive investigation of the transferability of deep convolutional neural network (CNN) for cross-subject and cross-experiment prediction of image Rapid Serial Visual Presentation (RSVP) events. We show that for both cross-subject and cross-experiment predictions, all convolutional layers and fully connected layers contain both general and subject/experiment-specific features and transfer learning with weights fine-tuning can improve the prediction performance over that without transfer. However, for cross-subject prediction, the convolutional layers capture more subject-specific features, whereas for cross-experiment prediction, the convolutional layers capture more general features across experiment. Our study provides important information that will guide the design of more sophisticated deep transfer learning algorithms for EEG based classifications in BCI applications.", "Deep neural network  Sentence selection  Transfer learning In this paper, we propose Constrained Deep Neural Network (CDNN) a simple deep neural model for answer sentence selection. CDNN makes its predictions based on neural reasoning compound with some symbolic constraints. It integrates pattern matching technique into sentence vector learning. When trained using enough samples, CDNN outperforms regular models. We show how using other sources of training data as a mean of transfer learning can enhance the performance of the network. In a well-studied dataset for answer sentence selection, our network improves the state of the art in answer sentence selection significantly.", "Theme analysis  Deep learning  Transfer learning This paper puts forward theme analysis problem in order to automatically solve composition writing questions in Chinese college entrance examination. Theme analysis is to distillate the embedded semantic information from the given materials or documents. We proposes a hierarchical neural network framework to address this problem. Two deep learning based models under the proposed framework are presented. Besides, two transfer learning strategies based on the proposed deep learning models are tried to deal with the lack of large training data for composition theme analysis problems. Experimental results on two tag recommendation data sets show the effect of the proposed deep learning based theme analysis models. Also, we show the effect of the proposed model with transfer learning on a composition writing questions data set built by ourself.", "Expedited convolutional neural network  Mild cognitive impairment  Tucker decomposition  Magnetic resonance imaging Few studies have focused on the potential of applying deep learning algorithms into magnetic resonance imaging (MRI) for automatic recognition of subjects with mild cognitive impairment (MCI). In this work, we propose the expedited convolutional neural networks involving Tucker decomposition to recognize MCI using MRI images. We employ transfer learning and data augmentation to deal with limited training data. The effect of Tucker decomposition on saving computational time is discussed. The experimental results show that the proposed model outperforms the previous methods. The expedited convolutional neural networks can provide a good guidance for the applications of deep learning in real-world classification with large training dataset.", "Image fusion  Layer division  Transfer learning  Coupled dictionary A novel layer based image fusion method is proposed in this paper. It exploits and utilizes the implicated patterns among source images with two parts: (i) proposed a more precise model roots in transfer learning and coupled dictionary for layering source images  (ii) designed appropriate fusion scheme which bases on multi-scale transformation for recombining layers into final fused image efficiently. Rigorous experimental comparison in subjective and objective demonstrates that proposed image fusion method achieves better result in visual perception and computer process.", "Individual healthcare  Transfer learning  Neural networks  Disease prediction  Unlabeled data Nowadays, emerging mobile medical technology and disease prevention become new trends of disease prevention and control. Based on this technology, we present disease prediction models based on transfer learning. Breast cancer disease data has been used to build our model. According to the neural networks, the basic model has been provided. With unlabeled data, transfer learning is a appropriate way to revise the module to increase accuracy. The test results show that the algorithm is suitable for data classification, especially for unlabeled health data.", "Face recognition  Service robot  Audio-visual fusion  Deep learning In the research of family service robot, face recognition has become a hotspot because of its wide application prospect in home security, human-computer interaction and authentication. The robot must first solve the user's identity registration problem before performing the task of face recognition. To make the registration process more natural and convenient, this paper combines the human audio-visual information, first use the voice interactive way to collect the user's name and face image information, and then use the improved deep neural network and transfer learning method quickly training out the face recognition model to achieve the user's identity registration. The experimental results show that the proposed method not only improves the convenience of identity registration but also shortens the time it takes for user's identity registration.", "Deep convolutional neural network  DCE-MRI  Breast  Cancer Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) is gaining popularity as a complementary diagnostic method for early detection and diagnosis of breast cancer. However, due to the large amount of data, DCE-MRI can hardly be inspected without the use of a Computer Aided Diagnosis (CAD) system. Among the major issues in developing CAD for breast DCE-MRI there is the classification of regions of interest according to their aggressiveness. For this task newer hand-crafted features are continuously proposed by domain experts. On the other hand, deep learning approaches have gained popularity in many pattern recognition tasks, being able to outperform classical machine learning techniques in different fields, by learning compact hierarchical representations of an image which well fit the specific task to solve. The aim of this work is to explore the applicability of Convolutional Neural Networks (CNN) in automatic lesion malignancy assessment for breast DCE-MRI data. Our findings show that while promising results in treating DCE-MRI can be obtained by using transfer learning, CNNs have to be carefully designed and tuned in order to outperform approaches specifically designed to exploit all the available data information.", "Transfer Learning  Deep Learning  Faster R-CNN  Sequential Monte Carlo Filter (SMC)  Pedestrian Detection The performance of a generic pedestrian detector decreases significantly when it is applied to a specific scene due to the large variation between the source dataset used to train the generic detector and samples in the target scene. In this paper, we suggest a new approach to automatically specialize a scene-specific pedestrian detector starting with a generic detector in video surveillance without further manually labeling any samples under a novel transfer learning framework. The main idea is to consider a deep detector as a function that generates realizations from the probability distribution of the pedestrian to be detected in the target. Our contribution is to approximate this target probability distribution with a set of samples and an associated specialized deep detector estimated in a sequential Monte Carlo filter framework. The effectiveness of the proposed framework is demonstrated through experiments on two public surveillance datasets. Compared with a generic pedestrian detector and the state-of-the-art methods, our proposed framework presents encouraging results.", "Deep Learning  Convolution Neural Networks  Fusion  Transfer Learning Deep learning, in particular convolutional neural networks, has increasingly been applied to medical images. Advances in hardware coupled with availability of increasingly large data sets have fueled this rise. Results have shattered expectations. But it would be premature to cast aside conventional machine learning and image processing techniques. All that deep learning comes at a cost, the need for very large datasets. We discuss the role of conventional manually tuned features combined with deep learning. This process of fusing conventional image processing techniques with deep learning can yield results that are superior to those obtained by either learning method in isolation. In this article, we review the rise of deep learning in medical image processing and the recent onset of fusion of learning methods. We discuss supervision equilibrium point and the factors that favor the role of fusion methods for histopathology and quasi-histopathology modalities.", "Braid Hairstyle Recognition  Convolutional Neural Networks In this paper, we present a novel braid hairstyle recognition system based on Convolutional Neural Networks (CNNs). We first build a hairstyle patch dataset that is composed of braid hairstyle patches and non-braid hairstyle patches (straight hairstyle patches, curly hairstyle patches, and kinky hairstyle patches). Then we train our hairstyle recognition system via transfer learning on a pre-trained CNN model in order to extract the features of different hairstyles. Our hairstyle recognition CNN model achieves the accuracy of 92.7% on image patch dataset. Then the CNN model is used to perform braid hairstyle detection and recognition in full-hair images. The experiment results shows that the patch-level trained CNN model can successfully detect and recognize braid hairstyle in image-level.", "Pedestrian Counting  Deep Learning  Convolutional Neural Networks  Synthetic Images  Transfer Learning  Cross Entropy Cost Function  Squared Error Cost Function Counting pedestrians in surveillance applications is a common scenario. However, it is often challenging to obtain sufficient annotated training data, especially so for creating models using deep learning which require a large amount of training data. To address this problem, this paper explores the possibility of training a deep convolutional neural network (CNN) entirely from synthetically generated images for the purpose of counting pedestrians. Nuances of transfer learning are exploited to train models from a base model trained for image classification. A direct approach and a hierarchical approach are used during training to enhance the capability of the model for counting higher number of pedestrians. The trained models are then tested on natural images of completely different scenes captured by different acquisition systems not experienced by the model during training. Furthermore, the effectiveness of the cross entropy cost function and the squared error cost function are evaluated and analyzed for the scenario where a model is trained entirely using synthetic images. The performance of the trained model for the test images from the target site can be improved by fine-tuning using the image of the background of the target site.", "Fingerprint Classification  Transfer Learning  Convolutional Neural Networks Reducing the number of comparisons in automated fingerprint identification systems is essential when dealing with a large database. Fingerprint classification allows to achieve this goal by dividing fingerprints into several categories, but it presents still some challenges due to the large intra-class variations and the small inter-class variations. The vast majority of the previous methods uses global characteristics, in particular the orientation image, as features of a classifier. This makes the feature extraction stage highly dependent on preprocessing techniques and usually computationally expensive. In this work we evaluate the performance of two pretrained convolutional neural networks fine-tuned on the NIST SD4 benchmark database. The obtained results show that this approach is comparable with other results in the literature, with the advantage of a fast feature extraction stage.", "Image tagging  Convolutional neural networks  Personalized tag recommendation  Factorization models Image tag recommendation in social media systems provides the users with personalized tag suggestions which facilitate the users' tagging task and enable automatic organization and many image retrieval tasks. Factorization models are a widely used approach for personalized tag recommendation and achieve good results. These methods rely on the user's tagging preferences only and ignore the contents of the image. However, it is obvious that especially the contents of the image, such as the objects appearing in the image, colors, shapes or other visual aspects, strongly influence the user's tagging decisions. We present a personalized content-aware image tag recommendation approach that combines both historical tagging information and image-based features in a factorization model. Employing transfer learning, we apply state of the art deep learning image classification and object detection techniques to extract powerful features from the images. Both, image information and tagging history, are fed to an adaptive factorization model to recommend tags. Empirically, we can demonstrate that the visual and object-based features can improve the performance up to 1.5% over the state of the art.", "Electricity load forecasting  Boosting  Multiple kernel learning  Transfer learning Accurate electricity load forecasting is of crucial importance for power system operation and smart grid energy management. Different factors, such as weather conditions, lagged values, and day types may affect electricity load consumption. We propose to use multiple kernel learning (MKL) for electricity load forecasting, as it provides more flexibilities than traditional kernel methods. Computation time is an important issue for short-term load forecasting, especially for energy scheduling demand. However, conventional MKL methods usually lead to complicated optimization problems. Another practical aspect of this application is that there may be very few data available to train a reliable forecasting model for a new building, while at the same time we may have prior knowledge learned from other buildings. In this paper, we propose a boosting based framework for MKL regression to deal with the aforementioned issues for short-term load forecasting. In particular, we first adopt boosting to learn an ensemble of multiple kernel regressors, and then extend this framework to the context of transfer learning. Experimental results on residential data sets show the effectiveness of the proposed algorithms.", "component  Fuzzy Transfer Learning  Brain-Computer Interface (BCI)  Generalized Hidden-Mapping Ridge Regression (GHRR)  Classification  Fuzzy Rule Generation In brain-computer interfaces (BCI), the statistical distribution of the data could differ across subjects as well as across sessions for an individual subject. Moreover, the lack of data due to the difficulties in collecting data in BCI is a common challenge in training the systems. Since most of machine learning tools are based on the assumption that the distribution of training and testing data are the same and they need adequate training data, they would fail in such situations. To overcome this problem and because of the vague and uncertain essence of EEG data, in this paper, we used a fuzzy transfer learning (FTL) method based on Generalized Hidden-Mapping Ridge Regression (GHRR) to improve the classification task in BCI. Takagi-Sugeno-Kang fuzzy logical system (TSK) with proposed modified Wang-Mendel fuzzy rule generation were employed for classification. Then the session-to-session transfer of knowledge is adopted. The results demonstrate the effectiveness of our proposed method in classification and outperform the well-known SVM classifier.", "Unconstrained age estimation  Convolutional Neural Network  Transform learning Age estimation has always hit people's eyes. While most previous works have focused on constrained images taken under lab condition, which is far from real-world age estimation. The benchmark we used in this paper is the unconstrained Adience [3], which is believed to better reflect the traits of age in the wild condition. In this paper, we adapted contrastive loss to fine-tune the pre-trained VGG-16 over FG-NET to get a better start point and proposed AvgOut-FC Layer to enhance the performance of the models over Audience. We have achieved better results over the Adience benchmark than previous works, which demonstrated the effectiveness of our methods.", "Convolutional neural networks  End-to-End  Webpage saliency prediction  Two-stage transfer learning With the great success of convolutional neural networks (CNN) achieved on various computer vision tasks in recent years, CNN has also been applied in natural image saliency prediction. As a specific visual stimuli, web-pages exhibit evident similarities whereas also significant differences from natural image. Consequently, the learned CNN for natural image saliency prediction cannot be directly used to predict webpage saliency. Only a few researches on webpage saliency prediction have been developed till now. In this paper, we propose a simple yet effective scheme of two-stage transfer learning of end-to-end CNN to predict the webpage saliency. In the first stage, the output layer of two typical CNN architectures with instances of AlexNet and VGGNet are reconstructed, and the parameters between the fully connected layers are relearned from a large natural image database for image saliency prediction. In the second stage, the parameters between the fully connected layers are relearned from a scarce webpage database for webpage saliency prediction. In fact, the two-stage transfer learning can be regarded as a task transfer in the first stage and a domain transfer in the second stage, respectively. The experimental results indicate that the proposed two-stage transfer learning of end-to-end CNN can obtain a substantial performance improvement for webpage saliency prediction.", "Big data  Transfer learning  Temperature forecast It is necessary for mining meteorological big data to build a machine learning model by using historical data to predict the future meteorological elements. This work is significant and has a technical challenge. However, the maintained data of the small cities and the medium cities are very limited due to historical reasons. It is adverse to build an accurate forecast model. Aiming at this problem, a temperature forecast method based on transfer learning technique is proposed. It extends the data of the target city by transferring the data from related cities. It builds a forecast model based on the extended dataset, and then solves the problem of the insufficient samples in machine learning. In this experiment, the temperature sequence of Gaoyao weather station in Zhaoqing area is extended according to the yearly average temperature from 1884 to 1997 of Hongkong. It is corrected by Macau data. Temperature trend of Zhaoqing area is modeled by the time power function and the least square method. The fitting curves and the regression function of the temperature change are obtained. The forecasting model is tested by the actual temperature data of 2014, 2015 and 2016. The results support the effectiveness of the proposed method and they also justify the superiority of applying data transfer to temperature forecast.", "Human carrying baggage classification  Convolution neural network  Transfer learning  Direction attribute  Region division Human carrying baggage classification is one of the important stages in identifying the owner of unattended baggage for a vision-based intelligent surveillance system. In this paper, an approach to classifying human carrying baggage region on surveillance video is proposed. The proposed approach utilized transfer learning strategy under convolution neural network with human pose direction attribute. For this purpose, we first constructed convolution neural network with the target including the presence of baggage and viewing direction of the human region. The network kernels are then fine-tuned to learning a new task in verifying whether the human carrying baggage or not. Rather than using the entire human region as input to the network, we divided the region into several sub-regions and assign them as a channel of the input layer. In the experiment, the standard public dataset is re-annotated with direction information of human pose to evaluate the effectiveness of the proposed approach.", "Incremental learning  Transfer learning  Concept drift Concept drift is one of the key challenges that incremental learning needs to deal with. So far, a lot of algorithms have been proposed to cope with it, but it is still difficult to response quickly to the change of concept. In this paper, a novel method named Selective Transfer Incremental Learning (STIL) is proposed to deal with this tough issue. STIL uses a selective transfer strategy based on the well-known chunk-based ensemble algorithm. In this way, STIL can adapt to the new concept of data well through transfer learning, and prevent negative transfer and overfitting that may occur in the transfer learning effectively by an appropriate selective policy. The algorithm was evaluated on 15 synthetic datasets and three real-world datasets, the experiment results show that STIL performs better in almost all of the datasets compared with five other state-of-the-art methods.", "Object  transfer learning  data mining  dynamic-TrAdaBoost  K-means Transfer learning can use the knowledge of closed fields to enhance complete the learning tasks in the target field. After several years of development, it has been widely used in research, data mining and deep learning. The problem that the sample weight of TrAdaBoost is prone to polarization in the source domain and the target domain when is in the transfer learning process. So we propose a transfer learning algorithm which based on dynamic-TrAdaBoost and k-means algorithm. This experiment proved to be a good solution to this problem.", "sentiment classification  neural network  Gated Recurrent Unit  transfer learning Sentiment classification for product reviews is of great significance for business feedback for manufactures, sellers and users. However, since a large amount of training data for a specific product domain is not always available, transfer learning is often utilized to do sentiment analysis applications. Specifically, after a pre-training of the large Chinese corpus by a word-embedding method, a larger size of training data for a specific domain was trained using a Gated Recurrent Unit. And then the trained model was used for testing the sentiment classification for a smaller amount of product reviews. The performances of this transfer learning method was also examined, especially to testify different factors affecting the performance of the transfer learning. The experimental results showed that different wording in the review domain (which we call it noise) will have a greater impact on transfer learning. We also calculate the difference of the wording to verify our hypothesis. According to these results, we have explored the impacts of the dataset wording, while we are doing Chinese text sentiment classification. We also shed a light in optimizing the transfer learning effect in general.", "Transfer learning  Long short-term memory  Parameters transfer  Cross-domain sentiment classification Social media sentiment classification has important theoretical research value and broad application prospects. Deep neural networks have been applied into social media sentiment mining tasks successfully with excellent representation learning and high efficiency classification abilities. However, it is very difficult to collect and label large scale training data for deep learning. In this case, deep transfer learning (DTL) can transfer abundant source domain knowledge to target domain using deep neural networks. In this paper, we propose a two-stage bidirectional long short-term memory (Bi-LSTM) and parameters transfer framework for short texts cross-domain sentiment classification tasks. Firstly, Bi-LSTM networks are pre-trained on a large amount of fine-labeled source domain training data. We fine-tune the pre-trained Bi-LSTM networks and transfer the parameters using target domain training data and continuing back propagation. The fine-tuning strategy is to transfer bottom-layer (general features) and retrain top-layer (specific features) to the target domain. Extensive experiments on four Chinese social media data sets show that our method outperforms other baseline algorithms for cross-domain sentiment classification tasks.", "Transfer learning  Traditional machine learning  Test framework In the field of supervised machine learning, a transfer learning environment is defined as the training data having different distribution characteristics than the testing data. This is due to the lack of available labeled data for the domain of interest, which prompts an alternate domain to be used as the training data. Because there is insufficient labeled data from the domain of interest, validation techniques cannot be reliably used for the algorithm selection process in a transfer learning environment. A transfer learning algorithm is typically comprised of a domain adaptation step followed by a learning step. The learning step is usually implemented using a traditional machine learning algorithm. In this paper, we examine and analyze the impact that the traditional machine learning algorithm (the learning step) has on the overall performance of a transfer learning algorithm. Using the transfer learning test framework, we test five state-of-the-art transfer learning algorithms coupled with seven different traditional learning algorithms for a total of 35 unique transfer learning algorithms. For our experiment, no labeled data from the domain of interest is available for the training process. Since validation techniques cannot be reliably used for the algorithm selection process in a transfer learning environment, it is important for machine learning researchers and practitioners to understand the impact of a traditional machine learner on the overall performance of a transfer learning algorithm.", "Ship License Numbers localization  Convolution Neural Network  Text Detection Ship License Numbers (SLNs) localization is an important part of waterway intelligent transportation systems. Unfortunately, this issue has been neglected for a long time. In this paper, we present an effective approach for localizing multi-style SLNs in nature scenes. The problem of locating SLNs is posed as the detection of character sequences which possess SLNs prior features. First, faced with the difficulty of no training data, a transfer learning-based deep convolutional neural network is designed to detect character sequences in SLNs. In the second step, to accurately locate SLNs from the detected character sequences, the prior features of SLNs are considered. Three SLNs prior features are summarized. An SLNs region generating algorithm and a low-level similarity-based fake SLNs filtering algorithm are presented, respectively. The accurate positions of the SLNs in the input image are obtained in this stage. The proposed approach is finally tested on ZJUSHIPS950 dataset. The approach achieves a FPPI of 0.42 and a F-measure of 0.614 on 1374 labeled SLNs, surpassing several related methods by a large margin. Controlled experiment results also prove the impressive performances of the proposed SLNs region generating and fake SLNs filtering algorithms.", "surveillance  moving object detection  object tracking  aerial image classification  deep learning  transfer learning  real time  low altitude  aerial video  automation Unnamed Aerial Vehicles (UAVs) are becoming increasingly popular and widely used for surveillance and reconnaissance. There are some recent studies regarding moving object detection, tracking, and classification from UAV videos. A unifying study, which also extends the application scope of such previous works and provides real-time results, is absent from the literature. This paper aims to fill this gap by presenting a framework that can robustly detect, track and classify multiple moving objects in real-time, using commercially available UAV systems and a common laptop computer. The framework can additionally deliver practical information about the detected objects, such as their coordinates and velocities. The performance of the proposed framework, which surpasses human capabilities for moving object detection, is reported and discussed.", "Face anti-spoofing  Transfer learning  Deep learning  Face recognition Face recognition systems are gaining momentum with current developments in computer vision. At the same time, tactics to mislead these systems are getting more complex, and counter-measure approaches are necessary. Following the current progress with convolutional neural networks (CNN) in classification tasks, we present an approach based on transfer learning using a pre-trained CNN model using only static features to recognize photo, video or mask attacks. We tested our approach on the REPLAY-ATTACK and 3DMAD public databases. On the REPLAY-ATTACK database our accuracy was 99.04% and the half total error rate (HTER) of 1.20%. For the 3DMAD, our accuracy was of 100.00% and HTER 0.00%. Our results are comparable to the state-of-the-art.", "Face analysis  Smile detection  Gender recognition  Deep learning Person gender detection is an important feature in many vision-based research fields including surveillance, human computer interaction, Biometrics, stratified behavior understanding, and content based indexing. Researchers are still facing big challenges to establish automated systems to recognize gender from images where human face represents the most important source of information. In the present study, we elaborated and validated a methodology for gender perception by transfer learning. First, the face is located and the corresponding cropped image is fed to a pre-trained convolutional neural network, the generated deep latent features are used to train a linear-SVM classifier. The overall classification performance reached 90.69% on the FotW validation set and 91.52% on the private test set. In this paper, we investigated also whether these features can deliver a smile recognizer. A similar trained architecture for classification of smiling and non-smiling faces gave a rate of 88.14% on the validation set and 82.12% on the private test set.", "Active contour  Convolutional Neural Netwrok  transfer learning  image segmentation  Support Vector machine  Pistachio sorting Convolutional neural networks have proved to be prominent in various fields of machine vision and image classification. Although it necessitates a large-scale dataset for promising performance, the mid-level representation of these networks can be exploited for specified tasks with smaller annotated image dataset. To this end, by evaluating the generality specificity of the desired layer as a feature extractor layer, the parameters of Convolutional Neural Networks learned on massive-size dataset like ImageNet can be transferred to a new model. In this study, the images of different sort of pistachios including trashes have been acquired to feed into a new model using a support vector classifier. The ultimate goal of our machine vision system is to separate the desired open-shell pistachios from other defected pistachios as well as trashes. For image segmentation, we use active contour method to detect objects and form both new images of each object type and their augmented images. Since our dataset is not large-scale compared to ImageNet classes, a feature reduction method is performed after the feature extractor layer of pre-trained Convolutional Neural Network. The results show the better performance of the proposed approach in detection of desired-formed pistachio facing unseen test set of images compared to basic approaches.", "DBN  RBM  Semi-supervised  Rectified Linear Units  Transfer Learning A fault diagnosis method based on improved Deep Belief Network is proposed. This paper proposed an network named Semi-DBN, which adapts an adaptive semi supervised method to train the RBM structure to improve the performance of DBN, replaces the Sigmoid activation function with Rectified Linear Units to improve the performance of the network, and combines the model with Transfer Learning to solve the problem of lack of data. It can get rid of the dependence of the traditional machine learning methods on the extraction of the sample characteristics, and effectively overcome the problem of gradient vanishing, local extremum and so on. The identification and generalization ability of the method is verified when we used it in the experiment of rolling bearing data.", "Transfer Learning  Reinforcement Learning  Linear Multi-variable Mapping  Inter-task Mapping  Keepaway Though popular in many agent learning tasks, reinforcement learning still faces problems, such as long learning time in complex environment. Transfer learning could shorten the learning time and improve the performance in reinforcement learning by reusing the knowledge acquired from different but related source task. Due to the difference in state space and/or action space of the target and source task, transfer via inter-task mapping is a popular method. The design of the inter-task mapping is very critical to this transfer learning method. In this paper, we propose a linear multi-variable mapping (LMVM) for the transfer learning to make a better use of the knowledge learned from the source task. Unlike the inter-task mapping used before, the LMVM is not a one-to-one mapping but a one-to-many mapping, which is based on the idea that the element in target task is related with several similar elements from source task. We test transfer learning via our new mapping on the Keepaway platform. The experimental results show that our method could make the reinforcement learning agents learn much faster than those without transfer and those transfer with simpler mappings.", "Convolutional Neural Network  object detection  region proposal  regression  datasets With the development of intelligent device and social media, the data bulk on Internet has grown with high speed. As an important aspect of image processing, object detection has become one of the international popular research fields. In recent years, the powerful ability with feature learning and transfer learning of Convolutional Neural Network (CNN) has received growing interest within the computer vision community, thus making a series of important breakthroughs in object detection. So it is a significant survey that how to apply CNN to object detection for better performance. First the paper introduced the basic concept and architecture of CNN. Secondly the methods that how to solve the existing problems of conventional object detection are surveyed, mainly analyzing the detection algorithm based on region proposal and based on regression. Thirdly it mentioned some means which improve the performance of object detection. Then the paper introduced some public datasets of object detection and the concept of evaluation criterion. Finally, it combed the current research achievements and thoughts of object detection, summarizing the important progress and discussing the future directions.", "image sentiment analysis  multi-modal embedding  heterogeneous transfer learning  multimodal deep learning Most traditional methods of image sentiment analysis focus on the design of visual features, and the usefulness of texts associated to the images have not been sufficiently investigated. Heterogeneous transfer learning has recently gained much attention as a new machine learning paradigm in which knowledge can be transferred from source domain feature space to target domain feature space. This paper proposes a novel approach that exploits deep latent correlation between visual and textual modalities. In our proposed method, we build a latent embedding space for symmetric heterogeneous feature transfer. The latent space is able to generate domain-specific and maximally correlative cross-domain features which are regarded as the semantic-intensive visual feature representation and used to train sentiment polarity classifiers. The results of experiments conducted on real-world data sets show that the proposed approach can achieve better sentiment classification accuracy by using multi-layer neural network to capture deeper internal relations.", " Predicting failure in Hard Disk Drives (HDD) is of fundamental importance for data loss avoidance as well as to lower the downtime costs of a system. As a consequence, an increasing effort may be observed from both universities and industry to find suitable failure prediction methods. Despite the encouraging performances achieved by various methods one important aspect that shall be noticed is the lack of data available to build reliable models. Considering the HDD failure prediction task, this problem arises with the numerous HDD models. To overcome such problem, transfer learning strategies offer a valid alternative since it can be used to transfer learning from HDD model with enough data to build failure prediction methods for HDD models with lack of data. In this work we evaluate several transfer learning strategies in the task of HDD failure prediction. Additionally we propose a new strategy to build information sources based on the clustering of similar HDD models. This approach may be a valid alternative when no HDD model has enough data to generate a reliable model. Results showed that all transfer learning scenarios can improve the performance of HDDs failure prediction methods, mainly for HDDs with very limited data. Moreover, the clustering-based information source also results in performance gains in all transfer methods and HDD models tested.", " Detecting the travel modes such as walking and driving a car is an important task for user behavior understanding as well as transportation planning and management. Existing solutions for this task mainly train a generic classifier for all users although the walking or driving behaviors may differ greatly from one user to another. In this paper, we propose to build a personalized travel mode detection method. In particular, the proposed method can be divided into two stages. First, for a given target user, it applies user similarity computation to borrow data from a set of pre-collected data for transfer learning. Second, it estimates the data distribution in feature space, and uses it to reweight the borrowed data so as to minimize the model loss with respect to the target user. Experimental evaluations on real travel data show that the proposed method outperforms the generic method and the transfer learning method with kernel mean matching in terms of prediction accuracy.", "Cross-Domain Collaborative Filtering  transfer learning  recommender system  similarity network fusion Cross-Domain Collaborative Filtering(CDCF) methods transfer knowledge from auxiliary domains (e.g., books) to improve recommendation in a target domain (e.g. movies). Most CDCF methods exploit homogeneous user feedback, e.g. numeric ratings, from auxiliary domains as the knowledge source. However, in a typical recommender system, the usage data is usually heterogeneous and therefore is potential to better improve recommendation in other domains. In this paper, we propose a novel and generic CDCF solution called Heterogeneous Knowledge Transfer via Domain Regularization (HKT-DR). Our solution is able to mine high quality knowledge from heterogeneous knowledge sources, i.e. both explicit and implicit feedbacks from multiple auxiliary domains, by building a fused user similarity network, and to incorporate the knowledge by imposing domain regularization to constrain matrix factorization objective function. Extensive experiments on real world datasets show that the proposed HKT-DR model outperforms the state-of-the-art CDCF solutions.", "Breast cancer diagnosis  mammographic image analysis  deep learning  transfer learning World Health Organization report shows 519,000 deaths due to breast cancer in 2014 and it was much more in 2008. Therefore, it is required to take early steps in detection and diagnosis of breast cancer to decrease the associated death rate. Computer Aided Diagnosis (CAD) is useful in mass screening of breast cancer datasets. Data mining and machine learning technologies have already achieved significant success in many knowledge engineering areas including classification, regression and clustering, and most recently, have been employed to assist the diagnosis of cancers with promising outcomes. Traditional machine learning models are characterized by training and testing data with the same input feature space and data distribution. But when distribution changes, most machine learning models need to be modified or rebuilt from scratch to work on newly collected data. In many real world applications, it is expensive or impossible to recollect the needed data and rebuild the models. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred as Transfer Learning. In this paper, we explore the usage of transfer learning, specially, unsupervised domain adaptation for breast cancer diagnosis to address the issues of fewer training data on target image dataset. On the strength of recent developed deep descriptors, we are able to adapt recent transfer learning methodologies, e.g., TCA (Transfer Component Analysis), CORAL (Correlation Alignment), BDA(Balanced Distribution Adaptation) to breast cancer diagnosis across multiple mammographic image databases including CBIS-DDSM, InBreast, MIAS, etc, and evaluate their performance. Experiments demonstrate that, without any labels in the target database, transfer learning is able to help improve the classification accuracy.", " The performance of many artificial intelligence systems critically depends on classification methods. Unfortunately, most classifiers are highly sensitive to distortions in the input distribution, for example, due to sensor failures or a switch of the sensor system. In such a case, new labeled data has to be recorded to retrain the classifier under the distorted input distribution, which can be expensive. Transfer learning addresses this issue by reusing the original model and adapting it to the new domain using as few new data points as possible. In this contribution, we present a novel supervised transfer learning scheme for the Large Margin Nearest Neighbor classifier. We evaluate our approach on a real-world example of a transfer between two hyper-spectral sensors applied to chemical molecule classification. Our results show that transfer learning outperforms the original model as well as a newly trained model on the new data and requires only very little data to be trained.", "Transfer Learning  Convolutional Neural Network (CNN)  Facial Recognition  Humanoid Robot Applications of transfer learning for convolutional neural networks (CNNs) have shown to be an efficient alternative for solving recognition tasks rather than designing and training a new neural network from scratch. However, there exists several popular CNN architectures available for various recognition tasks. Therefore, choosing an appropriate network for a specific recognition task, specifically designed for a humanoid robotic platform, is often challenging. This study evaluates the performance of two well-known CNN architectures  AlexNet, and VCC-Face for a face recognition task. This is accomplished by applying the transfer learning concept to the networks pre-trained for different recognition tasks. The proposed face recognition framework is then implemented on a humanoid robot known as NAO to demonstrate the practicality and flexibility of the algorithm. The results suggest that the proposed pipeline shows excellent performance in recognizing a new person from a single example image under varying distance and resolution conditions usually applicable to a mobile humanoid robotic platform.", "transfer learning  traffic sign recognition  stacked auto-encoder  local coordinate coding  deep learning Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. Supervised algorithms have achieved superior results on German Traffic Sign Recognition Bench-mark (GTSRB) database. However, these models cannot transfer knowledge across domains, e.g. transfer knowledge learned from Synthetic Signs database to recognize the traffic signs in GTSRB database. Through Synthetic Signs database shares exactly the same class label with GTSRB, the data distribution between them are divergent. Such task is called transfer learning, that is a basic ability for human being but a challenge problem for machines. In order to make these algorithms have ability to transfer knowledge between domains, we propose a variant of Generalized Auto-Encoder (GAE) in this paper. Traditional transfer learning algorithms, e.g. Stacked Autoencoder(SA), usually attempt to reconstruct target data from source data or man-made corrupted data. In contrast, we assume the source and target data are two different corrupted versions of a domain-invariant data. And there is a latent subspace that can reconstruct the domain-invariant data as well as preserve the local manifold of it. Therefore, the domain-invariant data can be obtained not only by de-noising from the nearest source and target data but also by reconstructing from the latent subspace. In order to make the statistical and geometric property preserved simultaneously, we additionally propose a Local Coordinate Coding (LCC)-based relational function to construct the deep nonlinear architecture. The experimental results on several benchmark datasets demonstrate the effectiveness of our proposed approach in comparison with several traditional methods.", "Automatic speech recognition  acoustic model  transfer learning  deep neural network  multi-task learning  children's speech processing Children's speech processing is more challenging than that of adults due to lacking of large scale children's speech corpora. With the developing of the physical speech organ, high inter speaker and intra speaker variabilities are observed in children's speech. On the other hand, data collection on children is difficult as children usually have short attention span and their language proficiency is limited. In this paper, we propose to improve children's automatic speech recognition performance with transfer learning technique. We compare two transfer learning approaches in enhancing children's speech recognition performance with adults' data. The first method is to perform acoustic model adaptation on the pre-trained adult model. The second is to train acoustic model with deep neural network based multi-task learning approach: the adults' and children's acoustic characteristics are learnt jointly in the shared hidden layers, while the output layers are optimized with different speaker groups. Our experiment results show that both transfer learning approaches are effective in transferring rich phonetic and acoustic information from adults' model to children model. The multi-task learning approach outperforms the acoustic adaptation approach. We further show that the speakers' acoustic characteristics in languages can also benefit the target language under the multi-task learning framework.", "Sports sununarization  Encoder Decoder model  Transfer learning This paper describes an inning sununarization method for a baseball game by using an encoder-decoder model. Each inning in a baseball game contains some events, such as hits, strikeouts, homeruns and scoring. Simplified description of the events leads to the improvement of readability of the inning information. Our method learns a relation between play-by-play data in each inning and inning reports. We also incorporate sophisticated expressions acquired from game summaries with the model. We call them Game-changing Phrase, GP. One problem in our task is the size of training data for the learning. To solve this problem, we apply a transfer learning approach into our method. In the experiment, we evaluate the effectiveness of our method with the transfer learning.", "Computational geometry  Graph theory  Hamilton cycles This study explored the viability of out-the-box, pre-trained ConvNet models as a tool to generate features for large-scale classification tasks. A juxtaposition with generative methods for vocabulary generation was drawn. Both methods were chosen in an attempt to integrate other datasets (transfer learning) and unlabelled data, respectively. Both methods were used together, studying the viability of a ConvNet model to estimate category labels of unlabelled images. All experiments pertaining to this study were carried out over a two-class set, later expanded into a 5-category dataset. The pre-trained models used were obtained from the Caffe Model Zoo. The study showed that the pre-trained model achieved best results for the binary dataset, with an accuracy of 0.945. However, for the 5-class dataset, generative vocabularies outperformed the ConvNet (0.91 vs. 0.861). Furthermore, when replacing labelled images with unlabelled ones during training, acceptable accuracy scores were obtained (as high as 0.903). Additionally, it was observed that linear kernels perform particularly well when utilized with generative models. This was especially relevant when compared to ConvNets, which require days of training even when utilizing multiple GPUs for computations.", "Transfer learning  Regularization  Cervical cancer  Digital colposcopy Cervical cancer remains a significant cause of mortality in low-income countries. As in many other diseases, the existence of several screening/diagnosis methods and subjective physician preferences creates a complex ecosystem for automated methods. In order to diminish the amount of labeled data from each modality/expert we propose a regularization-based transfer learning strategy that encourages source and target models to share the same coefficient signs. We instantiated the proposed framework to predict cross-modality individual risk and cross-expert subjective quality assessment of colposcopic images for different modalities. Thus, we are able to transfer knowledge gained from one expert/modality to another.", "Transfer learning  Convolutional neural network  Manifold learning Deep learning has been recently proposed to learn robust representation for various tasks and deliver state-of-the-art performance in the past few years. Most researchers attribute such success to the substantially increased depth of deep learning models. However, training a deep model is time-consuming and need huge amount of data. Though techniques like fine-tuning can ease those pains, the generalization performance drops significantly in transfer learning setting with little or without target domain data. Since the representation in higher layers must transition from general to specific eventually, generalization performance degrades without integrating sufficient label information of target domain. To address such problem, we propose a transfer learning framework called manifold regularized convolutional neural networks (MRCNN). Specifically, MRCNN fine-tunes a very deep convolutional neural network on source domain, and simultaneously tries to preserve the manifold structure of target domain. Extensive experiments demonstrate the effectiveness of MRCNN compared to several state-of-the-art baselines.", "Brain-computer interface  event-related potential  EEG  active learning  domain adaptation  semi-supervised learning  transfer learning Single-trial classification of event-related potentials in electroencephalogram (EEG) signals is a very important paradigm of brain-computer interface (BCI). Because of individual differences, usually some subject-specific calibration data are required to tailor the classifier for each subject. Transfer learning has been extensively used to reduce such calibration data requirement, by making use of auxiliary data from similar/relevant subjects/tasks. However, all previous research assumes that all auxiliary data have been labeled. This paper considers a more general scenario, in which part of the auxiliary data could be unlabeled. We propose active semi-supervised transfer learning (ASTL) for offline BCI calibration, which integrates active learning, semi-supervised learning, and transfer learning. Using a visual evoked potential oddball task and three different EEG headsets, we demonstrate that ASTL can achieve consistently good performance across subjects and headsets, and it outperforms some state-of-the-art approaches in the literature.", " Motor imagery based brain computer interface (BCI) has drawback of long subject dependent calibration session times. This can be a very exhausting and a time consuming process. In order to alleviate it, transfer learning and active learning approaches can be utilised. Informative instances are selected by applying active learning concept from other subjects under similar circumstances. Then, they are transferred to target user domain which has low number of training data. This informative transfer learning approach is associated with common spatial pattern (CSP) as feature extraction method in our previous attempt. CSP features are widely used for motor imagery-based BCI systems. However, the classical CSP algorithm will perform poorly when operational frequency bands are inadequately selected. Therefore, in the present study, filter bank common spatial pattern (FBCSP) algorithm has been applied for extracting features from the multi-class motor imagery data. FBCSP algorithm selects subject-specific operational frequency bands for extracting discriminative features. We incorporated FBCSP features into informative instance transfer learning framework to investigate the effect of subject specific feature selection. Results show that performance of new users can be improved with reduced number of training samples when FBCSP features are used compared to the classical CSP-based features.", " Diabetic Foot Ulcer (DFU) is a major complication of Diabetes, which if not managed properly can lead to amputation. DFU can appear anywhere on the foot and can vary in size, colour, and contrast depending on various pathologies. Current clinical approaches to DFU treatment rely on patients and clinician vigilance, which has significant limitations such as the high cost involved in the diagnosis, treatment and lengthy care of the DFU. We introduce a dataset of 705 foot images. We provide the ground truth of ulcer region and the surrounding skin that is an important indicator for clinicians to assess the progress of ulcer. Then, we propose a two-tier transfer learning from bigger datasets to train the Fully Convolutional Networks (FCNs) to automatically segment the ulcer and surrounding skin. Using 5-fold cross-validation, the proposed two-tier transfer learning FCN Models achieve a Dice Similarity Coefficient of 0.794 (+/- 0.104) for ulcer region, 0.851 (+/- 0.148) for surrounding skin region, and 0.899 (+/- 0.072) for the combination of both regions. This demonstrates the potential of FCNs in DFU segmentation, which can be further improved with a larger dataset.", " In the realm of surface electromyography (sEMG) gesture recognition, deep learning algorithms are seldom employed. This is due in part to the large quantity of data required for them to train on. Consequently, it would be prohibitively time consuming for a single user to generate a sufficient amount of data for training such algorithms. In this paper, two datasets of 18 and 17 able-bodied participants respectively are recorded using a low-cost, low-sampling rate (200Hz), 8-channel, consumer-grade, dry electrode sEMG device named Myo armband (Thalmic Labs). A convolutional neural network (CNN) is augmented using transfer learning techniques to leverage inter-user data from the first dataset and alleviate the data generation burden imposed on a single individual. The results show that the proposed classifier is robust and precise enough to guide a 6DoF robotic arm (in conjunction with orientation data) with the same speed and precision as with a joystick. Furthermore, the proposed CNN achieves an average accuracy of 97.81% on seven hand/wrist gestures on the 17 participants of the second dataset.", "Machine Learning  Deep Learning  Convolutional Neural Network (CNN)  Support Vector Machine  SVM  Ensemble Subspace Discriminant  Visual Impairment In this paper, a system to aid the visually impaired by providing contextual information of the surroundings using 360 degrees view camera combined with deep learning is proposed. The system uses a 360 degrees view camera with a mobile device to capture surrounding scene information and provide contextual information to the user in the form of audio. The scene information from the spherical camera feed is classified by identifying objects that contain contextual information of the scene. That is achieved using convolutional neural networks (CNN) for classification by leveraging CNN transfer learning properties using the pre-trained VGG-19 network. There are two challenges related to this paper, a classification and a segmentation challenge. As an initial prototype, we have experimented with general classes such restaurants, coffee shops and street signs. We have achieved a 92.8% classification accuracy in this paper.", " Sleep plays an important role in recovering physical and mental functions. Sleep position is known to affect sleep quality, hence, managing sleep position is beneficial for patients suffering from sleep disorders. For a long-term sleep management, we propose a sleep position tracking system using two wristbands. From the data collected from the wristbands, the system detects sleep positions and their changes. We define a sleep position motion model that consists of seven transitions between three sleep positions. Then, we propose pre-processing methods to overcome difficulties in analyzing sleep motion data, i.e., discontinuity, uncertainty, and time-variability. We tested experimental data in state-of-art pre-trained convolution neural networks by transfer learning. The accuracy of our proposed system was 96.03% and 88.02% in pilot experiment and on-site sleep experiment, respectively. Our experimental results demonstrate that the proposed system effectively and accurately keeps track of sleep positions without causing any inconvenience to users, and hence, serves as a key building block for cost-effective 24/7 sleep monitoring solutions", "Terms bibliography extraction  CRF  confidence measure  active learning  transfer learning The effective use of digital libraries demands maintenance of bibliographic databases. Useful bibliographic information appears in the reference fields of academic papers, so we are developing a method for automatic extraction of bibliographic information from reference strings using a conditional random field (CRF). However, at least a few hundred reference strings are necessary to learn an accurate CRF. In this paper, we propose active learning and transfer learning techniques to reduce the required training data for CRFs. We evaluate extraction accuracies and the associated training cost by experiments.", "Hyperspectral remote sensing  Classification  Active Learning  Transfer Learning Classification of hyperspectral remote sensing images is key to extract abundant information. The researchers are focusing on the development of algorithms for accurate classifiers from last few decades. With the technological advancement and new modern methods of learning provide confidence for efficient and accurate classification when compared to the direct implementation of conventional learning algorithms. The variations in the conventional algorithm leads to active learning based and transfer learning based approaches and provide promising results. This paper attempt to explore various recent technologies applied to Hyperspectral imagery for classification.", "sentiment analysis  topic adaption  transfer learning Twitter sentiment analysis model trained from data of one topic may per-forms worse on another topic. While tweets have diverse topics, and the topic of target data of sentiment analysis task is change with the application requirement, which makes it hard to get a good performance. We also notice that one model cannot performs well on every topic, and tweets have no sentiment label to train the model. On the other hand, there are plenty of available text data with sentiment label and topic information such as online movie and product reviews. So we propose a method based on transfer learning, which use topic information and term distribution as a bridge between target tweets and texts from other sources. It could quickly find appropriate in-stances from other sources, and then use them to train a model adapting to target tweets of specific topic for getting better performance. The experiment result shows the effectiveness of the proposed method.", "Sentiment Analysis  Transfer Learning  Support Vector Machine  Twitter Sentiment analysis is an automatic process of understanding, extracting and processing textual data to obtain the sentiment information. From machine learning point of view, the sentiment analysis is a supervised learning problem whose training and predicting data come from a similar domain. When domain changes, the machine learning model must be rebuilt from scratch using new training data. New training data requires manual labeling process which is very costly and time-consuming. Therefore, it would be more effective and efficient using transfer learning which uses the training data from an already available domain to deal with the estimating data on different domains. In this paper, we evaluate the accuracy of the transfer learning on sentiment analysis for Indonesian tweets. Our simulations show that the accuracy of the transfer learning is still lower than that of the supervised learning. Moreover, the bi-gram features can improve the accuracy of the transfer learning.", "Arabic script based languages  Generalized OCR  MDLSTM  CTC Many languages use Arabic script for written communication either in basic or augmented form. These languages include Urdu, Pashto, Persian, etc. As the primary characters are shared among all these languages, it is possible to take advantage of the visual similarities for Optical Character Recognition (OCR). OCR models optimized for individual languages have been proposed. However, to the best of our knowledge, there is no attempt to develop a single system for more than one language. The contributions of the presented work are: First, it investigates the effect on the recognition accuracy when different languages are combined (A pioneering study). Second, it introduces publicly available synthetic datasets for Arabic and Pashto languages for experimental purposes. Third, this paper provides statistical analysis as clues for transfer learning concerning OCR systems for Arabic, Urdu, and Pashto languages.", "convolutional neural networks  deep learning  transfer learning  skin lesions Deep learning methods for image analysis have shown impressive performance in recent years. In this paper, we present deep learning based approaches to solve two problems in skin lesion analysis using a dermoscopic image containing skin tumor. In the first problem, we use a fully convolutional-deconvolutional architecture to automatically segment skin tumor from the surrounding skin. In the second problem, we use a simple convolutional neural network and VGG-16 architecture using transfer learning to address the two different tasks in skin tumor classification. The proposed models are trained and evaluated on standard benchmark datasets from the International Skin Imaging Collaboration (ISIC) 2017 Challenge, which consists of 2000 training samples and 600 testing samples. The result shows that the proposed methods achieve promising performances. In the first problem, the average value of Jaccard index for lesion segmentation using hilly convolutional-deconvolutional architecture is 0.507. In the second problem, the values of area under the receiver operating characteristic curve (AUC) on two different lesion classifications using VGG16 with transfer learning are 0.763 and 0.869, respectively  the average value of AUC in two tasks is 0.816.", "image quality  no reference  transfer learning Transfer learning has emerged from recent years' great success of applying convolutional neural networks to object recognition tasks. In this work, we describe a transfer learning framework that learns an image quality estimator end-to-end in classification or regression. Experiments show this approach achieves state of the art performance on four publicly available image quality databases. Image quality is ignored in visual recognition challenges such as PASCAL VOC and ILSVRC. However, it poses a practical challenge to the design of robust vision applications. This work responds to such challenge without impacting the performance of recognition tasks.", " Precise localization of robots is imperative for their safe and autonomous navigation in both indoor and outdoor environments. In outdoor scenarios, the environment typically undergoes significant perceptual changes and requires robust methods for accurate localization. Monocular camerabased approaches provide an inexpensive solution to such challenging problems compared to 3D LiDAR-based methods. Recently, approaches have leveraged deep convolutional neural networks (CNNs) to perform place recognition and they turn out to outperform traditional handcrafted features under challenging perceptual conditions. In this paper, we propose an approach for directly regressing a 6-DoF camera pose using CNNs and a single monocular RGB image. We leverage the idea of transfer learning for training our network as this technique has shown to perform better when the number of training samples are not very high. Furthermore, we propose novel data augmentation in 3D space for additional pose coverage which leads to more accurate localization. In contrast to the traditional visual metric localization approaches, our resulting map size is constant with respect to the database. During localization, our approach has a constant time complexity of O (1) and is independent of the database size and runs in real-time at similar to 80 Hz using a single GPU. We show the localization accuracy of our approach on publicly available datasets and that it outperforms CNN-based state-of-the-art methods.", " In this paper, we present a multi-sensory terrain classification algorithm with a generalized terrain representation using semantic and geometric features. We compute geometric features from lidar point clouds and extract pixel-wise semantic labels from a fully convolutional network that is trained using a dataset with a strong focus on urban navigation. We use data augmentation to overcome the biases of the original dataset and apply transfer learning to adapt the model to new semantic labels in off-road environments. Finally, we fuse the visual and geometric features using a random forest to classify the terrain traversability into three classes: safe, risky and obstacle. We implement the algorithm on our four-wheeled robot and test it in novel environments including both urban and off-road scenes which are distinct from the training environments and under summer and winter conditions. We provide experimental result to show that our algorithm can perform accurate and fast prediction of terrain traversability in a mixture of environments with a small set of training data.", " This paper proposes a new method for human posture recognition from top-view depth maps on small training datasets. There are two strategies developed to leverage the capability of convolution neural network (CNN) in mining the fundamental and generic features for recognition. First, the early layers of CNN should serve the function to extract feature without specific representation. By applying the concept of transfer learning, the first few layers from the pre-learned VGG model can be used directly without further fine-tuning. To alleviate the computational loading and to increase the accuracy of our partially transferred model, a cross-layer inheriting feature fusion (CLIFF) is proposed by using the information from the early layer in fully connected layer without further processing. The experimental result shows that combination of partial transferred model and CLIFF can provide better performance than VGG16 [1] model with re-trained FC layer and other hand-crafted features like RBPs [2].", " Multi-robot transfer learning allows a robot to use data generated by a second, similar robot to improve its own behavior. The potential advantages are reducing the time of training and the unavoidable risks that exist during the training phase. Transfer learning algorithms aim to find an optimal transfer map between different robots. In this paper, we investigate, through a theoretical study of single-input single-output (SISO) systems, the properties of such optimal transfer maps. We first show that the optimal transfer learning map is, in general, a dynamic system. The main contribution of the paper is to provide an algorithm for determining the properties of this optimal dynamic map including its order and regressors (i.e., the variables it depends on). The proposed algorithm does not require detailed knowledge of the robots' dynamics, but relies on basic system properties easily obtainable through simple experimental tests. We validate the proposed algorithm experimentally through an example of transfer learning between two different quadrotor platforms. Experimental results show that an optimal dynamic map, with correct properties obtained from our proposed algorithm, achieves 60-70% reduction of transfer learning error compared to the cases when the data is directly transferred or transferred using an optimal static map.", " A deep neural network (DNN) is called as a deep rectified network (DRN), if using Rectified Linear Units (ReLUs) as its activation function. In this paper, we show its parameters can be seen to play two important roles simultaneously: one for determining the subnetworks corresponding to the inputs and the other for the parameters of those subnetworks. This observation leads our paper to proposing a method to combine a DNN and an SVM, as a deep classifier. For a DRN trained by a common tuning algorithm, a multilayer gated bilinear classifier is designed to mimic its functionality. Its parameter set is duplicated into two independent sets, playing different roles. One set is used to generate gate signals so as to determine subnetworks corresponding to its inputs, and keeps fixed when optimizing the classifier. The other set serves as parameters of subnetworks, which are linear classifiers. Therefore, their parameters can be implicitly optimized by applying SVM optimizations. Since the DRN is only to generate gate signals, we show in experiments, that it can be trained by using supervised, or unsupervised learning, and even by transfer learning.", " Transfer learning has attracted more and more attention, and many scholars proposed some useful strategies. Boosting is the main strategy for transfer learning. In boosting, resampling is preferred over reweighting, and it can be applied to any base learner. In this paper, we propose a weighted-resampling method for transfer learning, called TrResampling. Firstly, resampling is applied to the data with heaven weight in the source domain, and the resampled data is used with the target data as the training data to build a classifier. Then the TrAdaBoost algorithm is used to adjust the weights of source data and target data. We discuss Decision Tree, Naive Bayes, and SVM as the base learner in TrResampling, and choose the suitable for TrResampling. In order to illustrate the performance of the proposed algorithm, we compare TrResampling with the state-of-the-art algorithm TrAdaBoost and the base learner Decision Tree, experimental results on UCI data sets indicate that TrResampling is superior to TrAdaBoost and Decision Tree on many data sets.", "transfer learning  machine learning  support vector machine  cancer prediction Many medical applications face a situation that the on-hand data cannot fully fit an existing predictive model or on-line tool, since these models or tools only use the most common predictors and the other valuable features collected in the current scenario are not considered altogether. On the other hand, the training data in the current scenario is not sufficient to learn a predictive model effectively yet. In order to overcome these problems and construct an efficient classifier, for these real situations in medical fields, in this work we present an approach based on the least squares support vector machine (LS-SVM), which utilizes a transfer learning framework to make maximum use of the data and guarantee its enhanced generalization capability. The proposed approach is capable of effectively learning a target domain with limited samples by relying on the probabilistic outputs from the other previously learned model using a heterogeneous method in the source domain. Moreover, it autonomously and quickly decides how much output knowledge to transfer from source domain to the target one using a fast leave-one-out cross validation strategy. This approach is applied on a real-world clinical dataset to predict 5-year mortality of bladder cancer patients after radical cystectomy, and the experimental results indicate that the proposed method can achieve better performances compared to traditional machine learning methods, consistently showing the potential of the proposed method under the circumstances with insufficient data.", "action recognition  deep learning  transfer learning  hybrid classifier Human action recognition is an imperative research area in the field of computer vision due to its numerous applications. Recently, with the emergence and successful deployment of deep learning techniques for image classification, object recognition, and speech recognition, more research is directed from traditional handcrafted to deep learning techniques. This paper presents a novel method for human action recognition based on a pre-trained deep CNN model for feature extraction & representation followed by a hybrid Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) classifier for action recognition. It has been observed that already learnt CNN based representations on large-scale annotated dataset could be transferred to action recognition task with limited training dataset. The proposed method is evaluated on two well-known action datasets, i.e., UCF sports and KTH. The comparative analysis confirms that the proposed method achieves superior performance over state-of-the-art methods in terms of accuracy.", " The output encodings of neural nets determine the structure of the space in which inference occurs. Yet, they are generally given very little thought. It is common practice for neural nets to use 1-Hot encoding when training to discriminate among many classes. The primary exceptions to this are error correcting output codes, and semantic output encodings. Output encodings based upon semantic descriptors cause a net to learn responses for classes to which it has not been exposed, provided those classes may be characterized by the same semantic descriptors. This raises a number of questions, such as Can a net implicitly learn encodings for unobserved classes in the absence of a semantic encoding, or any encoding that requires some form of prior knowledge and hand crafting?. Also, are some output encodings better than others for learning these implicit encodings? In this paper, we will compare how effectively different non-semantic encodings are at causing a neural net to implicitly learn encodings for unobserved classes. Also, while evaluating the efficacy of these implicit encodings, we will look for evidence of a phenomenon akin to over-training. Specifically, as training on the observed classes occurs, we initially see improvement in how well the implicitly learned encodings can be used to differentiate among the classes which are unobserved during the net's training. However, as training continues to improve discrimination among the observed classes, the efficacy of the implicit codes either remains steady, or undergoes a degradation. This degradation is akin to the overtraining that one generally tries to guard against when training a neural net.", " In short-term load forecasting, dataset construction plays a vital role in the improvement of forecasting accuracy. This paper begins with filtering the load data of the target city, transferring and expanding the load data of the nearby cities, and analysing the influence of the periodicity of load, holiday and load growth rate on dataset construction. A dataset construction method combining compress-filtering and transfer-expanding is then proposed based on the analysis. In this method, firstly, we apply the mean compress method to compress monthly data into weekly data in which both the periodic trend and the randomness of other related factors are taken into consideration. The training set is selected according to the similarity between the load variation pattern of the predicted month and that of the historical data. Secondly, based on the analysis of the similarity of the load data between the target city and the nearby cities, we introduce the load growth rate to measure the difference of the load variation patterns between different cities. Based on the load growth rate, a transfer learning method is put forward which transfers the data of the source city to the target city. The case study on real load data shows that, compared with the mutual information filtering-based predicting method and the knowledge transfer expanding method, the mean absolute percent error is decreased by 26% and 9.5%, respectively.", " The paper deals with realizations of transfer learning for classification, i.e. the adaptation of a classifier model to a changed data distribution. This change could be a data drift or a more complex transformation. We propose to model those data changes by manifolds describing continuous transformations of the data. This description can be seen as a generalization of function based transfer models considered so far. The manifold description of the transfer function allows either to adjust the classifier model to the changed data distribution or a back-transformation of those data to the original data space. To get the approach feasible, the manifold is approximated by the affine part of the Taylor expansion of the manifold structure. Moreover, the affine approximation shows mathematical correspondences to tangent metric leaning, which was developed for handling of data with drifts in classification methods. The paper provides the mathematical background for manifold based transfer data learning. Further, the approach is exemplarily applied for the generalized learning vector quantization classifier. This classifier is a prominent method which frequently achieves a high performance and a robust behavior while the classifier complexity is low compared to more sophisticated approaches like deep learning architectures or support vector machines. Moreover, the good interpretability of learning vector quantization classifiers also contributes to an intuitive practical understanding of transfer learning.", " Facial expression recognizers based on hand-crafted features have achieved satisfactory performance on many databases. Recently, deep neural networks, e.g. deep convolutional neural networks (CNNs) have been shown to boost performance on vision tasks. However, the mechanisms exploited by CNNs are not well established. In this paper, we establish the existence and utility of feature maps selective to action units in a deep CNN trained by transfer learning. We transfer a network pre-trained on the Image-Net dataset to the facial expression recognition task using the Karolinska Directed Emotional Faces (KDEF), Radboud Faces Database(RaFD) and extended Cohn-Kanade (CK+) database. We demonstrate that higher convolutional layers of the deep CNN trained on generic images are selective to facial action units. We also show that feature selection is critical in achieving robustness, with action unit selective feature maps being more critical in the facial expression recognition task. These results support the hypothesis that both human and deeply learned CNNs use similar mechanisms for recognizing facial expressions.", " One of the challenges in applying convolutional neural networks to automated optical inspection is the lack of sufficient training data. In this paper we show that transfer learning can be successfully applied using image data from an entirely different domain. Focusing on optical inspection of texture images, we transfer weights from a source network trained with arbitrary unrelated images from the ImageNet dataset. Inspection experiments using our method show that one epoch of fine-tuning is sufficient to achieve 99.95% classification accuracy, while conventional transfer learning without fine-tuning achieves only 78.76%. An in-depth analysis of the effects of fine-tuning reveals that after fine-tuning, most of the unnecessary features encoded in the weights of the source network are deactivated, while meaningful features of the target data are amplified to capture new variations in the target domain.", " Convolutional Neural Networks (CNNs) have become the state-of-the-art in various computer vision tasks, but they are still premature for most sensor data, especially in pervasive and wearable computing. A major reason for this is the limited amount of annotated training data. In this paper, we propose the idea of leveraging the discriminative power of pre-trained deep CNNs on 2-dimensional sensor data by transforming the sensor modality to the visual domain. By three proposed strategies, 2D sensor output is converted into pressure distribution imageries. Then we utilize a pre-trained CNN for transfer learning on the converted imagery data. We evaluate our method on a gait dataset of floor surface pressure mapping. We obtain a classification accuracy of 87.66%, which outperforms the conventional machine learning methods by over 10%.", " A reinforcement learning (RL) agent needs a fair amount of experience to find a near-optimal policy. Transfer learning has been investigated as a means to reduce the amount of experience required. Transfer learning, however, requires another similar reinforcement learning task as a transfer source, which can also be costly in the amount of experience required. In this research, we examine the possible practice approach that transfers knowledge from a non-RL task to a target RL task to avoid the expensive data sampling. We analyze how practice captures the distributions of state and action spaces in an environment. For this, we develop a novel learning approach that acquires important samples from practice and then applies them to a target RL task without changing learned bases. Results show an improved learning efficiency through practice in classical benchmark problems and limitations in OpenAI Gym problems.", " Learning complex manipulation tasks often requires to collect a large training dataset to obtain a model of a specific skill. This process may become laborious when dealing with high-DoF robots, and even more tiresome if the skill needs to be learned by multiple robots. In this paper, we investigate how this learning process can be accelerated by using shared latent variable models for knowledge transfer among similar robots in an imitation setting. For this purpose, we take advantage of a shared Gaussian process latent variable model to learn a common latent representation of robot skills. Such representation is then reused as prior information to train new robots by reducing the learning process to a latent-to-output mapping. We show that our framework exhibits faster training convergence and similar performance when compared to single-and multi-robot models. All experiments were conducted in simulation on three different robotic platforms: WALK-MAN, COMAN and CENTAURO robots.", " Horizon or skyline detection plays a vital role towards mountainous visual geo-localization, however most of the recently proposed visual geo-localization approaches rely on user-in-the-loop skyline detection methods. Detecting such a segmenting boundary fully autonomously would definitely be a step forward for these localization approaches. This paper provides a quantitative comparison of four such methods for autonomous horizon/sky line detection on an extensive data set. Specifically, we provide the comparison between four recently proposed segmentation methods  one explicitly targeting the problem of horizon detection[2], second focused on visual geo-localization but relying on accurate detection of skyline [15] and other two proposed for general semantic segmentation Fully Convolutional Networks (FCN) [21] and SegNet[22]. Each of the first two methods is trained on a common training set [11] comprised of about 200 images while models for the third and fourth method are fine tuned for sky segmentation problem through transfer learning using the same data set. Each of the method is tested on an extensive test set (about 3K images) covering various challenging geographical, weather, illumination and seasonal conditions. We report average accuracy and average absolute pixel error for each of the presented formulation.", "Incremental Learning  Transfer Learning  Convolutional Neural Networks  Associative Memories Thanks to their ability to absorb large amounts of data, Convolutional Neural Networks (CNNs) have become state-of-the-art in numerous vision challenges, sometimes even on par with biological vision. They rely on optimisation routines that typically require intensive computational power, thus the question of embedded architectures is a very active field of research. Of particular interest is the problem of incremental learning, where the device adapts to new observations or classes. To tackle this challenging problem, we propose to combine pre-trained CNNs with binary associative memories, using product random sampling as an intermediate between the two methods. The obtained architecture requires significantly less computational power and memory usage than existing counterparts. Moreover, using various challenging vision datasets we show that the proposed architecture is able to perform one-shot learning-and even use only a small portion of the dataset-while keeping very good accuracy.", "smart healthcare  smart diagnosis  MRI  artificial intelligence  pattern recognition  Alzheimer's disease Modern pattern recognition and artificial intelligence systems can help in providing better health care and medical solutions. The performance of human diagnosis degrades due to fatigue, cognitive biases, systems faults, and distractions. However, artificial intelligence based diagnosis systems are less error prone and give safe support to clinicians in detection and decision making. This work presents a smart and reliable way of diagnosing Alzheimer's disease (AD) and its possible early stage i.e., mild cognitive impairment. Alzheimer's is a neurodegenerative disease and leads to severe memory loss and inability to cope with daily life tasks. The diagnosis of AD from structural images requires great skill and is challenging for human diagnostics. The presented framework is based on deep learning and detects Alzheimer's and its initial stages accurately from structural MRI scans. The framework analyzes four different classes simultaneously in a single setup. The testing accuracy of diagnosis obtained by the method is 98.88%. Experiments are also performed on binary data and transfer learning is applied for multiclass classification achieving 99.7% accuracy. Once a good trained model is obtained, the decision for an unseen test scan is given within a few seconds. These models are free from the factors that are responsible for causing human diagnostic errors. Therefore, these models are dependable and can provide much faster diagnosis.", " Post-traumatic stress disorder (PTSD) is a traumatic-stressor related disorder developed by exposure to a traumatic or adverse environmental event that caused serious harm or injury. Structured interview is the only widely accepted clinical practice for PTSD diagnosis but suffers from several limitations including the stigma associated with the disease. Diagnosis of PTSD patients by analyzing speech signals has been investigated as an alternative since recent years, where speech signals are processed to extract frequency features and these features are then fed into a classification model for PTSD diagnosis. In this paper, we developed a deep belief network (DBN) model combined with a transfer learning (TL) strategy for PTSD diagnosis. We computed three categories of speech features and utilized the DBN model to fuse these features. The TL strategy was utilized to transfer knowledge learned from a large speech recognition database, TIMIT, for PTSD detection where PTSD patient data is difficult to collect. We evaluated the proposed methods on two PTSD speech databases, each of which consists of audio recordings from 26 patients. We compared the proposed methods with other popular methods and showed that the state-of-the-art support vector machine (SVM) classifier only achieved an accuracy of 57.68%, and TL strategy boosted the performance of the DBN from 61.53% to 74.99%. Altogether, our method provides a pragmatic and promising tool for PTSD diagnosis.", "Transfer learning  domain adaptation  distribution adaptation  class imbalance Transfer learning has achieved promising results by leveraging knowledge from the source domain to annotate the target domain which has few or none labels. Existing methods often seek to minimize the distribution divergence between domains, such as the marginal distribution, the conditional distribution or both. However, these two distances are often treated equally in existing algorithms, which will result in poor performance in real applications. Moreover, existing methods usually assume that the dataset is balanced, which also limits their performances on imbalanced tasks that are quite common in real problems. To tackle the distribution adaptation problem, in this paper, we propose a novel transfer learning approach, named as Balanced Distribution Adaptation (BDA), which can adaptively leverage the importance of the marginal and conditional distribution discrepancies, and several existing methods can be treated as special cases of BDA. Based on BDA, we also propose a novel Weighted Balanced Distribution Adaptation (W-BDA) algorithm to tackle the class imbalance issue in transfer learning. W-BDA not only considers the distribution adaptation between domains but also adaptively changes the weight of each class. To evaluate the proposed methods, we conduct extensive experiments on several transfer learning tasks, which demonstrate the effectiveness of our proposed algorithms over several state-of-the-art methods.", "region based CNN  fast feature pyramids  detection  gender  man woman  transfer learning Human gender detection from body profile is an important task for surveillance. Most surveillance cameras are placed at a distance such that it is not possible to see people's face clearly. In this paper, we report the comparison between fast-feature pyramids and deep region-based convolutional neural network (RCNN) to detect a person in surveillance images. Since RCNN performs better in detecting a person, further training is applied to the RCNN to detect man and woman. Transfer learning strategy is used due to a small number of training images. The result shows that the trained RCNN can detect man and woman with promising result.", "Transfer Learning  knowledge transfer  sentiment analysis  Twitter  Yelp reviews Transfer learning is an emerging research area which extracts knowledge from one or more than one source domains and utilizes this gained knowledge to perform some task in a target domain. It has emerged as a popular topic in recent years, because this technique is considered to be helpful in reducing the cost of labeling. It has many applications on different domains such as Natural Language Processing, Image and Video Processing, etc. The aim is to study transfer learning and implement it for Sentiment Analysis of Tweets by using the knowledge of Yelp reviews. We find that transfer Learning approach is faster than the conventional machine learning approach and give comparable accuracy at much smaller dataset.", " Face verification is a task to determine whether a pair of given facial images belong to the same person. In unconstrained real applications, inter and intra variations, including illumination, pose, occlusion, and expression, will seriously decrease the verification performance. Due to the lack of annotated data for face verification, extended datasets for face recognition with large samples are used to assist learning a robust feature representation generally. However, the extended data for face recognition is different from face verification on distribution and task. In this paper, a transfer learning based on PCA-SVM is proposed to alleviate above problem. The original feature representation is learnt from a deep convolutional neural network by face classification. Then a PCA-SVM based transfer method is used for feature reprojection from the source domain (face recognition) to the target domain (face verification), which reduces the divergence of feature distribution and task inconsistency. The proposed framework yields comparable results and the accuracy is 98.5% on LFW dataset.", " We propose a novel 3D face recognition algorithm using a deep convolutional neural network (DCNN) and a 3D face expression augmentation technique. The performance of 2D face recognition algorithms has significantly increased by leveraging the representationalp ower of deep neural networks and the use of large-scale labeled training data. In this paper, we show that transfer learning from a CNN trained on 2D face images can effectively work for 3D face recognition by fine-tuning the CNN with an extremely small number of 3D facial scans. We also propose a 3D face expression augmentation technique which synthesizes a number of different facial expressions from a single 3D face scan. Our proposed method shows excellent recognition results on Bosphorus, BU-3DFE, and 3D-TEC datasets without using hand-crafted features. The 3D face identification using our deep features also scales well for large databases.", " Active appearance models (AAMs) have seen tremendous success in face analysis. However model learning depends on the availability of detailed annotation of canonical landmark points. As a result, when accurate AAM fitting is required on a different set of variations (expression, pose, identity), a new dataset is collected and annotated. To overcome the need for time consuming data collection and annotation, transfer learning approaches have received recent attention. The goal is to transfer knowledge from previously available datasets (source) to a new dataset (target). We propose a subspace transfer learning method, in which we select a subspace from the source that best describes the target space. We propose a metric to compute the directional similarity between the source eigenvectors and the target subspace. We show an equivalence between this metric and the variance of target data when projected onto source eigenvectors. Using this equivalence, we select a subset of source principal directions that capture the variance in target data. To define our model, we augment the selected source subspace with the target subspace learned from a handful of target examples. In experiments done on six public datasets, we show that our approach outperforms the state of the art in terms of the RMS fitting error as well as the percentage of test examples for which AAM fitting converges to the ground truth.", "Single-Image Super Resolution  Iris Recognition  Transfer Learning  Convolutional Neural Networks Increasingly, iris recognition towards more relaxed conditions has issued a new super-resolution field direction. In this work we evaluate the use of deep learning and transfer learning for single image super resolution applied to iris recognition. For this purpose, we explore if the nature of the images as well as if the pattern from the iris can influence the CNN transfer learning and, consequently, the results in the recognition process. The good results obtained by the texture transfer learning using a deep architecture suggest that features learned by Convolutional Neural Networks used for image super-resolution can be highly relevant to increase iris recognition rate.", "Deep learning  Small dataset  Stacked autoencoder Many studies have shown that deep learning outperforms traditional machine learning methods in many applications. To prevent overfitting, a huge number of training samples is usually required in training process of deep learning. However, collecting such a large dataset is time-consumed and costly. Recently, several methods have been proposed to effectively learn the models with a limited number of training samples. In this paper, we experimentally evaluate the data augmentation and the transfer learning methods on the MNIST dataset with stacked autoencoders. The experiment results suggest that the transfer learning method can improve the performance of the SAE when training samples are insufficient.", "Arousal recognition  Affect  Deep neural networks  Machine learning  Transfer learning  Stress  Emotions Affect recognition is an important task in ubiquitous computing, in particular in health and human-computer interaction. In the former, it contributes to the timely detection and treatment of emotional and mental disorders, and in the latter, it enables indigenous interaction and enhanced user experience. We present an inter-domain study for affect recognition on seven different datasets, recorded with six different sensors, three different sensor placements, 211 subjects and nearly 1000 hours of labelled data. The datasets are processed and translated into a common spectrotemporal space. The data represented in the common spectro-temporal space is used to train a deep neural network (DNN) for arousal recognition that benefits from the large amounts of data even when the data are heterogeneous (i.e., different sensors and different datasets). The DNN approach outperforms the classical machine-learning approaches in six out of seven datasets.", "Deep Neural Network  Transfer Learning  Feature Learning  Brain Decoding Is there a general representation of the information content of human brain, which can be extracted from the functional magnetic resonance imaging (fMRI) data? Is it possible to learn this representation automatically from big data sets by unsupervised learning methods? Is it possible to transfer this representation to learn and decode a set of cognitive states in other fMRI data sets? This study addresses partial answers to the above questions by using transfer learning in deep architectures. First, a hierarchical representation for fMRI data is learned from a large data set in Human Connectome Project (HCP) by a 3-layered stacked denoising autoencoder (SDAE). Then, the learned representations are used to train and recognize the cognitive states recorded by a relatively small data set of one-back repetition detection experiment. Results show that, it is possible to learn a general representation and transfer the learned representation of an fMRI data set to another dataset for brain decoding problem. The learned representation has a better discriminative power compared to the Pearson correlation features. Results also show us that deep neural networks transfer representations better than factor models commonly used in pattern recognition and neuroscience literature.", "Online Advertising  Transfer Learning  CTR Prediction As the main revenue source of Internet companies, online advertising is always a significant topic, where click-through rate (CTR) prediction plays a central role. In online advertising systems, there are often many advertisement products. Due to the competition in the bidding mechanism, some advertising products may get lots of data to train the CTR prediction model while some may lack highquality data. However, to predict accurate CTR, a large amount of data is needed. Therefore, transfer knowledge from the large product (source) to the small product (target) is necessary. We propose a transfer learning method that iteratively updates the data weights to selectively combine source data with target data for training. To efficiently process huge advertisement data, we design a sampling strategy based on the gradient information, and implement the algorithm with a MapReduce-like machine learning framework. We do experiments on real advertisement datasets. The results show that our approach improves the accuracy of CTR prediction compared to the supervised learning method.", "User Profile  Personalized Recommendation  Transfer Learning Cold start is always a key challenge for building real-life recommendation systems. Thanks to the ever-growing multi-modal data in the mobile Internet age and the latest deep learning techniques, transfer-learning based cross-domain recommendation starts to play a crucial role in tackling the cold start problem and to provide warm-start recommendation for new users. At Cheetah Mobile, we apply transfer learning to build personalized recommendation systems for both advertisement and content scenarios, serving 600+ millions monthly active mobile users. In particular, we leveraged the app install & usage and many other mobile data, built a Unified User Profile (UUP) by the state-of-the-art deep learning techiniques, and developed cross-domain personalized Ad and news recommendation. Our approaches enable us to solve the cold start problem with close to full coverage of our user base while yielding significant CTR increase and better user experience.", "cross-domain  knowledge transfer  recommender system Cross-domain recommender systems and transfer learning approaches are useful to help integrate knowledge from different places, so that we alleviate some existing problems (such as the cold-start problem), or improve the quality of recommender systems. With the advantages of these techniques, we host the first international workshop on intelligent recommender systems by knowledge transfer and learning (RecSysKTL) to provide such a forum for academia researchers and application developers from around the world to present their work and discuss exciting research ideas or outcomes. The workshop is held in conjunction with the ACM Conference on Recommender Systems 2017 on August 27th at Como, Italy.", "Fashion recommendation  deep learning  cross-domain knowledge transfer  transfer learning  domain adaptation  CNN With the increasing number of online shopping services, the number of users and the quantity of visual and textual information on the Internet, there is a pressing need for intelligent recommendation systems that analyze the user's behavior amongst multiple domains and help them to find the desirable information without the burden of search. However, there is little research that has been done on complex recommendation scenarios that involve knowledge transfer across multiple domains. This problem is especially challenging when the involved data sources are complex in terms of the limitations on the quantity and quality of data that can be crawled. The goal of this paper is studying the connection between visual and textual inputs for better analysis of a certain domain, and to examine the possibility of knowledge transfer from complex domains for the purpose of efficient recommendations. The methods employed to achieve this study include both design of architecture and algorithms using deep learning technologies to analyze the effect of deep pixel-wise semantic segmentation and text integration on the quality of recommendations. We plan to develop a practical testing environment in a fashion domain.", " Most machine learning methods assume that previous and future data have same distribution in same feature space. This paper presents a real-world problem that violates the common assumption, and we propose a practical methodology to handle the problem. In the steel making industry, automated marking systems are widely used to inscribe slab identification numbers (SINS). In the previous work, a deep learning based algorithm was developed to automatically extract regions of printed SINs. However, as the marking system is outdated, few SINs are marked by hand in uncommon situations, and the existing algorithm does not work for the handwritten SINs. This paper proposes a practical method that uses very small training data (10 images) to localize handwritten SINs. The knowledge of mid-level layers or entire layers in the pre-trained deep convolutional neural network is transferred to overcome the shortage of training data in the target domain. Experiments were conducted with actual industrial data to demonstrate the effectiveness of the proposed algorithm.", "Convolutional Neural Network(CNN)  Transfer Learning (TL)  Machine Learning (ML)  Computer Vision Automated medical assistance system is in high demand with the advances in research in the machine learning area. In many such applications, availability of labeled medical dataset is a primary challenge and dataset of dental diseases is not an exception. An attempt towards accurate classification of dental diseases is addressed in this paper. Labeled dataset consisting of 251 Radio Visiography (RVG) x-ray images of 3 different classes is used for classification. Convolutional neural network (CNN) has become a most effective tool in machine learning which enables solving the problems like image recognition, segmentation, classification, etc., with high order of accuracy. It is found from literature that CNN performs well in natural image classification problems where large dataset is available. In this paper we experimented on the performance of CNN for diagnosis of small labeled dental dataset. In addition, transfer learning is used to improve the accuracy. Experimental results are presented for three different architectures of CNN. Overall accuracy achieved is very encouraging.", "Nutritional assessment  maize leaf analysis  deep learning  texture analysis  transfer learning  convolutional neural networks Every year, efficient maize production is very important to the economy of many countries. Since nutritional deficiencies in maize plants are directly reflected in their grains productivity, early detection is needed to maximize the chances of proper recovery of these plants. Traditional texture methods recently showed interesting results in the identification of nutritional deficiencies. On the other hand, deep learning techniques are increasingly outperforming hand-crafted features on many tasks. In this paper, we propose a simple transfer learning approach from pre-trained cnn models and compare their results with those from traditional texture methods in the task of nitrogen deficiency identification. We perform experiments in a real-world dataset that contains digitalized images of maize leaves at different growth stages and with different levels of nitrogen fertilization. The results show that deep learning based descriptors achieve better success rates than traditional texture methods.", "optimization  transfer learning  CMA-ES  Powell's Conjugate Gradient Optimization is one of the most important problems in science and engineering. Common optimization algorithms are designed to work for a large set of problems, but not necessarily to be efficient for specific domains. We propose a new transfer learning approach to adapt optimization algorithms to specific problem domains. Our approach analyzes solved problems of a domain to identify areas in the search space where good solutions are expected for this domain. Knowledge of these areas is used to improve the optimization algorithm performance of unseen problems of the same domain. Because of its general design, our method can be applied to a wide range of problems and algorithms.", "domain adaptation  transfer learning  fuzzy features  heterogeneous feature space Domain adaptation is a transfer learning approach that has been widely studied in the last decade. However, existing works still have two limitations: 1) the feature spaces of the domains are homogeneous, and 2) the target domain has at least a few labeled instances. Both limitations significantly restrict the domain adaptation approach when knowledge is transferred across domains, especially in the current era of big data. To address both issues, this paper proposes a novel fuzzy-based heterogeneous unsupervised domain adaptation approach. This approach maps the feature spaces of the source and target domains onto the same latent space constructed by fuzzy features. In the new feature space, the label spaces of two domains are maintained to reduce the probability of negative transfer occurring. The proposed approach delivers superior performance over current benchmarks, and the heterogeneous unsupervised domain adaptation (HeUDA) method provides a promising means of giving a learning system the associative ability to judge unknown things using related knowledge.", "machine learning  fuzzy rules  transfer learning  classification As the age of big data approaches, methods of massive scale data management are rapidly evolving. The traditional machine learning methods can no longer satisfy the exponential development of big data  there is a common assumption in these data-driving methods that the distribution of both the training data and testing data should be equivalent. A model built using today's data will not adequately address the classification tasks tomorrow if the distribution of the data item values has changed. Transfer learning is emerging as a solution to this issue, and many methods have been proposed. Few of the existing methods, however, explicitly indicate the solution to the case where the labels' distributions in two domains are different. This work proposes the fuzzy rule-based methods to deal with transfer learning problems where the discrepancy between the two domains shows in the label spaces. The presented methods are validated in both the synthetic and real-world datasets, and the experimental results verify the effectiveness of the introduced methods.", "Transfer learning  Image classification  Deep learning  Video surveillance  Intrusion detection Its crucial for financial systems to have sound security measures in place. For security reasons customers are not allowed to wear a helmet while using ATM(Automated Teller Machine). An automated helmet detection using ATM surveillance camera feed can help improve security significantly. Recently deep convolutional neural network (DCNN) have shown state of the art accuracy in object detection and localization. In this work, a pretrained Google's inception model have been used and have achieved an accuracy of 95.3% by training the model on a proprietary ATM surveillance dataset. Transferred information from inception model has been feed to multiple fully connected layers with drop outs to achieve better accuracy. (C) 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of the 7th International Conference on Advances in Computing & Communications.", " This paper proposes a new face verification method that uses multiple deep convolutional neural networks (DCNNs) and a deep ensemble, that extracts two types of low dimensional but discriminative and high-level abstracted features from each DCNN, then combines them as a descriptor for face verification. Our DCNNs are built from stacked multi-scale convolutional layer blocks to present multi-scale abstraction. To train our DCNNs, we use different resolutions of triplets that consist of reference images, positive images, and negative images, and triplet-based loss function that maximize the ratio of distances between negative pairs and positive pairs and minimize the absolute distances between positive face images. A deep ensemble is generated from features extracted by each DCNN, and used as a descriptor to train the joint Bayesian learning and its transfer learning method. On the LFW, although we use only 198, 018 images and only four different types of networks, the proposed method with the joint Bayesian learning and its transfer learning method achieved 98.33% accuracy. In addition to further increase the accuracy, we combine the proposed method and high dimensional LBP based joint Bayesian method, and achieved 99.08% accuracy on the LFW. Therefore, the proposed method helps to improve the accuracy of face verification when training data is insufficient to train DCNNs.", " Matching facial sketches to digital face images has widespread application in law enforcement scenarios. Recent advancements in technology have led to the availability of sketch generation tools, minimizing the requirement of a sketch artist. While these sketches have helped in manual authentication, matching composite sketches with digital mugshot photos automatically show high modality gap. This research aims to address the task of matching a composite face sketch image to digital images by proposing a transfer learning based evolutionary algorithm. A new feature descriptor, Histogram of Image Moments, has also been presented for encoding features across modalities. Moreover, IIITD Composite Face Sketch Database of 150 subjects is presented to fill the gap due to limited availability of databases in this problem domain. Experimental evaluation and analysis on the proposed dataset show the effectiveness of the transfer learning approach for performing cross-modality recognition.", " Autonomous cars establish driving strategies using the positions of ego lanes. The previous methods detect lane points and select ego lanes with heuristic and complex post-processing with strong geometric assumptions. We propose a sequential end-to-end transfer learning method to estimate left and right ego lanes directly and separately without any postprocessing. We redefined a point-detection problem as a region-segmentation problem  as a result, the proposed method is insensitive to occlusions and variations of environmental conditions, because it considers the entire content of an input image during training. Also, we constructed an extensive dataset that is suitable for a deep neural network training by collecting a variety of road conditions, annotating ego lanes, and augmenting them systematically. The proposed method demonstrated improved accuracy and stability on input variations compared with a recent method based on deep learning. Our approach does not involve postprocessing, and is therefore flexible to change of target domain.", "Convolution neural networks  Transfer learning  Dataset complexity This paper makes use of diverse domains' datasets to analyze the impact of image complexity and diversity on the task of transfer learning in deep neural networks. As the availability of labels and quality instances for several domains are still scarce, it is imperative to use the knowledge acquired from similar problems to improve classifier performance by transferring the learned parameters. We performed a statistical analysis through several experiments in which the convolutional neural networks (LeNet-5, AlexNet, VGG-11 and VGG-16) were trained and transferred to different target tasks layer by layer. We show that when working with complex low-quality images and small datasets, fine-tuning the transferred features learned from a low complexity source dataset gives the best results.", " The intermediate map responses of a Convolutional Neural Network (CNN) contain contextual knowledge about its input. In this paper, we present a framework that uses these activation maps from several layers of a CNN as features to a Deep Belief Network (DBN) using transfer learning to provide an understanding of an input image. We create a representation of these features and the training data and use them to extract more information from an image at the pixel level, hence gaining understanding of the whole image. We experimentally demonstrate the usefulness of our framework using a pretrained model and use a DBN to perform segmentation on the BAERI dataset of Synthetic Aperture Radar (SAR) imagery and the CAMVID dataset with a relatively smaller training dataset.", "Deep Learning  Neural Network  Geospatial  Automation  Satellite Imagery The importance of satellite imagery analysis has increased dramatically over the last several years, keeping pace with the rapid improvements seen in both remote sensing platforms and sensors. As this field expands, so too does the interest in using machine learning methods to automate parts of the imagery analyst's workflow. In this paper we address one aspect of this challenge: the development of a method for the automatic extraction of parking lots from aerial imagery. To the best of our knowledge, there has been no prior work conducted on the development of an end-to-end pipeline for this particular task. Due to the limited size of our dataset and to accommodate the potentially limited size of future datasets, we propose a deep learning approach using transfer learning. This process hinges upon the use of state of the art Convolutional Neural Networks (CNNs), trained on general image classification datasets. These networks were then fine-tuned on our custom dataset, to establish a comprehensive benchmark for this task. Our method exhibits promising results for automatic parking lot extraction, and is generalizable enough to work with different input types, including high resolution aerial orthoimagery, satellite imagery, full motion video (FMV), and UAV imagery.", " Different types of vehicles, such as buses and cars, can be quite different in shapes and details. This makes it more difficult to try to learn a single feature vector that can detect all types of vehicles using a single object class. We proposed an approach to perform vehicle detection with Sub-Classes categories learning using R-CNN in order to improve the performance of vehicle detection. Instead of using a single object class, which is a vehicle in this experiment, to train on the R-CNN, we used multiple sub-classes of vehicles so that the network can better learn the features of each individual type. In the experiment, we also evaluated the result of using a transfer learning approach to use a pre-trained weights on a new dataset.", " The object detection is a challenging problem in computer vision with various potential real-world applications. The objective of this study is to evaluate the deep learning based object detection techniques for detecting drones. In this paper, we have conducted experiments with different Convolutional Neural Network (CNN) based network architectures namely Zeiler and Fergus (ZF), Visual Geometry Group (VGGI6) etc. Due to sparse data available for training, networks are trained with pre-trained models using transfer learning. The snapshot of trained models is saved at regular interval during training. The best models having high mean Average Precision (mAP) for each network architecture are used for evaluation on the test dataset. The experimental results show that VGGI6 with Faster R-CNN perform better than other architectures on the training dataset. Visual analysis of the test dataset is also presented.", "Transfer learning  weight transfer  LF-MMI  multi-task learning  Automatic Speech Recognition It is common in applications of ASR to have a large amount of data out-of-domain to the test data and a smaller amount of in-domain data similar to the test data. In this paper, we investigate different ways to utilize this out-of-domain data to improve ASR models based on Lattice-free MMI (LF-MMI). In particular, we experiment with multi-task training using a network with shared hidden layers  and we try various ways of adapting previously trained models to a new domain. Both types of methods are effective in reducing the WER versus in-domain models, with the jointly trained models generally giving more improvement.", "speech recognition  end-to-end neural network  raw speech waveform Conventional automatic speech recognition (ASR) typically performs multi-level pattern recognition tasks that map the acoustic speech waveform into a hierarchy of speech units. But, it is widely known that information loss in the earlier stage can propagate through the later stages. After the resurgence of deep learning, interest has emerged in the possibility of developing a purely end-to-end ASR system from the raw waveform to the transcription without any predefined alignments and hand-engineered models. However, the successful attempts in end-to-end architecture still used spectral-based features, while the successful attempts in using raw waveform were still based on the hybrid deep neural network -Hidden Markov model (DNN-HMM) framework. In this paper, we construct the first end-to-end attention-based encoder-decoder model to process directly from raw speech waveform to the text transcription. We called the model as Attention-based Wav2Text. To assist the training process of the end-to-end model, we propose to utilize a feature transfer learning. Experimental results also reveal that the proposed Attention-based Wav2Text model directly with raw waveform could achieve a better result in comparison with the attentional encoder-decoder model trained on standard front-end filterbank features.", "Multi-genre broadcast  Automatic speech recognition  Lightly-supervised training  LF-MMI  Segmentation This paper describes the JHU team's Kaldi system submission to the Arabic MGB-3: The Arabic speech recognition in the Wild Challenge for ASRU-2017. We use a weights transfer approach to adapt a neural network trained on the out-of-domain MGB-2 multi-dialect Arabic TV broadcast corpus to the MGB-3 Egyptian YouTube video corpus. The neural network has a TDNN-LSTM architecture and is trained using lattice-free maximum mutual information (LF-MMI) objective followed by sMBR discriminative training. For supervision, we fuse transcripts from 4 independent transcribers into confusion network training graphs. We also describe our own approach for speaker diarization and audio-transcript alignment. We use this to prepare lightly supervised transcriptions for training the seed system used for adaptation to MGB3. Our primary submission to the challenge gives a multi-reference WER of 32.78% on the MGB-3 test set.", "Modality invariance  unsupervised speech processing  multimodal language processing  variational methods  regularization In this paper, we explore the unsupervised learning of a semantic embedding space for co-occurring sensory inputs. Specifically, we focus on the task of learning a semantic vector space for both spoken and handwritten digits using the TIDIGITs and MNIST datasets. Current techniques encode image and audio/textual inputs directly to semantic embeddings. In contrast, our technique maps an input to the mean and log variance vectors of a diagonal Gaussian from which sample semantic embeddings are drawn. In addition to encouraging semantic similarity between co-occurring inputs, our loss function includes a regularization term borrowed from variational autoencoders (VAEs) which drives the posterior distributions over embeddings to be unit Gaussian. We can use this regularization term to filter out modality information while preserving semantic information. We speculate this technique may be more broadly applicable to other areas of cross-modality/domain information retrieval and transfer learning.", "dialogue state tracking  belief tracking  dialogue systems  transfer learning Dialogue state tracking (DST) is a key component of task-oriented dialogue systems. DST estimates the user's goal at each user turn given the interaction until then. State of the art approaches for state tracking rely on deep learning methods, and represent dialogue state as a distribution over all possible slot values for each slot present in the ontology. Such a representation is not scalable when the set of possible values are unbounded (e.g., date, time or location) or dynamic (e.g., movies or usernames). Furthermore, training of such models requires labeled data, where each user turn is annotated with the dialogue state, which makes building models for new domains challenging. In this paper, we present a scalable multi-domain deep learning based approach for DST. We introduce a novel framework for state tracking which is independent of the slot value set, and represent the dialogue state as a distribution over a set of values of interest (candidate set) derived from the dialogue history or knowledge. Restricting these candidate sets to be bounded in size addresses the problem of slot-scalability. Furthermore, by leveraging the slot-independent architecture and transfer learning, we show that our proposed approach facilitates quick adaptation to new domains.", " In this paper, we study the task of 3D human pose estimation in the wild. This task is challenging due to lack of training data, as existing datasets are either in the wild images with 2D pose or in the lab images with 3D pose. We propose a weakly-supervised transfer learning method that uses mixed 2D and 3D labels in a unified deep neutral network that presents two-stage cascaded structure. Our network augments a state-of-the-art 2D pose estimation sub-network with a 3D depth regression sub-network. Unlike previous two stage approaches that train the two sub-networks sequentially and separately, our training is end-to-end and fully exploits the correlation between the 2D pose and depth estimation sub-tasks. The deep features are better learnt through shared representations. In doing so, the 3D pose labels in controlled lab environments are transferred to in the wild images. In addition, we introduce a 3D geometric constraint to regularize the 3D pose prediction, which is effective in the absence of ground truth depth labels. Our method achieves competitive results on both 2D and 3D benchmarks.", " We propose a novel deep learning approach to solve simultaneous alignment and recognition problems (referred to as Sequence-to-sequence learning). We decompose the problem into a series of specialised expert systems referred to as SubUNets. The spatio-temporal relationships between these SubUNets are then modelled to solve the task, while remaining trainable end-to-end. The approach mimics human learning and educational techniques, and has a number of significant advantages. SubUNets allow us to inject domain-specific expert knowledge into the system regarding suitable intermediate representations. They also allow us to implicitly perform transfer learning between different interrelated tasks, which also allows us to exploit a wider range of more varied data sources. In our experiments we demonstrate that each of these properties serves to significantly improve the performance of the overarching recognition system, by better constraining the learning problem. The proposed techniques are demonstrated in the challenging domain of sign language recognition. We demonstrate state-of-the-art performance on hand-shape recognition (outperforming previous techniques by more than 30%). Furthermore, we are able to obtain comparable sign recognition rates to previous research, without the need for an alignment step to segment out the signs for recognition.", " We present an algorithm for test-time combination of a set of reference predictors with unknown parametric forms. Existing multi-task and transfer learning algorithms focus on training-time transfer and combination, where the parametric forms of predictors are known and shared. However, when the parametric form of a predictor is unknown, e.g., for a human predictor or a predictor in a precompiled library, existing algorithms are not applicable. Instead, we empirically evaluate predictors on sampled data points to measure distances between different predictors. This embeds the set of reference predictors into a Riemannian manifold, upon which we perform manifold denoising to obtain the refined predictor. This allows our approach to make no assumptions about the underlying predictor forms. Our test-time combination algorithm equals or outperforms existing multi-task and transfer learning algorithms on challenging real-world datasets, without introducing specific model assumptions.", " Subspace learning is one of the most foundational tasks in computer vision with applications ranging from dimensionality reduction to data denoising. As geometric objects, subspaces have also been successfully used for efficiently representing certain types of invariant data. However, methods for subspace learning from subspace-valued data have been notably absent due to incompatibilities with standard problem formulations. To fill this void, we introduce Approximate Grassmannian Intersections (AGI), a novel geometric interpretation of subspace learning posed as finding the approximate intersection of constraint sets on a Grassmann manifold. Our approach can naturally be applied to input subspaces of varying dimension while reducing to standard subspace learning in the case of vector-valued data. Despite the nonconvexity of our problem, its globally-optimal solution can be found using a singular value decomposition. Furthermore, we also propose an efficient, general optimization approach that can incorporate additional constraints to encourage properties such as robustness. Alongside standard subspace applications, AGI also enables the novel task of transfer learning via subspace completion. We evaluate our approach on a variety of applications, demonstrating improved invariance and generalization over vector-valued alternatives.", " Region of Interest (ROI) crowd counting can be formulated as a regression problem of learning a mapping from an image or a video frame to a crowd density map. Recently, convolutional neural network (CNN) models have achieved promising results for crowd counting. However, even when dealing with video data, CNN-based methods still consider each video frame independently, ignoring the strong temporal correlation between neighboring frames. To exploit the otherwise very useful temporal information in video sequences, we propose a variant of a recent deep learning model called convolutional LSTM (ConvLSTM) for crowd counting. Unlike the previous CNN-based methods, our method fully captures both spatial and temporal dependencies. Furthermore, we extend the ConvLSTM model to a bidirectional ConvLSTM model which can access long-range information in both directions. Extensive experiments using four publicly available datasets demonstrate the reliability of our approach and the effectiveness of incorporating temporal information to boost the accuracy of crowd counting. In addition, we also conduct some transfer learning experiments to show that once our model is trained on one dataset, its learning experience can be transferred easily to a new dataset which consists of only very few video frames for model adaptation.", " We introduce a novel method for representation learning that uses an artificial supervision signal based on counting visual primitives. This supervision signal is obtained from an equivariance relation, which does not require any manual annotation. We relate transformations of images to transformations of the representations. More specifically, we look for the representation that satisfies such relation rather than the transformations that match a given representation. In this paper, we use two image transformations in the context of counting: scaling and tiling. The first transformation exploits the fact that the number of visual primitives should be invariant to scale. The second transformation allows us to equate the total number of visual primitives in each tile to that in the whole image. These two transformations are combined in one constraint and used to train a neural network with a contrastive loss. The proposed task produces representations that perform on par or exceed the state of the art in transfer learning benchmarks.", "personalization  smart device  user preference  distributed learning  context aware Providing a personalized service to a user in a smart environment has been one of the key goals in the area of pervasive computing. The proliferation of individually developed smart devices in the name of Internet of Things opens up a possibility of providing personalized services to a user in an autonomous and distributed manner. As a user's task often involves services supported by multiple devices, capturing a device-specific service preference is not enough to maximize a user's comfort. In this paper, we propose a distributed learning scheme for capturing multiple device service preferences in a smart environment. We exploit multi-agent reinforcement learning (MARL) method where each smart device acts as a reinforcement learning agent to incrementally and cooperatively capture a user specific preference of a task. Experiments confirm that smart devices with the proposed scheme are able to capture multiple device service preferences from a small number of interactions with a user and an environment. Also, the proposed transfer learning method improves learning performance for a new task.", " Recent years have seen increased attention being given to Blood Pressure (BP) monitoring. Among all kinds of measurements, the monitors based on Pulse Transit Time (PTT) have gain plenty of attention due to its continuous and cuffless features. Additionally, several studies proposed a fancy way to estimate photoplethysmography (PPG) signal simply via a regular webcam. Nevertheless, literatures on issues of integrating these two advanced techniques have emerged on a slowly and scattered way. Furthermore, accuracy of BP prediction model based on PTT is often limited due to the lack of data. To address the above-mentioned problems, we proposed an image based BP measurement algorithm using k-nearest neighbor and transfer learning results from MIMICII database to real task. The study also introduces newly defined PTT features which are especially suitable for image based PPG and domain adaptation. Compared with the state-of-the-art algorithm, root mean square error of SBP evaluation has been reduced from 15.08 to 14.02.", " A significant improvement in big data analytics has motivated the radical change in the scientific study of crime and criminals. In terms of criminal activities, it has been observed that alcohol has a great influence in most of the cases. The main goals of our research are to analyze different types of violence happening in Nova Scotia and to apply machine learning techniques to model the relationships between alcohol consumption and violence. In many machine learning algorithms, it is assumed that, the training and testing data must be in the same distribution and feature space. Because of limited amount of Nova Scotia criminal activity data, the need of transfer learning arises which helps to gain knowledge from different domains. The results of our studies show a very satisfactory classification performance on Nova Scotia data.", " We propose DeepExpr, a novel expression transfer approach from humans to multiple stylized characters. We first train two Convolutional Neural Networks to recognize the expression of humans and stylized characters independently. Then we utilize a transfer learning technique to learn the mapping from humans to characters to create a shared embedding feature space. This embedding also allows human expression-based image retrieval and character expression-based image retrieval. We use our perceptual model to retrieve character expressions corresponding to humans. We evaluate our method on a set of retrieval tasks on our collected stylized character dataset of expressions. We also show that the ranking order predicted by the proposed features is highly correlated with the ranking order provided by a facial expression expert and Mechanical Turk experiments.", "deep learning  convolutional neural network  transfer learning  inception model  class activation map  face detection  face localization  thermography Recently, capabilities of many computer vision tasks have significantly improved due to advances in Convolutional Neural Networks. In our research, we demonstrate that it can be also used for face detection from low resolution thermal images, acquired with a portable camera. The physical size of the camera used in our research allows for embedding it in a wearable device or indoor remote monitoring solution for elderly and disabled people. The benefits of the proposed architecture were experimentally verified on the thermal video sequences, acquired in various scenarios to address possible limitations of remote diagnostics: movements of the person performing a diagnose and movements of the examined person. The achieved short processing time ( 42.05 +/- 0.21ms) along with high model accuracy ( false positives -0.43%  true positives for the patient focused on a certain task -89.2%) clearly indicates that the current state of the art in the area of image classification and face tracking in thermography was significantly outperformed.", " Image based vehicle insurance processing is an important area with large scope for automation. In this paper we consider the problem of car damage classification, where some of the categories can be fine-granular. We explore deep learning based techniques for this purpose. Initially, we try directly training a CNN. However, due to small set of labeled data, it does not work well. Then, we explore the effect of domain-specific pre-training followed by fine-tuning. Finally, we experiment with transfer learning and ensemble learning. Experimental results show that transfer learning works better than domain specific fine-tuning. We achieve accuracy of 89.5% with combination of transfer and ensemble learning.", "transfer learning  keyword spotting  cross lingual  small-footprint This paper presents our work on building a small footprint keyword spotting system for a resource-limited language, which requires low CPU, memory and latency. Our keyword spotting system consists of deep neural network (DNN) and hidden Markov model (HMM), which is a hybrid DNN-HMM decoder. We investigate different transfer learning techniques to leverage knowledge and data from a resource-abundant source language to improve the keyword DNN training for a target language which has limited in-domain data. The approaches employed in this paper include training a DNN using source language data to initialize the target language DNN training, mixing data from source and target languages together in a multi-task DNN training setup, using logits computed from a DNN trained on the source language data to regularize the keyword DNN training in the target language, as well as combinations of these techniques. Given different amounts of target language training data, our experimental results show that these transfer learning techniques successfully improve keyword spotting performance for the target language, measured by the area under the curve (AUC) of DNN-HMM decoding detection error tradeoff (DET) curves using a large in-house far-field test set.", "Transfer learning  Domain class imbalance  Traditional machine learning Transfer learning is a subclass of machine learning, which uses training data (source) drawn from a different domain than that of the testing data (target). A transfer learning environment is characterized by the unavailability of labeled data from the target domain, due to data being rare or too expensive to obtain. However, there exists abundant labeled data from a different, but similar domain. These two domains are likely to have different distribution characteristics. Transfer learning algorithms attempt to align the distribution characteristics of the source and target domains to create high-performance classifiers. This paper provides comparative performance analysis between state-of-the-art transfer learning algorithms and traditional machine learning algorithms under the domain class imbalance condition. The domain class imbalance condition is characterized by the source and target domains having different class probabilities, which can create marginal distribution differences between the source and target data. Statistical analysis is provided to show the significance of the results.", "anomaly detection  non-stationary time series  automatic diagnosis  reconstruction error  statistical process control  transfer learning Anomaly detection in database management systems (DBMSs) is difficult because of increasing number of statistics (stat) and event metrics in big data system. In this paper, I propose an automatic DBMS diagnosis system that detects anomaly periods with abnormal DB stat metrics and finds causal events in the periods. Reconstruction error from deep autoencoder and statistical process control approach are applied to detect time period with anomalies. Related events are found using time series similarity measures between events and abnormal stat metrics. After training deep autoencoder with DBMS metric data, efficacy of anomaly detection is investigated from other DBMSs containing anomalies. Experiment results show effectiveness of proposed model, especially, batch temporal normalization layer. Proposed model is used for publishing automatic DBMS diagnosis reports in order to determine DBMS configuration and SQL tuning.", " The advance of computer graphics techniques comes revolutionizing games and movie's industries. Creating very realistic characters totally from computer graphics models is, nowadays, a reality. However, this advance comes with a big price: the realism of images is so big that it is difficult to realize when we are facing a computer generated image or a real photo. In this paper we propose a new approach for highly realistic computer generated images detection by exploring inconsistencies into the region of the eyes. Such inconsistencies are captured exploring the expression power of features extracted via transfer learning approach with VGG19 Deep Neural Network model. Unlike the state-of-the-art approaches, which looks to evaluate the entire image, proposed method focuses in specific regions (eyes) where computer graphics modeling still needs improvements. Experiments conducted over two different datasets containing extremely realistic images achieved an accuracy of 0.80 and an AUC of 0.88.", " A major assumption in many machine learning algorithms is that the training and testing data must come from the same feature space or have the same distributions. However, in real applications, this strong hypothesis does not hold. In this paper, we introduce a new framework for transfer where the source and target domains are represented by subspaces described by eigenvector matrices. To unify subspace distribution between domains, we propose to use a fast efficient approximative SVD for fast features generation. In order to make a transfer learning between domains, we firstly use a subspace learning approach to develop a domain adaption algorithm where only target knowledge is transferable. Secondly, we use subspace alignment trick to propose a novel transfer domain adaptation method. To evaluate the proposal, we use large scale data sets. Numerical results, based on accuracy and computational time are provided with comparison with state-of-the-art methods.", " Malicious software (malware) has been extensively used for illegal activity and new malware variants are discovered at an alarmingly high rate. The ability to group malware variants into families with similar characteristics makes possible to create mitigation strategies that work for a whole class of programs. In this paper, we present a malware family classification approach using a deep neural network based on the ResNet-50 architecture. Malware samples are represented as byteplot grayscale images and a deep neural network is trained freezing the convolutional layers of ResNet-50 pre-trained on the ImageNet dataset and adapting the last layer to malware family classification. The experimental results on a dataset comprising 9,339 samples from 25 different families showed that our approach can effectively be used to classify malware families with an accuracy of 98.62%.", " Learning speaker turn embeddings has shown considerable improvement in situations where conventional speaker modeling approaches fail. However, this improvement is relatively limited when compared to the gain observed in face embedding learning, which has proven very successful for face verification and clustering tasks. Assuming that face and voices from the same identities share some latent properties (like age, gender, ethnicity), we propose two transfer learning approaches to leverage the knowledge from the face domain learned from thousands of identities for tasks in the speaker domain. These approaches, namely target embedding transfer and clustering structure transfer, utilize the structure of the source face embedding space at different granularities to regularize the target speaker turn embedding space as optimizing terms. Our methods are evaluated on two public broadcast corpora and yield promising advances over competitive baselines in verification and audio clustering tasks, especially when dealing with short speaker utterances. The analysis gives insight into characteristics of the embedding spaces and shows their potential applications.", " In this paper, we propose an encoder-decoder convolutional neural network (CNN) architecture for estimating camera pose (orientation and location) from a single RGB-image. The architecture has a hourglass shape consisting of a chain of convolution and up-convolution layers followed by a regression part. The up-convolution layers are introduced to preserve the fine-grained information of the input image. Following the common practice, we train our model in end-to-end manner utilizing transfer learning from large scale classification data. The experiments demonstrate the performance of the approach on data exhibiting different lighting conditions, reflections, and motion blur. The results indicate a clear improvement over the previous state-of-the-art even when compared to methods that utilize sequence of test frames instead of a single frame.", " We address the problem of object recognition from RGB-D images using deep convolutional neural networks (CNNs). We advocate the use of 3D CNNs to fully exploit the 3D spatial information in depth images as well as the use of pretrained 2D CNNs to learn features from RGB-D images. There exists currently no large scale dataset available comprising depth information as compared to those for RGB data. Hence transfer learning from 2D source data is key to be able to train deep 3D CNNs. To this end, we propose a hybrid 2D/3D convolutional neural network that can be initialized with pretrained 2D CNNs and can then be trained over a relatively small RGB-D dataset. We conduct experiments on the Washington dataset involving RGB-D images of small household objects. Our experiments show that the features learnt from this hybrid structure, when fused with the features learnt from depth-only and RGB-only architectures, outperform the state of the art on RGB-D category recognition.", " Subspace learning and reconstruction have been widely explored in recent transfer learning work and generally a specially designed projection and reconstruction transfer matrix are wanted. However, existing subspace reconstruction based algorithms neglect the class prior such that the learned transfer function is biased, especially when data scarcity of some class is encountered. Different from those previous methods, in this paper, we propose a novel reconstruction-based transfer learning method called Class-specific Reconstruction Transfer Learning (CRTL), which optimizes a well-designed transfer loss function without class bias. Using a class-specific reconstruction matrix to align the source domain with the target domain which provides help for classification with class prior modeling. Furthermore, to keep the intrinsic relationship between data and labels after feature augmentation, a projected Hilbert-Schmidt Independence Criterion (pHSIC), that measures the dependency between two sets, is first proposed by mapping the data from original space to RKHS in transfer learning. In addition, combining low-rank and sparse constraints on the class-specific reconstruction coefficient matrix, the global and local data structures can be effectively preserved. Extensive experiments demonstrate that the proposed method outperforms conventional representation based domain adaptation methods.", " Inconsistent data distributions among multiple views is one of the most crucial aspects of person re-identification. To solve the problem, this paper presents a novel strategy called consistent iterative multi-view transfer learning model. The proposed model captures seven groups of multi-view visual words (MvVW) through an unsupervised cluster method (K-means) from human body. For each group of MvVW, a multi-view discriminative common subspace can be obtained by the fusion of transfer learning and discriminative analysis. In these common subspaces, the original samples can be reconstructed based on MvVW under the low-rank and sparse constraints. Then, we solve it via the inexact augmented Lagrange multiplier method. The proposed strategy is performed on three different challenging person re-identification databases (i.e., VIPeR, CUHK01 and PRID450S), which shows that our model outperforms several state-of-the-art models with improving of 6.36%, 7.7% and 4.0% respectively.", " Multi-modal ego-centric data from inertial measurement units (IMU) and first-person videos (FPV) can be effectively fused to recognise proprioceptive activities. Existing IMU-based approaches mostly employ cascades of handcrafted triaxial motion features or deep frameworks trained on limited data. FPV approaches generally encode scene dynamics with motion and pooled appearance features. In this paper, we propose a multi-modal ego-centric proprioceptive activity recognition that uses a convolutional neural network (CNN) followed by a long short-term memory (LSTM) network, transfer learning and a merit-based fusion of IMU and/or FPV streams. The CNN encodes short-term temporal dynamics of the ego-motion and the LSTM exploits the long-term temporal dependency among activities. The merit of a stream is evaluated with a sparsity measure of its initial classification output. We validate the proposed framework on multiple visual and inertial datasets.", " Smile detection is an interesting topic in computer vision and has received increasing attention in recent years. However, the challenge caused by age variations has not been sufficiently focused on before. In this paper, we first highlight the impact of the discrepancy between infants and adults in a quantitative way on a newly collected database. We then formulate this issue as an unsupervised domain adaptation problem and present the solution of deep transfer learning, which applies the state of the art transfer learning methods, namely Deep Adaptation Networks (DAN) and Joint Adaptation Network (JAN), to two baseline deep models, i.e. AlexNet and ResNet. Thanks to DAN and JAN, the knowledge learned by deep models from adults can be transferred to infants, where very limited labeled data are available for training. Cross-dataset experiments are conducted and the results evidently demonstrate the effectiveness of the proposed approach to smile detection across such an age gap.", " Recently, low-shot learning has been proposed for handling the lack of training data in machine learning. Despite of the importance of this issue, relatively less efforts have been made to study this problem. In this paper, we aim to increase the size of training dataset in various ways to improve the accuracy and robustness of face recognition. In detail, we adapt a generator from the Generative Adversarial Network (GAN) to increase the size of training dataset, which includes a base set, a widely available dataset, and a novel set, a given limited dataset, while adopting transfer learning as a backend. Based on extensive experimental study, we conduct the analysis on various data augmentation methods, observing how each affects the identification accuracy. Finally, we conclude that the proposed algorithm for generating faces is effective in improving the identification accuracy and coverage at the precision of 99% using both the base and novel set.", " In this paper, we introduce a new regularization technique for transfer learning. The aim of the proposed approach is to capture statistical relationships among convolution filters learned from a well-trained network and transfer this knowledge to another network. Since convolution filters of the prevalent deep Convolutional Neural Network (CNN) models share a number of similar patterns, in order to speed up the learning procedure, we capture such correlations by Gaussian Mixture Models (GMMs) and transfer them using a regularization term. We have conducted extensive experiments on the CIFAR10, Places2, and CM-Places datasets to assess generalizability, task transferability, and cross-model transferability of the proposed approach, respectively. The experimental results show that the feature representations have efficiently been learned and transferred through the proposed statistical regularization scheme. Moreover, our method is an architecture independent approach, which is applicable for a variety of CNN architectures.", " Biological datasets, such as our case of study, coral segmentation, often present scarce and sparse annotated image labels. Transfer learning techniques allow us to adapt existing deep learning models to new domains, even with small amounts of training data. Therefore, one of the main challenges to train dense segmentation models is to obtain the required dense labeled training data. This work presents a novel pipeline to address this pitfall and demonstrates the advantages of applying it to coral imagery segmentation. We fine tune state-of-the-art encoder-decoder CNN models for semantic segmentation thanks to a new proposed augmented labeling strategy. Our experiments run on a recent coral dataset [4], proving that this augmented ground truth allows us to effectively learn coral segmentation, as well as provide a relevant score of the segmentation quality based on it. Our approach provides a segmentation of comparable or better quality than the baseline presented with the dataset and a more flexible end-to-end pipeline.", " In spite of the large number of existing methods, pedestrian detection remains an open challenge. In recent years, deep learning classification methods combined with multi-modality images within different fusion schemes have achieved the best performance. It was proven that the late-fusion scheme outperforms both direct and intermediate integration of modalities for pedestrian recognition. Hence, in this paper, we focus on improving the late-fusion scheme for pedestrian classification on the Daimler stereo vision data set. Each image modality, Intensity, Depth and Flow, is classified by an independent Convolutional Neural Network (CNN), the outputs of which are then fused by a Multi-layer Perceptron (MLP) before the recognition decision. We propose different methods based on Cross-Modality deep learning of CNNs: (1) a correlated model where a unique CNN is trained with Intensity, Depth and Flow images for each frame, (2) an incremental model where a CNN is trained with the first modality images frames, then a second CNN, initialized by transfer learning on the first one is trained on the second modality images frames, and finally a third CNN initialized on the second one, is trained on the last modality images frames. The experiments show that the incremental cross-modality deep learning of CNNs improves classification performances not only for each independent modality classifier, but also for the multi-modality classifier based on late-fusion. Different learning algorithms are also investigated.", " Parking is one basic function of autonomous vehicles. However, parking still remains difficult to be implemented, since it requires to generate a relatively long-term series of actions to reach a certain objective under complicated constraints. One recently proposed method used deep neural networks(DNN) to learn the relationship between the actual parking trajectories and the corresponding steering actions, so as to find the best parking trajectory via direct recalling. However, this method can only handle a special vehicle whose dynamic parameters are well known. In this paper, we use transfer learning technique to further extend this direct trajectory planning method and master general parking skills. We aim to mimic how human drivers make parking by using a specially designed deep neural network. The first few layers of this DNN contain the general parking trajectory planning knowledge for all kinds of vehicles  while the last few layers of this DNN can be quickly tuned to adapt various kinds of vehicles. Numerical tests show that, combining transfer learning and direct trajectory planning solution, our new approach enables automated vehicles to convey the knowledge of trajectory planning from one vehicle to another with a few try-and-tests.", " Computer graphics techniques for image generation are living an era where, day after day, the quality of produced content is impressing even the more skeptical viewer. Although it is a great advance for industries like games and movies, it can become a real problem when the application of such techniques is applied for the production of fake images. In this paper we propose a new approach for computer generated images detection using a deep convolutional neural network model based on ResNet-50 and transfer learning concepts. Unlike the state-of-the-art approaches, the proposed method is able to classify images between computer generated or photo generated directly from the raw image data with no need for any pre-processing or hand-crafted feature extraction whatsoever. Experiments on a public dataset comprising 9700 images show an accuracy higher than 94%, which is comparable to the literature reported results, without the drawback of laborious and manual step of specialized features extraction and selection.", " Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a deep transfer learning scheme, called selective joint fine-tuning, for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task. Experiments demonstrate that our deep transfer learning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, and fine-grained classification problems (Oxford Flowers 102 and Stanford Dogs 120). In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model. Codes and models are available at https://github.com/ZYYSzj/Selective-Joint-Fine-tuning.", " We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve crosschannel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.", " In this paper, a self-learning approach is proposed towards solving scene-specific pedestrian detection problem without any human' annotation involved. The self-learning approach is deployed as progressive steps of object discovery, object enforcement, and label propagation. In the learning procedure, object locations in each frame are treated as latent variables that are solved with a progressive latent model (PLM). Compared with conventional latent models, the proposed PLM incorporates a spatial regularization term to reduce ambiguities in object proposals and to enforce object localization, and also a graph-based label propagation to discover harder instances in adjacent frames. With the difference of convex (DC) objective functions, PLM can be efficiently optimized with a concave-convex programming and thus guaranteeing the stability of self-learning. Extensive experiments demonstrate that even without annotation the proposed self-learning approach outperforms weakly supervised learning approaches, while achieving comparable performance with transfer learning and fully supervised approaches.", " We consider the problem of data augmentation, i.e., generating artificial samples to extend a given corpus of training data. Specifically, we propose attributed-guided augmentation (AGA) which learns a mapping that allows synthesis of data such that an attribute of a synthesized sample is at a desired value or strength. This is particularly interesting in situations where little data with no attribute annotation is available for learning, but we have access to an external corpus of heavily annotated samples. While prior works primarily augment in the space of images, we propose to perform augmentation in feature space instead. We implement our approach as a deep encoder-decoder architecture that learns the synthesis function in an end-to-end manner. We demonstrate the utility of our approach on the problems of (1) one-shot object recognition in a transfer-learning setting where we have no prior knowledge of the new classes, as well as (2) object-based one-shot scene recognition. As external data, we leverage 3D depth and pose information from the SUN RGB-D dataset. Our experiments show that attribute-guided augmentation of high-level CNN features considerably improves one-shot recognition performance on both problems.", " We present a novel visual attention tracking technique based on Shared Attention modeling. By considering the viewer as a participant in the activity occurring in the scene, our model learns the loci of attention of the scene actors and use it to augment image salience. We go beyond image salience and instead of only computing the power of image regions to pull attention, we also consider the strength with which the scene actors push attention to the region in question, thus the term Attentional Push. We present a convolutional neural network (CNN) which augments standard saliency models with Attentional Push. Our model contains two pathways: an Attentional Push pathway which learns the gaze location of the scene actors and a saliency pathway. These are followed by a shallow augmented saliency CNN which combines them and generates the augmented saliency. For training, we use transfer learning to initialize and train the Attentional Push CNN by minimizing the classification error of following the actors' gaze location on a 2-D grid using a large-scale gaze-following dataset. The Attentional Push CNN is then fine-tuned along with the augmented saliency CNN to minimize the Euclidean distance between the augmented saliency and ground truth fixations using an eye-tracking dataset, annotated with the head and the gaze location of the scene actors. We evaluate our model on three challenging eye fixation datasets, SALICON, iSUN and CAT2000, and illustrate significant improvements in predicting viewers' fixations in social scenes.", " Intense interest in applying convolutional neural networks (CNNs) in biomedical image analysis is wide spread, but its success is impeded by the lack of large annotated datasets in biomedical imaging. Annotating biomedical images is not only tedious and time consuming, but also demanding of costly, specialty-oriented knowledge and skills, which are not easily accessible. To dramatically reduce annotation cost, this paper presents a novel method called AIFT (active, incremental fine-tuning) to naturally integrate active learning and transfer learning into a single framework. AIFT starts directly with a pre-trained CNN to seek worthy samples from the unannotated for annotation, and the (fine-tuned) CNN is further fine-tuned continuously by incorporating newly annotated samples in each iteration to enhance the CNN's performance incrementally. We have evaluated our method in three different biomedical imaging applications, demonstrating that the cost of annotation can be cut by at least half. This performance is attributed to the several advantages derived from the advanced active and incremental capability of our AIFT method.", " In a transfer-learning scheme, the intermediate layers of a pre-trained CNN are employed as universal image representation to tackle many visual classification problems. The current trend to generate such representation is to learn a CNN on a large set of images labeled among the most specific categories. Such processes ignore potential relations between categories, as well as the categorical-levels used by humans to classify. In this paper, we propose Multi Categorical-Level Networks (MuCaLe-Net) that include human-categorization knowledge into the CNN learning process. A MuCaLe-Net separates generic categories from each other while it independently distinguishes specific ones. It thereby generates different features in the intermediate layers that are complementary when combined together. Advantageously, our method does not require additive data nor annotation to train the network. The extensive experiments over four publicly available benchmarks of image classification exhibit state-of-the-art performances.", " In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.", " This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.", " We introduce a novel technique for knowledge transfer, where knowledge from a pretrained deep neural network (DNN) is distilled and transferred to another DNN. As the DNN maps from the input space to the output space through many layers sequentially, we define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers. When we compare the student DNN and the original network with the same size as the student DNN but trained without a teacher network, the proposed method of transferring the distilled knowledge as the flow between two layers exhibits three important phenomena: (1) the student DNN that learns the distilled knowledge is optimized much faster than the original model  (2) the student DNN outperforms the original DNN  and (3) the student DNN can learn the distilled knowledge from a teacher DNN that is trained at a different task, and the student DNN outperforms the original DNN that is trained from scratch.", " Breast Density Classification is a problem in Medical Imaging domain that aims to assign an American College of Radiology's BIRADS category (I-IV) to a mammogram as an indication of tissue density. This is performed by radiologists in an qualitative way, and thus subject to variations from one physician to the other. In machine learning terms it is a 4-ordered-classes classification task with highly unbalance training data, as classes are not equally distributed among populations, even with variations among ethnicities. Deep Learning techniques in general became the state-of-the-art for many imaging classification tasks, however, dependent on the availability of large datasets. This is not often the case for Medical Imaging, and thus we explore Transfer Learning and Dataset Augmentationn. Results show a very high squared weighted kappa score of 0.81 (0.95 C.I. [0.77, 0.85]) which is high in comparison to the 8 medical doctors that participated in the dataset labeling 0.82 (0.95 CI [0.77, 0.87]).", "deep learning  breast cancer  histopathological image  CNN  classification  massive image data The automatic and precision classification for breast cancer histopathological image has a great significance in clinical application. However, the existing analysis approaches are difficult to addressing the breast cancer classification problem because the feature subtle differences of inter-class histopathological image and the classification accuracy still hard to meet the clinical application. Recent advancements in data-driven sharing processing and multi-level hierarchical feature learning have made available considerable chance to dope out a solution to this problem. To address the challenging problem, we propose a novel breast cancer histopathological image classification method based on deep convolutional neural networks, named as BiCNN model, to address the two-class breast cancer classification on the pathological image. This deep learning model considers class and sub-class labels of breast cancer as prior knowledge, which can restrain the distance of features of different breast cancer pathological images. In addition, an advanced data augmented method is proposed to fit tolerance whole slide image recognition, which can full reserve image edge feature of cancerization region. The transfer learning and fine-tuning method are adopted as an optimal training strategy to improve breast cancer histopathological image classification accuracy. The experiment results show that the proposed method leads to a higher classification accuracy (up to 97%) and displays good robustness and generalization, which provides efficient tools for breast cancer clinical diagnosis.", "Machine Learning  Computer Vision  Convolution Neural Network  Transfer Learning Convolution neural networks (CNNs) eliminate the need for feature extraction which is one of the most important and time-consuming part of traditional machine learning (ML) methods. However, the challenge of training a deep CNN model with a limited amount of training data still remains. Transfer learning and parameter fine-tuning have emerged as solutions to this problem. Following the recent trends, we address the task of visual identification of leaves in images by modifying a trained model on a similar problem. In particular, we show that a pretrained CNN model on a large dataset (ImageNet) can be used to train a model from a small training set (ImageCLEF2013 Plant Identification). The resulting model outperforms the classical machine learning methods using local binary patterns (LBPs), a well utilized feature in the field.", "cross-domain  recommender system  transfer learning  cluster The technique of collaborative filtering in recommender system suffers from data sparsity and cold start. In this paper, a cluster based approach is proposed for alleviating the problem of sparsity by transferring the knowledge from a more densely rated concomitant domain. The paper focuses on providing recommendation in a sparsely rated domain by transferring the knowledge from the highly rated domain with the same users rating the items in both the domains. This cross domain system transfers the affinity among users from the highly rated domain to the sparsely rated domain to make more accurate recommendation at the target domain. An algorithm is proposed to link the users' affinity between the domains. The proposed cross domain algorithm is tested with various clustering methods. The experiments are performed considering restaurant - tourist attraction as cross domains and results show that hierarchical agglomerative clustering performs better when transferring user affinity between associated domains.", " Face recognition performance evaluation has traditionally focused on one-to-one verification, popularized by the Labeled Faces in the Wild dataset [1] for imagery and the YouTubeFaces dataset [2] for videos. In contrast, the newly released IJB-A face recognition dataset [3] unifies evaluation of one-to-many face identification with one-to-one face verification over templates, or sets of imagery and videos for a subject. In this paper, we study the problem of template adaptation, a form of transfer learning to the set of media in a template. Extensive performance evaluations on IJB-A show a surprising result, that perhaps the simplest method of template adaptation, combining deep convolutional network features with template specific linear SVMs, outperforms the state-of-the-art by a wide margin. We study the effects of template size, negative set construction and classifier fusion on performance, then compare template adaptation to convolutional networks with metric learning, 2D and 3D alignment. Our unexpected conclusion is that these other methods, when combined with template adaptation, all achieve nearly the same top performance on IJB-A for templatebased face verification and identification.", " Zero shot learning (ZSL) is about being able to recognize gesture classes that were never seen before. This type of recognition involves the understanding that the presented gesture is a new form of expression from those observed so far, and yet carries embedded information universal to all the other gestures (also referred as context). As part of the same problem, it is required to determine what action/command this new gesture conveys, in order to react to the command autonomously. Research in this area may shed light to areas where ZSL occurs, such as spontaneous gestures. People perform gestures that may be new to the observer. This occurs when the gesturer is learning, solving a problem or acquiring a new language. The ability of having a machine recognizing spontaneous gesturing, in the same manner as humans do, would enable more fluent human-machine interaction. In this paper, we describe a new paradigm for ZSL based on adaptive learning, where it is possible to determine the amount of transfer learning carried out by the algorithm and how much knowledge is acquired from a new gesture observation. Another contribution is a procedure to determine what are the best semantic descriptors for a given command and how to use those as part of the ZSL approach proposed.", " Facial expression recognition plays an increasingly important role in human behavior analysis and human computer interaction. Facial action units (AUs) coded by the Facial Action Coding System (FACS) provide rich cues for the interpretation of facial expressions. Much past work on AU analysis used only frontal view images, but natural images contain a much wider variety of poses. The FG 2017 Facial Expression Recognition and Analysis challenge (FERA 2017) requires participants to estimate the AU occurrence and intensity under nine different pose angles. This paper proposes a multi-task deep network addressing the AU intensity estimation sub-challenge of FERA 2017. The network performs the tasks of pose estimation and pose-dependent AU intensity estimation simultaneously. It merges the pose-dependent AU intensity estimates into a single estimate using the estimated pose. The two tasks share transferred bottom layers of a deep convolutional neural network (CNN) pre-trained on ImageNet. Our model outperforms the baseline results, and achieves a balanced performance among nine pose angles for most AUs.", " In this paper, we propose an Attention-Based Template Adaptation (termed as ABTA) algorithm for face recognition in the unconstrained environment. This ABTA algorithm can be divided into two modules, which consist of an attention-based neural network (feature extractor module) to integrate the template features of various lengths to a single fixed length feature representation according to the attention mechanism, and a template adaptation module (transfer module) which is used to transfer the knowledge of a hold-out dataset to the test templates to improve the performance via transfer learning. The feature extractor module is invariant to the order of the images and videos and can save both memory and computation resources due to its compactness. As for the transfer module, we apply the one-shot similarity to get the scores between the test template pairs, which demonstrates its power in recent research. Our method produces results comparable to the state-of-the-art in the challenging face dataset, IJB-A.", "flower classification  TensorFlow  inception-v3  transfer learning The study of flower classification system is a very important subject in the field of Botany. A classifier of flowers with high accuracy will also bring a lot of fun to people's lives. However, because of the complex background of flowers, the similarity between the different species of flowers, and the differences among the same species of flowers, there are still some challenges in the recognition of flower images. The traditional flower classification is mainly based on the three features: color, shape and texture, this classification requires people to select features for classification, and the accuracy is not very high. In this paper, based on Inception-v3 model of TensorFlow platform, we use the transfer learning technology to retrain the flower category datasets, which can greatly improve the accuracy of flower classification.", "Deep Learning  Temporal ConvNets  Transfer Learning  Text Classification  Sentiment Analysis Temporal (one-dimensional) Convolutional Neural Network ( Temporal CNN, ConvNet) is an emergent technology for text understanding. The input for the ConvNets could be either a sequence of words or a sequence of characters. In the latter case there are no needs for natural language processing that depends on a language such as morphological analysis. Past studies showed that the character-level ConvNets worked well for news category classification and sentiment analysis / classification tasks in English and romanized Chinese text corpus. In this article we apply the character-level ConvNets to Japanese text understanding. We also attempt to reuse meaningful representations that are learned in the ConvNets from a large-scale dataset in the form of transfer learning, inspired by its success in the field of image recognition. As for the application to the news category classification and the sentiment analysis and classification tasks in Japanese text corpus, the ConvNets outperformed N-gram-based classifiers. In addition, our ConvNets transfer learning frameworks worked well for a task which is similar to one used for pre-training.", "Transfer Learning  Kernel  SVM  Maximum Mean Discrepancy This paper is a contribution to solving the domain adaptation problem where no labeled target data is available. A new SVM approach is proposed by imposing a zero-valued Maximum Mean Discrepancy-like constraint. This heuristic allows us to expect a good similarity between source and target data, after projection onto an efficient subspace of a Reproducing Kernel Hilbert Space. Accordingly, the classifier will perform well on source and target data. We show that this constraint does not modify the quadratic nature of the optimization problem encountered in classic SVM, so standard quadratic optimization tools can be used. Experimental results demonstrate the competitiveness and efficiency of our method.", "Biometrics  User Identification  Electrocardiogram (ECG)  Deep Learning  Feature Learning  Transfer Learning  Deep Autoencoder Biometric identification is the task of recognizing an individual using biological or behavioral traits and, recently, electrocardiogram has emerged as a prominent trait. In addition, deep learning is a fast-paced research field where several models, training schemes and applications are being actively investigated. In this paper, an ECG-based biometric system using a deep autoencoder to learn a lower dimensional representation of heartbeat templates is proposed. A superior identification performance is achieved, validating the expressiveness of such representation. A transfer learning setting is also explored and results show practically no loss of performance, suggesting that these deep learning methods can be deployed in systems with offline training.", " In this work, we design and evaluate a computational learning model that enables a human-robot team to co-develop joint strategies for performing novel tasks that require coordination. The joint strategies are learned through perturbation training, a human team-training strategy that requires team members to practice variations of a given task to help their team generalize to new variants of that task. We formally define the problem of human-robot perturbation training and develop and evaluate the first end-to-end framework for such training, which incorporates a multi-agent transfer learning algorithm, human robot co-learning framework and communication protocol. Our transfer learning algorithm, Adaptive Perturbation Training (AdaPT), is a hybrid of transfer and reinforcement learning techniques that learns quickly and robustly for new task variants. We empirically validate the benefits of AdaPT through comparison to other hybrid reinforcement and transfer learning techniques aimed at transferring knowledge from multiple source tasks to a single target task. We also demonstrate that AdaPT's rapid learning supports live interaction between a person and a robot, during which the human-robot team trains to achieve a high level of performance for new task variants. We augment AdaPT with a co-learning framework and a computational bi-directional communication protocol so that the robot can co-train with a person during live interaction. Results from large-scale human subject experiments (n=48) indicate that AdaPT enables an agent to learn in a manner compatible with a human's own learning process, and that a robot undergoing perturbation training with a human results in a high level of team performance. Finally, we demonstrate that human robot training using AdaPT in a simulation environment produces effective performance for a team incorporating an embodied robot partner.", "Food classification  Convolutional neural network  Deep learning  Transfer learning Automatic classification of foods is a way to control food intake and tackle with obesity. However, it is a challenging problem since foods are highly deformable and complex objects. Results on ImageNet dataset have revealed that Convolutional Neural Network has a great expressive power to model natural objects. Nonetheless, it is not trivial to train a ConyNet from scratch for classification of foods. This is due to the fact that ConyNets require large datasets and to our knowledge there is not a large public dataset of food for this purpose. Alternative solution is to transfer knowledge from trained ConyNets to the domain of foods. In this work, we study how transferable are state-of-art ConyNets to the task of food classification. We also propose a method for transferring knowledge from a bigger ConyNet to a smaller ConyNet by keeping its accuracy similar to the bigger ConyNet. Our experiments on UECFood256 datasets show that Googlenet, VGG and residual networks produce comparable results if we start transferring knowledge from appropriate layer. In addition, we show that our method is able to effectively transfer knowledge to the smaller ConyNet using unlabelled samples.", "convolutional neural network  deep learning  pedestrian  gender classification  k-means  transfer learning In this work, we studied a strategy for training a convolutional neural network in pedestrian gender classification with limited amount of labeled training data. Unsupervised learning by k-means clustering on pedestrian images was used to learn the filters to initialize the first layer of the network. As a form of pre-training, supervised learning for the related task of pedestrian classification was performed. Finally, the network was fine-tuned for gender classification. We found that this strategy improved the network's generalization ability in gender classification, achieving better test results when compared to random weights initialization and slightly more beneficial than merely initializing the first layer filters by unsupervised learning. This shows that unsupervised learning followed by pre-training with pedestrian images is an effective strategy to learn useful features for pedestrian gender classification.", " Recognising detailed clothing characteristics (fine-grained attributes) in unconstrained images of people in-the-wild is a challenging task for computer vision, especially when there is only limited training data from the wild whilst most data available for model learning are captured in well-controlled environments using fashion models (well lit, no background clutter, frontal view, high-resolution). In this work, we develop a deep learning framework capable of model transfer learning from well-controlled shop clothing images collected from web retailers to in-the-wild images from the street. Specifically, we formulate a novel MultiTask Curriculum Transfer (MTCT) deep learning method to explore multiple sources of different types of web annotations with multi-labelled fine-grained attributes. Our multi-task loss function is designed to extract more discriminative representations in training by jointly learning all attributes, and our curriculum strategy exploits the staged easy-to-hard transfer learning motivated by cognitive studies. We demonstrate the advantages of the MTCT model over the state-of-the-art methods on the X-Domain benchmark, a large scale clothing attribute dataset. Moreover, we show that the MTCT model has a notable advantage over contemporary models when the training data size is small.", " Since the introduction of deep convolutional neural networks (CNNs), object detection in imagery has witnessed substantial breakthroughs in state-of-the-art performance. The defense community utilizes overhead image sensors that acquire large field-of-view aerial imagery in various bands of the electromagnetic spectrum, which is then exploited for various applications, including the detection and localization of man-made objects. In this work, we utilize a recent state-of-the art object detection algorithm, faster R-CNN, to train a deep CNN for vehicle detection in multimodal imagery. We utilize the vehicle detection in aerial imagery (VEDAI) dataset, which contains overhead imagery that is representative of an ISR setting. Our contribution includes modification of key parameters in the faster R-CNN algorithm for this setting where the objects of interest are spatially small, occupying less than 1.5 x 10(-3) of the total image pixels. Our experiments show that (1) an appropriately trained deep CNN leads to average precision rates above 93% on vehicle detection, and (2) transfer learning between imagery modalities is possible, yielding average precision rates above 90% in the absence of fine-tuning.", " Studying marine plankton is critical to assessing the health of the world's oceans. To sample these important populations, oceanographers are increasingly using specially engineered in situ digital imaging systems that produce very large data sets. Most automated annotation efforts have considered data from individual systems in isolation. This is predicated on the assumption that the images from each system are so different that there would be little benefit to considering out-of-domain data. Meanwhile, in the computer vision community, much effort has been dedicated to understanding how using out-of-domain images can improve the performance of machine classifiers. In this paper, we leverage these advances to evaluate how well weights transfer between Convolutional Neural Networks (CNNs) trained on data from two radically different plankton imaging systems. We also examine the utility of CNNs as feature extractors on a third unique plankton data set. Our results indicate that these data sets are perhaps more similar in the eyes of a machine classifier than previously assumed. Further, these tests underscore the value of using the rich feature representations learned by CNNs to classify data in vastly different domains.", "Convolutional networks  deep learning  greedy layer-wise training This work proposes a supervised layer-wise strategy to train deep convolutional neural networks (DCNs) particularly suited for small, specialized image datasets. DCNs are increasingly being used with considerable success in image classification tasks and trained over large datasets (with more than 1M images and 10K classes). Pre-trained successful DCNs can then be used for new smaller datasets (10K to 100K images) through a transfer learning process which cannot guarantee competitive a-priori performance if the new data is of different or specialized nature (medical imaging, plant recognition, etc.). We therefore seek out to find competitive techniques to train DCNs for such small datasets, and hereby describe a supervised greedy layer-wise method analogous to that used in unsupervised deep networks. Our method consistently outperforms the traditional methods that train a full DCN architecture in a single stage, yielding an average of over 20% increase in classification performance across all DCN architectures and datasets used in this work. Furthermore, we obtain more interpretable and cleaner visual features. Our method is better suited for small, specialized datasets since we require a training cycle for each DCN layer and this increases its computing time almost linearly with the number of layers. Nevertheless, it still remains as a fraction of the computing time required to generate pre-trained models with large generic datasets, and poses no additional requirements on hardware. This constitutes a solid alternative for training DCNs when transfer learning is not possible and, furthermore, suggests that state of the art DCN performance with large datasets might yet be improved at the expense of a higher computing time.", "Transfer learning  unsupervised domain adaptation  feature weighting  instance clustering  maximum mean discrepancy Learning invariant features across domains is of vital importance to unsupervised domain adaptation, where classifiers trained on the training examples (source domain) need to adapt to a different set of test examples (target domain) in which no labeled examples are available. In this paper, we propose a novel approach to find the invariant features in the original space and transfer the knowledge across domains. We extract invariant features of input data by a kernel-based feature weighting approach, which exploits distribution difference and instance clustering to find desired features. The proposed method is called the kernel-based feature weighting (KFW) approach and benefits from the maximum mean discrepancy to measure the difference between domains. KFW uses condensed clusters in the reduced domains, the domains that do not contain variant features, to enhance the classification performance. Simultaneous use of feature weighting and instance clustering increases the adaptation and classification performance. Our approach automatically discovers the invariant features across domains and employs them to bridge between source and target domains. We demonstrate the effectiveness of our approach in the task of artificial and real world dataset examinations. Empirical results show that the proposed method outperforms other state-of-the-art methods on the standard transfer learning benchmark datasets.", "Dimensionality reduction  domain adaptation  kernel method  multiobjective optimization  transfer learning Domain adaptation learning (DAL) investigates how to perform a task across different domains. In this paper, we present a kernelized local-global approach to solve domain adaptation problems. The basic idea of the proposed method is to consider the global and local information regarding the domains (e.g., maximum mean discrepancy and intraclass distance) and to convert the domain adaptation problem into a bi-object optimization problem via the kernel method. A solution for the optimization problem will help us identify a latent space in which the distributions of the different domains will be close to each other in the global sense, and the local properties of the labeled source samples will be preserved. Therefore, classic classification algorithms can be used to recognize unlabeled target domain data, which has a significant difference on the source samples. Based on the analysis, we validate the proposed algorithm using four different sources of data: synthetic, textual, object, and facial image. The experimental results indicate that the proposed method provides a reasonable means to improve DAL algorithms.", "Discriminant tracking  tensor samples  semi-supervised learning  graph embedding space An appearance model adaptable to changes in object appearance is critical in visual object tracking. In this paper, we treat an image patch as a two-order tensor which preserves the original image structure. We design two graphs for characterizing the intrinsic local geometrical structure of the tensor samples of the object and the background. Graph embedding is used to reduce the dimensions of the tensors while preserving the structure of the graphs. Then, a discriminant embedding space is constructed. We prove two propositions for finding the transformation matrices which are used to map the original tensor samples to the tensor-based graph embedding space. In order to encode more discriminant information in the embedding space, we propose a transfer-learning-based semi-supervised strategy to iteratively adjust the embedding space into which discriminative information obtained from earlier times is transferred. We apply the proposed semi-supervised tensor-based graph embedding learning algorithm to visual tracking. The new tracking algorithm captures an object's appearance characteristics during tracking and uses a particle filter to estimate the optimal object state. Experimental results on the CVPR 2013 benchmark dataset demonstrate the effectiveness of the proposed tracking algorithm.", "Deep convolutional neural networks  Transfer learning  Texture descriptors  Texture classification  Ensemble of descriptors In this work, we propose an unorthodox approach for describing a given image. Each image is represented by a feature vector whose elements are the scores assigned to object classes by deep convolutional neural networks that were not related to those that built the given image classification problem. The deep neural networks are trained using 1000 classes  therefore, each image is described by 1000 scores, which are fed to a support vector machine. The proposed approach could be considered a transfer learning method, where, instead of repurposing the learned features to a second classification problem, we use the scores obtained by trained convolutional neural networks. Methods based on state of the art handcrafted descriptors, and the novel approach presented here are compared, together with selected ensembles of such methods. The fusion between a standard approach and the new unorthodox method boosts the performance of the standard approach. The Wilcoxon signed rank test is used to compare the different methods. The novel method is applied to 21 different datasets to demonstrate its generality. The MATLAB source code to replicate our experiments will be available at(https://www.dei.unipd.it/node/2357 +Pattern Recognition and Ensemble Classifiers). ", "neural networks  transfer learning  progressive neural networks  computational paralinguistics  emotion recognition Many paralinguistic tasks are closely related and thus representations learned in one domain can be leveraged for another. In this paper, we investigate how knowledge can be transferred between three paralinguistic tasks: speaker, emotion, and gender recognition. Further, we extend this problem to cross-dataset tasks. asking how knowledge captured in one emotion dataset can be transferred to another. We focus on progressive neural networks and compare these networks to the conventional deep learning method of pre-training and fine-tuning. Progressive neural networks provide a way to transfer knowledge and avoid the forgetting effect present when pre-training neural networks on different tasks. Our experiments demonstrate that: (I) emotion recognition can benefit from using representations originally learned for different paralinguistic tasks and (2) transfer learning can effectively leverage additional datasets to improve the performance of emotion recognition systems.", "Transfer Learning (TL)  sincerity prediction  deception prediction Transfer learning (TL) involves leveraging information from sources outside the domain at hand for enhancing model performances. Popular TL methods either directly use the data or adapt the models learned on out-of-domain resources and incorporate them within in-domain models. TL methods have shown promise in several applications such as text classification, cross domain language classification and emotion recognition. In this paper, we propose TL methods to computational human behavioral trait modeling. Many behavioral traits are abstract constructs (e.g., sincerity of an individual), and are often conceptually related to other constructs (e.g., level of deception) making TL methods an attractive option for their modeling. We consider the problem of automatically predicting human sincerity and deception from behavioral data while leveraging transfer of knowledge from each other. We compare our methods against baseline models trained only on in-domain data. Our best models achieve an Unweighted Average Recall (UAR) of 72.02% in classifying deception (baseline: 69.64%). Similarly, applied methods achieve Spearman's/Pearson's correlation values of 49.37%/48.52% between true and predicted sincerity scores (baseline: 46.51%/41.58%), indicating the success and the potential of TL for such human behavior tasks.", "cross-lingual speech recognition  probabilistic transcription  deep neural networks  multi-task learning We examine a scenario where we have no access to native transcribers in the target language. This is typical of language communities that are under-resourced. However, turkers (online crowd workers) available in online marketplaces can serve as valuable alternative resources for providing transcripts in the target language. We assume that the turkers neither speak nor have any familiarity with the target language. Thus, they are unable to distinguish all phone pairs in the target language  their transcripts therefore specify, at best, a probability distribution called a probabilistic transcript (PT). Standard deep neural network (DNN) training using PTs do not necessarily improve error rates. Previously reported results have demonstrated some success by adopting the multi-task learning (MTL) approach. In this study, we report further improvements by introducing a deep auto-encoder based MTL. This method leverages large amounts of untranscribed data in the target language in addition to the PTs obtained from turkers. Furthermore, to encourage transfer learning in the feature space, we also examine the effect of using monophones from transcripts in well-resourced languages. We report consistent improvement in phone error rates (PER) for Swahili, Amharic, Dinka, and Mandarin.", "speech recognition  deep neural network  cross lingual  cross-bandwidth  transfer learning The success of deep neural network (DNN) acoustic models is partly owed to large amounts of training data available for different applications. This work investigates ways to improve DNN acoustic models for Bluetooth narrowband mobile applications when relatively small amounts of in-domain training data are available. To address the challenge of limited in domain data, we use cross-bandwidth and cross-lingual transfer learning methods to leverage knowledge from other domains with more training data (different bandwidth and/or languages). Specifically, narrowband DNNs in a target language are initialized using the weights of DNNs trained on bandlimited wide band data in the same language or those trained on a different (resource-rich) language. We investigate multiple recipes involving such methods with different data resources. For all languages in our experiments, these recipes achieve up to 45% relative WER reduction, compared to training solely on the Blue tooth narrowband data in the target language. Furthermore, these recipes are very beneficial even when over two hundred hours of manually transcribed in-domain data is available, and we can achieve better accuracy than the baselines with as little as 20 hours of in-domain data.", "speech recognition  low-resource languages  transfer learning  distillation Deep neural networks (DNN) require large amount of training data to build robust acoustic models for speech recognition tasks. Our work is intended in improving the low-resource language acoustic model to reach a performance comparable to that of a high-resource scenario with the help of data/model parameters from other high-resource languages. we explore transfer learning and distillation methods, where a complex high resource model guides or supervises the training of low resource model. The techniques include (i) multi-lingual framework of borrowing data from high-resource language while training the low-resource acoustic model. The KL divergence based constraints are added to make the model biased towards low-resource language. (ii) distilling knowledge from the complex high-resource model to improve the low-resource acoustic model, The experiments were performed on three Indian languages namely Hindi, Tamil and Kannada. All the techniques gave improved performance and the multi-lingual framework with KL divergence regularization giving the best results. In all the three languages a performance close to or better than high-resource scenario was obtained.", "Automated Speech Recognition  Speech Therapy  Speech Assessment Tools This paper describes the continued development of a system to provide early assessment of speech development issues in children and better triaging to professional services. Whilst corpora of children's speech are increasingly available, recognition of disordered children's speech is still a data-scarce task. Transfer learning methods have been shown to be effective at leveraging out-of-domain data to improve ASR performance in similar data-scarce applications. This paper combines transfer learning, with previously developed methods for constrained decoding based on expert speech pathology knowledge and knowledge of the target text. Results of this study show that transfer learning with out-of-domain adult speech can improve phoneme recognition for disordered children's speech. Specifically. a Deep Neural Network (DNN) trained on adult speech and finetuned on a corpus of disordered children's speech reduced the phoneme error rate (PER) of a DNN trained on a children's corpus from 16.3% to 14.2%. Furthermore, this fine-tuned DNN also improved the performance of a Hierarchal Neural Network based acoustic model previously used by the system with a PER of 19.3%. We close with a discussion of our planned future developments of the system.", "transfer learning  knowledge distillation  ensemble  neural network  punctuation prediction This paper proposes an approach to distill knowledge from an ensemble of models to a single deep neural network (DNN) student model for punctuation prediction. This approach makes the DNN student model mimic the behavior of the ensemble. The ensemble consists of three single models. Kullback-Leibler (KL) divergence is used to minimize the difference between the output distribution of the DNN student model and the behavior of the ensemble. Experimental results on English IWSLT2011 dataset show that the ensemble outperforms the previous state-of-the-art model by up to 4.0% absolute in overall F-I-score. The DNN student model also achieves up to 13.4% absolute overall F-I-score improvement over the conventionally-trained baseline models.", "sound event detection (SED)  connectionist temporal classification (CTC)  transfer learning  convolutional neural networks (CNN) Sound event detection is the task of detecting the type, onset time, and offset time of sound events in audio streams. The mainstream solution is recurrent neural networks (RNNs). which usually predict the probability of each sound event at every time step. Connectionist temporal classification (CTC) has been applied in order to relax the need for exact annotations of onset and offset times: the CTC output layer is expected to generate a peak for each event boundary where the acoustic signal is most salient. However, with limited training data, the CTC network has been found to train slowly, and generalize poorly to new data. In this paper, we try to introduce knowledge learned from a much larger corpus into the CTC network. We train two variants of SoundNet, a deep convolutional network that takes the audio tracks of videos as input, and tries to approximate the visual information extracted by an image recognition network. A lower part of SoundNet or its variants is then used as a feature extractor for the CTC network to perform sound event detection. We show that the new feature extractor greatly accelerates the convergence of the CTC network, and slightly improves the generalization.", "Computational Paralinguistics  speaker states  drowsiness detection  transfer learning  feature analysis In this work, we study the drowsy state of a speaker. induced by alcohol intoxication or sleep deprivation. In particular, we investigate the coherence between the two pivotal causes of drowsiness. as featured in the Intoxication and Sleepiness tasks of the INTERSPEECH Speaker State Challenge. In this way, we aim to exploit the interrelations between these different, yet highly correlated speaker states, which need to be reliably recognised in safety and security critical environments. To this end, we perform cross-domain classification of alcohol intoxication and sleepiness, thus leveraging the acoustic similarities of these speech phenomena for transfer learning. Further, we conducted in-depth feature analysis to quantitatively assess the task relatedness and to determine the most relevant features for both tasks. To test our methods in realistic contexts, we use the Alcohol Language Corpus and the Sleepy Language Corpus containing in total 60 hours of genuine intoxicated and sleepy speech. In the result, cross-domain classification combined with feature selection yields up to 60.3 % unweighted average recall, which is significantly above-chance (50 %) and highly notable given the mismatch in the training and validation data. Finally, we show that an effective, general drowsiness classifier can be obtained by aggregating the training data from both domains.", "keyword spotting  time delay neural network  singular value decomposition  small-footprint In this paper we investigate a time delay neural network (TDNN) for a keyword spotting task that requires low CPU, memory and latency. The TDNN is trained with transfer learning and multi-task learning. Temporal subsampling enabled by the time delay architecture reduces computational complexity. We propose to apply singular value decomposition (SVD) to further reduce TDNN complexity. This allows us to first train a larger full-rank TDNN model which is not limited byCPU/memory constraints. The larger TDNN usually achieves better performance. Afterwards, its size can be compressed by SVD to meet the budget requirements. Hidden Markov models (HMM) are used in conjunction with the networks to perform keyword detection and performance is measured in terms of area under the curve (AUC) for detection error tradeoff (DET) curves. Our experimental results on a large in-house far-field corpus show that the full-rank TDNN achieves a 19.7% DET AUC reduction compared to a similar-size deep neural network (DNN) baseline. If we train a larger size full-rank TDNN first and then reduce it via SVD to the comparable size of the DNN, we obtain a 37.6% reduction in DET AUC compared to the DNN baseline.", "breast mass classification  transfer learning  deep learning  convolutional neural networks  mammogram  computer aided diagnosis  fine-tuning In the U.S., breast cancer is diagnosed in about 12% of women during their lifetime and it is the second leading reason for women's death. Since early diagnosis could improve treatment outcomes and longer survival times for breast cancer patients, it is significant to develop breast cancer detection techniques. The Convolutional Neural Network (CNN) can extract features from images automatically and then perform classification. To train the CNN from scratch, however, requires a large number of labeled images, which is infeasible for some kinds of medical image data such as mammographic tumor images. A promising solution is to apply transfer learning in CNN. In this paper, we firstly tested three training methods on the MIAS database: 1) trained a CNN from scratch, 2) applied the pre-trained VGG-16 model to extract features from input mammograms and used these features to train a Neural Network (NN)-classifier, 3) updated the weights in several final layers of the pre-trained VGG-16 model by back-propagation (fine-tuning) to detect abnormal regions. We found that method 2) is ideal for study because the classification accuracy of fine-tuning model was just 0.008 higher than that of feature extraction model but time cost of feature extraction model was only about 5% of that of the fine-tuning model. Then, we used method 2) to classify regions: benign vs. normal, malignant vs. normal and abnormal vs. normal from the DDSM database with 10-fold cross validation. The average validation accuracy converged at about 0.905 for abnormal vs. normal cases, and there was no obvious overfitting. This study shows that applying transfer learning in CNN can detect breast cancer from mammograms, and training a NN-classifier by feature extraction is a faster method in transfer learning.", "Deep Learning  Inception  Landmark recognition  neural networks In this paper, we have classified the famous Indian Monuments across the Golden Quadrilateral. A well-known Deep Learning architectural model has been adopted to provide on-time and striking accuracies for classifying the images. A deep learning library has been used for all the training computations on the classifier for the dataset generated using a web crawler. The concept of Transfer learning has been used to prune the computational load. The last layer of the architecture is retrained as per the training set. Once the model is fully trained, the model is tested on a few arbitrary images to determine the test accuracy of the model.", " One of the most crucial parts in the diagnosis of a wide variety of ailments is cytopathological testing. This process is often laborious, time consuming and requires skill. These constraints have led to interests in automating the process. Several deep learning based methods have been proposed in this domain to enable machines to gain human expertise. In this paper, we investigate the effectiveness of transfer learning using fine-tuned features from modified deep neural architectures and certain ensemble learning methods for classifying the leukemia cell lines HL60, MOLT, and K562. Microfluidics-based imaging flow cytometry (mIFC) is used for obtaining the images instead of image cytometry. This is because mIFC guarantees significantly higher throughput and is easy to set up with minimal expenses. We find that the use of fine-tuned features from a modified deep neural network for transfer learning provides a substantial improvement in performance compared to earlier works. We also identify that without any fine tuning, feature selection using ensemble methods on the deep features also provide comparable performance on the considered Leukemia cell classification problem. These results show that automated methods can in fact be a valuable guide in cytopathological testing especially in resource limited settings.", "Audio emotion prediction  Transfer learning  Echo State Network  Regression Processing generalized sound events with the purpose of predicting the emotion they might evoke is a relatively young research field. Tools, datasets, and methodologies to address such a challenging task are still under development, far from any standardized format. This work aims to cover this gap by revealing and exploiting potential similarities existing during the perception of emotions evoked by sound events and music. o this end we propose (a) the usage of temporal modulation features and (b) a transfer learning module based on an Echo State Network assisting the prediction of valence and arousal measurements associated with generalized sound events. The effectiveness of the proposed transfer learning solution is demonstrated after a thoroughly designed experimental phase employing both sound and music data. The results demonstrate the importance of transfer learning in the specific field and encourage further research on approaches which manage the problem in a cooperative way.", "Skin lesion  Melanoma detection  Computer Vision-based diagnostic systems  Convolutional neural networks - CNN  CNN architectures  Transfer learning In this work, we report the use of convolutional neural networks for the detection of malignant melanomas against nevus skin lesions in a dataset of dermoscopic images of the same magnification. The technique of transfer learning is utilized to compensate for the limited size of the available image dataset. Results show that including transfer learning in training CNN architectures improves significantly the achieved classification results.", "Deep learning  Thermal imaging  Marine mammals Marine mammals in the Arctic are threatened by a changing climate and increasing human activity in the region. International laws protect these animals, however detecting and identifying them is not always easy. We have developed a multimodal approach using an omnidirectional thermal camera system, and an optical band stereo system operating in parallel. Using a unified framework for transfer learning with convolutional neural networks in both modalities we have trained a system to detect and classify mammals as well as habitat indicators in the images from both camera systems. Our experiments show that mammal habitat can be identified reliably using these techniques, and our analysis provides a framework for real world use cases.", "Distributed Multi-Task Learning  Transfer Learning Multi-task learning aims to learn multiple tasks jointly by exploiting their relatedness to improve the generalization performance for each task. Traditionally, to perform multi-task learning, one needs to centralize data from all the tasks to a single machine. However, in many real-world applications, data of different tasks may be geo-distributed over different local machines. Due to heavy communication caused by transmitting the data and the issue of data privacy and security, it is impossible to send data of different task to a master machine to perform multi-task learning. Therefore, in this paper, we propose a distributed multi-task learning framework that simultaneously learns predictive models for each task as well as task relationships between tasks alternatingly in the parameter server paradigm. In our framework, we first offer a general dual form for a family of regularized multi-task relationship learning methods. Subsequently, we propose a communication-efficient primal-dual distributed optimization algorithm to solve the dual problem by carefully designing local subproblems to make the dual problem decomposable. Moreover, we provide a theoretical convergence analysis for the proposed algorithm, which is specific for distributed multi-task relationship learning. We conduct extensive experiments on both synthetic and real-world datasets to evaluate our proposed framework in terms of effectiveness and convergence.", "Black-Box Optimization  Bayesian Optimization  Gaussian Processes  Hyperparameters  Transfer Learning  Automated Stopping Any sufficiently complex system acts as a black box when it becomes easier to experiment with than to understand. Hence, black-box optimization has become increasingly important as systems have become more complex. In this paper we describe Google Vizier, a Google-internal service for performing black-box optimization that has become the de facto parameter tuning engine at Google. Google Vizier is used to optimize many of our machine learning models and other systems, and also provides core capabilities to Google's Cloud Machine Learning HyperTune subsystem. We discuss our requirements, infrastructure design, underlying algorithms, and advanced features such as transfer learning and automated early stopping that the service provides.", "Transfer Learning  Multi-task Learning  Visual Aesthetic Quality Assessment Visual aesthetic quality assessment has played an important role in increasing number of computer vision applications. Particularly, estimating the quality score precisely is a main task of aesthetic quality assessment, but the training samples labeled with score are usually expensive to obtain. In this paper, we propose a transfer learning method which can improve the performance of aesthetic score prediction by using the coarse labeled samples, which are much easier to obtain. The proposed method incorporates the coarse information from source domain into the target domain by a novel multi-task framework, which can revise the model in target task. The effectiveness of our method is proven by experimental results that the error is reduced obviously with the help of source domain.", "person re-identification  convolutional neural network  knowledge transfer Person re-identification (Re-ID) has become an increasingly popular computer vision problem. It remains challenging, especially when there are non-overlapping cameras. In this paper, we review the two representative architecture, i.e., identification and verification models. They both have their advancements and limitations. We present a novel method to address the Re-ID problem. First, combine the two models to consist a more effective fusion loss function. Second, we find that CNNs which are pre-trained on large image datasets learn more discriminative knowledge with objective semantic, which can be transferred to subsequent layers to promote accuracy significantly. Experiments on four benchmark datasets show the superiority of our method over the state-of-the-art alternatives.", "Protein-ATP binding site prediction  Protein-ADP binding site prediction  Instance transfer learning  Random under sampling Protein-ATP and protein-ADP interactions are ubiquitous in a wide variety of biological processes. Accurately identifying ATP-binding and ADP-binding sites or pockets is of significant importance for both protein function analysis and drug design. Although much progress has been made, challenges remain, especially in the post-genome era where large volume of proteins without being functional annotated are quickly accumulated. In this study, we report an instance-transfer-learning-based predictor, ATP&ADPsite, to target both ATP-binding and ADP-binding residues from protein sequence and structural information. ATP&ADPsite first employs evolutionary information, predicted secondary structure, and predicted solvent accessibility to represent each residue sample. In the above feature space, a supervised instance-transfer-learning method is proposed to improve the ATP-binding/ADP-binding residues prediction by combining ATP-binding and ADP-binding proteins. Random under-sampling is lastly employed to solve the imbalanced data learning problem. Experimental results demonstrate that the proposed ATP&ADPsite achieves a better prediction performance and outperforms many existing sequence-based predictors. The ATP&ADPsite web-server is available at http ://csbio.njust. edu.cn/bioinf/ATP&ADPsite.", "common representation learning  multi-view data  transfer learning  deep learning Deep learning techniques have been successfully used in learning a common representation for multi-view data, wherein the different modalities are projected onto a common subspace. In a broader perspective, the techniques used to investigate common representation learning falls under the categories of canonical correlation-based approaches and autoencoder based approaches. In this paper, we investigate the performance of deep autoencoder based methods on multi-view data. We propose a novel step-based correlation multi-modal CNN (CorrMCNN) which reconstructs one view of the data given the other while increasing the interaction between the representations at each hidden layer or every intermediate step. Finally, we evaluate the performance of the proposed model on two benchmark datasets - MNIST and XRMB. Through extensive experiments, we find that the proposed model achieves better performance than the current state-of-the-art techniques on joint common representation learning and transfer learning tasks.", " We propose a CNN-based approach for 3D human body pose estimation from single RGB images that addresses the issue of limited generalizability of models trained solely on the starkly limited publicly available 3D pose data. Using only the existing 3D pose data and 2D pose data, we show state-of-the-art performance on established benchmarks through transfer of learned features, while also generalizing to in-the-wild scenes. We further introduce a new training set for human body pose estimation from monocular images of real humans that has the ground truth captured with a multi-camera marker-less motion capture system. It complements existing corpora with greater diversity in pose, human appearance, clothing, occlusion, and viewpoints, and enables an increased scope of augmentation. We also contribute a new benchmark that covers outdoor and indoor scenes, and demonstrate that our 3D pose dataset shows better in-the-wild performance than existing annotated data, which is further improved in conjunction with transfer learning from 2D pose data. All in all, we argue that the use of transfer learning of representations in tandem with algorithmic and data contributions is crucial for general 3D body pose estimation.", "Deep Learning  Convolutional Neural Networks  Transfer Learning  Malware Classification Malicious software has been growing exponentially during the past years. One of the major challenges for antimalware industry is the vast amounts of data and files which need to be evaluated for potential malicious content. To effectively analyze such large amounts of files, machine learning based malware classification approaches have been developed to classify malware into families based on the same forms of malicious behaviors. This paper presents our design and implementation of a malware classification approach using the Convolutional Neural Networks (CNNs), a prime example of deep learning algorithms. It makes use of CNNs to learn a feature hierarchy for classifying samples of malware binary files, represented as gray-scale images, to their corresponding families. It also uses transfer learning techniques to facilitate model building. Three different models of CNNs were developed and these implemented methods achieved validation accuracy around 97% using the large malware dataset provided for the Microsoft Malware Classification Challenge (BIG 2015).", "Object recognition  Zero-shot learning  Transfer learning  Visual-Semantic Embedding  Entity embedding Zero-shot object recognition is aiming to attach unseen category labels to images which are out of the training set. The key challenge in Zero-shot learning is building the map between visual domain and semantic domain. However, previous Visual-Semantic Embedding ignores the essential difference between the vectors of category names and the vectors of the entities. Hybrid model, moreover, computes the middle vector with a fixed size candidate set which limits the generalization on different images. So we propose a novel framework named Hierarchical-Dynamic Embedding. First, Hierarchical Network Embedding (HNE) takes advantage of the internal hierarchical taxonomy of the category names. We then provide Dynamic Hybrid Model (DHM) to map unseen images from visual vectors to entity vectors. Furthermore, we conduct the experiments on 1,000 seen categories and 1,548 unseen categories to show the state-of-the-art performance of our proposed framework.", "convolutional neural networks  facial recognition  transfer learning  FERET image dataset The challenge of developing facial recognition systems has been the focus of many research efforts in recent years and has numerous applications in areas such as security, entertainment, and biometrics. Recently, most progress in this field has come from training very deep neural networks on massive datasets. Here, we use a pre-trained face recognition model and perform transfer learning to produce a network that is capable of making accurate predictions on a much smaller dataset. We also compare our results with results produced by a selection of classical algorithms on the same dataset.", "bilingualism  handwritten character recognition  deep learning  hierarchical generative models  restricted Boltzmann machines  transfer learning In this study, we investigated the effects of mastering multiple scripts in handwritten character recognition by means of computational simulations. In particular, we trained a set of deep neural networks on two different datasets of handwritten characters: the HODA dataset, which is a collection of images of handwritten Persian digits, and the MNIST dataset, which contains Latin handwritten digits. We simulated native language individuals (trained on a single dataset) as well as bilingual individuals (trained on both datasets), and compared their performance in a recognition task performed under different noisy conditions. Our results show the superior performance of bilingual networks in handwritten digit recognition in comparison to the monolingual networks, thereby suggesting that mastering multiple languages might facilitate knowledge transfer across similar domains.", "Image classification  CNN  deep learning  transfer learning  remote sensing In this paper we propose a stacking approach for Convolutional Neural Network (CNN) transfer learning ensemble for remote sensing imagery, in particular for the task of scene classification. We propose to use a combination of features produced by an ensemble of CNNs as one feature vector for classification. At the same time the original data set can be processed with different up-sampling and image enhancement methods and then used to obtain more features from pretrained networks. We investigate both fine-tuning and non fine-tuning approaches for transfer learning. We have selected Brazilian Coffee Scenes data set as a benchmark to measure the classification accuracy. Proposed method in case of a non fine-tuned model shows 89.18% classification accuracy. For a fine-tuned model the best classification rate is 96.11%. We analyzed how networks that have appeared recently (VGG-19 and SqueezeNet), can be applied to the task of transfer learning for remote sensing. Also we describe a method of decreasing processing time and memory consumption while preserving classification accuracy by using feature selection based on feature importance.", "Image retrieval  medical imaging  deep learning  CNNs  digital pathology  image classification  deep features  VGG  Inception We explore the problem of classification within a medical image data-set based on a feature vector extracted from the deepest layer of pre-trained Convolution Neural Networks. We have used feature vectors from several pre-trained structures, including networks with/without transfer learning to evaluate the performance of pre-trained deep features versus CNNs which have been trained by that specific dataset as well as the impact of transfer learning with a small number of samples. All experiments are done on Kimia Path24 dataset which consists of 27,055 histopathology training patches in 24 tissue texture classes along with 1,325 test patches for evaluation. The result shows that pre-trained networks are quite competitive against training from scratch. As well, fine-tuning does not seem to add any tangible improvement for VGG16 to justify additional training while we observed considerable improvement in retrieval and classification accuracy when we fine-tuned the Inception structure.", "Multiple source domain adaptation  Transfer learning  Supervised low rank representation  Visual classification How to guarantee the robustness of multi-source adaptation visual classification is an important challenge in current visual learning community. To this end, we address in this paper the problem of robust visual classification with few labeled samples from the target domain of interest by leveraging multiple prior source models. Motivated by the recent success of low rank representation, we formulate this problem as a robust multi-source adaptation visual classification (RMAVC) model with supervised low rank representation by combining the strength of discriminative information from the target domain and the prior models from multiple source domains. Specifically, we propose a joint supervised low rank representation and multi-source adaptation visual classification framework, which achieves dual goals of finding the most discriminative low rank representation and multi-source adaptation classifier parameters for the target domain. While it is showed in this paper that the proposed RMAVC framework is effective and can produce high accuracy on several tasks of multi-source adaptation visual classification, this framework fails to consider the local geometrical structure of the target data and the heterogeneousness among multiple source domains. Hence, under this framework, we further present two effective extensions or variants, i.e., RMAVCK and RMAVC_FM, by exploiting multiple kernel trick and flexible manifold regularization, respectively. The proposed framework and its variants are robust for classifying visual objects accurately and the experimental results demonstrate the effectiveness of our methods on several types of image and video datasets. (C) 2016 Elsevier Ltd. All rights reserved.", "Person re-identification  Covariance features  Deep leaning  Transfer learning This paper introduces a novel type of features based on covariance descriptors - the convolutional covariance features (CCF). Differently from the traditional and handcrafted way to obtain covariance descriptors, CCF is computed from adaptive and trainable features, which come from a coarse-to-fine transfer learning (CFL) strategy. CFL provides a generic-to-specific knowledge and noise-invariant information for person re-identification. After training the deep features, convolutional and flat features are extracted from, respectively, intermediate and top layers of a hybrid deep network. Intermediate layer features are then wrapped in covariance matrices, composing the so-called CCF, which are integrated to the top layer features, called here flat features. Integration of CCF and flat features demonstrated to improve the proposed person re-identification in comparison with the use of the component features alone. Our person re-identification method achieved the best top 1 performance, when compared with other 18 state-of-the-art methods over VIPeR, i-LIDS, CUHK01 and CUHK03 data sets. The compared methods are based on deep learning, covariance descriptors, or handcrafted features and similarity functions. (C) 2016 Elsevier Ltd. All rights reserved.", "Transfer learning  Speaker adaptation  Deep neural network  Multi-task learning In this paper, we present a unified approach to transfer learning of deep neural networks (DNNs) to address performance degradation issues caused by a potential acoustic mismatch between the training and testing conditions due to inter-speaker variability in state-of-the-art connectionist (a.k.a., hybrid) automatic speech recognition (ASR) systems. Different schemes to transfer knowledge of deep neural networks related to speaker adaptation can be developed with ease under such a unifying concept as demonstrated in the three frameworks investigated in this study. In the first solution, knowledge is transferred between homogeneous domains, namely the source and the target domains. Moreover the transfer takes place in a sequential manner from the target to the source speaker to boost the ASR accuracy on spoken utterances from a surprise target speaker. In the second solution, a multi-task approach is adopted to adjust the connectionist parameters to improve the ASR system performance on the target speaker. Knowledge is transferred simultaneously among heterogeneous tasks, and that is achieved by adding one or more smaller auxiliary output layers to the original DNN structure. In the third solution, DNN output classes are organised into a hierarchical structure in order to adjust the connectionist parameters and close the gap between training and testing conditions by transferring prior knowledge from the root node to the leaves in a structural maximum a posteriori fashion. Through a series of experiments on the Wall Street Journal (WSJ) speech recognition task, we show that the proposed solutions result in consistent and statistically significant word error rate reductions. Most importantly, we show that transfer learning is an enabling technology for speaker adaptation, since it outperforms both the transformation-based adaptation algorithms usually adapted in the speech community, and the multi-condition training (MCT) schemes, which is a data combination methods often adopted to cover more acoustic variabilities in speech when data from the source and target domains are both available at the training time. Finally, experimental evidence demonstrates that all proposed solutions are robust to negative transfer even when only a single sentence from the target speaker is available. ", "Transfer learning  reinforcement learning  Gaussian processes  prototype generation In machine learning, learning a task is expensive (many training samples are needed) and it is therefore of general interest to be able to reuse knowledge across tasks. This is the case in aerial robotics applications, where an autonomous aerial robot cannot interact with the environment hazard free. Prototype generation is a well known technique commonly used in supervised learning to help reduce the number of samples needed to learn a task. However, little is known about how such techniques can be used in a reinforcement learning task. In this work we propose an algorithm that, in order to learn a new (target) task, first generates new samplesprototypesbased on samples acquired previously in a known (source) task. The proposed approach uses Gaussian processes to learn a continuous multidimensional transition function, rendering the method capable of reasoning directly in continuous (states and actions) domains. We base the prototype generation on a careful selection of a subset of samples from the source task (based on known filtering techniques) and transforming such samples using the (little) knowledge acquired in the target task. Our experimental evidence gathered in known reinforcement learning benchmark tasks, as well as a challenging quadcopter to helicopter transfer task, suggests that prototype generation is feasible and, furthermore, that the filtering technique used is not as important as a correct transformation model.", "Histograms of Topographical (HoT) features  Spectral regression  Transfer learning  Temporal filtering  Continuous pain intensity estimation Pain assessment through observational pain scales is necessary for special categories of patients such as neonates, patients with dementia, and critically ill patients. The recently introduced Prkachin Solomon score allows pain assessment directly from facial images opening the path for multiple assistive applications. In this paper, we proposed a system built upon the Histograms of Topographical (HoT) features, which are a generalization of the topographical primal sketch, for the description of the face parts contributing to the mentioned score. We further propose a semi-supervised, clustering oriented self-taught learning procedure developed on the Cohn-Kanade emotion oriented database by adapting the spectral regression. To make use of inter-frame pain correlation we introduce a machine learning based temporal filtering. We use this procedure to improve the discrimination between different pain intensity levels and the generalization with respect to the monitored persons, while testing on the UNBC McMaster Shoulder Pain database. ", " This study considers the common situation in data analysis when there are few observations of the distribution of interest or the target distribution, while abundant observations are available from auxiliary distributions. In this situation, it is natural to compensate for the lack of data from the target distribution by using data sets from these auxiliary distributionsin other words, approximating the target distribution in a subspace spanned by a set of auxiliary distributions. Mixture modeling is one of the simplest ways to integrate information from the target and auxiliary distributions in order to express the target distribution as accurately as possible. There are two typical mixtures in the context of information geometry: the The is applied in a variety of research fields because of the presence of the well-known expectation-maximazation algorithm for parameter estimation, whereas the e-mixture is rarely used because of its difficulty of estimation, particularly for nonparametric models. The e-mixture, however, is a well-tempered distribution that satisfies the principle of maximum entropy. To model a target distribution with scarce observations accurately, this letter proposes a novel framework for a nonparametric modeling of the e-mixture and a geometrically inspired estimation algorithm. As numerical examples of the proposed framework, a transfer learning setup is considered. The experimental results show that this framework works well for three types of synthetic data sets, as well as an EEG real-world data set.", "Deep metric learning  deep transfer metric learning  transfer learning  face verification  person reidentification Conventional metric learning methods usually assume that the training and test samples are captured in similar scenarios so that their distributions are assumed to be the same. This assumption does not hold in many real visual recognition applications, especially when samples are captured across different data sets. In this paper, we propose a new deep transfer metric learning (DTML) method to learn a set of hierarchical nonlinear transformations for cross-domain visual recognition by transferring discriminative knowledge from the labeled source domain to the unlabeled target domain. Specifically, our DTML learns a deep metric network by maximizing the inter-class variations and minimizing the intra-class variations, and minimizing the distribution divergence between the source domain and the target domain at the top layer of the network. To better exploit the discriminative information from the source domain, we further develop a deeply supervised transfer metric learning (DSTML) method by including an additional objective on DTML, where the output of both the hidden layers and the top layer are optimized jointly. To preserve the local manifold of input data points in the metric space, we present two new methods, DTML with autoencoder regularization and DSTML with autoencoder regularization. Experimental results on face verification, person re-identification, and handwritten digit recognition validate the effectiveness of the proposed methods.", "Recommender system  Collaborative filtering  Logistic regression  White-space customer In commercial sales and services, recommender systems have been widely adopted to predict customers' purchase interests using their prior purchasing behaviors. Cold-start is a known challenge to existing recommendation techniques, e.g., the popular collaborative filtering method is not applicable to predict the interests of white-space customers since they have no prior purchasing history in the targeted product categories. This paper presents SalesExplorer, a new recommendation algorithm to address white-space customer issue in the commercial sales and services segment. To predict the interests of customers who are new to a product category, we propose a statistical inference method using customers' existing purchase records from other product categories, a Probabilistic Latent Semantic Analysis (PLSA)-based transfer learning method using customers' business profile content, and a kernel logistic regression-based model which combines these two recommendations to produce the final results with higher accuracy. Experimental study using real-world enterprise sales data demonstrates that, comparing with a baseline and two state-of-the-art methods, the proposed combinatorial algorithm improves recommendation accuracy by 32.14%, 13.13% and 9.85%, respectively. ", "Multi-task learning  Model-agnostic framework  Meta algorithm  Classification  Regression Learning from small number of examples is a challenging problem in machine learning. An effective way to improve the performance is through exploiting knowledge from other related tasks. Multi-task learning (MTL) is one such useful paradigm that aims to improve the performance through jointly modeling multiple related tasks. Although there exist numerous classification or regression models in machine learning literature, most of the MTL models are built around ridge or logistic regression. There exist some limited works, which propose multi-task extension of techniques such as support vector machine, Gaussian processes. However, all these MTL models are tied to specific classification or regression algorithms and there is no single MTL algorithm that can be used at a meta level for any given learning algorithm. Addressing this problem, we propose a generic, model-agnostic joint modeling framework that can take any classification or regression algorithm of a practitioner's choice (standard or custom-built) and build its MTL variant. The key observation that drives our framework is that due to small number of examples, the estimates of task parameters are usually poor, and we show that this leads to an under-estimation of task relatedness between any two tasks with high probability. We derive an algorithm that brings the tasks closer to their true relatedness by improving the estimates of task parameters. This is achieved by appropriate sharing of data across tasks. We provide the detail theoretical underpinning of the algorithm. Through our experiments with both synthetic and real datasets, we demonstrate that the multi-task variants of several classifiers/regressors (logistic regression, support vector machine, K-nearest neighbor, Random Forest, ridge regression, support vector regression) convincingly outperform their single-task counterparts. We also show that the proposed model performs comparable or better than many state-of-the-art MTL and transfer learning baselines.", "Transfer learning  Information granules  Text sequence recognition  Type-2 fuzzy set  (Hidden Markov Model) HMM Context information plays an important role in text sequence recognition, but it is difficult to harness the uncertainty caused by conflicting implications. In this paper, we propose a novel Granular Transfer (GT) learning with type-2 fuzzy Hidden Markov Model (HMM) called GT2HMM, in which interpretable granules-representation is introduced to describe the contextual uncertainty for its transfer learning. The correspondences among words are transformed into information granules using fuzzy c-means. To fulfill the utilization of granular information in sequence recognition, we construct a type-2 fuzzy HMM which fuses labeled data and unlabeled observations. With a tunable granularity, correspondence information is refined in a coarse-to-fine manner in GT2HMM. Experiments on transductive and inductive transfer learning in part-of-speech (POS) tagging tasks verify the effectiveness of our proposed GT2HMM. ", " ", "Cross-dataset  Action recognition  Neural network  Transfer learning  Domain adaptation The emergence of large-scale human action datasets poses a challenge to efficient action labeling. Hand labeling large-scale datasets is tedious and time consuming  thus a more efficient labeling method would be beneficial. One possible solution is to make use of the knowledge of a known dataset to aid the labeling of a new dataset. To this end, we propose a new transfer learning method for cross-dataset human action recognition. Our method aims at learning generalized feature representation for effective cross-dataset classification. We propose a novel dual many-to-one encoder architecture to extract generalized features by mapping raw features from source and target datasets to the same feature space. Benefiting from the favorable property of the proposed many-to-one encoder, cross-dataset action data are encouraged to possess identical encoded features if the actions share the same class labels. Experiments on pairs of benchmark human action datasets achieved state-of-the-art accuracy, proving the efficacy of the proposed method. ", "Co-occurrence data  directed cyclic network (DCN)  heterogeneous transfer learning  multidomain  transferred weight One of the main research problems in heterogeneous transfer learning is to determine whether a given source domain is effective in transferring knowledge to a target domain, and then to determine how much of the knowledge should be transferred from a source domain to a target domain. The main objective of this paper is to solve this problem by evaluating the relatedness among given domains through transferred weights. We propose a novel method to learn such transferred weights with the aid of co-occurrence data, which contain the same set of instances but in different feature spaces. Because instances with the same category should have similar features, our method is to compute their principal components in each feature space such that co-occurrence data can be rerepresented by these principal components. The principal component coefficients from different feature spaces for the same instance in the co-occurrence data have the same order of significance for describing the category information. By using these principal component coefficients, the Markov Chain Monte Carlo method is employed to construct a directed cyclic network where each node is a domain and each edge weight is the conditional dependence from one domain to another domain. Here, the edge weight of the network can be employed as the transferred weight from a source domain to a target domain. The weight values can be taken as a prior for setting parameters in the existing heterogeneous transfer learning methods to control the amount of knowledge transferred from a source domain to a target domain. The experimental results on synthetic and real-world data sets are reported to illustrate the effectiveness of the proposed method that can capture strong or weak relations among feature spaces, and enhance the learning performance of heterogeneous transfer learning.", "Facial expression recognition  Transfer learning  Subspace learning  Multi-view learning  Biometrics In this paper, we propose a transfer subspace learning approach cross-dataset facial expression recognition. To our best knowledge, this problem has been seldom addressed in the literature. While many facial expression recognition methods have been proposed in recent years, most of them assume that face images in the training and testing sets are collected under the same conditions so that they are independently and identically distributed. In many real applications, this assumption does not hold as the testing data are usually collected online and are generally more uncontrollable than the training data. Hence, the testing samples are likely different from the training samples. In this paper, we define this problem as cross-dataset facial expression recognition as the training and testing. data are considered to be collected from different datasets due to different acquisition conditions. To address this, we propose a transfer subspace learning approach to learn a feature subspace which transfers the knowledge gained from the source domain (training samples) to the target domain (testing samples) to improve the recognition performance. To better exploit more complementary information for multiple feature representations of face images, we develop a multi-view transfer subspace learning approach where multiple different yet related subspaces are learned to transfer information from the source domain to the target domain. Experimental results are presented to demonstrate the efficacy of these proposed methods for the cross-dataset facial expression recognition task. ", "Distribution diversity  electroencephalogram (EEG)  epilepsy detection  feature extraction  Takagi-Sugeno-Kang (TSK) fuzzy logic system (FLS)  transfer learning The intelligent recognition of electroencephalogram (EEG) signals has become an important approach to the detection of epilepsy. Among existing intelligent identification methods, fuzzy logic systems (FLSs) have shown a distinctive advantage in identifying epileptic EEG signals because of their strong learning abilities and interpretability. Like many conventional intelligent methods for recognizing EEG signals, in the training of FLS, it is assumed that the training dataset and test dataset are drawn from data that are identically distributed. However, this assumption is not necessarily valid in practice as it is not uncommon for the two datasets to have different distributions. To overcome this problem, a strategy is presented in this paper to construct a Takagi-Sugeno-Kang (TSK) FLS based on transductive transfer learning for identifying epileptic EEG signals. Two novel objective functions, achieved by integrating the transductive transfer learning mechanism, are proposed for the training of the TSK FLS. As regression and binary classification are two common approaches to multiclass classification, the TSK transfer learning FLS algorithms for regression and binary classification are developed, respectively, to construct the corresponding TSK FLS. Both algorithms are further used to perform a multiclass classification to recognize epileptic EEG signals. Their performance in the epileptic EEG datasets indicates promise in dealing with situations where the training and test datasets differ with regard to data distribution.", "Fuzzy c-means (FCM)  fuzzy subspace clustering (FSC)  knowledge leverage  prototype-based clustering  transfer learning Traditional prototype-based clustering methods, such as the well-known fuzzy c-means (FCM) algorithm, usually need sufficient data to find a good clustering partition. If available data are limited or scarce, most of them are no longer effective. While the data for the current clustering task may be scarce, there is usually some useful knowledge available in the related scenes/domains. In this study, the concept of transfer learning is applied to prototype-based fuzzy clustering (PFC). Specifically, the idea of leveraging knowledge from the source domain is exploited to develop a set of transfer PFC algorithms. First, two representative PFC algorithms, namely, FCM and fuzzy subspace clustering, have been chosen to incorporate with knowledge leveraging mechanisms to develop the corresponding transfer clustering algorithms based on an assumption that there are the same number of clusters between the target domain (current scene) and the source domain (related scene). Furthermore, two extended versions are also proposed to implement the transfer learning for the situation that there are different numbers of clusters between two domains. The novel objective functions are proposed to integrate the knowledge from the source domain with the data in the target domain for the clustering in the target domain. The proposed algorithms have been validated on different synthetic and real-world datasets. Experimental results demonstrate their effectiveness in comparison with both the original PFC algorithms and the related clustering algorithms like multitask clustering and coclustering.", "Enhanced KL-TSK-FS  fuzzy systems  knowledge leverage  missing data  fuzzy modeling  transfer learning The knowledge-leverage-based Takagi-Sugeno-Kang fuzzy system (KL-TSK-FS) modeling method has shown promising performance for fuzzy modeling tasks where transfer learning is required. However, the knowledge-leverage mechanism of the KL-TSK-FS can be further improved. This is because available training data in the target domain are not utilized for the learning of antecedents and the knowledge transfer mechanism from a source domain to the target domain is still too simple for the learning of consequents when a Takagi-Sugeno-Kang fuzzy system (TSK-FS) model is trained in the target domain. The proposed method, that is, the enhanced KL-TSK-FS (EKL-TSK-FS), has two knowledge-leverage strategies for enhancing the parameter learning of the TSK-FS model for the target domain using available information from the source domain. One strategy is used for the learning of antecedent parameters, while the other is for consequent parameters. It is demonstrated that the proposed EKL-TSK-FS has higher transfer learning abilities than the KL-TSK-FS. In addition, the EKL-TSK-FS has been further extended for the scene of the multisource domain.", "Image segmentation  Brain parcellation  Medical imaging With the increasing use of efficient multimodal 3D imaging, clinicians are able to access longitudinal imaging to stage pathological diseases, to monitor the efficacy of therapeutic interventions, or to assess and quantify rehabilitation efforts. Analysis of such four-dimensional (4D) image data presenting pathologies, including disappearing and newly appearing lesions, represents a significant challenge due to the presence of complex spatio-temporal changes. Image analysis methods for such 4D image data have to include not only a concept for joint segmentation of 3D datasets to account for inherent correlations of subject-specific repeated scans but also a mechanism to account for large deformations and the destruction and formation of lesions (e.g., edema, bleeding) due to underlying physiological processes associated with damage, intervention, and recovery. In this paper, we propose a novel framework that provides a joint segmentation-registration framework to tackle the inherent problem of image registration in the presence of objects not present in all images of the time series. Our methodology models 4D changes in pathological anatomy across time and also provides an explicit mapping of a healthy normative template to a subject's image data with pathologies. Since atlas-moderated segmentation methods cannot explain appearance and locality pathological structures that are not represented in the template atlas, the new framework provides different options for initialization via a supervised learning approach, iterative semisupervised active learning, and also transfer learning, which results in a fully automatic 4D segmentation method. We demonstrate the effectiveness of our novel approach with synthetic experiments and a 4D multi modal MRI dataset of severe traumatic brain injury (TBI), including validation via comparison to expert segmentations. However, the proposed methodology is generic in regard to different clinical applications requiring quantitative analysis of 4D imaging representing spatio-temporal changes of pathologies. (C) 2016 Elsevier Inc. All rights reserved.", "Transfer learning  semi-supervised learning  support vector machine (SVM)  confident classifiers  self-paced learning  easy-to-hard  facial action unit (AU) detection Facial action unit (AU) detection from video has been a long-standing problem in the automated facial expression analysis. While progress has been made, accurate detection of facial AUs remains challenging due to ubiquitous sources of errors, such as inter-personal variability, pose, and low-intensity AUs. In this paper, we refer to samples causing such errors as hard samples, and the remaining as easy samples. To address learning with the hard samples, we propose the confidence preserving machine (CPM), a novel two-stage learning framework that combines multiple classifiers following an easy-to-hard strategy. During the training stage, CPM learns two confident classifiers. Each classifier focuses on separating easy samples of one class from all else, and thus preserves confidence on predicting each class. During the test stage, the confident classifiers provide virtual labels for easy test samples. Given the virtual labels, we propose a quasi-semi-supervised (QSS) learning strategy to learn a person-specific classifier. The QSS strategy employs a spatio-temporal smoothness that encourages similar predictions for samples within a spatio-temporal neighborhood. In addition, to further improve detection performance, we introduce two CPM extensions: iterative CPM that iteratively augments training samples to train the confident classifiers, and kernel CPM that kernelizes the original CPM model to promote nonlinearity. Experiments on four spontaneous data sets GFT, BP4D, DISFA, and RU-FACS illustrate the benefits of the proposed CPM models over baseline methods and the state-of-the-art semi-supervised learning and transfer learning methods.", "Machine learning  Classification  Computer aided diagnosis  Transfer learning Machine learning approaches are increasingly successful in image-based diagnosis, disease prognosis, and risk assessment. This paper highlights new research directions and discusses three main challenges related to machine learning in medical imaging: coping with variation in imaging protocols, learning from weak labels, and interpretation and evaluation of results. ", "Crowdsourcing  Contest  Recommendation system  Transfer learning  Feature-based matrix factorization We propose a novel participation recommendation approach for crowdsourcing contests including probabilistic modeling of contest participation and winner determination. Our method estimates the winning and participation probability of each worker and offers ranked lists of recommended contests. Since there is only one winner in most contests, standard recommendation techniques fail to estimate the accurate winning probability using only the extremely sparse winning information of completed contests. Our solution is to utilize contest participation information and features of workers and contests as auxiliary information. We use the concept of a transfer learning method for matrices and a feature-based matrix factorization method. Experiments conducted using real crowdsourcing contest datasets show that the use of auxiliary information is crucial for improving the performance of contest recommendation, and also reveal several important common skills. (C) 2016 Elsevier Ltd. All rights reserved.", "Convolutional neural networks  transfer learning  representation learning  deep learning  visual recognition Evidence is mounting that Convolutional Networks (ConvNets) are the most effective representation learning method for visual recognition tasks. In the common scenario, a ConvNet is trained on a large labeled dataset (source) and the feed-forward units activation of the trained network, at a certain layer of the network, is used as a generic representation of an input image for a task with relatively smaller training set (target). Recent studies have shown this form of representation transfer to be suitable for a wide range of target visual recognition tasks. This paper introduces and investigates several factors affecting the transferability of such representations. It includes parameters for training of the source ConvNet such as its architecture, distribution of the training data, etc. and also the parameters of feature extraction such as layer of the trained ConvNet, dimensionality reduction, etc. Then, by optimizing these factors, we show that significant improvements can be achieved on various (17) visual recognition tasks. We further show that these visual recognition tasks can be categorically ordered based on their similarity to the source task such that a correlation between the performance of tasks and their similarity to the source task w.r.t. the proposed factors is observed.", "Signature verification  Metric Learning  Multitask Learning  Transfer Learning This paper presents a novel classification method, Deep Multitask Metric Learning (DMML), for offline signature verification. Unlike existing methods that to verify questioned signatures of an individual merely consider the training samples of that class, DMML uses the knowledge from the similarities and dissimilarities between the genuine and forged samples of other classes too. To this end, using the idea of multitask and transfer learning, DMML train a distance metric for each class together with other classes simultaneously. DMML has a structure with a shared layer acting as a writer-independent approach, that is followed by separated layers which learn writer-dependent factors. We compare the proposed method against SVM, writer-dependent and writer-independent Discriminative Deep Metric Learning method on four offline signature datasets (UTSig, MCYT-75, GPDSsynthetic, and GPDS960GraySignatures) using Histogram of Oriented Gradients (HOG) and Discrete Radon Transform (DRT) features. Results of our experiments show that DMML achieves better performance compared to other methods in verifying genuine signatures, skilled and random forgeries. ", "Recommender systems  Transfer learning  Collaborative filtering  Implicit ratings  Explicit ratings  Sparsity In recent years, transfer learning has been used successfully to improve the predictive performance of collaborative filtering (CF) for sparse data by transferring patterns across domains. In this work, we advance transfer learning (TL) in recommendation systems (RSs), facilitating improvement within a domain rather than across domains. Specifically, we utilize TL for in-domain usage. This reduces the need to obtain information from additional domains, while achieving stronger single domain results than other state-of-the-art CF methods. We present two new algorithms  the first utilizes different event data within the same domain and boosts recommendations of the target event (e.g., the buy event), and the second algorithm transfers patterns from dense subspaces of the dataset to sparse subspaces. Experiments on real-life and publically available datasets reveal that the proposed methods outperform existing state-of-the-art CF methods. ", "Bayesian networks parameter learning  Transfer learning  Bayesian model comparison  Bayesian model averaging Learning Bayesian networks from scarce data is a major challenge in real-world applications where data are hard to acquire. Transfer learning techniques attempt to address this by leveraging data from different but related problems. For example, it may be possible to exploit medical diagnosis data from a different country. A challenge with this approach is heterogeneous relatedness to the target, both within and across source networks. In this paper we introduce the Bayesian network parameter transfer learning (BNPTL) algorithm to reason about both network and fragment (sub-graph) relatedness. BNPTL addresses (i) how to find the most relevant source network and network fragments to transfer, and (ii) how to fuse source and target parameters in a robust way. In addition to improving target task performance, explicit reasoning allows us to diagnose network and fragment relatedness across Bayesian networks, even if latent variables are present, or if their state space is heterogeneous. This is important in some applications where relatedness itself is an output of interest. Experimental results demonstrate the superiority of BNPTL at various scarcities and source relevance levels compared to single task learning and other state-of-the-art parameter transfer methods. Moreover, we demonstrate successful application to real-world medical case studies. (C) 2016 Elsevier Ltd. All rights reserved.", "Change detection  cluster analysis  mean shift clustering A nonparametric unsupervised method for analyzing changes in complex datasets is proposed. It is based on the mean shift clustering algorithm. Mean shift is used to cluster the old and new datasets and compare the results in a nonparametric manner. Each point from the new dataset naturally belongs to a cluster of points from its dataset. The method is also able to find to which cluster the point belongs in the old dataset and use this information to report qualitative differences between that dataset and the new one. Changes in local cluster distribution are also reported. The report can then be used to try to understand the underlying reasons which caused the changes in the distributions. On the basis of this method, a transductive transfer learning method for automatically labeling data from the new dataset is also proposed. This labeled data is used, in addition to the old training set, to train a classifier better suited to the new dataset. The algorithm has been implemented and tested on simulated and real (a stereo image pair) datasets. Its performance was also compared with several state-of-the-art methods.", "Object recognition  domain adaptation  transfer learning  low-rank reconstruction  boosting  late fusion We aim at improving the object recognition with few training data in the target domain by leveraging abundant auxiliary data in the source domain. The major issue obstructing knowledge transfer from source to target is the limited correlation between the two domains. Transferring irrelevant information from the source domain usually leads to performance degradation in the target domain. To address this issue, we propose a transfer learning framework with the two key components, such as discriminative source data reconstruction and dual-domain boosting. The former correlates the two domains via reconstructing source data by target data in a discriminative manner. The latter discovers and delivers only knowledge shared by the target data and the reconstructed source data. Hence, it facilitates recognition in the target. The promising experimental results on three benchmarks of object recognition demonstrate the effectiveness of our approach.", "Active learning  Entropy-based sampling  Image classification  Bag-of-visual-words  Cognitive model Active learning is an effective method for iteratively selecting a subset of images from an unlabeled dataset. One of the most widely used active learning strategies is uncertainty sampling. However, traditional sampling strategies do not take the category of samples into consideration, and the selected images do not reflect the desired training distribution, leading to the result that additional labeling work needs to be done. To deal with these problems, from the aspect of visual perception, we improve the traditional entropy-based uncertainty sampling strategy by introducing a certainty measurement estimated by a bag-of-visual-words (BoVW). The Rescorla-Wagner perceptive model is utilized to quantify the stop criterion. This method differs from previous approaches that treated sampling and classifying process separately: we treat the learning process as a uniform model by proposing a new evolving sample selection method that uses the unified negative-accelerated learning principle and takes category distribution into consideration. A classifier is trained to provide category distributions for the sampling process to improve its sampling performance and reduce additional annotation costs for the human annotator. During the training process, weights for both modules are adaptively initialized by the inner similarity of sample set measured by structural similarity (SSIM), and dynamically adjusted according to the learning process of the human. In addition to the regular tests that are commonly utilized by traditional sampling methods, the transfer test, based on transfer learning theory, is utilized to further evaluate the performance of different sampling strategies. Experimental results on real world datasets show that our active sampling framework outperforms both baseline and state-of-the-art adaptive active learning strategies. ", " Heterogeneous one-class feedback has been recognized as an important source of information in recommendation systems. Joint similarity learning has the merit of being able to connect two seemingly unrelated items along sparse positive feedback only.", "Rare class  Transfer learning  Class imbalance  AdaBoost  Weighted majority algorithm  HealthCare informatics  Text mining A fundamental problem in data mining is to effectively build robust classifiers in the presence of skewed data distributions. Class imbalance classifiers are trained specifically for skewed distribution datasets. Existing methods assume an ample supply of training examples as a fundamental prerequisite for constructing an effective classifier. However, when sufficient data are not readily available, the development of a representative classification algorithm becomes even more difficult due to the unequal distribution between classes. We provide a unified framework that will potentially take advantage of auxiliary data using a transfer learning mechanism and simultaneously build a robust classifier to tackle this imbalance issue in the presence of few training samples in a particular target domain of interest. Transfer learning methods use auxiliary data to augment learning when training examples are not sufficient and in this paper we will develop a method that is optimized to simultaneously augment the training data and induce balance into skewed datasets. We propose a novel boosting-based instance transfer classifier with a label-dependent update mechanism that simultaneously compensates for class imbalance and incorporates samples from an auxiliary domain to improve classification. We provide theoretical and empirical validation of our method and apply to healthcare and text classification applications.", "Policy reuse  Reinforcement learning  Online learning  Online bandits  Transfer learning  Bayesian optimisation  Bayesian decision theory A long-lived autonomous agent should be able to respond online to novel instances of tasks from a familiar domain. Acting online requires 'fast' responses, in terms of rapid convergence, especially when the task instance has a short duration such as in applications involving interactions with humans. These requirements can be problematic for many established methods for learning to act. In domains where the agent knows that the task instance is drawn from a family of related tasks, albeit without access to the label of any given instance, it can choose to act through a process of policy reuse from a library in contrast to policy learning. In policy reuse, the agent has prior experience from the class of tasks in the form of a library of policies that were learnt from sample task instances during an offline training phase. We formalise the problem of policy reuse and present an algorithm for efficiently responding to a novel task instance by reusing a policy from this library of existing policies, where the choice is based on observed 'signals' which correlate to policy performance. We achieve this by posing the problem as a Bayesian choice problem with a corresponding notion of an optimal response, but the computation of that response is in many cases intractable. Therefore, to reduce the computation cost of the posterior, we follow a Bayesian optimisation approach and define a set of policy selection functions, which balance exploration in the policy library against exploitation of previously tried policies, together with a model of expected performance of the policy library on their corresponding task instances. We validate our method in several simulated domains of interactive, short-duration episodic tasks, showing rapid convergence in unknown task variations.", "Transfer learning  Domain adaptation  Non-negative matrix factorization Domain adaptation is a field of machine learning that addresses the problem occurring when a classifier is trained and tested on domains from different distributions. This kind of paradigm is of vital importance as it allows a learner to generalize the knowledge across different tasks. In this paper, we present a new method for fully unsupervised domain adaptation that seeks to align two domains using a shared set of basis vectors derived from eigenvectors of each domain. We use non-negative matrix factorization (NMF) techniques to generate a non-negative embedding that minimizes the distance between projections of source and target data. We present a theoretical justification for our approach by showing the consistency of the similarity function defined using the obtained embedding. We also prove a theorem that relates the source and target domain errors using kernel embeddings of distribution functions. We validate our approach on benchmark data sets and show that it outperforms several state-of-art domain adaptation methods. ", "Writer adaptation  Domain adaptation  Handwriting recognition  Transfer learning  Discriminant learning Performance of handwritten character recognition systems degrades significantly when they are trained and tested on different databases. In this paper, we propose a novel large margin domain transfer algorithm, which is able to jointly reduce the data distribution mismatch of training (source) and test (target) datasets, as well as learning a target classifier by relying on a set of pre-learned classifiers with the labeled source data in addition to a few available target labels. The proposed method optimizes the combination coefficients of pre-learned classifiers to obtain the minimum mismatch between results on the source and target datasets. Our method is applicable both in semi-supervised and unsupervised domain adaptation scenarios, while most of the previous competing domain adaptation methods work only in semi-supervised scenario. Experiments on adaptation to different handwritten digit datasets demonstrate that this method achieves superior classification accuracy on target sets, comparing to the state of the art methods. Quantitative evaluation shows that an unsupervised adaptation reduces the error rates by 40.2% comparing with the SVM classifier trained by the labeled samples from the source domain. (C) 2016 Elsevier Ltd. All rights reserved.", "Emotion recognition in the wild  Facial expression recognition  Speech emotion recognition  Sparse transductive transfer linear discriminant analysis  Transfer learning  Domain adaptation Recently, emotion recognition in the wild has been attracted in computer vision and affective computing. In contrast to classical emotion recognition, emotion recognition in the wild becomes more challenging since the databases are collected under real scenarios. In such databases, there would inevitably be various adverse samples, whose emotion labels are considerably hard to be identified using many ideal databases based classical emotion recognition methods. Therefore, it significantly increases the difficulty of emotion recognition task based on the wild databases. In this paper, we propose to use a transductive transfer learning framework to handle the problem of emotion recognition in the wild. We develop a sparse transductive transfer linear discriminant analysis (STTLDA) for facial expression recognition and speech emotion recognition under real-world environments, respectively. As far as we know, the novelty of our method is that we are the first to consider emotion recognition in the wild as a transfer learning problem and use the transductive transfer learning method to eliminate the distribution difference between training and testing samples caused by the wild. We conduct extensive experiments on SFEW 2.0, AFEW 4.0 and 5.0 (audio part) databases, which were used in Emotion Recognition in the Wild Challenge (EmotiW 2014 and 2015) to evaluate our proposed method. Experimental results demonstrate that our proposed STTLDA achieves a satisfactory performance compared with the baseline provided by the challenge organizers and some competitive methods. In addition, we report our previous results in static image based facial expression recognition challenge of EmotiW 2015. In this competition, we achieve an accuracy of 50 % on the Test set and this result has a 10.87 % improvement compared with the baseline released by challenge organizers.", "Extreme learning machine  Transfer learning (TL)  Free sparse representation In this paper, we propose a general framework for Extreme Learning Machine via free sparse transfer representation, which is referred to as transfer free sparse representation based on extreme learning machine (TFSR-ELM). This framework is suitable for different assumptions related to the divergence measures of the data distributions, such as a maximum mean discrepancy and K-L divergence. We propose an effective sparse regularization for the proposed free transfer representation learning framework, which can decrease the time and space cost. Different solutions to the problems based on the different distribution distance estimation criteria and convergence analysis are given. Comprehensive experiments show that TFSR-based algorithms outperform the existing transfer learning methods and are robust to different sizes of training data.", "Multi-view clustering  Ensemble clustering  Similarity measure  Transfer learning  Co-clustering In many clustering problems, we have access to multiple sources of data representing different aspects of the problem. Each of these data separately represents an association between entities. Multi-view clustering involves integrating clustering information from these heterogeneous sources of data and has been shown to improve results over a single-view clustering. On the other hand, co-clustering has been widely used as a technique to improve clustering results on a single view by exploiting the duality between objects and their attributes. In this paper, we propose a multi-view clustering setting in the context of a co-clustering framework. Our underlying assumption is that similarity values generated from the individual data can be transferred from one view to the other(s) resulting in a better clustering of the data. We provide empirical evidence to show that this framework results in a better clustering accuracy than those obtained from any of the single views, tested on different datasets.", "Dictionary learning  cross-view  action recognition  transfer learning Discriminative appearance features are effective for recognizing actions in a fixed view, but may not generalize well to a new view. In this paper, we present two effective approaches to learn dictionaries for robust action recognition across views. In the first approach, we learn a set of view-specific dictionaries where each dictionary corresponds to one camera view. These dictionaries are learned simultaneously from the sets of correspondence videos taken at different views with the aim of encouraging each video in the set to have the same sparse representation. In the second approach, we additionally learn a common dictionary shared by different views to model view-shared features. This approach represents the videos in each view using a view-specific dictionary and the common dictionary. More importantly, it encourages the set of videos taken from the different views of the same action to have the similar sparse representations. The learned common dictionary not only has the capability to represent actions from unseen views, but also makes our approach effective in a semi-supervised setting where no correspondence videos exist and only a few labeled videos exist in the target view. The extensive experiments using three public datasets demonstrate that the proposed approach outperforms recently developed approaches for cross-view action recognition.", "Cross-view action recognition  Transfer learning  Discriminant analysis  Heterogeneous domain adaption We propose an approach of cross-view action recognition, in which the samples from different views are represented by features with different dimensions. Inspired by linear discriminant analysis (LDA), we introduce a discriminative common feature space to bridge the source and target views. Two different projection matrices are learned to respectively map the action data from two different views into the common space by simultaneously maximizing the similarity of intra-class samples, minimizing the similarity of inter-class samples and reducing the mismatch between data distributions of two views. In addition, the locality information is incorporated into the discriminant analysis as a constraint to make the discriminant function smooth on the data manifold. Our method is neither restricted to the corresponding action instances in the two views nor restricted to a specific type of feature. We evaluate our approach on the IXMAS multi-view action dataset and N-UCLA dataset. The experimental results demonstrate the effectiveness of our method. ", "Transfer Learning  Non-Negative Matrix Tri-Factorization  Multi-Layer  Cross-domain classification Transfer learning, which leverages labeled data in a source domain to train an accurate classifier for classification tasks in a target domain, has attracted extensive research interests recently for its effectiveness proven by many studies. Previous approaches adopt a common strategy that models the shared structure as a bridge across different domains by reducing distribution divergences. However, those approaches totally ignore specific latent spaces, which can be utilized to learn non-shared concepts. Only specific latent spaces contain specific latent factors, lacking which will lead to ineffective distinct concept learning. Additionally, only learning latent factors in one latent feature space layer may ignore those in the other layers. The missing latent factors may also help us to model the latent structure shared as the bridge. This paper proposes a novel transfer learning method Multi-Layer Transfer Learning (MLTL). MLTL first generates specific latent feature spaces. Second, it combines these specific latent feature spaces with common latent feature space into one latent feature space layer. Third, it generates multiple layers to learn the corresponding distributions on different layers with their pluralism simultaneously. Specifically, the pluralism of the distributions on different layers means that learning the distributions on one layer can help us to learn the distributions on the others. Furthermore, an iterative algorithm based on Non-Negative Matrix Tri-Factorization is proposed to solve the optimization problem. Comprehensive experiments demonstrate that MLTL can significantly outperform the state-of-the-art learning methods on topic and sentiment classification tasks. ", "Age estimation  Deep Convolutional Neural Networks  Transfer learning  Loss function Automatic age estimation has attracted much attention due to its potential applications. Most of the proposed approaches have mainly used low-level handcraft features to encode facial age related visual information and train an age estimation model. In this paper, we focus on age classification task in which face image is assigned to a label that represents an age range. We proposed a deep learning based framework for age classification task. In our proposed algorithm, Deep Convolutional Neural Networks (Deep ConvNets) are used to extract high-level complex age related visual features and predict age range of input face image. Due to lack of age labeled face images, we use the transfer learning strategy to train the Deep ConvNets. In addition, to describe the relationships between labels that compose an ordered sequence, we define a new loss function in the training process of age classification task. The experiments are conducted on a widely used age estimation dataset-Images of Groups of People. The experimental results demonstrate the excellent performance of our proposed algorithm against the state-of-the-art methods. ", "Domain adaptation  Pedestrian detection  Feature learning  Scene-specific detector  Transfer learning Successful detection and localisation of pedestrians is an important goal in computer vision which is a core area in Artificial Intelligence. State-of-the-art pedestrian detectors proposed in literature have reached impressive performance on certain datasets. However, it has been pointed out that these detectors tend not to perform very well when applied to specific scenes that differ from the training datasets in some ways. Due to this, domain adaptation approaches have recently become popular in order to adapt existing detectors to new domains to improve the performance in those domains. There is a real need to review and analyse critically the state-of-the-art domain adaptation algorithms, especially in the area of object and pedestrian detection. In this paper, we survey the most relevant and important state-of-the-art results for domain adaptation for image and video data, with a particular focus on pedestrian detection. Related areas to domain adaptation are also included in our review and we make observations and draw conclusions from the representative papers and give practical recommendations on which methods should be preferred in different situations that practitioners may encounter in real life. (C) 2016 Elsevier Ltd. All rights reserved.", "IR pedestrian detection  Scenarios adaptation  Transfer learning  Boosting  Classification disagreement The presence of inevitable disparity in distributions between the training data and test data is one of the main reasons that result in downgraded performance for infrared (IR) pedestrian detection across distinct scenarios, and it is expensive and sometimes difficult to label sufficient new training data from target scenarios to re-train a scene-specific detector. In this paper, a novel boosting-style method for data-level transfer learning termed DTLBoost is proposed. Specifically, sample importance measurement is first presented to evaluate the similarities between the samples across distinct scenarios using k-nearest neighbors-based model. Then the most informative samples from former scenarios are selected to extend the training data and help to build a base learner iteratively. In addition, degree of classification disagreement among base learners is formulated and incorporated into the weight updating rules of training samples, which helps to select the samples from former scenarios with positive transferability and encourage different base learners to learn different parts or aspects of samples from target scenarios. The proposed method has been evaluated on two types of IR pedestrian detection applications, including pedestrian detection for both driving assistance systems and video surveillance. Experimental results demonstrate that the proposed method achieves promising improvement on detection performance toward both new scenes and viewpoints adaptation.", "Transfer learning  Non-negative matrix tri-factorization  Multi-bridge  Cross-domain classification Transfer learning, which aims to exploit the knowledge in the source domains to promote the learning tasks in the target domains, has attracted extensive research interests recently. The general idea of the previous approaches is to model the shared structure in one latent space as the bridge across domains by reducing the distribution divergences. However, there exist some latent factors in the other latent spaces, which can also be utilized to draw the corresponding distributions closer for establishing the bridges. In this paper, we propose a novel transfer learning method, referred to as Multi-Bridge Transfer Learning (MBTL), to learn the distributions in the different latent spaces together. Therefore, more latent factors shared can be utilized to transfer knowledge. Additionally, an iterative algorithm with convergence guarantee based on non-negative matrix tri-factorization techniques is proposed to solve the optimization problem. Comprehensive experiments demonstrate that MBTL can significantly outperform state-of-theart learning methods on the topic and sentiment classification tasks. ", "Boosting  Domain adaptation  Transfer learning Domain adaptation (DA) is a new learning framework dealing with learning problems where the target test data are drawn from a distribution different from the one that has generated the learning source data. In this article, we introduce self-labeling domain adaptation boosting (SLDAB), a new DA algorithm that falls both within the theory of DA and the theory of Boosting, allowing us to derive strong theoretical properties. SLDAB stands in the unsupervised DA setting where labeled data are only available in the source domain. To deal with this more complex situation, the strategy of SLDAB consists in jointly minimizing the empirical error on the source domain while limiting the violations of a natural notion of pseudo-margin over the target domain instances. Another contribution of this paper is the definition of a new divergence measure aiming at penalizing models that induce a large discrepancy between the two domains, reducing the production of degenerate models. We provide several theoretical results that justify this strategy. The practical efficiency of our model is assessed on two widely used datasets.", "Transfer learning  domain adaptation  visualcategorization  heterogeneous data We propose a novel reconstruction-based transfer learning method called latent sparse domain transfer (LSDT) for domain adaptation and visual categorization of heterogeneous data. For handling cross-domain distribution mismatch, we advocate reconstructing the target domain data with the combined source and target domain data points based on l(1)-norm sparse coding. Furthermore, we propose a joint learning model for simultaneous optimization of the sparse coding and the optimal subspace representation. In addition, we generalize the proposed LSDT model into a kernel-based linear/nonlinear basis transformation learning framework for tackling nonlinear subspace shifts in reproduced kernel Hilbert space. The proposed methods have three advantages: 1) the latent space and the reconstruction are jointly learned for pursuit of an optimal subspace transfer  2) with the theory of sparse subspace clustering, a few valuable source and target data points are formulated to reconstruct the target data with noise (outliers) from source domain removed during domain adaptation, such that the robustness is guaranteed  and 3) a nonlinear projection of some latent space with kernel is easily generalized for dealing with highly nonlinear domain shift (e.g., face poses). Extensive experiments on several benchmark vision data sets demonstrate that the proposed approaches outperform other state-of-the-art representation-based domain adaptation methods.", " ", "Transfer learning  Hierarchical representation  Attributes  Metric learning  Action similarity The goal of transfer learning is to exploit previous experiences and knowledge in order to improve learning in a novel domain. This is especially beneficial for the challenging task of learning classifiers that generalize well when only few training examples are available. In such a case, knowledge transfer methods can help to compensate for the lack of data. The performance and robustness against negative transfer of these approaches is influenced by the interdependence between knowledge representation and transfer type. However, this important point is usually neglected in the literature  instead the focus lies on either of the two aspects. In contrast, we study in this work the effect of various high-level semantic knowledge representations on different transfer types in a novel generic transfer metric learning framework. Furthermore, we introduce a hierarchical knowledge representation model based on the embedded structure in the semantic attribute space. The evaluation of the framework on challenging transfer settings in the context of action similarity demonstrates the effectiveness of our approach compared to state-of-the-art. ", "Online action recognition  Online interaction recognition  Hierarchical  Transfer learning Recognising human actions in real-time can provide users with a natural user interface (NUI) enabling a range of innovative and immersive applications. A NUI application should not restrict users' movements  it should allow users to transition between actions in quick succession, which we term as compound actions. However, the majority of action recognition researchers have focused on individual actions, so their approaches are limited to recognising single actions or multiple actions that are temporally separated. This paper proposes a novel online action recognition method for fast detection of compound actions. A key contribution is our hierarchical body model that can be automatically configured to detect actions based on the low level body parts that are the most discriminative for a particular action. Another key contribution is a transfer learning strategy to allow the tasks of action segmentation and whole body modelling to be performed on a related but simpler dataset, combined with automatic hierarchical body model adaption on a more complex target dataset. Experimental results on a challenging and realistic dataset show an improvement in action recognition performance of 16% due to the introduction of our hierarchical transfer learning. The proposed algorithm is fast with an average latency of just 2 frames (66 ms) and outperforms state of the art action recognition algorithms that are capable of fast online action recognition. ", "Group-based verification  open-world re-identification  transfer relative distance comparison Solving the problem of matching people across non-overlapping multi-camera views, known as person re-identification (re-id), has received increasing interests in computer vision. In a real-world application scenario, a watch-list (gallery set) of a handful of known target people are provided with very few (in many cases only a single) image(s) (shots) per target. Existing re-id methods are largely unsuitable to address this open-world re-id challenge because they are designed for (1) a closed-world scenario where the gallery and probe sets are assumed to contain exactly the same people, (2) person-wise identification whereby the model attempts to verify exhaustively against each individual in the gallery set, and (3) learning a matching model using multi-shots. In this paper, a novel transfer local relative distance comparison (t-LRDC) model is formulated to address the open-world person re-identification problem by one-shot group-based verification. The model is designed to mine and transfer useful information from a labelled open-world non-target dataset. Extensive experiments demonstrate that the proposed approach outperforms both non-transfer learning and existing transfer learning based re-id methods.", "Active learning  Multiple domains  Arbitrarily different distributions  Transfer learning This paper addresses a novel learning problem where data from multiple domains are generated from distributions that are allowed to be arbitrarily different. Domain adaptation and transfer learning have been addressed previously for two domains with different distributions, but based on the assumption that the distributions are sufficiently related to each other. However, in many real-world applications, multiple domains may have arbitrarily different distributions. In this paper, a general framework is proposed, called Multi-ATL, for bridging knowledge from multiple domains with arbitrarily different distributions. The proposed framework consists of three key components: latent feature space extraction, co-training with multiple views, and active sample selection. The basic concept is to explore each domain and characterize it with an extracted latent feature space, and then apply a co-training algorithm to the latent spaces to use multiple views simultaneously for training. In the co-training process, active sample selection is used to determine how much knowledge should be transferred from a source domain to the target domain. Experimental results on one synthetic and two real-world datasets show that the proposed framework significantly improves classification accuracy (13.5% on the synthetic and +17.79%, +23.87% on the real datasets) with only a few (2-5) active selections. ", "Collaborative recommendation  Auxiliary data  Transfer learning Intelligent recommendation technology has been playing an increasingly important role in various industry applications such as e-commerce product promotion and Internet advertisement display. Besides user feedbacks (e.g., numerical ratings) on items as usually exploited by some typical recommendation algorithms, there are often some additional data such as users' social circles and other behaviors. Such auxiliary data are usually related to user preferences on items behind numerical ratings. Collaborative recommendation with auxiliary data (CRAD) aims to leverage such additional information so as to improve personalized services. It has received much attention from both researchers and practitioners. Transfer learning (TL) is proposed to extract and transfer knowledge from some auxiliary data in order to assist the learning task on the target data. In this survey, we consider the CRAD problem from a transfer learning view, especially on how to enable knowledge transfer from some auxiliary data, and discuss the representative transfer learning techniques. Firstly, we give a formal definition of transfer learning for CRAD (TL-CRAD). Secondly, we extend the existing categorization of TL techniques with three knowledge transfer strategies. Thirdly, we propose a novel and generic knowledge transfer framework for TL-CRAD. Fourthly, we describe some representative works of each specific knowledge transfer strategy in detail, which are expected to inspire further works. Finally, we conclude the survey with some summarized discussions and several future directions. ", "Source domain  target domain  low-rank and sparse constraints  knowledge transfer  subspace learning In this paper, we address the problem of unsupervised domain transfer learning in which no labels are available in the target domain. We use a transformation matrix to transfer both the source and target data to a common subspace, where each target sample can be represented by a combination of source samples such that the samples from different domains can be well interlaced. In this way, the discrepancy of the source and target domains is reduced. By imposing joint low-rank and sparse constraints on the reconstruction coefficient matrix, the global and local structures of data can be preserved. To enlarge the margins between different classes as much as possible and provide more freedom to diminish the discrepancy, a flexible linear classifier (projection) is obtained by learning a non-negative label relaxation matrix that allows the strict binary label matrix to relax into a slack variable matrix. Our method can avoid a potentially negative transfer by using a sparse matrix to model the noise and, thus, is more robust to different types of noise. We formulate our problem as a constrained low-rankness and sparsity minimization problem and solve it by the inexact augmented Lagrange multiplier method. Extensive experiments on various visual domain adaptation tasks show the superiority of the proposed method over the state-of-the art methods. The MATLAB code of our method will be publicly available at https://www.yongxu.org/lunwen.html.", " ", "Bagging  Ensemble method  Transfer learning  Machine learning Nowadays, transfer learning is one of the main research areas in machine learning that is helpful for labeling the data with low cost. In this paper, we propose a novel bagging-based ensemble transfer learning (BETL). The BETL framework includes three operations: Initiate, Update, and Integrate. In the Initiate operation, we use bootstrap sampling to divide the source data into many subsets, and add the labeled data from the target domain into these subsets separately so that the source data and the target data arrive at a reasonable ratio, then we learn as many initial classifiers as the elements of an ensemble. In the Update operation, we utilize the initial classifiers and an updateable classifier to repeatedly label the data that hasn't been labeled yet in the target domain, and then, add the newly labeled data into the target domain to renew the updateable classifier. In the Integrate operation, we integrate the updated classifiers from each iteration into a pool to predict the labels of the test data via the majority vote strategy. In order to demonstrate the effectiveness of our method in the classification process, we conduct experiments on UCI data set, real world data set, and text data set. The results show that our method can effectively label the unlabeled data in the target domain, which greatly enhances the performance of target domain.", "Unsupervised domain adaptation  transfer learning  maximum mean discrepancy  Kernel-based feature weighting  feature-label relation One of the serious challenges in machine learning and pattern recognition is to transfer knowledge from related but different domains to a new unlabeled domain. Feature selection with maximum mean discrepancy (f-MMD) is a novel and effective approach to transfer knowledge from source domain (training set) into target domain (test set) where training and test sets are drawn from different distributions. However, f-MMD has serious challenges in facing datasets with large number of samples and features. Moreover, f-MMD ignores the feature-label relation in finding the reduced representation of dataset. In this paper, we exploit jointly transfer learning and class discrimination to cope with domain shift problem on which the distribution difference is considerably large. We therefore put forward a novel transfer learning and class discrimination approach, referred to as RandOm k-samplesets feature Weighting Approach (ROWA). Specifically, ROWA reduces the distribution difference across domains in an unsupervised manner where no label is available in the test set. Moreover, ROWA exploits feature-label relation to separate various classes alongside the domain transfer, and augments the relation of selected features and source domain labels. In this work, we employ disjoint/overlapping small-sized samplesets to iteratively converge to final solution. Employment of local sets along with a novel optimization problem constructs a robust and effective reduced representation for adaptation across domains. Extensive experiments on real and synthetic datasets verify that ROWA can significantly outperform state-of-the-art transfer learning approaches.", " Common representation learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, has been receiving a lot of attention recently. Two popular paradigms here are canonical correlation analysis (CCA)-based approaches and autoencoder (AE)-based approaches. CCA-based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. AE-based methods learn a common representation by minimizing the error of reconstructing the two views. Each of these approaches has its own advantages and disadvantages. For example, while CCA-based approaches outperform AE-based approaches for the task of transfer learning, they are not as scalable as the latter. In this work, we propose an AE-based approach, correlational neural network (CorrNet), that explicitly maximizes correlation among the views when projected to the common subspace. Through a series of experiments, we demonstrate that the proposed CorrNet is better than AE and CCA with respect to its ability to learn correlated common representations. We employ CorrNet for several cross-language tasks and show that the representations learned using it perform better than the ones learned using other state-of-the-art approaches.", "Multi-task  Transfer learning  Optimization  Healthcare  Data mining  Statistical analysis Prognosis, such as predicting mortality, is common in medicine. When confronted with small numbers of samples, as in rare medical conditions, the task is challenging. We propose a framework for classification with data with small numbers of samples. Conceptually, our solution is a hybrid of multi-task and transfer learning, employing data samples from source tasks as in transfer learning, but considering all tasks together as in multi-task learning. Each task is modelled jointly with other related tasks by directly augmenting the data from other tasks. The degree of augmentation depends on the task relatedness and is estimated directly from the data. We apply the model on three diverse real-world data sets (healthcare data, handwritten digit data and face data) and show that our method outperforms several state-of-the-art multi-task learning baselines. We extend the model for online multi-task learning where the model parameters are incrementally updated given new data or new tasks. The novelty of our method lies in offering a hybrid multi-task/transfer learning model to exploit sharing across tasks at the data-level and joint parameter learning.", "Transfer learning  complex action recognition  relationship matrix Recognizing complex human actions is very challenging, since training a robust learning model requires a large amount of labeled data, which is difficult to acquire. Considering that each complex action is composed of a sequence of simple actions which can be easily obtained from existing data sets, this paper presents a simple to complex action transfer learning model (SCA-TLM) for complex human action recognition. SCA-TLM improves the performance of complex action recognition by leveraging the abundant labeled simple actions. In particular, it optimizes the weight parameters, enabling the complex actions to be learned to be reconstructed by simple actions. The optimal reconstruct coefficients are acquired by minimizing the objective function, and the target weight parameters are then represented as a combination of source weight parameters. The main advantage of the proposed SCA-TLM compared with existing approaches is that we exploit simple actions to recognize complex actions instead of only using complex actions as training samples. To validate the proposed SCA-TLM, we conduct extensive experiments on two well-known complex action data sets: 1) Olympic Sports data set and 2) UCF50 data set. The results show the effectiveness of the proposed SCA-TLM for complex action recognition.", "Soft-partition clustering  Fuzzy c-means  Maximum entropy  Diversity index  Transfer learning  Cross-domain clustering Conventional, soft-partition clustering approaches, such as fuzzy c-means (FCM), maximum entropy clustering (MEC) and fuzzy clustering by quadratic regularization (FC-QR), are usually incompetent in those situations where the data are quite insufficient or much polluted by underlying noise or outliers. In order to address this challenge, the quadratic weights and Gini-Simpson diversity based fuzzy clustering model (QWGSD-FC), is first proposed as a basis of our work. Based on QWGSD-FC and inspired by transfer learning, two types of cross-domain, soft-partition clustering frameworks and their corresponding algorithms, referred to as type-I/type-II knowledge-transfer-oriented c-means (TI-KT-CM and TII-KT-CM), are subsequently presented, respectively. The primary contributions of our work are four-fold: (1) The delicate QWGSD-FC model inherits the most merits of FCM, MEC and FC-QR. With the weight factors in the form of quadratic memberships, similar to FCM, it can more effectively calculate the total intra-cluster deviation than the linear form recruited in MEC and FC-QR. Meanwhile, via Gini-Simpson diversity index, like Shannon entropy in MEC, and equivalent to the quadratic regularization in FC-QR, QWGSD-FC is prone to achieving the unbiased probability assignments, (2) owing to the reference knowledge from the source domain, both TI-KT-CM and TII-KT-CM demonstrate high clustering effectiveness as well as strong parameter robustness in the target domain, (3) TI-MT-CM refers merely to the historical cluster centroids, whereas TB-KT-CM simultaneously uses the historical cluster centroids and their associated fuzzy memberships as the reference. This indicates that TII-KT-CM features more comprehensive knowledge learning capability than TI-KT-CM and TII-KT-CM consequently exhibits more perfect cross-domain clustering performance and (4) neither the historical cluster centroids nor the historical cluster centroid based fuzzy memberships involved in TI-MT-CM or TII-KT-CM can be inversely mapped into the raw data. This means that both TI-KT-CM and TII-KT-CM can work without disclosing the original data in the source domain, i.e. they are of good privacy protection for the source domain. In addition, the convergence analyses regarding both TI-KT-CM and TB-KT-CM are conducted in our research. The experimental studies thoroughly evaluated and demonstrated our contributions on both synthetic and real-life data scenarios. ", "Extreme learning machine  Transfer learning (TL)  Classification The extreme learning machine (ELM) is a new method for using Single-hidden Layer Feed-forward Networks (SLFNs) with a much simpler training method. While conventional extreme learning machine are based on the training and test data which should be under the same distribution, in reality it is often desirable to learn an accurate model using only a tiny amount of new data and a large amount of old data. Transfer learning (TL) aims to solve related but different target domain problems by using plenty of labeled source domain data. When the task from one new domain comes, new domain samples are relabeled costly, and it would be a waste to discard all the old domain data. Therefore, an algorithm called TL-ELM based on the ELM algorithm is proposed, which uses a small amount of target domain tag data and a large number of source domain old data to build a high-quality classification model. The method inherits the advantages of ELM and makes up for the defects that traditional ELM cannot transfer knowledge. Experimental results indicate that the performance of the proposed methods is superior to or at least comparable with existing benchmarking methods. In addition, a novel domain adaptation kernel extreme learning machine (TL-DAKELM) based on the kernel extreme learning machine was proposed with respect to the TL-ELM. Experimental results show the effectiveness of the proposed algorithm. ", "Image-based localization  Active transfer learning  Thermal imaging Indoor localization is one of the key problems in robotics research. Most current localization systems use cellular base stations and Wifi signals, whose localization accuracy is largely dependent on the signal strength and is sensitive to environmental changes. With the development of camera-based technologies, image-based localization may be employed in an indoor environment where the GPS signal is weak. Most of the existing image-based localization systems are based on color images captured by cameras, but this is only feasible in environments with adequate lighting conditions. In this paper, we introduce an image-based localization system based on thermal imaging to make the system independent of light sources, which are especially useful during emergencies such as a sudden power outage in a building. As thermal images are not obtained as easily as color images, we apply active transfer learning to enrich the thermal image classification learning, where normal RGB images are treated as the source domain, and thermal images are the target domain. The application of active transfer learning avoids random target training sample selection and chooses the most informative samples in the learning process. Through the proposed active transfer learning, the query thermal images can be accurately used to indicate the location. Experiments show that our system can be efficiently deployed to perform indoor localization in a dark environment. ", " The emergence of big data greatly promotes the development of data-driven machine learning technologies. The common assumption in machine learning that the training data and the test data have identical feature spaces with underlying distributions impedes the development of machine learning. To deal with this issue, transfer learning is studied to exploit the knowledge accumulated from data in auxiliary domains to facilitate predictive modelling consisting of different data patterns in the current domain. There have been a significant number of methods proposed to address the classification, as the task, through transfer learning, but the studies targeted at regression problems are still scarce. In this paper, we propose a new transfer learning method to deal with the regression task in the target domain where few data are available. Takagi-Sugeno fuzzy model is used to construct the model for regression task in the source domain, and the prototypes and linear functions of the existing model are modified to make the model more compatible for the target domain. The experimental results demonstrate that our method can improve the performance of the existing model of the source domain on addressing current task in the target domain.", "keyword spotting  DNN  Deep Neural Network  multi-task learning  weighted cross entropy We propose improved Deep Neural Network (DNN) training loss functions for more accurate single keyword spotting on resource-constrained embedded devices. The loss function modifications consist of a combination of multi-task training and weighted cross entropy. In the multi-task architecture, the keyword DNN acoustic model is trained with two tasks in parallel - the main task of predicting the keyword-specific phone states, and an auxiliary task of predicting LVCSR senones. We show that multi-task learning leads to comparable accuracy over a previously proposed transfer learning approach where the keyword DNN training is initialized by an LVCSR DNN of the same input and hidden layer sizes. The combination of LVCSR-initialization and Multi-task training gives improved keyword detection accuracy compared to either technique alone. We also propose modifying the loss function to give a higher weight on input frames corresponding to keyword phone targets, with a motivation to balance the keyword and background training data. We show that weighted cross-entropy results in additional accuracy improvements. Finally, we show that the combination of 3 techniques-LVCSR-initialization, multi-task training and weighted cross-entropy gives the best results, with significantly lower False Alarm Rate than the LVCSR-initialization technique alone, across a wide range of Miss Rates.", "Multilingual  cross-lingual  semi-supervised training  deep neural network Semi-supervised and cross-lingual knowledge transfer learnings are two strategies for boosting performance of low resource speech recognition systems. In this paper, we propose a unified knowledge transfer learning method to deal with these two learning tasks. Such a knowledge transfer learning is realized by fine-tuning of Deep Neural Network (DNN). We demonstrate its effectiveness in both monolingual based semi supervised learning task and cross-lingual knowledge transfer learning task. We then combine these two learning strategies to obtain further performance improvement.", "Colloquial Arabic  dialectical Arabic  language modelling  transfer learning  machine translation Modern standard Arabic (MSA) is the official language of spoken and written Arabic media. Colloquial Arabic (CA) is the set of spoken variants of modem Arabic that exist in the form of regional dialects. CA is used in informal and everyday conversations while MSA is formal communication. An Arabic speaker switches between the two variants according to the situation. Developing an automatic speech recognition system always requires a large collection of transcribed speech or text, and for CA dialects this is an issue. CA has limited textual resources because it exists only as a spoken language, without a standardised written form unlike MSA. This paper focuses on the data sparsity issue in CA textual resources and proposes a strategy to emulate a native speaker in colloquialising MSA to be used in CA language models (LMs) by use of a machine translation (MT) framework. The empirical results in Levantine CA show that using LMs estimated from colloquialised MSA data outperformed MSA LMs with a perplexity reduction up to 68% relative. In addition, interpolating colloquialised MSA LMs with a CA LMs improved speech recognition performance by 4% relative.", "speech recognition  whispered speech  deep neural networks  bottleneck feature networks  multilingual bottlenecks  low-resource speech recognition Previous work on whispered speech recognition has shown that acoustic models (AM) trained on whispered speech can somewhat classify unwhispered (neutral) speech sounds, but not vice versa. In fact, AMs trained purely on neutral speech completely fail to recognize whispered speech. Meanwhile, recipes used to train neutral AMs will work just as well for whispered speech, but such methods require a large volume of transcribed whispered speech which is expensive to gather. In this work, we propose and investigate the use of bottleneck feature networks to normalize differences between whispered and neutral speech modes. Our extensive experiments show that this type of speech variability can be effectively normalized. We also show that it is possible to transfer this knowledge from two source languages with whispered speech (Mandarin and English), to a new target language (Malay) without whispered speech. Furthermore, we report a substantial reduction in word error rate for cross-mode speech recognition, effectively demonstrate that it is possible to train acoustic models capable of classifying both types of speech without needing any additional whispered speech.", "speaker verification  transfer learning  PLDA  short utterance Short utterance lacks enough discriminative information and its duration variation will propagate uncertainty into a probability linear discriminant analysis (PLDA) classifier. For speaker verification on short utterances, it can be considered as a domain with limited amount of long utterances. Therefore, transfer learning of PLDA can be adopted to learn discriminative information from other domain with a large amount of long utterances. In this paper, we explore the effectiveness of transfer learning based PLDA (TL-PLDA) on the NIST SRE and Switchboard (SWB) corpus. Experimental results showed that it could produce the largest gain of performance compared with the traditional PLDA, especially for short utterances with the duration of 5s and 10s.", "Computational Paralinguistics  Transfer Learning  Cross-Task Labelling  Multi-Task Learning  Deception and Sincerity Identification In this work, we investigate the coherence between inferable deception and perceived sincerity in speech, as featured in the Deception and Sincerity tasks of the INTERSPEECH 2016 Computational Paralinguistics ChallengE (ComParE). We demonstrate an effective approach that combines the corpora of both Challenge tasks to achieve higher classification accuracy. We show that the naive label mapping method based on the assumption that sincerity and deception are just 'two sides of the same coin', i.e., taking deceptive speech as equivalent to non-sincere speech and vice versa, does not yield satisfactory results. However, we can exploit the interplay and synergies between these characteristics. To achieve this, we combine our previously introduced approach for data aggregation by semi-supervised cross-task label completion with multi-task learning, and knowledge-based instance selection. In the result, our approach achieves significant error rate reductions compared to the official Challenge baseline.", "speech emotion classification  human-computer interaction  computational paralinguistics Speech emotion recognition is an important problem with applications as varied as human-computer interfaces and affective computing. Previous approaches to emotion recognition have mostly focused on extraction of carefully engineered features and have trained simple classifiers for the emotion task. There has been limited effort at representation learning for affect recognition, where features are learnt directly from the signal waveform or spectrum. Prior work also does not investigate the effect of transfer learning from affective attributes such as valence and activation to categorical emotions. In this paper, we investigate emotion recognition from spectrogram features extracted from the speech and glottal flow signals  spectrogram encoding is performed by a stacked autoencoder and an RNN (Recurrent Neural Network) is used for classification of four primary emotions. We perform two experiments to improve RNN training : (1) Representation Learning - Model training on the glottal flow signal to investigate the effect of speaker and phonetic invariant features on classification performance (2) Transfer Learning - RNN training on valence and activation, which is adapted to a four emotion classification task. On the USC-IEMOCAP dataset, our proposed approach achieves a performance comparable to the state of the art speech emotion recognition systems.", "deep learning  emotion recognition  neural networks  speech recognition  transfer learning The correlation between Automatic Speech Recognition (ASR) and Speech Emotion Recognition (SER) is poorly understood. Studying such correlation may pave the way for integrating both tasks into a single system or may provide insights that can aid in advancing both systems such as improving ASR in dealing with emotional speech or embedding linguistic input into SER. In this paper, we quantify the relation between ASR and SER by studying the relevance of features learned between both tasks in deep convolutional neural networks using transfer learning. Experiments are conducted using the TIMIT and IEMOCAP databases. Results reveal an intriguing correlation between both tasks, where features learned in some layers particularly towards initial layers of the network for either task were found to be applicable to the other task with varying degree.", "cross-lingual speech recognition  transfer learning  deep neural networks  probabilistic transcription In this study, a transfer learning technique is presented for cross lingual speech recognition in an adverse scenario where there are no natively transcribed transcriptions in the target language. The transcriptions that are available during training are transcribed by crowd workers who neither speak nor have any familiarity with the target language. Hence, such transcriptions are likely to be inaccurate. Training a deep neural network (DNN) in such a scenario is challenging  previously reported results have described DNN error rates exceeding the error rate of an adapted Gaussian Mixture Model (GMM). This paper investigates multi-task learning techniques using deep neural networks which are suitable for this scenario. We report, for the first time, absolute improvement in phone error rates (PER) in the range 1.3-6.2% over GMMs adapted to probabilistic transcriptions. Results are reported for Swahili, Hungarian, and Mandarin.", " Multiverse networks were recently proposed as a method for promoting more effective transfer learning. While an extensive analysis was proposed, this analysis failed to capture two main aspects of these networks: (i) the rank of the representation is much lower than the rank predicted by the analysis  and (ii) the contribution of increased multiplicity in such networks diminishes quickly. In this work, we propose additional analysis of multiverse networks which addresses both deficits. A major contribution of our work is quantifying the Rademacher complexity of the multiverse network. It is shown that the complexity upper bound of multiverse networks is significantly lower than that of conventional networks, and diminishes by a factor of p k, k being the multiplicity. In addition, we generalize the notion of multiverse networks to multilayer multiverse networks. We derive the Rademacher complexity formula to such networks and present experimental results.", " The ability of a human being to extrapolate previously gained knowledge to other domains inspired a new family of methods in machine learning called transfer learning. Transfer learning is often based on the assumption that objects in both target and source domains share some common feature and/or data space. In this paper, we propose a simple and intuitive approach that minimizes iteratively the distance between source and target task distributions by optimizing the kernel target alignment (KTA). We show that this procedure is suitable for transfer learning by relating it to Hilbert-Schmidt Independence Criterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We run our method on benchmark computer vision data sets and show that it can outperform some state-of-art methods.", " Mortality prediction of rare cancer types with a small number of high-dimensional samples is a challenging task. We propose a transfer learning model where both classes in rare cancers (target task) are modeled in a joint framework by transferring knowledge from the source task. The knowledge transfer is at the data level where only related data points are chosen to train the target task. Moreover, both positive and negative class in training enhances the discrimination power of the proposed framework. Overall, this approach boosts the generalization performance of target task with a small number of data points. The formulation of the proposed framework is convex and expressed as a primal problem. We convert this to a dual problem and efficiently solve by alternating direction multipliers method. Our experiments with both synthetic and three real-world datasets show that our framework outperforms state-of-the-art single-task, multi-task, and transfer learning baselines.", " A supervised learning system requires labeled data during the training phase. Obtaining labels can be an expensive process, especially in medical imaging applications where a qualified expert may be needed to carefully analyze images and annotate them. This constrains the amount of labeled data available. This study explores the possibility of incorporating labeling behavior (viz., labeling latency) in a supervised convolutional neural network (CNN) framework in order to improve its performance in the presence of limited labeled data. The problem of spot detection in MRI scans is considered in this work. In this two-class problem, (a) labeling behavior is available only during the training phase unlike traditional features that are available both during training and testing  and (b) the labeling behavior is associated with only one class (the positive samples) unlike other side information that is available for all classes. To address these issues, a new CNN architecture referred to as L-CNN is designed. The proposed method utilizes the labeling behavior of the expert to cluster the labeled data into multiple categories  a source CNN is then trained to distinguish between these categories. Next, a transfer learning paradigm is used where a target CNN is initialized using this source CNN and its weights updated with the limited labeled data that is available. Experimental results on an existing MRI database show that the proposed L-CNN performs better than a conventional CNN and, further, significantly outperforms the previous state-of-the-art, thereby establishing a new baseline for spot detection in MRI.", " Micro-expression recognition is a challenging task in computer vision field due to the repressed facial appearance and short duration. Previous work for micro-expression recognition have used hand-crafted features like LBP-TOP, Gabor filter and optical flow. This paper is the first work to explore the possible use of deep learning for micro-expression recognition task. Due to the lack of data for micro-expression, training a CNN model from micro-expression data is not feasible. Instead, transfer learning from objects and facial expressions based CNN models are used. The aim is to use feature selection to remove the irrelevant deep features for our task. This work extends evolutionary algorithms to search an optimal set of deep features so that it does not overfit the training data and generalizes well for the test data. Promising results are presented for various micro-expression datasets.", " Hyperparameters play a crucial role in the model selection of machine learning algorithms. Tuning these hyperparameters can be exhaustive when the data is large. Bayesian optimisation has emerged as an efficient tool for hyperparameter tuning of machine learning algorithms. In this paper, we propose a novel framework for tuning the hyperparameters for big data using Bayesian optimisation. We divide the big data into chunks and generate hyperparameter configurations for the chunks using the standard Bayesian optimisation. We utilise this information from the chunks for hyperparameter tuning on big data using a transfer learning setting. We evaluate the performance of the proposed method on the task of tuning hyperparameters of two machine learning algorithms. We show that our method achieves the best available hyperparameter configuration within less computational time compared to the state-of-art hyperparameter tuning methods.", " This paper considers the problem of material recognition. Motivated by observation of close interconnections between material and object recognition, we study how to select and integrate multiple features obtained by different models of Convolutional Neural Networks (CNNs) trained in a transfer learning setting. To be specific, we first compute activations of features using representations on images to select a set of samples which are best represented by the features. Then, we measure uncertainty of the features by computing entropy of class distributions for each sample set. Finally, we compute contribution of each feature to representation of classes for feature selection and integration. Experimental results show that the proposed method achieves state-of-the-art performance on two benchmark datasets for material recognition. Additionally, we introduce a new material dataset, named EFMD, which extends Flickr Material Database (FMD). By the employment of the EFMD for transfer learning, we achieve 84.0% +/- 1.8% accuracy on the FMD dataset, which is close to the reported human performance 84.9%.", " In this work, we address the problem of transfer learning for sequential recommendation model. Most of the state-of-the-art recommendation systems consider user preference and give customized results to different users. However, for those users without enough data, personalized recommendation systems cannot infer their preferences well or rank items precisely. Recently, transfer learning techniques are applied to address this problem. Although the lack of data in target domain may result in underfitting, data from auxiliary domains can be utilized to assist model training. Most of recommendation systems combined with transfer learning aim at the rating prediction problem whose user feedback is explicit and not sequential. In this paper, we apply transfer learning techniques to a model utilizing user preference and sequential information. To the best of our knowledge, no previous works have addressed the problem. Experiments on real-world datasets are conducted to demonstrate our framework is able to improve prediction accuracy by utilizing auxiliary data.", "Multimodal emotion recognition  Bag-of-audio-words  Transfer learning  Long short-term memory  Convolutional neural networks This paper presents the University of Passau's approaches for the Multimodal Emotion Recognition Challenge 2016. For audio signals, we exploit Bag-of-Audio-Words techniques combining Extreme Learning Machines and Hierarchical Extreme Learning Machines. For video signals, we use not only the information from the cropped face of a video frame, but also the broader contextual information from the entire frame. This information is extracted via two Convolutional Neural Networks pre-trained for face detection and object classification. Moreover, we extract facial action units, which reflect facial muscle movements and are known to be important for emotion recognition. Long Short-Term Memory Recurrent Neural Networks are deployed to exploit temporal information in the video representation. Average late fusion of audio and video systems is applied to make prediction for multimodal emotion recognition. Experimental results on the challenge database demonstrate the effectiveness of our proposed systems when compared to the baseline.", "Speech emotion recognition  Deep neural network  Transfer learning  Feed forward network Deep learning has made great impact in several areas, where large size dataset usually plays a great role in its effect. As emotion recognition task usually lacks in labelled data, transfer learning is proposed to initiate from models of relevant classification task with amount of data and fine tune some part of network with target emotion domain data. An ensemble method based on posterior probability difference is performed to take advantage of different networks. Experiments show that method in this paper outperforms others like random forest and ranks top two in audio subtask of the CCPR multimedia emotion recognition challenge.", " Several studies on cross-domain users' behaviour revealed generic personality trails and behavioural patterns. This paper, proposes quantitative approaches to use the knowledge of player behaviour in one game to seed the process of building player experience models in another. We investigate two settings: in the supervised feature mapping method, we use labeled datasets about players' behaviour in two games. The goal is to establish a mapping between the features so that the models build on one dataset could be used on the other by simple feature replacement. For the unsupervised transfer learning scenario, our goal is to find a shared space of correlated features based on unlabelled data. The features in the shared space are then used to construct models for one game that directly work on the transferred features of the other game. We implemented and analysed the two approaches and we show that transferring the knowledge of player experience between domains is indeed possible and ultimately useful when studying players' behaviour and when designing user studies.", " Bayesian models and neural models have demonstrated their respective advantage in topic modeling. Motivated by the dark knowledge transfer approach proposed by [3], we present a novel method that combines the advantages of the two model families. Particularly, we present a transfer learning method that uses LDA to supervise the training of a deep neural network (DNN), so that the DNN can approximate the LDA inference with less computation. Our experimental results show that by transfer learning, a simple DNN can approximate the topic distribution produced by LDA pretty well, and deliver competitive performance as LDA on document classification, with much faster computation.", " Message-level and word-level polarity classification are two popular tasks in Twitter sentiment analysis. They have been commonly addressed by training supervised models from labelled data. The main limitation of these models is the high cost of data annotation. Transferring existing labels from a related problem domain is one possible solution for this problem. In this paper, we propose a simple model for transferring sentiment labels from words to tweets and vice versa by representing both tweets and words using feature vectors residing in the same feature space. Tweets are represented by standard NLP features such as unigrams and part-of-speech tags. Words are represented by averaging the vectors of the tweets in which they occur. We evaluate our approach in two transfer learning problems: 1) training a tweet-level polarity classifier from a polarity lexicon, and 2) inducing a polarity lexicon from a collection of polarity-annotated tweets. Our results show that the proposed approach can successfully classify words and tweets after transfer.", "Domain Adaptation  Transfer Learning  Ensemble  Deep Learning  Sentiment Analysis Real world applications of machine learning in natural language processing can span many different domains and usually require a huge effort for the annotation of domain specific training data. For this reason, domain adaptation techniques have gained a lot of attention in the last years. In order to derive an effective domain adaptation, a good feature representation across domains is crucial as well as the generalisation ability of the predictive model. In this paper we address the problem of domain adaptation for sentiment classification by combining deep learning, for acquiring a cross-domain high-level feature representation, and ensemble methods, for reducing the cross-domain generalization error. The proposed adaptation framework has been evaluated on a benchmark dataset composed of reviews of four different Amazon category of products, significantly outperforming the state of the art methods.", "Transfer learning  Domain adaptation  Traditional machine learning  Test framework  Distortion profiles Previous research focusing on the evaluation of transfer learning algorithms has predominantly used real-world datasets to measure an algorithm's performance. A test with a real-world dataset exposes an algorithm to a single instance of distribution difference between the training (source) and test (target) datasets. These previous works have not measured performance over a wide-range of source and target distribution differences. We propose to use a test framework that creates many source and target datasets from a single base dataset, representing a diverse-range of distribution differences. These datasets will be used as a stress test to measure an algorithm's performance. The stress test process will measure and compare different transfer learning algorithms and traditional learning algorithms. The unique contributions of this paper, with respect to transfer learning, are defining a test framework, defining multiple distortion profiles, defining a stress test suite, and the evaluation and comparison of different transfer learning and traditional machine learning algorithms over a wide-range of distributions.", "Transfer Learning  Linear Transformation  Distribution Matching  Weighted Mean In this paper, we propose a novel learning framework for the problem of domain transfer learning. We map the data of two domains to one single common space, and learn a classifier in this common space. Then we adapt the common classifier to the two domains by adding two adaptive functions to it respectively. In the common space, the target domain data points are weighted and matched to the target domain in term of distributions. The weighting terms of source domain data points and the target domain classification responses are also regularized by the local reconstruction coefficients. The novel transfer learning framework is evaluated over some benchmark cross-domain data sets, and it outperforms the existing state-of-the-art transfer learning methods.", " Transfer learning aims to extract the knowledge from a label-rich source domain to enhance the predictive model of a target domain. Previous methods achieve knowledge transfer by detecting a shared low-dimensional feature representation from source domain to target domain. Along this line, many algorithms, e.g., dual transfer learning (DTL), triplex transfer learning (TRi-TL) etc., have been proposed and widely used for text classification. However, we argue that it is difficult for models to distinguish exactly the common concepts or identical concepts across different domains through the existing algorithms, even though source and target domains are related but different. So we propose to use the similar feature clusters as knowledge transfer, that is, we only guarantee the approximate similarity of common word clusters across different domains, rather than the exactly same. Based on the above assumption, the derived association matrices between word clusters and document classes should be slightly different to account for the word clusters variations. To take the above assumptions into account, we propose a novel Nonnegative Matrix Tri-Factorization based transfer learning by linking similar feature clusters (LSF-TL) for sentiment classification, in which an approximate constraint between similar word clusters matrices is added to allow differences while keeping the knowledge transferring function. Besides, LSF-TL also provides the same approximate constraint for the derived clusters association matrices. Then we employ an iterative updating algorithm with sound theoretical proof to find the local optimal solution. Last, we evaluate our method by conducting extensive experiments on Amazon product reviews. The results show that our approach achieves better classification accuracy than the state-of-the-art methods for both Cross-lingual sentiment classification(CLSC) and Cross-lingual cross-domain sentiment classification(CLCDSC) tasks.", "Monocular  manifold  sequential labeling In this paper, the problem of depth estimation from single monocular image is considered. The depth cues such as motion, stereo correspondences are not present in single image which makes the task more challenging. We propose a machine learning based approach for extracting depth information from single image. The deep learning is used for extracting features, then, initial depths are generated using manifold learning in which neighborhood preserving embedding algorithm is used. Then, fixed point supervised learning is applied for sequential labeling to obtain more consistent and accurate depth maps. The features used are initial depths obtained from manifold learning and various image based features including texture, color and edges which provide useful information about depth. A fixed point contraction mapping function is generated using which depth map is predicted for new structured input image. The transfer learning approach is also used for improvement in learning in a new task through the transfer of knowledge from a related task that has already been learned. The predicted depth maps are reliable, accurate and very close to ground truth depths which is validated using objective measures: RMSE, PSNR, SSIM and subjective measure: MOS score.", " Finding appropriate feature representations from radiological images is a vital task for prediction and diagnosis. Deep convolutional neural networks have recently achieved state-of-the-art performance in classification problems from several different domains. Research has also shown the feasibility of using a pre-trained deep neural network as a feature extractor when only a small dataset is available. This paper proposes a novel image feature extraction method for predicting survival time from brain tumor magnetic resonance images using pre-trained deep neural networks. Since all tumors are different sizes, we also explore different image resizing methods in the paper. We demonstrate that deep features can result in better survival time prediction with the highest accuracy of 95.45% versus conventional feature extraction methods from magnetic resonance images of the brain.", "Neural Network  Biterm Topic Model  Pre-training  Social Emotion Detection  Short Text Short text is prevalent on the Web, but it brings challenges to content analysis methods for the lack of contextual information. Biterm topic model (BTM) is a variant of latent Dirichlet allocation, which effectively infers the latent topic distribution of short text by modeling the generation of biterms in the whole corpus. However, it needs fine-tuning from labels to reduce noise when applied to supervised learning. Motivated by the transfer learning approach, we propose the hybrid neural networks based on BTM and conventional neural networks, which first make the hidden layer of neural networks approximate the inference of BTM. Following this initial pre-training phase, we then use the simple back-propagation algorithm to fine-tune the topic distribution learned from BTM, so as to improve the performance of supervised learning. Our experiment on two diverse collections of short text validates the effectiveness of the proposed hybrid neural networks for social emotion detection.", " Brand recognition is a very challenging topic with many useful applications in localization recognition, advertisement and marketing. In this paper we present an automatic graphic logo detection system that robustly handles unconstrained imaging conditions. Our approach is based on Fast Region-based Convolutional Networks (FRCN) proposed by Ross Girshick, which have shown state-of-the-art performance in several generic object recognition tasks (PASCAL Visual Object Classes challenges). In particular, we use two CNN models pre-trained with the ILSVRC ImageNet dataset and we look at the selective search of windows 'proposals' in the pre-processing stage and data augmentation to enhance the logo recognition rate. The novelty lies in the use of transfer learning to leverage powerful Convolutional Neural Network models trained with large-scale datasets and repurpose them in the context of graphic logo detection. Another benefit of this framework is that it allows for multiple detections of graphic logos using regions that are likely to have an object. Experimental results with the FlickrLogos-32 dataset show not only the promising performance of our developed models with respect to noise and other transformations a graphic logo can be subject to, but also its superiority over state-of-the-art systems with hand-crafted models and features.", " Person re-identification has become a hot research topic due to its importance in surveillance and forensics applications. The purpose of person re-identification is to find the same person from disjoint camera views at different time. Most of the existing methods try to identify the person by measuring the similarity of two still images from different camera views, which only uses intra-image features such as color, shape and texture. In this paper, we propose a person re-identification architecture which analyzes a sequence pair while not an image pair, so not only intra-image features but also the gait feature is also considered. In contrast to existing works that use handcrafted features, our method automatically learns spatio-temporal features optimal for the person re-identification task with a deep convolutional network. To learn a discriminant metric, we use a subspace and metric learning method called Cross-view Quadratic Discriminant Analysis (XQDA). The process of XQDA is also called transfer learning. Experiments show that our method significantly outperforms the state of the art on both a large dataset (CUHK03) and a medium-sized dataset (CUHK01). We also get a better performance on a small dataset (VIPeR) with a pre-trained network without fine-tuning.", " Unsupervised transfer learning has attracted a lot of attention in the big data era, due to its capability of extracting knowledge from large-scale unlabeled samples in multiple data domains. Existing unsupervised transfer learning methods mainly focus on learning a common latent space for source and target domains, while the data representation and subspace structure in target domain are usually ignored. In this paper, we develop an Unsupervised Transfer learning approach based on Low-Rank Coding (UTLRC), in order to take advantages of the high-level structural information in the target domain. A dictionary is shared by samples from source domain and target domain, which contains bases vectors for low-level visual patterns. By utilizing the cross-domain dictionary, UTLRC is able to effectively encode the samples in target domain. In addition, a low-rank constraint is incorporated to model the subspace structure in target domain, and a sparse constraint is introduced in the source domain. We formulate UTLRC as a rank-minimization problem, and design an effective optimization algorithm based on the alternating direction method of multipliers (ADMM) to solve it. Existing evidence already indicates the connections between neural mechanisms and sparse coding, the low-rank coding strategy studied in this work would further inspire the understanding of the neural mechanisms of cognition. We apply UTLRC to image clustering, and evaluate its performance on several benchmark datasets. Extensive experimental results demonstrate the effectiveness of UTLRC, compared to some representative subspace clustering methods.", " Transfer learning plays a powerful role in mitigating the discrepancy between test data (target) and auxiliary data (source). There is often the case that multiple sources are available in transfer learning. However, naively combining multiple sources does not lead to valid results, since they will introduce negative transfer as well. Furthermore, each single source from multiple sources may not cover all the labels of the target data. In this paper, we consider the problem that how to better utilize multiple incomplete sources for effective knowledge transfer. To this end, we propose a Bi-directional Low-Rank Transfer learning framework (BLRT). First, we adapt the conventional low-rank transfer learning to multiple sources knowledge transfer scenario. Second, an iterative structure learning is proposed to better use prior knowledge for transfer learning coefficients matrix. Third, a cross-source regularizer is added to couple the same labels from multiple incomplete sources, so that they could jointly compensate missing data from other sources. Experimental results on three groups of databases including face and object images have demonstrated that our method can successfully inherit knowledge from incomplete multiple sources and adapt to the target data successfully.", "Transfer learning  sparse coding  discriminative subspace  domain adaptation In this paper, we propose a l(2,1)-norm based discriminative robust transfer learning (DKTL) method for domain adaptation tasks. The key idea is to simultaneously learn discriminative subspaces by using the proposed domain-class-consistency (DCC) metric, and the representation based robust transfer model between source domain and target domain via l(21)-norm minimization. The DCC metric includes two parts: domain-consistency used to measure the between-domain distribution discrepancy and class-consistency used to measure the within-domain class separability. The objective of transfer learning is to maximize the proposed metric, while for easily formulating this metric in model, we propose to minimize the domain-class-inconsistency, such that both domain distribution mismatch and class inseparability are well addressed. Two advantages of the proposed method are that on one hand the robust sparse coding selects a few valuable source data with noises (outliers) removed during knowledge transfer, and the proposed DCC metric can help to pursue discriminative subspaces of different domains for classification based transfer learning tasks. Extensive experiments demonstrate the superiority of the proposed method over other state-of-the-art domain adaptation methods.", " The performance of most conventional classification systems relies on appropriate data representation and much of the efforts arc dedicated to feature engineering, a difficult and time-consuming process that uses prior expert domain knowledge of the data to create useful features. On the other hand, deep learning can extract and organize the discriminative information from the data, not requiring the design of feature extractors by a domain expert. Convolutional Neural Networks (CNNs) are a particular type of deep, feedforward network that have gained attention from research community and industry, achieving empirical successes in tasks such as speech recognition, signal processing, object recognition, natural language processing and transfer learning. In this paper, we conduct some preliminary experiments using the deep learning approach to classify breast cancer histopathological images from BreaKHis, a publicly dataset available at https://web.inf.ufpr.brivri/breast-cancer-database. We propose a method based on the extraction of image patches for training the CNN and the combination of these patches tier final classification. This method aims to allow using the high resolution histopathological images from BreaKHis as input to existing CNN, avoiding adaptations of the model that can lead to a more complex and computationally costly architecture. The CNN performance is better when compared to previously reported results obtained by other machine learning models trained with hand-crafted textural descriptors. Finally, we also investigate the combination of different CNNs using simple fusion rules, achieving some improvement in recognition rates.", " The recent success of representation learning is built upon the learning of relevant features, in particular from unlabelled data available in different domains. This raises the question of how to transfer and reuse such knowledge effectively so that the learning of a new task can be made easier or be improved. This poses a difficult challenge for the area of transfer learning where there is no label in the source data, and no source data is ever transferred to the target domain. In previous work, the most capable approach has been self-taught learning which, however, relies heavily upon the compatibility across the domains. In this paper, we propose a novel transfer learning framework called Adaptive Transferred-profile Likelihood Learning (aTPL), which performs adaptation on the representations to be transferred, so that they become more compatible with the target domain. At the same time, it learns supplementary knowledge about the target domain. Experiments on five images datasets and a sentiment dataset demonstrate the effectiveness of the approach in comparison with self-taught learning and other common feature extraction methods. The results also indicate that the new transfer method is less sensitive to negative transfer.", "reinforcement learning  transfer learning  dynamic environment Reinforcement learning (RL) allows an intelligent agent to learn optimal behavior as it interacts with its environment. Conventional model-based RL algorithms learn rapidly, but can be slow to adapt to sudden changes in the environment. Animals' brains, however, are thought to employ model-based RL mechanisms for learning, but are able to adapt to changes with relative ease. By employing transfer learning, they can recycle previously learned information to solve new problems with minimal new learning. We developed two brain-inspired methods that can allow model-based RL to cope with changes to the underlying process being learned: hierarchical state abstraction, and context-switching. Hierarchical state abstraction allows a previously-learned model to be efficiently adapted for use in a new task, while context switching allows learned models to be saved and recalled at the appropriate times. We test these mechanisms using grid-world simulations in which the goal remains constant, but contingencies for reaching it frequently change. These mechanisms allow an agent to significantly outperform a conventional model-based RL algorithm in the task.", " Visual features trained from large scale image data by the deep convolutional neural network can be used for the other visual tasks. This paper investigates the effects of the learning of multiple tasks for such transfer learning from the source domains to the target domain. Two methods of the learning of multiple tasks are considered. Also we investigate which hidden layers should be re-trained for the target task in the fine-tuning process by selecting a subset of the hidden layers and updating only the parameters of the selected subset. Through a detail experiments, we confirmed the effectiveness of the learning of multiple tasks for pre-training. Also we showed that the first layer is not always required to be trained for the target task and the fully-connected layer, the classifier layer, and the last hidden layer should be retrained for the target task in the fine-tuning. These results suggests that the first and the second layers in the deep convolutional neural network trained by the learning of multiple tasks can extract general low level visual features.", " Non-invasive EEG signal based brain computer interface (BCI) for motor imagery task - classification requires large number of subject specific training samples for each user session that reduces the user feasibility of BCI. A generalized classifier using few subject specific sample will ease the real world implementation of motor imagery based BCI. At first, this paper applies an improved active transfer learning (ATL) on motor imagery based BCI. Then, it proposes a noble method of transferring selective instances (selected by few new subject specific data) from other subjects to new subject combining with selecting most informative subject specific data determined by active learning. Experimental results on BCI competition IV 2B dataset show that improved ATL works well on six out of nine subjects and proposed SIITAL method overcomes ATL limitation for other subjects. This means, it can achieve similar or better accuracy with a lower quantity of subject specific training data. Thus, it reduces the calibration effort.", " In order to better model complex real-world data and to develop robust features that capture relevant information, we usually employ unsupervised feature learning to learn a layer of features representations from unlabeled data. However, developing domain-specific features for each task is expensive, time-consuming and requires expertise of the data. In this paper, we introduce multi-instance clustering and graphical learning to unsupervised transfer learning. For a better clustering efficient, we proposed a set of algorithms on the application of traffic data learning, instance feature representation, distance calculation of multi-instance clustering, multi-instance graphical cluster initialisation, multi-instance multi-cluster update, and graphical multi-instance transfer clustering (GMITC). In the end of this paper, we examine the proposed algorithms on the Eastwest datasets by couples of baselines. The experiment results indicate that our proposed algorithms can get higher clustering accuracy and much higher programming speed.", "Transfer learning  survival analysis  regularization  regression  high-dimensional data survival analysis, the primary goal is to monitor several entities and model the occurrence of a particular event of interest. In such applications, it is quite often the case that the event of interest may not always be observed during the study period and this gives rise to the problem of censoring which cannot be easily handled in the standard regression approaches. In addition, obtaining sufficient labeled training instances for learning a robust prediction model is a very time consuming process and can be extremely difficult in practice. In this paper, we propose a transfer learning based Cox method, called Transfer-Cox, which uses auxiliary data to augment learning when there are insufficient amount of training examples. The proposed method aims to extract useful knowledge from the source domain and transfer it to the target domain, thus potentially improving the prediction performance in such timeto-event data. The proposed method uses the l(2,1)-normpenalty to encourage multiple predictors to share similar sparsity patterns, thus learns a shared representation across source and target domains, potentially improving the model performance on the target task. To speedup the computation, we apply the screening approach and extend the strong rule to sparse survival analysis models in multiple high-dimensional censored datasets. We demonstrate the performance of the proposed transfer learning method using several synthetic and high-dimensional microarray gene expression benchmark datasets and compare with other related competing state-of-the-art methods. Our results show that the proposed screening approach significantly improves the computational efficiency of the proposed algorithm without compromising the prediction performance. We also demonstrate the scalability of the proposed approach and show that the time taken to obtain the results is linear with respect to both the number of instances and features.", "Intelligent transportation systems  Deep learning  Spatio-temporal  Time series prediction  Convolutional neural network Traffic speed prediction is a long-standing and critically important topic in the area of Intelligent Transportation Systems (ITS). Recent years have witnessed the encouraging potentials of deep neural networks for real-life applications of various domains. Traffic speed prediction, however, is still in its initial stage without making full use of spatio-temporal traffic information. In light of this, in this paper, we propose a deep learning method with an Error-feedback Recurrent Convolutional Neural Network structure (eRCNN) for continuous traffic speed prediction. By integrating the spatio-temporal traffic speeds of contiguous road segments as an input matrix, eRCNN explicitly leverages the implicit correlations among nearby segments to improve the predictive accuracy. By further introducing separate error feedback neurons to the recurrent layer, eRCNN learns from prediction errors so as to meet predictive challenges rising from abrupt traffic events such as morning peaks and traffic accidents. Extensive experiments on real-life speed data of taxis running on the 2nd and 3rd ring roads of Beijing city demonstrate the strong predictive power of eRCNN in comparison to some state-of-the-art competitors. The necessity of weight pre-training using a transfer learning notion has also been testified. More interestingly, we design a novel influence function based on the deep learning model, and showcase how to leverage it to recognize the congestion sources of the ring roads in Beijing.", "Transfer Learning  Multi-Source Domain Adaptation  Constraint Clustering Domain adaptation has achieved promising results in many areas, such as image classification and object recognition. Although a lot of algorithms have been proposed to solve the task with different domain distributions, it remains a challenge for multi-source unsupervised domain adaptation. In addition, most of the existing algorithms learn a classifier on the source domain and predict the labels for the target data, which indicates that only the knowledge derived from the hyperplane is transferred to the target domain and the structure information is ignored. In light of this, we propose a novel algorithm for multi-source unsupervised domain adaptation. Generally speaking, we aim to preserve the whole structure from source domains and transfer it to serve the task on the target domain. The source and target data are put together for clustering, which simultaneously explores the structures of the source and target domains. The structure-preserved information from source domain further guides the clustering process on the target domain. Extensive experiments on two widely used databases on object recognition and face identification show the substantial improvement of our proposed approach over several state-of-the-art methods. Especially, our algorithm can take use of multi-source domains and achieve robust and better performance compared with the single source domain adaptation methods.", " Driven by recent developments in the area of Artificial Intelligence research, a promising new technology for building intelligent agents has evolved. The technology is termed Deep Reinforcement Learning (DRL) and combines the classic field of Reinforcement Learning (RL) with the representational power of modern Deep Learning approaches. DRL enables solutions for difficult and high dimensional tasks, such as Atari game playing, for which previously proposed RL methods were inadequate. However, these new solution approaches still take a long time to learn how to actuate in such domains and so far are mainly researched for single task scenarios. The ability to generalize gathered knowledge and transfer it to another task has been researched for classical RL, but remains an open problem for the DRL domain. Consequently, in this article we evaluate under which conditions the application of Transfer Learning (TL) to the DRL domain improves the learning of a new task. Our results indicate that TL can greatly accelerate DRL when transferring knowledge from similar tasks, and that the similarity between tasks plays a key role in the success or failure of knowledge transfer.", " This study investigates the impact of genotypic and behavioral diversity maintenance methods on controller evolution in multi-robot (RoboCup keep-away soccer) tasks. The focus is to examine the impact of these methods on the transfer learning of behaviors, first evolved in a source task before being transferred for further evolution in different but related target tasks. The goal is to ascertain an appropriate controller design (NE: Neuro-Evolution) method for facilitating improved effectiveness given policy transfer between source and target tasks. Effectiveness is defined as the average task performance of transferred behaviors. The study comparatively tests and evaluates the efficacy of coupling policy transfer with several NE variants. Results indicate a hybrid of behavioral diversity maintenance and objective-based search yields significantly improved effectiveness for evolved behaviors across increasingly complex target tasks. Results also highlight the efficacy of coupling policy transfer with the hybrid of behavioral diversity maintenance and objective based search in order to address bootstrapping and deception problems endemic to complex tasks.", "deep neural networks  transfer learning  retraining  drift detection  emotion recognition  on line facial expression analysis This paper presents a new methodology for detecting deterioration in performance of deep neural networks when applied to on line visual analysis problems and enabling fine-tuning, or retraining, of the network to the current data characteristics. Pre-trained deep neural networks which have a satisfactory performance on the problem under study constitute the basis of the approach, with efficient transfer learning being performed whenever drift is detected in network operation. The method is applied and validated on the problem of emotion detection using line facial expression analysis based on a dimensional emotion representation.", " In recent years, with the popularization of SNS, the incidents called flaming, in which a large number of negative comments are retweeted and spread to many followers on SNS, are increasing. Since a flaming event sometimes causes severe criticism by public people, it is becoming a great thread to companies and therefore it is important for companies to protect their reputation from such flaming events. In order to protect companies from serious damages in reputation, we propose a machine learning approach to the detection of flaming events by monitoring the sentiment polarity of SNS comments. From the nature of SNS comments such as the spread of a large number of retweets with the same content for a short time, the word distributions are often strongly biased and it leads to poor performance in sentiment polarity prediction. To alleviate this problem, we introduce transfer learning into the conventional Naive Bayes classifier. More concretely, in the Naive Bayes classifier, the occurrence probabilities of words on a target domain are recalculated using those on other domains, where a domain corresponds to a company to be protected. The experimental results demonstrate that the proposed transfer learning contribute to the improvement in the sentiment polarity prediction for SNS comments. In addition, we show that the proposed system can detect flaming events correctly by monitoring the number of negative comments.", " Most existing person re-identification (Re-ID) approaches follow a supervised learning framework, in which a large number of labelled matching pairs are required for training. This severely limits their scalability in real-world applications. To overcome this limitation, we develop a novel cross-dataset transfer learning approach to learn a discriminative representation. It is unsupervised in the sense that the target dataset is completely unlabelled. Specifically, we present an multi-task dictionary learning method which is able to learn a dataset-shared but target-data-biased representation. Experimental results on five benchmark datasets demonstrate that the method significantly outperforms the state-of-the-art.", " Deep learning techniques are renowned for supporting effective transfer learning. However, as we demonstrate, the transferred representations support only a few modes of separation and much of its dimensionality is unutilized. In this work, we suggest to learn, in the source domain, multiple orthogonal classifiers. We prove that this leads to a reduced rank representation, which, however, supports more discriminative directions. Interestingly, the softmax probabilities produced by the multiple classifiers are likely to be identical. Experimental results, on CIFAR-100 and LFW, further demonstrate the effectiveness of our method.", " Feature learning with deep models has achieved impressive results for both data representation and classification for various vision tasks. Deep feature learning, however, typically requires a large amount of training data, which may not be feasible for some application domains. Transfer learning can be one of the approaches to alleviate this problem by transferring data from data-rich source domain to data-scarce target domain. Existing transfer learning methods typically perform one-shot transfer learning and often ignore the specific properties that the transferred data must satisfy. To address these issues, we introduce a constrained deep transfer feature learning method to perform simultaneous transfer learning and feature learning by performing transfer learning in a progressively improving feature space iteratively in order to better narrow the gap between the target domain and the source domain for effective transfer of the data from source domain to target domain. Furthermore, we propose to exploit the target domain knowledge and incorporate such prior knowledge as constraint during transfer learning to ensure that the transferred data satisfies certain properties of the target domain. To demonstrate the effectiveness of the proposed constrained deep transfer feature learning method, we apply it to thermal feature learning for eye detection by transferring from the visible domain. We also applied the proposed method for cross-view facial expression recognition as a second application. The experimental results demonstrate the effectiveness of the proposed method for both applications.", " We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning  the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.", "convolutional neural network  digit classification  large data  universal OCR  transfer learning Convolutional Neural Networks (CNN) are on the forefront of accurate character recognition. This paper explores CNNs at their maximum capacity by implementing the use of large datasets. We show a near-perfect performance by using a dataset of about 820,000 real samples of isolated handwritten digits, much larger than the conventional MNIST database. In addition, we report a near-perfect performance on the recognition of machine-printed digits and multi-font digital born digits. Also, in order to progress toward a universal OCR, we propose methods of combining the datasets into one classifier. This paper reveals the effects of combining the datasets prior to training and the effects of transfer learning during training. The results of the proposed methods also show an almost perfect accuracy suggesting the ability of the network to generalize all forms of text.", "diabetic retinopathy recognition  hybrid color space  convolutional neural networks  LGI  CKML Net  VNXK  transfer learning Automatic diabetes retinopathy (DR) recognition can help DR carriers to receive treatment in early stages and avoid the risk of vision loss. In this paper, we emphasize the role of multiple filter sizes in learning fine-grained discriminant features and propose: (i) two deep convolutional neural networks - Combined Kernels with Multiple Losses Network (CKML Net) and VGGNet with Extra Kernel (VNXK), which are an improvement upon GoogLeNet and VGGNet in context of DR tasks. Learning from existing research, (ii) we propose a hybrid color space, LGI, for DR recognition via proposed nets. (iii) Transfer learning is applied to solve the challenge of imbalanced dataset. The effectiveness of proposed new nets and color space is evaluated using two grand challenge retina datasets: EyePACS and Messidor. Our experimental results show: (iv) CKML Net improves upon GoogLeNet and VNXK improves upon VGGNet on both datasets using the LGI color space. Additionally, proposed methodology improves upon other state of the art results on Messidor dataset for referable/non-referable screening.", "Transfer learning  Domain class imbalance  Traditional machine learning  Class imbalance A transfer learning environment is characterized by a machine learning algorithm being trained with data from one domain (the source domain) and being tested on data from a different domain (the target domain). In a transfer learning scenario, the class probability of the source domain may be different from the class probability of the target domain, which is referred to as domain class imbalance. Domain class imbalance is different from class imbalance. Class imbalance refers to the condition of a single domain having unequal class probabilities. In traditional machine learning, the training and testing data are drawn from a single domain. The effects of class imbalance in traditional machine learning are well studied  however, the issue of domain class imbalance in the field of transfer learning has received little research attention. This paper provides a comparative performance test of state-of-the-art transfer learning algorithms, using a wide-range of domain class imbalance combinations. A detailed discussion on the relative performances of the different algorithms, with statistical validation, is presented for the different domain class imbalance scenarios.", "Autoencoder  deep learning  long short-term memory  nontechnical loss  unsupervised learning  transfer learning Fraud detection in electricity consumption is a major challenge for power distribution companies. While many pattern recognition techniques have been applied to identify electricity theft, they often require extensive handcrafted feature engineering. Instead, through deep layers of transformation, nonlinearity, and abstraction, Deep Learning (DL) automatically extracts key features from data. In this paper, we design spatial and temporal deep learning solutions to identify nontechnical power losses (NTL), including Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM) and Stacked Autoencoder. These models are evaluated in a modified IEEE 123-bus test feeder. For the same tests, we also conduct comparison experiments using three conventional machine learning approaches: Random Forest, Decision Trees and shallow Neural Networks. Experimental results demonstrate that the spatiotemporal deep learning approaches outperform conventional machine learning approaches.", "transfer learning  imbalance  adaptive threshold Domain adaptation methods can be highly sensitive to class balance, particularly the usually unknown balance of the unlabeled test set. In this work, we analyze the effect of imbalance on a well-known algorithm, ARTL (Adaptation Regularization Transfer Learning) and propose four approaches for mitigating the adverse effects of imbalance. These include (1) balancing the training set for pseudo-label calculation, (2) applying adaptive thresholding to pseudo-label calculation, (3) using class reweighting in the optimization objective, and (4) applying adaptive thresholding to the output objective. We tested these methods with the UCI newsgroup dataset and on three types of imbalanced EEG (electroencephalogram) classification problems. We observed significant improvements, particularly for cases of extreme imbalance, which are not well addressed by standard classification techniques.", "Convolutional neural networks  Transfer learning  Celiac Disease  Automated diagnois  Endoscopy In this work, four well known convolutional neural networks (CNNs) that were pretrained on the ImageNet database are applied for the computer assisted diagnosis of celiac disease based on endoscopic images of the duodenum. The images are classified using three different transfer learning strategies and a experimental setup specifically adapted for the classification of endoscopic imagery. The CNNs are either used as fixed feature extractors without any fine-tuning to our endoscopic celiac disease image database or they are fine-tuned by training either all layers of the CNN or by fine-tuning only the fully connected layers. Classification is performed by the CNN SoftMax classifier as well as linear support vector machines. The CNN results are compared with the results of four state-of-the-art image representations. We will show that fine-tuning all the layers of the nets achieves the best results and outperforms the comparison approaches.", "3D ultrasound  First-trimester scan  Random Forests  Convolutional Neural Networks  Fetal plane localization In this paper, we present a fully automated machine-learning based solution to localize the fetus and extract the best fetal biometry planes for the head and abdomen from 11-13(+6days) week 3D fetal ultrasound (US) images. Our method to localize the whole fetus in the sagittal plane utilizes Structured Random Forests (SRFs) and classical Random Forests (RFs). A transfer learning Convolutional Neural Network (CNNs) is then applied to axial images to localize one of three classes (head, body and non-fetal). Finally, the best fetal head and abdomen planes are automatically extracted based on clinical knowledge of the position of the fetal biometry planes within the head and body. Our hybrid method achieves promising localization of the best biometry fetal planes with 1.6 mm and 3.4 mm for head and abdomen plane localization respectively compared to the best manually chosen biometry planes.", " Recent years have seen a growing interest in the use of deep neural networks as function approximators in reinforcement learning. In this paper, an experience replay method is proposed that ensures that the distribution of the experiences used for training is between that of the policy and a uniform distribution. Through experiments on a magnetic manipulation task it is shown that the method reduces the need for sustained exhaustive exploration during learning. This makes it attractive in scenarios where sustained exploration is in-feasible or undesirable, such as for physical systems like robots and for life long learning. The method is also shown to improve the generalization performance of the trained policy, which can make it attractive for transfer learning. Finally, for small experience databases the method performs favorably when compared to the recently proposed alternative of using the temporal difference error to determine the experience sample distribution, which makes it an attractive option for robots with limited memory capacity.", "Transfer Learning  Atanassov Intuitionistic fuzzy sets  negative transfer learning  Fuzzy domain adaption  Hausdorff Intuitionistic similarity metric  Yager-generating function Transfer learning framework is designed to use previously acquired knowledge to solve a new but somewhat related task (like humans do). Non-availability of sufficient and relevant information in building a learning model is a major bottleneck in this research area. However, such models are highly susceptible to negative transfer learning (NTL) during transferral of knowledge due to the hesitancy in the decision making. Negative transfer learning may cause chaotic learning and have a profound effect on their predictive precision. In this paper, we have proposed a novel Intuitionistic Fuzzy Domain Adaptation (IFDA) algorithm, which uses Yager-generating function over Atanassov's Intuitionistic fuzzy set theory in conjunction with modified Hausdorff Intuitionistic similarity metric to build a fuzzy domain adaptation algorithm which is independent of supervised machine learning technique. It exploits the hesitancy margin in intuitionistically fuzzified features by eradicating similar looking but useless instances. Therefore, it selects optimal source instances from a previous problem in bridging the knowledge gap, in order to solve a new target problem, by containing negative transfer learning.", "machine learning  cnn  k-nn  similarity search  transfer learning  fine-tuning  features  convolutional neural networks  digits  caffe The purpose of this work is to evaluate possible minimisation of the time needed for expansion of the scope of a CNN (convolutional neural network) classifier without the need to fully re-train it. We investigate the effects of applying k-NN (k-Nearest Neighbours) based classification and transfer learning (via fine-tuning) for the purpose of adding new classes to an existing deep convolutional neural network without the need to re-train it with the whole set of the existing plus the newly added classes. Our main contribution is the thorough comparison of the overall and per-class classification accuracy in the different scenarios. We use our own selection of ImageNet images for the CIFAR-10 classes plus four more purposefully selected classes. The motivation behind this investigation is the challenge for significant time reduction that would be possible in case the hypothesis that adding new classes to an existing classifier, using transfer learning and k-NN classification does not significantly underperform training with the whole expanded set of classes is true. Though this hypothesis is proved wrong it still sets the foundation for us and/or other researchers to go further into looking for strategies on how to do partial re-training.", "student classification  transfer learning  co-training  educational data mining  academic credit system Student classification is one of the popular educational data mining tasks to early predict in-trouble students in an educational system for appropriate and timely support. Besides, an academic credit system is nowadays widely-used all over the world due to its flexibility in order that teaching and learning activities can be efficiently conducted. Nevertheless, its flexibility might lead to the heterogeneity in educational data gathered in the system over time. Also, the changes in one educational program or the differences between the educational programs might hinder the exploitation of knowledge discovered in educational data. Indeed, a conventional student classification model built on one program can not be utilized for other programs. In this paper, our work proposes a novel approach to student classification in an academic credit system by combining transfer learning and co-training. A resulting model can predict a study status of a student enrolled in one educational program effectively by using a classification model enhanced by transfer learning and co-training techniques on educational data from another program. In addition, our approach can deal with the sparseness in educational data sets for early in-trouble student prediction. Experimental results on real data sets have shown that our approach could provide an effective solution to student classification in an academic credit system.", "Domain adaptation  transfer learning  covariate shift  risk minimization Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real-world problems show that FLDA performs on par with state-of-the-art domain adaptation techniques.", "soft biometrics  age classification  gender classification  convolutional neural networks  deep learning  transfer learning Age and gender are complementary soft biometric traits for face recognition. Successful estimation of age and gender from facial images taken under real-world conditions can contribute improving the identification results in the wild. In this study, in order to achieve robust age and gender classification in the wild, we have benefited from Deep Convolutional Neural Networks based representation. We have explored transferability of existing deep convolutional neural network (CNN) models for age and gender classification. The generic AlexNet-like architecture and domain specific VGG-Face CNN model are employed and fine-tuned with the Adience dataset prepared for age and gender classification in uncontrolled environments. In addition, task specific GilNet CNN model has also been utilized and used as a baseline method in order to compare with transferred models. Experimental results show that both transferred deep CNN models outperform the GilNet CNN model, which is the state-of-the-art age and gender classification approach on the Adience dataset, by an absolute increase of 7% and 4.5% in accuracy, respectively. This outcome indicates that transferring a deep CNN model can provide better classification performance than a task specific CNN model, which has a limited number of layers and trained from scratch using a limited amount of data as in the case of GilNet. Domain specific VGG-Face CNN model has been found to be more useful and provided better performance for both age and gender classification tasks, when compared with generic AlexNet-like model, which shows that transfering from a closer domain is more useful.", "Computer Vision  Deep Learning  Transfer Learning  Large Scale Learning A major barrier towards scaling visual recognition systems is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) trained used 1.2M+ labeled images have emerged as clear winners on object classification benchmarks. Unfortunately, only a small fraction of those labels are available with bounding box localization for training the detection task and even fewer pixel level annotations are available for semantic segmentation. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect scene-centric images with precisely localized labels. We develop methods for learning large scale recognition models which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. We provide a novel formulation of a joint multiple instance learning method that includes examples from object-centric data with image-level labels when available, and also performs domain transfer learning to improve the underlying detector representation. We then show how to use our large scale detectors to produce pixel level annotations. Using our method, we produce a >7.6K category detector and release code and models at lsda.berkeleyvision.org.", "learning-to-learn  multitask learning  representation learning  statistical learning theory  transfer learning We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.", " Reinforcement Learning (RL) is a successful technique for learning the solutions of control problems from an agent's interaction in its domain. However, RL is known to be inefficient for real-world applications. In this paper we propose to use a combination of case-based reasoning (CBR) and heuristically accelerated reinforcement learning methods aiming to speed up a Reinforcement Learning algorithm in a transfer learning problem. We show results of applying this method on a robot soccer domain, where the use of the proposed method led to a significant improvement in the learning rate.", "Memetic computation  Bi-level combinatorial optimization  Bi-level Vehicles routing problems Bi-level optimization problems are a class of challenging optimization problems, that contain two levels of optimization tasks. In these problems, the optimal solutions to the lower level problem become possible feasible candidates to the upper level problem. Such a requirement makes the optimization problem difficult to solve, and has kept the researchers busy towards devising methodologies, which can efficiently handle the problem. In recent decades, it is observed that many efficient optimizations using modern advanced EAs have been achieved via the incorporation of domain specific knowledge. In such a way, the embedment of domain knowledge about an underlying problem into the search algorithms can enhance properly the evolutionary search performance. Motivated by this issue, we present in this paper a Memetic Evolutionary Algorithm for Bi-level Combinatorial Optimization (M-CODBA) based on a new recently proposed CODBA algorithm with transfer learning to enhance future bi-level evolutionary search. A realization of the proposed scheme is investigated on the Bi-CVRP and Bi-MDVRP problems. The experimental studies on well established benchmarks are presented to assess and validate the benefits of incorporating knowledge memes on bi-level evolutionary search. Most notably, the results emphasize the advantage of our proposal over the original scheme and demonstrate its capability to accelerate the convergence of the algorithm.", " enetic programming (GP) is a well established evolutionary computation technique that automatically generates a computer program to solve a given problem. GP has been successfully used to solve optimization, symbolic regression and classification problems. Transfer learning in GP has been investigated to learn various Boolean and symbolic regression problems. However, there has been not much work on transfer learning in GP for image classification problems. In this paper, we propose a new technique to use transfer learning in GP to learn image classification problems. The developed method has been compared with the baseline GP method on three image classification benchmarks. The obtained results indicate that transfer learning has significantly improved the classification accuracy in learning various rotated and noisy versions of the tested image classification problems.", " \\Transfer learning is an important approach in machine learning, which aims to solve a problem by utilising the knowledge learnt from another problem domain. There has been extensive research and great achievement on transfer learning for image analysis and other tasks, but research on transfer learning in genetic programming (GP) for symbolic regression is still in the very early stage. However, GP has a natural way of expressing knowledge by trees or subtrees, which can be automatically discovered during the evolutionary process. An initial work on GP with transfer learning was proposed to transfer knowledge through best trees or subtrees from to source domain to facilitate the learning in the target domain. However, there are still a number of important issues remaining not investigated. This paper further investigates the ability of GP with transfer learning on different types of transfer scenarios, investigates the influence of a key parameter and the effect of transfer learning on the evolutionary training process, and also analyses how the knowledge learnt from the source domain was utilised during the learning process on the target domain. The results show that GP with transfer learning can generally perform well on different types of transfer scenarios. The transferred knowledge can provide a good initial population for the GP learning on the target domain, speed up the convergence, and help obtain better final solutions. However, the benefits of transfer learning varies in different scenarios.", " Finding the location of a mobile user is a classical and important problem in pervasive computing, because location provides a lot of information about the situation of a person from which adaptive computer systems can be created. While the inference of location outside buildings is possible with GPS or similar satellite systems, these are unavailable inside buildings. A large number of methods has been proposed to overcome this limitation and provide indoor location to mobile devices such as smartphones. With this paper, we propose a novel visual indoor positioning system DeepMoVIPS, which exploits the image classification power of deep convolutional neural networks for symbolic indoor geolocation. We further show, how to transfer visual features from deep learned networks to the application domain and give encouraging results of more than 95% classification accuracy for datasets modelling work environments using 16 rooms and evaluation over a time frame of four weeks.", "Audio-visual emotion recognition  Convolutional Neural Network  Long Short-Term Memory  Transfer learning This paper presents the techniques used in our contribution to Emotion Recognition in the Wild 2016's video based subchallenge. The purpose of the sub-challenge is to classify the six basic emotions (angry, sad, happy, surprise, fear & disgust) and neutral. Compared to earlier years' movie based datasets, this year's test dataset introduced reality TV videos containing more spontaneous emotion. Our proposed solution is the fusion of facial expression recognition and audio emotion recognition subsystems at score level. For facial emotion recognition, starting from a network pre-trained on ImageNet training data, a deep Convolutional Neural Network is fine-tuned on FER2013 training data for feature extraction. The classifiers, i.e., kernel SVM, logistic regression and partial least squares are studied for comparison. An optimal fusion of classifiers learned from different kernels is carried out at the score level to improve system performance. For audio emotion recognition, a deep Long ShortTerm Memory Recurrent Neural Network (LSTM-RNN) is trained directly using the challenge dataset. Experimental results show that both subsystems individually and as a whole can achieve state-of-the art performance. The overall accuracy of the proposed approach on the challenge test dataset is 53.9%, which is better than the challenge baseline of 40.47%", "Image super resolution  Transfer learning  Example learning-based Example learning-based super-resolution (SR) methods are effective to generate a high-resolution (HR) image from a single low-resolution (LR) input. And these SR methods have shown a great potential for many practical applications. Unfortunately, most of popular example learning-based approaches extract features from limited training images. These training images are insufficient for super resolution task. Our work is to transfer some supplemental information from other domains. Therefore, in this paper, a new algorithm Transfer Learning based on A+ (TLA) is proposed for image super-resolution task. First, we transfer supplemental information from other datasets to construct a new dictionary. Then, in sample selection, more training samples are supplemented to the basic training samples. In experiments, we seek to explore what types of images can provide more appropriate information for super-resolution task. Experimental results indicate that our approach is superior to A+ when transferring images containing similar content with original data.", "convolutional neural network  transfer learning  historical Chinese character recognition Historical Chinese character recognition has been suffering from the problem of lacking sufficient labeled training samples. A transfer learning method based on Convolutional Neural Network (CNN) for historical Chinese character recognition is proposed in this paper. A CNN model L is trained by printed Chinese character samples in the source domain. The network structure and weights of model L are used to initialize another CNN model T, which is regarded as the feature extractor and classifier in the target domain. The model T is then fine-tuned by a few labeled historical or handwritten Chinese character samples, and used for final evaluation in the target domain. Several experiments regarding essential factors of the CNN-based transfer learning method are conducted, showing that the proposed method is effective.", "Process-oriented case-based reasoning  Transfer learning  CRM application This paper studies the feasibility of using transfer learning for process-oriented case-based reasoning. The work introduces a novel approach to transfer workflow cases from a loosely related source domain to a target domain. The idea is to develop a representation mapper based on workflow generalization, workflow abstraction, and structural analogy between the domain vocabularies. The approach is illustrated by a pair of sample domains in two sub-fields of customer relationship management that have similar process objectives but different tasks and data to fulfill them. An experiment with expert ratings of transferred cases is conducted to test the feasibility of the approach with promising results for workflow modeling support.", "Cross-database facial expression recognition  Domain adapatation  Transfer learning  Dictionary learning Dictionary learning based methods have achieved state-of-the-art performance in the task of conventional facial expression recognition (FER), where the distributions between training and testing data are implicitly assumed to be matched. But in the practical scenes this assumption is usually broken, especially when testing samples and training samples come from different databases, a.k.a. the cross-database FER problem. To address this problem, we propose a novel method called unsupervised domain adaptive dictionary learning (UDADL) to deal with the unsupervised case that all samples in target database are completely unlabeled. In UDADL, to obtain more robust representations of facial expressions and to reduce the time complexity in training and testing phases, we introduce a dual dictionary pair consisting of a synthesis one and an analysis one to mutually bridge the samples and their codes. Meanwhile, to relieve the distribution disparity of source and target samples, we further integrate the learning of unlabeled testing data into UDADL to adaptively adjust the misaligned distribution in an embedded space, where geometric structures of both domains are also encourage to be preserved. The UDADL model can be solved by an iterate optimization strategy with each sub-optimization in a closed analytic form. The extensive experiments on Multi-PIE and BU-3DFE databases demonstrate that the proposed UDADL is superior over most widely-used domain adaptation methods in dealing with cross-database FER, and achieves the state-of-the-art performance.", "Land-cover Classification  Remote Sensing  Domain Adaptation  Ensemble of Multilayer Perceptron  Transfer Learning Domain Adaptation (DA) for remotely sensed images using ensemble of Multilayer Perceptrons (MLP) is presented in this article. Using the labelled information from a source domain, the proposed method utilises the disagreement among the MLPs to figure out the 'most informative' patterns from the target domain using transfer learning. These selected patterns are then labelled by human expert and are used for training the ensemble. Finally the land-cover classes for a target region are predicted by applying a non-trainable majority voting rule among different MLPs in ensemble. Experiments have been conducted on multispectral images for two different regions in India obtained from Landsat-8 satellite and the proposed DA method outperforms the corresponding random sampling approach.", "Convolutional network  Transfer learning  Face clustering  Face recognition Clustering faces in movies or videos is extremely challenging since characters' appearance can vary drastically under different scenes. In addition, the various cinematic styles make it difficult to learn a universal face representation for all videos. Unlike previous methods that assume fixed handcrafted features for face clustering, in this work, we formulate a joint face representation adaptation and clustering approach in a deep learning framework. The proposed method allows face representation to gradually adapt from an external source domain to a target video domain. The adaptation of deep representation is achieved without any strong supervision but through iteratively discovered weak pairwise identity constraints derived from potentially noisy face clustering result. Experiments on three benchmark video datasets demonstrate that our approach generates character clusters with high purity compared to existing video face clustering methods, which are either based on deep face representation (without adaptation) or carefully engineered features.", "Domain adaptation  Object recognition  Deep learning  Convolutional networks  Transfer learning In this paper, we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. Specifically, we design a new model called Deep Reconstruction-Classification Network (DRCN), which jointly learns a shared encoding representation for two tasks: (i) supervised classification of labeled source data, and (ii) unsupervised reconstruction of unlabeled target data. In this way, the learnt representation not only preserves discriminability, but also encodes useful information from the target domain. Our new DRCN model can be optimized by using backpropagation similarly as the standard neural networks. We evaluate the performance of DRCN on a series of cross-domain object recognition tasks, where DRCN provides a considerable improvement (up to similar to 8% in accuracy) over the prior state-of-the-art algorithms. Interestingly, we also observe that the reconstruction pipeline of DRCN transforms images from the source domain into images whose appearance resembles the target dataset. This suggests that DRCN's performance is due to constructing a single composite representation that encodes information about both the structure of target images and the classification of source images. Finally, we provide a formal analysis to justify the algorithm's objective in domain adaptation context.", "Convolutional neural networks  Transfer learning  Multitask learning  Deep learning  Visual recognition When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning as standard practice for improved new task performance.", "Transfer learning  Domain adaptation  Neural Decision Forest  Neural network Heterogeneous domain adaptation (HDA) addresses the task of associating data not only across dissimilar domains but also described by different types of features. Inspired by the recent advances of neural networks and deep learning, we propose Transfer Neural Trees (TNT) which jointly solves cross-domain feature mapping, adaptation, and classification in a NN-based architecture. As the prediction layer in TNT, we further propose Transfer Neural Decision Forest (Transfer-NDF), which effectively adapts the neurons in TNT for adaptation by stochastic pruning. Moreover, to address semi-supervised HDA, a unique embedding loss term for preserving prediction and structural consistency between target-domain data is introduced into TNT. Experiments on classification tasks across features, datasets, and modalities successfully verify the effectiveness of our TNT.", "Small sample learning  Transfer learning  Object recognition  Model transformation  Deep regression networks We develop a conceptually simple but powerful approach that can learn novel categories from few annotated examples. In this approach, the experience with already learned categories is used to facilitate the learning of novel classes. Our insight is two-fold: (1) there exists a generic, category agnostic transformation from models learned from few samples to models learned from large enough sample sets, and (2) such a transformation could be effectively learned by high-capacity regressors. In particular, we automatically learn the transformation with a deep model regression network on a large collection of model pairs. Experiments demonstrate that encoding this transformation as prior knowledge greatly facilitates the recognition in the small sample size regime on a broad range of tasks, including domain adaptation, fine-grained recognition, action recognition, and scene classification.", "Action recognition  Image annotation  Fisher vectors  Recurrent Neural Networks Recurrent Neural Networks (RNNs) have had considerable success in classifying and predicting sequences. We demonstrate that RNNs can be effectively used in order to encode sequences and provide effective representations. The methodology we use is based on Fisher Vectors, where the RNNs are the generative probabilistic models and the partial derivatives are computed using backpropagation. State of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. We also show a surprising transfer learning result from the task of image annotation to the task of video action recognition.", " In this paper, we study online heterogeneous transfer learning (HTL) problems where offline labeled data from a source domain is transferred to enhance the online classification performance in a target domain. The main idea of our proposed algorithm is to build an offline classifier based on heterogeneous similarity constructed by using labeled data from a source domain and unlabeled co-occurrence data which can be easily collected from web pages and social networks. We also construct an online classifier based on data from a target domain, and combine the offline and online classifiers by using the Hedge weighting strategy to update their weights for ensemble prediction. The theoretical analysis of error bound of the proposed algorithm is provided. Experiments on a real-world data set demonstrate the effectiveness of the proposed algorithm.", "CNN  Multi-scale  Deep learning  VLAD  Transfer learning Global convolutional neural networks (CNNs) activations lack geometric invariance, and in order to address this problem, Gong et al. proposed multi-scale orderless pooling(MOP-CNN), which extracts CNN activations for local patches at multiple scale levels, and performs orderless VLAD pooling to extract features. However, we find that this method can improve the performance mainly because it extracts global and local representation simultaneously, and VLAD pooling is not necessary as the representations extracted by CNN is good enough for classification. In this paper, we propose a new method to extract multi-scale features of CNNs, leading to a new structure of deep learning. The method extracts CNN representations for local patches at multiple scale levels, then concatenates all the representations at each level separately, finally, concatenates the results of all levels. The CNN is trained on the ImageNet dataset to extract features and it is then transferred to other datasets. The experimental results obtained on the databases MITIndoor and Caltech-101 show that the performance of our proposed method is superior to the MOP-CNN.", "Food classification  Convolutional neural network  Deep learning  Transfer learning Automatic classification of foods is a challenging problem. Results on ImageNet dataset shows that ConvNets are very powerful in modeling natural objects. Nonetheless, it is not trivial to train a ConvNet from scratch for classification of foods. This is due to the fact that ConvNets require large datasets and to our knowledge there is not a large public dataset of foods for this purpose. An alternative solution is to transfer knowledge from already trained ConvNets. In this work, we study how transferable are state-of-art ConvNets to classification of foods. We also propose a method for transferring knowledge from a bigger ConvNet to a smaller ConvNet without decreasing the accuracy. Our experiments on UECFood256 dataset show that state-of-art networks produce comparable results if we start transferring knowledge from an appropriate layer. In addition, we show that our method is able to effectively transfer knowledge to a smaller ConvNet using unlabeled samples.", " In histopathological image assessment, there is a high demand to obtain fast and precise quantification automatically. Such automation could be beneficial to find clinical assessment clues to produce correct diagnoses, to reduce observer variability, and to increase objectivity. Due to its success in other areas, deep learning could be the key method to obtain clinical acceptance. However, the major bottleneck is how to train a deep CNN model with a limited amount of training data. There is one important question of critical importance: Could it be possible to use transfer learning and fine-tuning in biomedical image analysis to reduce the effort of manual data labeling and still obtain a full deep representation for the target task? In this study, we address this question quantitatively by comparing the performances of transfer learning and learning from scratch for cell nuclei classification. We evaluate four different CNN architectures trained on natural images and facial images.", " Machine learning processes consist in collecting data, obtaining a model and applying it to a given task. Given a new task, the standard approach is to restart the learning process and obtain a new model. However, previous learning experience can be exploited to assist the new learning process. The two most studied approaches for this are meta-learning and transfer learning. Metalearning can be used for selecting the predictive model to use on a new dataset. Transfer learning allows the reuse of knowledge from previous tasks. However, when multiple heterogeneous tasks are available as potential sources for transfer, the question is which one to use. One approach to address this problem is metalearning. In this paper we investigate the feasibility of this approach. We propose a method to transfer weights from a source trained neural network to initialize a network that models a potentially very different target dataset. Our experiments with 14 datasets indicate that this method enables faster convergence without significant difference in accuracy provided that the source task is adequately chosen. This means that there is potential for applying metalearning to support transfer between heterogeneous datasets.", " Bayesian optimisation is an efficient technique to optimise functions that are expensive to compute. In this paper, we propose a novel framework to transfer knowledge from a completed source optimisation task to a new target task in order to overcome the cold start problem. We model source data as noisy observations of the target function. The level of noise is computed from the data in a Bayesian setting. This enables flexible knowledge transfer across tasks with differing relatedness, addressing a limitation of the existing methods. We evaluate on the task of tuning hyperparameters of two machine learning algorithms. Treating a fraction of the whole training data as source and the whole as the target task, we show that our method finds the best hyperparameters in the least amount of time compared to both the state-of-art and no transfer method.", "Genetic programming  Transfer learning  Building blocks  Code fragments  Texture image classification Transfer learning is a process to transfer knowledge learned in one or more source tasks to a related but more complex, unseen target task, in an effort to facilitate learning in the target task. Genetic programming (GP) is an evolutionary approach to generating computer programs for solving a given problem automatically. Transfer learning in GP has been investigated in complex Boolean and symbolic regression problems, but not much in image classification. In this paper, we propose a novel approach to use transfer learning in GP for image classification problems. Specifically, the proposed novel approach extends an existing state-of-the-art GP method by incorporating the ability to extract useful knowledge from simpler problems of a domain and reuse the extracted knowledge to solve complex problems of the domain. The proposed system has been compared with the baseline system (i.e., GP without using transfer learning) on multi-class texture classification problems from three widely-used texture datasets with different rotations and different levels of noise. The experimental results showed that the ability to reuse the extracted knowledge in the proposed GP method helps achieve better classification accuracy than the baseline GP method.", "Reciprocal recommender system  Transfer learning This paper tackles the reciprocal recommendation task which has various applications such as online dating, employee recruitment and mentor-mentee matching. The major difference between traditional recommender systems and reciprocal recommender systems is that a reciprocal recommender has to satisfy the preference on both directions. This paper proposes a simple yet novel regularization term, the MutualAttraction Indicator, to model the mutual preferences of both parties. Given such indicator, we design a transfer-learning based CF model for reciprocal recommender. The experiments are based on two real world tasks, online dating and human resource matching, showing significantly improved performance over the original factorization model and stateof-the-art reciprocal recommenders.", " This paper focuses on the problem of explaining predictions of psychological attributes such as attractiveness, happiness, confidence and intelligence from face photographs using deep neural networks. Since psychological attribute datasets typically suffer from small sample sizes, we apply transfer learning with two base models to avoid overfitting. These models were trained on an age and gender prediction task, respectively. Using a novel explanation method we extract heatmaps that highlight the parts of the image most responsible for the prediction. We further observe that the explanation method provides important insights into the nature of features of the base model, which allow one to assess the aptitude of the base model for a given transfer learning task. Finally, we observe that the multiclass model is more feature rich than its binary counterpart. The experimental evaluation is performed on the 2222 images from the 10k US faces dataset containing psychological attribute labels as well as on a subset of KDEF images.", "Deep learning  Convolutional neural networks  Transfer learning  Learning curves  AlexNet In this paper we study the effect of target set size on transfer learning in deep learning convolutional neural networks. This is an important problem as labelling is a costly task, or for new or specific classes the number of labelled instances available may simply be too small. We present results for a series of experiments where we either train on a target of classes from scratch, retrain all layers, or subsequently lock more layers in the network, for the Tiny-ImageNet and MiniPlaces2 data sets. Our findings indicate that for smaller target data sets freezing the weights for the initial layers of the network gives better results on the target set classes. We present a simple and easy to implement training heuristic based on these findings.", "Caption Generation  CNN  LSTM  Fisher Kernel  Transfer Learning We propose a method to train image representation to improve the performance of image Caption Generation. We use transfer learning CNN on sentences, and extract image representation with deep Fisher kernel. With this representation, we generate sentence with gLSTM and show the improvements in performance. In the experiment, we validate each steps of our method by comparing with previous state-of-the-art models.", "Bearing  Local mode decomposition  Transfer learning strategy  Principal components analysis A novel method to solve the bearing fault recognition problem is proposed, which is based on local mode decomposition (LMD) to extract the characteristic features and the transfer learning strategy optimized support vector machine (TLSSVM) to achieve the fault state classification. Firstly, the gathered vibration signals were decomposed by the LMD to obtain the corresponding product functions (PFs). The morphological spectrums of the chosen PFs that include typical feature information are calculated and defined as the characteristic features. However, the extracted features remained high-dimensional, and excessive redundant information still existed. So, the principal components analysis (PCA) is used to extract the characteristic features to extract the characteristic features and reduce the dimension. The characteristic features are input into the SVM model, in order to get a higher accuracy of the diagnosis accuracy, because of the fuzzy features, the transfer learning strategy is used to optimized the SVM model, and the optimized model is used to construct the fault state identification model, the bearing fault state identification is realized. The fault states of a bearing normal and several inner races with different degree of fault were recognized, the results validate the effectiveness of the proposed algorithm.", " In this work we have investigated face verification based on deep representations from Convolutional Neural Networks (CNNs) to find an accurate and compact face descriptor trained only on a restricted amount of face image data. Transfer learning by fine-tuning CNNs pre-trained on large-scale object recognition has been shown to be a suitable approach to counter a limited amount of target domain data. Using model compression we reduced the model complexity without significant loss in accuracy and made the feature extraction more feasible for real-time use and deployment on embedded systems and mobile devices. The compression resulted in a 9-fold reduction in number of parameters and a 5-fold speed-up in the average feature extraction time running on a desktop CPU. With continued training of the compressed model using a Siamese Network setup, it outperformed the larger model.", " In this article, we propose a transfer learning method for deep neural networks (DNNs). Deep learning has been widely used in many applications. However, applying deep learning is problematic when a large amount of training data are not available. One of the conventional methods for solving this problem is transfer learning for DNNs. In the field of image recognition, state-of-the-art transfer learning methods for DNNs re-use parameters trained on source domain data except for the output layer. However, this method may result in poor classification performance when the amount of target domain data is significantly small. To address this problem, we propose a method called All-Transfer Deep Learning, which enables the transfer of all parameters of a DNN. With this method, we can compute the relationship between the source and target labels by the source domain knowledge. We applied our method to actual two-dimensional electrophoresis image (TDEI) classification for determining if an individual suffers from sepsis  the first attempt to apply a classification approach to TDEIs for proteomics, which has attracted considerable attention as an extension beyond genomics. The results suggest that our proposed method outperforms conventional transfer learning methods for DNNs.", " Automatic short answer grading (ASAG) is the task of automatically grading students answers which are a few words to a few sentences long. While supervised machine learning techniques (classification, regression) have been successfully applied for ASAG, they suffer from the constant need of instructor graded answers as labelled data. In this paper, we propose a transfer learning based technique for ASAG built on an ensemble of text classifier of student answers and a classifier using numeric features derived from various similarity measures with respect to instructor provided model answers. We present preliminary empirical results to demonstrate efficacy of the proposed technique.", "Cognitive Radio Network (CRN)  Cognitive Spectrum Decision (CSD)  Raptor codes  Machine learning  Transfer Actor-Critic Learning (TACT) In this research, we propose cognitive spectrum decision model comprised of spectrum adaptation (via Raptor codes) and spectrum handoff (via transfer learning) in Cognitive Radio Networks(CRN), in order to enhance the spectrum efficiency in multimedia communications. Raptor code enables the Secondary User (SU) to adapt to the dynamic channel conditions and maintain the Quality of Service (QoS) by prioritizing the data packets and learning the distribution of symbols transmission strategy called decoding-CDF through the history of symbol transmissions. Our scheme optimizes the acknowledgement (ACK) reception strategy in multimedia communications, and eventually increases the spectrum decision accuracy and allows the SUs to adapt to the channel variations. Moreover, to enhance spectrum decision in a long term process, we use TransferActor Critic Learning (TACT) model to allow the newly joined SU in a network to learn the spectrum decision strategies from historical spectrum decisions of the existing ` expert' SUs. Experimental results showthat our proposed modelworks better than themyopic spectrum decision which chooses the spectrum decision actions based on just short-term maximum immediate reward.", " This paper proposes a novel deep learning architecture for person re-identification. The proposed network is based on a coarse-to-fine learning (CFL) approach, attempting to acquire a generic-to-specific knowledge throughout a transfer learning process. The core of the method relies on a hybrid network composed of a convolutional neural network and a deep belief network denoising autoenconder. This hybrid network is in charge of extracting features invariant to illumination varying, certain image deformations, horizontal mirroring and image blurring, and is embedded in the CFL architecture. The proposed network achieved the best results when compared with other state-of-the-arts methods on i-LIDS, CUHK01 and CUHK03 data sets, and also a competitive performance on VIPeR data set.", "Recommender system  Quantity and quality  Pre-processing  Transfer learning Recommender system has become one of the most popular techniques to cope with the information overload problem. In the past years many algorithms have been proposed to obtain accurate recommendations. Such methods usually put all the collected user data into learning models without a careful consideration of the quantity and quality of individual user feedbacks. Yet in real applications, different types of users tend to represent preferences and opinions in various ways, thus resulting in user data with radically diverse quantity and quality. This characteristic of data influences the performance of recommendations. However, little attention has been devoted to the management of quantity and quality for user data in recommender systems. In this paper, we propose a generic framework to seamlessly exploit different pre-processing and recommendation approaches for ratings of different users. More specifically, we first classify users into groups based on the quantity and quality of their behavior data. In order to handle the user groups diversely, we further propose several data pre-processing strategies. Subsequently, we present a novel transfer latent factor model (TLMF) to transfer learnt models between groups. Finally, we conduct extensive experiments on a large data set and demonstrate the effectiveness of our proposed framework.", "3D object recognition  Transfer learning  Convolutional neural networks RGB-D data is getting ever more interest from the research community as both cheap cameras appear in the market and the applications of this type of data become more common. A current trend in processing image data is the use of convolutional neural networks (CNNs) that have consistently beat competition in most benchmark data sets. In this paper, we investigate the possibility of transferring knowledge between CNNs when processing RGB-D data with the goal of both improving accuracy and reducing training time. We present experiments that show that our proposed approach can achieve both these goals.", "Transfer learning  Reinforcement learning  Heterogeneous agents  Multi-agent system This paper presents a framework, called the knowledge co-creation framework (KCF), for the heterogeneous multi-robot transfer learning method with utilization of cloud-computing resources. A multi-agent robot system (MARS) that utilizes reinforcement learning and transfer learning methods has recently been deployed in real-world situations. In MARS, autonomous agents obtain behavior autonomously through multi-agent reinforcement learning and the transfer learning method enables the reuse of the knowledge of other robots' behavior, such as for cooperative behavior. These methods, however, have not been fully and systematically discussed. To address this, KCF leverages the transfer learning method and cloud-computing resources. In prior research, we developed a hierarchical transfer learning (HTL) method as the core technology of knowledge co-creation and investigated its effectiveness in a dynamic multi-agent environment. The HTL method hierarchically abstracts obtained knowledge by ontological methods. Here, we evaluate the effectiveness of HTL with two types of ontology: action and state.", " This paper addresses an open challenge in educational data mining, i.e., the problem of automatically mapping online courses from different providers (universities, MOOCs, etc.) onto a universal space of concepts, and predicting latent prerequisite dependencies (directed links) among both concepts and courses. We propose a novel approach for inference within and across course-level and concept-level directed graphs. In the training phase, our system projects partially observed course-level prerequisite links onto directed concept-level links  in the testing phase, the induced concept-level links are used to infer the unknown course-level prerequisite links. Whereas courses may be specific to one institution, concepts are shared across different providers. The bi-directional mappings enable our system to perform interlingua-style transfer learning, e.g. treating the concept graph as the interlingua and transferring the prerequisite relations across universities via the interlingua. Experiments on our newly collected datasets of courses from MIT, Caltech, Princeton and CMU show promising results.", "Real-time  Robot partner  Face recognition  Probabilistic  Transfer learning Face recognition is an integral part in robot partner's interaction with its human subject. With the task expected, open universe scenario face recognition needs to be performed under uncontrolled and uncooperative environment in real-time without imposing strain on its low powered processors. With the concept of informationally structured space where additional samples can contribute to constructing the prior for recognition, joint probabilistic face method is used for its accuracy, simplicity and transparency in data handling. A system is built upon the method that enables transfer learning from across different domains and real-time adaptive learning for continual information collection. Test result shows that the approach can perform well compared to other established methods under low number of prototype number. Test result on adaptive learning sheds light to the direction of future design. ", "Cross-domain clustering  maximum entropy clustering (MEC)  privacy protection  transfer learning  validity index The classical maximum entropy clustering (MEC) algorithm usually cannot achieve satisfactory results in the situations where the data is insufficient, incomplete, or distorted. To address this problem, inspired by transfer learning, the specific cluster prototypes and fuzzy memberships jointly leveraged (CPM-JL) framework for cross-domain MEC (CDMEC) is firstly devised in this paper, and then the corresponding algorithm referred to as CPM-JL-CDMEC and the dedicated validity index named fuzzy memberships-based cross-domain difference measurement (FM-CDDM) are concurrently proposed. In general, the contributions of this paper are fourfold: 1) benefiting from the delicate CPM-JL framework, CPM-JL-CDMEC features high-clustering effectiveness and robustness even in some complex data situations  2) the reliability of FM-CDDM has been demonstrated to be close to well-established external criteria, e.g., normalized mutual information and rand index, and it does not require additional label information. Hence, using FM-CDDM as a dedicated validity index significantly enhances the applicability of CPM-JL-CDMEC under realistic scenarios  3) the performance of CPM-JL-CDMEC is generally better than, at least equal to, that of MEC because CPM-JL-CDMEC can degenerate into the standard MEC algorithm after adopting the proper parameters, and which avoids the issue of negative transfer  and 4) in order to maximize privacy protection, CPM-JL-CDMEC employs the known cluster prototypes and their associated fuzzy memberships rather than the raw data in the source domain as prior knowledge. The experimental studies thoroughly evaluated and demonstrated these advantages on both synthetic and real-life transfer datasets.", "Object recognition  detection  semantic segmentation  convolutional networks  deep learning  transfer learning Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at https://www.cs.berkeley.edu/similar to rbg/rcnn.", " In multi-step learning, where a final learning task is accomplished via a sequence of intermediate learning tasks, the intuition is that successive steps or levels transform the initial data into representations more and more suited to the final learning task. A related principle arises in transfer-learning where Baxter (2000) proposed a theoretical framework to study how learning multiple tasks transforms the inductive bias of a learner. The most widespread multi-step learning approach is semi-supervised learning with two steps: unsupervised, then supervised. Several authors (Castelli-Cover, 1996  Balcan-Blum, 2005  Niyogi, 2008  Ben-David et al, 2008  Urner et al, 2011) have analyzed SSL, with Balcan-Blum (2005) proposing a version of the PAC learning framework augmented by a compatibility function to link concept class and unlabeled data distribution. We propose to analyze SSL and other multi-step learning approaches, much in the spirit of Baxter's framework, by defining a learning problem generatively as a joint statistical model on X.Y. This determines in a natural way the class of conditional distributions that are possible with each marginal, and amounts to an abstract form of compatibility function. It also allows to analyze both discrete and non-discrete settings. As tool for our analysis, we define a notion of gamma-uniform shattering for statistical models. We use this to give conditions on the marginal and conditional models which imply an advantage for multi-step learning approaches. In particular, we recover a more general version of a result of Poggio et al (2012): under mild hypotheses a multi-step approach which learns features invariant under successive factors of a finite group of invariances has sample complexity requirements that are additive rather than multiplicative in the size of the subgroups.", " Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.", " We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability and parameter transfer learnability of parametric feature mapping, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning.", "Transfer learning  Image classification  Kernel sparse coding  Label consistency When there are a few labeled images, the classifier trained performs poorly even we use sparse coding technique to process image features. So we utilize other data from related domains as source data to help classification tasks. In this paper, we propose a Supervised Transfer Kernel Sparse Coding (STKSC) algorithm to construct discriminative sparse representations for cross domain image classification tasks. Specifically, we map source and target data into a high dimensional feature space by using kernel trick, hence capturing the nonlinear image features. In order to make the sparse representations robust to the domain mismatch, we incorporate the Maximum Mean Discrepancy (MMD) criterion into the objective function of kernel sparse coding. We also use label information to learn more discriminative sparse representations. Furthermore, we provide a unified framework to learn the dictionary and the discriminative sparse representations, which can be further used for classification. The experiment results validate that our method outperforms many state-of-art methods. ", "Artificial intelligence  Machine learning  Support vector machines  Transfer learning  Parameter setting Machine Learning algorithms have a broad applicability, although generally a huge effort is necessary to find a good configuration for a given task. The tuning of free parameters, for example, is a task that directly affects the algorithm's performance but is often carried out as an ad hoc process. An alternative approach is to define the problem as a search in the parameters space, which can be computationally expensive and slow. Furthermore, to apply an algorithm to a different problem, all the work must be done from scratch. Transfer learning can be used to avoid this rework. In this paper we propose an approach to tune the parameter by means of transfer learning. The idea is to use data complexity characterization measures to evaluate the similarity among datasets and evaluate whether they share similar configurations of good parameters. To compare our approach, four performance measures were used: area under ROC Curve (AUC), accuracy, F1 and area under Precision-Recall Curve. Results show that the proposed approach may reduce the search space for the parameter tuning by exploiting parameter recommendation of similar datasets and provide competitive performance compared to other widely used techniques.", "Region classification  conformal framework  instance-transfer learning Conformal region classification focuses on developing region classifiers  i.e., classifiers that output regions (sets) of classes for new test instances. 2,13,16 Conformal region classifiers have been proven to be valid for any significance level epsilon is an element of [0, 1] in the sense that the probability the class regions do not contain the true instances' classes does not exceed e. In practice, however, conformal region classifiers need to be also efficient  i.e., they have to output non-empty and relatively small class regions. In this paper we show that conformal region classification can benefit from instance-transfer learning. Our new approach consists of the basic conformal region classifier with a nonconformity function that implements instance transfer. We propose to learn such a function using a new multi-class Transfer AdaBoost. M1 algorithm. The function and its relation to the conformal region classification are theoretically justified. The experiments showed that our approach is valid for any significance level epsilon is an element of [0, 1] and its efficiency can be improved with instance transfer.", "Classification  domain adaptation  fuzzy set-based approach  fuzzy similarity  transfer learning Machine learning plays an important role in data classification and data-based prediction. In some real-world applications, however, the training data (coming from the source domain) and test data (from the target domain) come from different domains or time periods, and this may result in the different distributions of some features. Moreover, the values of the features and/or labels of the datasets might be nonnumeric and involve vague values. Traditional learning-based prediction and classification methods cannot handle these two issues. In this study, we propose a multistep fuzzy bridged refinement domain adaptation algorithm, which offers an effective way to deal with both issues. It utilizes a concept of similarity to modify the labels of the target instances that were initially predicted by a shift-unaware model. It then refines the labels using instances that are most similar to a given target instance. These instances are extracted from mixture domains composed of source and target domains. The proposed algorithm is built on a basis of some data and refines the labels, thus performing completely independently of the shift-unaware prediction model. The algorithm uses a fuzzy set-based approach to deal with the vague values of the features and labels. Four different datasets are used in the experiments to validate the proposed algorithm. The results, which are compared with those generated by the existing domain adaptation methods, demonstrate a significant improvement in prediction accuracy in both the above-mentioned datasets.", "Transfer learning  Non-negative matrix tri-factorization  Quadruple Transfer Learning  Ambiguous concept Transfer learning focuses on leveraging the knowledge in source domains to complete the learning tasks in target domains, where the data distributions of the source and target domains are related but different in accordance with original features. To tackle the challenge of different data distributions, previous methods mine the high-level concepts (e.g., feature clusters) from original features, which shows to be suitable for the classification. The general strategies of the previous approaches are to utilize the identical concepts, the synonymous concepts or both of them as shared concepts to establish the bridge between the source and target domains. Besides the shared concepts, some methods use the different concepts for training model. Specifically, these methods assume that the identical concepts (e.g., feature clusters) in different domains can be mapped to the same example classes. However, some ambiguous concepts may exist in different domains and result in misleading classification in the target domains. Therefore, we need a general transfer learning framework, which can exploit four kinds of concepts including the identical concepts, the synonymous concepts, the different concepts and the ambiguous concepts simultaneously, for cross-domain classification. In this paper, we present a novel method, Quadruple Transfer Learning (QTL), which models these four kinds of concepts together to fit different situations on the data distributions. In addition, an iterative algorithm with convergence guarantee based on non-negative matrix tri-factorization techniques is presented to solve the optimization problem. Finally, systematic experiments demonstrate that QTL is more effective than all the compared baselines. ", "Text-to-image  heterogeneous transfer learning  robust matrix factorization  sparsity  non-negative matrix  noise removal Heterogeneous transfer learning has recently gained much attention as a new machine learning paradigm in which the knowledge can be transferred from source domains to target domains in different feature spaces. Existing works usually assume that source domains can provide accurate and useful knowledge to be transferred to target domains for learning. In practice, there may be noise appearing in given source (text) and target (image) domains data, and thus, the performance of transfer learning can be seriously degraded. In this paper, we propose a robust and non-negative collective matrix factorization model to handle noise in text-to-image transfer learning, and make a reliable bridge to transfer accurate and useful knowledge from the text domain to the image domain. The proposed matrix factorization model can be solved by an efficient iterative method, and the convergence of the iterative method can be shown. Extensive experiments on real data sets suggest that the proposed model is able to effectively perform transfer learning in noisy text and image domains, and it is superior to the popular existing methods for text-to-image transfer learning.", "Transducitve learning  multi-view Learning  transfer Learning  zero-shot Learning  heterogeneous hypergraph Most existing zero-shot learning approaches exploit transfer learning via an intermediate semantic representation shared between an annotated auxiliary dataset and a target dataset with different classes and no annotation. A projection from a low-level feature space to the semantic representation space is learned from the auxiliary dataset and applied without adaptation to the target dataset. In this paper we identify two inherent limitations with these approaches. First, due to having disjoint and potentially unrelated classes, the projection functions learned from the auxiliary dataset/domain are biased when applied directly to the target dataset/domain. We call this problem the projection domain shift problem and propose a novel framework, transductive multi-view embedding, to solve it. The second limitation is the prototype sparsity problem which refers to the fact that for each target class, only a single prototype is available for zero-shot learning given a semantic representation. To overcome this problem, a novel heterogeneous multi-view hypergraph label propagation method is formulated for zero-shot learning in the transductive embedding space. It effectively exploits the complementary information offered by different semantic representations and takes advantage of the manifold structures of multiple representation spaces in a coherent manner. We demonstrate through extensive experiments that the proposed approach (1) rectifies the projection shift between the auxiliary and target domains, (2) exploits the complementarity of multiple semantic representations, (3) significantly outperforms existing methods for both zero-shot and N-shot recognition on three image and video benchmark datasets, and (4) enables novel cross-view annotation tasks.", "Action recognition  transfer learning  deep features Action recognition in still images is a challenging problem in computer vision. To facilitate comparative evaluation independently of person detection, the standard evaluation protocol for action recognition uses an oracle person detector to obtain perfect bounding box information at both training and test time. The assumption is that, in practice, a general person detector will provide candidate bounding boxes for action recognition. In this paper, we argue that this paradigm is suboptimal and that action class labels should already be considered during the detection stage. Motivated by the observation that body pose is strongly conditioned on action class, we show that: 1) the existing state-of-the-art generic person detectors are not adequate for proposing candidate bounding boxes for action classification  2) due to limited training examples, the direct training of action-specific person detectors is also inadequate  and 3) using only a small number of labeled action examples, the transfer learning is able to adapt an existing detector to propose higher quality bounding boxes for subsequent action classification. To the best of our knowledge, we are the first to investigate transfer learning for the task of action-specific person detection in still images. We perform extensive experiments on two benchmark data sets: 1) Stanford-40 and 2) PASCAL VOC 2012. For the action detection task (i.e., both person localization and classification of the action performed), our approach outperforms methods based on general person detection by 5.7% mean average precision (MAP) on Stanford-40 and 2.1% MAP on PASCAL VOC 2012. Our approach also significantly outperforms the state of the art with a MAP of 45.4% on Stanford-40 and 31.4% on PASCAL VOC 2012. We also evaluate our action detection approach for the task of action classification (i.e., recognizing actions without localizing them). For this task, our approach, without using any ground-truth person localization at test time, outperforms on both data sets state-of-the-art methods, which do use person locations.", "Missing modality  transfer learning  latent factor Transfer learning is usually exploited to leverage previously well-learned source domain for evaluating the unknown target domain  however, it may fail if no target data are available in the training stage. This problem arises when the data are multi-modal. For example, the target domain is in one modality, while the source domain is in another. To overcome this, we first borrow an auxiliary database with complete modalities, then consider knowledge transfer across databases and across modalities within databases simultaneously in a unified framework. The contributions are threefold: 1) a latent factor is introduced to uncover the underlying structure of the missing modality from the known data  2) transfer learning in two directions allows the data alignment between both modalities and databases, giving rise to a very promising recovery  and 3) an efficient solution with theoretical guarantees to the proposed latent low-rank transfer learning algorithm. Comprehensive experiments on multi-modal knowledge transfer with missing target modality verify that our method can successfully inherit knowledge from both auxiliary database and source modality, and therefore significantly improve the recognition performance even when test modality is inaccessible in the training stage.", "Cross-view action recognition  transfer learning  heterogeneous features  multiple views adaptation In cross-view action recognition, what you saw in one view is different from what you recognize in another view, since the data distribution even the feature space can change from one view to another. In this paper, we address the problem of transferring action models learned in one view (source view) to another different view (target view), where action instances from these two views are represented by heterogeneous features. A novel learning method, called heterogeneous transfer discriminant-analysis of canonical correlations (HTDCC), is proposed to discover a discriminative common feature space for linking source view and target view to transfer knowledge between them. Two projection matrices are learned to, respectively, map data from the source view and the target view into a common feature space via simultaneously minimizing the canonical correlations of interclass training data, maximizing the canonical correlations of intraclass training data, and reducing the data distribution mismatch between the source and target views in the common feature space. In our method, the source view and the target view neither share any common features nor have any corresponding action instances. Moreover, our HTDCC method is capable of handling only a few or even no labeled samples available in the target view, and can also be easily extended to the situation of multiple source views. We additionally propose a weighting learning framework for multiple source views adaptation to effectively leverage action knowledge learned from multiple source views for the recognition task in the target view. Under this framework, different source views are assigned different weights according to their different relevances to the target view. Each weight represents how contributive the corresponding source view is to the target view. Extensive experiments on the IXMAS data set demonstrate the effectiveness of HTDCC on learning the common feature space for heterogeneous cross-view action recognition. In addition, the weighting learning framework can achieve promising results on automatically adapting multiple transferred source-view knowledge to the target view.", "Memory virtualization  Support vector machines  Transfer Virtualization is an essential technology in data centers allowing for a single machine to be used for multiple applications or users. With memory virtualization, two approaches, shadow paging (SP) and hardware-assisted paging (HAP), are taken by modern virtual machine memory managers. Neither memory mode is always preferred  previous studies have proposed to exploit the advantages of both modes by dynamically switching between these two paging modes based on the on-the-fly system behavior. However, the existing scheme makes the switching decision based on manual rules summarized for a specific architecture. This paper employs a machine learning approach that learns a decision model automatically and thus can adapt to different systems. Experimental results show that the performance of our switching mechanism can match or outperform either SP or HAP alone. Also, the results demonstrate that a machine learning-based decision model can match the performance of the hand-tuned model. Moreover, we further show that different hardware/software settings can affect on-the-fly system behavior and thus demand different decision models. Our scheme yields two effective decision models on two different machines. Additionally, transfer learning was used in order to efficiently train a model when faced with a new hardware configuration with only a limited number of training samples from the new machine.", "Bayesian networks  Temporal reasoning  Transfer learning  Knowledge transfer Traditional machine learning algorithms depend heavily on the assumption that there is sufficient data to learn a reliable model. This is not always the case, and in situations where data is limited, transfer learning can be applied to compensate for the lack of information by learning from several sources. In this work, we present a novel methodology for inducing a Temporal Nodes Bayesian Network (TNBN) when training data is scarce by applying a transfer learning strategy. A TNBN is a probabilistic graphical model that offers a compact representation for dynamic domains by defining multiple time intervals in which events can occur. Learning a TNBN poses additional challenges to learning traditional Bayesian networks due to the incorporation of time intervals. Our proposal incorporates novel approaches to transfer knowledge from several TNBNs to learn the structure, parameters and intervals of a target TNBN. To evaluate our algorithm, we performed experiments with a synthetic network, where we created auxiliary models by altering the structure, parameters and temporal intervals of the original model. Results show that the proposed algorithm is capable of retrieving a reliable model even when few records are available for the target domain. We also performed experiments with a real-world data set belonging to the medical domain of HIV, where we were able to learn some documented mutational pathways and their temporal relations by applying transfer learning.", "Object detection  topic modelling  weakly supervised learning  Bayesian domain transfer  probabilistic modelling We address the problem of localisation of objects as bounding boxes in images and videos with weak labels. This weakly supervised object localisation problem has been tackled in the past using discriminative models where each object class is localised independently from other classes. In this paper, a novel framework based on Bayesian joint topic modelling is proposed, which differs significantly from the existing ones in that: (1) All foreground object classes are modelled jointly in a single generative model that encodes multiple object co-existence so that explaining away inference can resolve ambiguity and lead to better learning and localisation. (2) Image backgrounds are shared across classes to better learn varying surroundings and push out objects of interest. (3) Our model can be learned with a mixture of weakly labelled and unlabelled data, allowing the large volume of unlabelled images on the Internet to be exploited for learning. Moreover, the Bayesian formulation enables the exploitation of various types of prior knowledge to compensate for the limited supervision offered by weakly labelled data, as well as Bayesian domain adaptation for transfer learning. Extensive experiments on the PASCAL VOC, ImageNet and YouTube-Object videos datasets demonstrate the effectiveness of our Bayesian joint model for weakly supervised object localisation.", "Collaborative filtering  Matrix factorization  Group-aware  Transfer learning  Recommender systems Group-aware collaborative filtering (CF) has recently become a hot research topic in recommender systems, which typically divides a large CF task on the entire data (i.e. rating matrix) into some smaller CF tasks on subgroups (i.e., sub-matrices). This leads to an effective way to improve current CF systems in accuracy and efficiency. However, existing approaches consider each subgroup separately, ignoring relationships among subgroups. In this paper, motivated by the intuition that there are similar users or items among different subgroups, we propose an improved group-aware CF algorithm which predicts a rating using a weighted sum of similar ratings from multiple subgroups. Our algorithm is based on Matrix Factorization and CodeBook Transfer (CBT), especially that we construct N matrix approximations based on N best sub-matrices, and then integrate the N approximations via a linear combination. We conduct experiments on real-life data to evaluate the performance of our algorithm in comparison with traditional CF algorithms and other state-of-the-art social and group-aware recommendation models. The empirical result and analysis demonstrate that our algorithm achieves a significant increase in recommendation accuracy. ", "Memetic computation  Evolutionary optimization of problems  Learning from past experiences  Culture-inspired  Evolutionary learning  Transfer learning A significantly under-explored area of evolutionary optimization in the literature is the study of optimization methodologies that can evolve along with the problems solved. Particularly, present evolutionary optimization approaches generally start their search from scratch or the ground-zero state of knowledge, independent of how similar the given new problem of interest is to those optimized previously. There has thus been the apparent lack of automated knowledge transfers and reuse across problems. Taking this cue, this paper presents a Memetic Computational Paradigm based on Evolutionary Optimization Transfer Learning for search, one that models how human solves problems, and embarks on a study towards intelligent evolutionary optimization of problems through the transfers of structured knowledge in the form of memes as building blocks learned from previous problem-solving experiences, to enhance future evolutionary searches. The proposed approach is composed of four culture-inspired operators, namely, Learning, Selection, Variation and Imitation. The role of the learning operator is to mine for latent knowledge buried in past experiences of problem-solving. The learning task is modelled as a mapping between past problem instances solved and the respective optimized solution by maximizing their statistical dependence. The selection operator serves to identify the high quality knowledge that shall replicate and transmit to future search, while the variation operator injects new innovations into the learned knowledge. The imitation operator, on the other hand, models the assimilation of innovated knowledge into the search. Studies on two separate established NP-hard problem domains and a realistic package collection/deliver problem are conducted to assess and validate the benefits of the proposed new memetic computation paradigm.", "Collaborative recommendation  Heterogeneous feedbacks  Factorization machine  Compressed knowledge  Transfer learning Collaborative recommendation has attracted various research works in recent years. However, an important problem setting, i.e., a user examined several items but only rated a few, has not received much attention yet. We coin this problem heterogeneous collaborative recommendation (HCR) from the perspective of users' heterogeneous feedbacks of implicit examinations and explicit ratings. In order to fully exploit such different types of feedbacks, we propose a novel and generic solution called compressed knowledge transfer via factorization machine (CKT-FM). Specifically, we assume that the compressed knowledge of user homophily and item correlation, i.e., user groups and item sets behind two types of feedbacks, are similar and then design a two-step transfer learning solution including compressed knowledge mining and integration. Our solution is able to transfer high quality knowledge via noise reduction, to model rich pairwise interactions among individual-level and cluster-level entities, and to adapt the potential inconsistent knowledge from implicit feedbacks to explicit feedbacks. Furthermore, the analysis on time complexity and space complexity shows that our solution is much more efficient than the state-of-the-art method for heterogeneous feedbacks. Extensive empirical studies on two large data sets show that our solution is significantly better than the state-of-the-art non-transfer learning method w.r.t. recommendation accuracy, and is much more efficient than that of leveraging the raw implicit examinations directly instead of compressed knowledge w.r.t. CPU time and memory usage. Hence, our CKT-FM strikes a good balance between effectiveness and efficiency of knowledge transfer in HCR. ", "Transfer learning  node classification  networked data In this paper, we present a novel transfer learning framework for network node classification. Our objective is to accurately predict the labels of nodes in a target network by leveraging information from an auxiliary source network. Such a transfer learning framework is potentially useful for broader areas of network classification, where emerging new networks might not have sufficient labeled information because node labels are either costly to obtain or simply not available, whereas many established networks from related domains are available to benefit the learning. In reality, the source and the target networks may not share common nodes or connections, so the major challenge of cross-network transfer learning is to identify knowledge/patterns transferable between networks and potentially useful to support cross-network learning. In this work, we propose to learn common signature subgraphs between networks, and use them to construct new structure features for the target network. By combining the original node content features and the new structure features, we develop an iterative classification algorithm, TrGraph, that utilizes label dependency to jointly classify nodes in the target network. Experiments on real-world networks demonstrate that TrGraph achieves the superior performance compared to the state-of-the-art baseline methods, and transferring generalizable structure information can indeed improve the node classification accuracy.", "Exemplar SVMs  Transfer learning  Object detection  Image retrieval  Feature mapping Exemplar SVMs (E-SVMs, Malisiewicz et al., ICCV 2011), where an SVM is trained with only a single positive sample, have found applications in the areas of object detection and content-based image retrieval (CBIR), amongst others. In this paper we introduce a method of part based transfer regularization that boosts the performance of E-SVMs, with a negligible additional cost. This enhanced E-SVM (EE-SVM) improves the generalization ability of E-SVMs by softly forcing it to be constructed from existing classifier parts cropped from previously learned classifiers. In CBIR applications, where the aim is to retrieve instances of the same object class in a similar pose, the EE-SVM is able to tolerate increased levels of intra-class variation, including occlusions and truncations, over E-SVM, and thereby increases precision and recall. In addition to transferring parts, we introduce a method for transferring the statistics between the parts and also show that there is an equivalence between transfer regularization and feature augmentation for this problem and others, with the consequence that the new objective function can be optimized using standard libraries. EE-SVM is evaluated both quantitatively and qualitatively on the PASCAL VOC 2007. and ImageNet datasets for pose specific object retrieval. It achieves a significant performance improvement over E-SVMs, with greater suppression of negative detections and increased recall, whilst maintaining the same ease of training and testing. ", "Case-based reasoning  Reinforcement learning  Transfer learning The goal of this paper is to propose and analyse a transfer learning meta-algorithm that allows the implementation of distinct methods using heuristics to accelerate a Reinforcement Learning procedure in one domain (the target) that are obtained from another (simpler) domain (the source domain). This meta-algorithm works in three stages: first, it uses a Reinforcement Learning step to learn a task on the source domain, storing the knowledge thus obtained in a case base  second, it does an unsupervised mapping of the source-domain actions to the target-domain actions  and, third, the case base obtained in the first stage is used as heuristics to speed up the learning process in the target domain. A set of empirical evaluations were conducted in two target domains: the 3D mountain car (using a learned case base from a 2D simulation) and stability learning for a humanoid robot in the Robocup 3D Soccer Simulator (that uses knowledge learned from the Acrobot domain). The results attest that our transfer learning algorithm outperforms recent heuristically-accelerated reinforcement learning and transfer learning algorithms. ", "Brain  Domain adaptation  Machine learning  MRI  Transfer learning Many automatic segmentation methods are based on supervised machine learning. Such methods have proven to perform well, on the condition that they are trained on a sufficiently large manually labeled training set that is representative of the images to segment. However, due to differences between scanners, scanning parameters, and patients such a training set may be difficult to obtain. We present a transfer-learning approach to segmentation by multi-feature voxelwise classification. The presented method can be trained using a heterogeneous set of training images that may be obtained with different scanners than the target image. In our approach each training image is given a weight based on the distribution of its voxels in the feature space. These image weights are chosen as to minimize the difference between the weighted probability density function (PDF) of the voxels of the training images and the PDF of the voxels of the target image. The voxels and weights of the training images are then used to train a weighted classifier. We tested our method on three segmentation tasks: brain-tissue segmentation, skull stripping, and white-matter-lesion segmentation. For all three applications, the proposed weighted classifier significantly outperformed an unweighted classifier on all training images, reducing classification errors by up to 42%. For brain-tissue segmentation and skull stripping our method even significantly outperformed the traditional approach of training on representative training images from the same study as the target image. ", "Transfer learning  Uncertain data  One-class classification One-class classification aims at constructing a distinctive classifier based on one class of examples. Most of the existing one-class classification methods are proposed based on the assumptions that: (1) there are a large number of training examples available for learning the classifier  (2) the training examples can be explicitly collected and hence do not contain any uncertain information. However, in real-world applications, these assumptions are not always satisfied. In this paper, we propose a novel approach called uncertain one-class transfer learning with support vector machine (UOCT-SVM), which is capable of constructing an accurate classifier on the target task by transferring knowledge from multiple source tasks whose data may contain uncertain information. In UOCT-SVM, the optimization function is formulated to deal with uncertain data and transfer learning based on one-class SVM. Then, an iterative framework is proposed to solve the optimization function. Extensive experiments have showed that UOCT-SVM can mitigate the effect of uncertain data on the decision boundary and transfer knowledge from source tasks to help build an accurate classifier on the target task, compared with state-of-the-art one-class classification methods.", "Transfer learning  Regularization  Electronic health records Transfer learning methods have been successfully applied in solving a wide range of real-world problems. However, there is almost no attempt of effectively using these methods in healthcare applications. In the healthcare domain, it becomes extremely critical to solve the when to transfer issue of transfer learning. In highly divergent source and target domains, transfer learning can lead to negative transfer. Most of the existing works in transfer learning are primarily focused on selecting useful information from the source to improve the performance of the target task, but whether the transfer learning can help and when the transfer learning should be applied in the target task are still some of the impending challenges. In this paper, we address this issue of when to transfer by proposing a sparse feature selection model based on the constrained elastic net penalty. As a case study of the proposed model, we demonstrate the performance using the diabetes electronic health records (EHRs) which contain patient records from all fifty states in the United States. Our approach can choose relevant features to transfer knowledge from the source to the target tasks. The proposed model can measure the differences between multivariate data distributions conditional on the predicted model, and based on this measurement we can avoid unsuccessful transfer. We successfully transfer the knowledge across different states to improve the diagnosis of diabetes in a certain state with insufficient records to build an individualized predictive model with the aid of information from other states.", "Machine learning  Multi-source learning  Domain adaptation  Transfer learning In many machine learning algorithms, a major assumption is that the training and the test samples are in the same feature space and have the same distribution. However, for many real applications this assumption does not hold. In this paper, we survey the problem where the training samples and the test samples are from different distributions. This problem can be referred as domain adaptation. The training samples, always with labels, are obtained from what is called source domains, while the test samples, which usually have no labels or only a few labels, are obtained from what is called target domains. The source domains and the target domains are different but related to some extent  the learners can learn some information from the source domains for the learning of the target domains. We focus on the multisource domain adaptation problem where there is more than one source domain available together with only one target domain. A key issue is how to select good sources and samples for the adaptation. In this survey, we review some theoretical results and well developed algorithms for the multi-source domain adaptation problem. We also discuss some open problems which can be explored in future work. ", "Action ordering  action selection  reinforcement learning  search pruning  transfer learning An agent tasked with solving a number of different decision making problems in similar environments has an opportunity to learn over a longer timescale than each individual task. Through examining solutions to different tasks, it can uncover behavioral invariances in the domain, by identifying actions to be prioritized in local contexts, invariant to task details. This information has the effect of greatly increasing the speed of solving new problems. We formalise this notion as action priors, defined as distributions over the action space, conditioned on environment state, and show how these can be learnt from a set of value functions. We apply action priors in the setting of reinforcement learning, to bias action selection during exploration. Aggressive use of action priors performs context based pruning of the available actions, thus reducing the complexity of lookahead during search. We additionally define action priors over observation features, rather than states, which provides further flexibility and generalizability, with the additional benefit of enabling feature selection. Action priors are demonstrated in experiments in a simulated factory environment and a large random graph domain, and show significant speed ups in learning new tasks. Furthermore, we argue that this mechanism is cognitively plausible, and is compatible with findings from cognitive psychology.", "Transfer learning  kernel learning  Nystrom method  text mining  image classification  video recognition Domain transfer learning generalizes a learning model across training data and testing data with different distributions. A general principle to tackle this problem is reducing the distribution difference between training data and testing data such that the generalization error can be bounded. Current methods typically model the sample distributions in input feature space, which depends on nonlinear feature mapping to embody the distribution discrepancy. However, this nonlinear feature space may not be optimal for the kernel-based learning machines. To this end, we propose a transfer kernel learning (TKL) approach to learn a domain-invariant kernel by directly matching source and target distributions in the reproducing kernel Hilbert space (RKHS). Specifically, we design a family of spectral kernels by extrapolating target eigensystem on source samples with Mercer's theorem. The spectral kernel minimizing the approximation error to the ground truth kernel is selected to construct domain-invariant kernel machines. Comprehensive experimental evidence on a large number of text categorization, image classification, and video event recognition datasets verifies the effectiveness and efficiency of the proposed TKL approach over several state-of-the-art methods.", "Design  Algorithms  Performance  Theory  Data mining  computer vision  big visual data  visual mining  transfer learning  visual knowledge base  RGB-D sensor Mining object-level knowledge, that is, building a comprehensive category model base, from a large set of cluttered scenes presents a considerable challenge to the field of artificial intelligence. How to initiate model learning with the least human supervision (i.e., manual labeling) and how to encode the structural knowledge are two elements of this challenge, as they largely determine the scalability and applicability of any solution. In this article, we propose a model-learning method that starts from a single-labeled object for each category, and mines further model knowledge from a number of informally captured, cluttered scenes. However, in these scenes, target objects are relatively small and have large variations in texture, scale, and rotation. Thus, to reduce the model bias normally associated with less supervised learning methods, we use the robust 3D shape in RGB-D images to guide our model learning, then apply the properly trained category models to both object detection and recognition in more conventional RGB images. In addition to model training for their own categories, the knowledge extracted from the RGB-D images can also be transferred to guide model learning for a new category, in which only RGB images without depth information in the new category are provided for training. Preliminary testing shows that the proposed method performs as well as fully supervised learning methods.", "Transfer learning  Computational intelligence  Neural network  Bayes  Fuzzy sets and systems  Genetic algorithm Transfer learning aims to provide a framework to utilize previously-acquired knowledge to solve new but similar problems much more quickly and effectively. In contrast to classical machine learning methods, transfer learning methods exploit the knowledge accumulated from data in auxiliary domains to facilitate predictive modeling consisting of different data patterns in the current domain. To improve the performance of existing transfer learning methods and handle the knowledge transfer process in real-world systems, computational intelligence has recently been applied in transfer learning. This paper systematically examines computational intelligence-based transfer learning techniques and clusters related technique developments into four main categories: (a) neural network-based transfer learning  (b) Bayes-based transfer learning  (c) fuzzy transfer learning, and (d) applications of computational intelligence-based transfer learning. By providing state-of-the-art knowledge, this survey will directly support researchers and practice-based professionals to understand the developments in computational intelligence-based transfer learning research and applications. ", "Action recognition  image classification  machine learning  object recognition  survey  transfer learning  visual categorization Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.", "Algorithms  Performance  Heterogeneous transfer learning  domain adaption  text classification  activity recognition Transfer learning aims to improve performance on a target task by utilizing previous knowledge learned from source tasks. In this paper we introduce a novel heterogeneous transfer learning technique, Feature-Space Remapping (FSR), which transfers knowledge between domains with different feature spaces. This is accomplished without requiring typical feature-feature, feature instance, or instance-instance co-occurrence data. Instead we relate features in different feature-spaces through the construction of metafeatures. We show how these techniques can utilize multiple source datasets to construct an ensemble learner which further improves performance. We apply FSR to an activity recognition problem and a document classification problem. The ensemble technique is able to outperform all other baselines and even performs better than a classifier trained using a large amount of labeled data in the target domain. These problems are especially difficult because, in addition to having different feature-spaces, the marginal probability distributions and the class labels are also different. This work extends the state of the art in transfer learning by considering large transfer across dramatically different spaces.", "Quadrupedal robot  gait optimization  transfer learning  particle swarm optimization  evolutionary algorithms Learning new gaits for compliant robots is a challenging multi-dimensional optimization task. Furthermore, to ensure optimal performance, the optimization process must be repeated for every variation in the environment, for example for every change in inclination of the terrain. This is unfortunately not possible using current approaches, since the time required for the optimization is simply too high. Hence, a sub-optimal gait is often used. The goal in this manuscript is to reduce the learning time of a particle swarm algorithm, such that the robot's gaits can be optimized over a wide variety of terrains. To facilitate this, we use transfer learning by sharing knowledge about gaits between the different environments. Our findings indicate that using transfer learning new robust gaits can be discovered faster compared to traditional methods that learn a gait for each environment independently.", "Machine learning  Bayesian networks  Transfer learning Bayesian network structure learning algorithms with limited data are being used in domains such as systems biology and neuroscience to gain insight into the underlying processes that produce observed data. Learning reliable networks from limited data is difficult  therefore, transfer learning can improve the robustness of learned networks by leveraging data from related tasks. Existing transfer learning algorithms for Bayesian network structure learning give a single maximum a posteriori estimate of network models. Yet, many other models may be equally likely, and so a more informative result is provided by Bayesian structure discovery. Bayesian structure discovery algorithms estimate posterior probabilities of structural features, such as edges. We present transfer learning for Bayesian structure discovery which allows us to explore the shared and unique structural features among related tasks. Efficient computation requires that our transfer learning objective factors into local calculations, which we prove is given by a broad class of transfer biases. Theoretically, we show the efficiency of our approach. Empirically, we show that compared to single-task learning, transfer learning is better able to positively identify true edges. We apply the method to whole-brain neuroimaging data.", "Customer churn prediction  Transfer ensemble model  Feature selection  GMDH-type neural network  Transfer learning Customer churn prediction is one of the key steps to maximize the value of customers for an enterprise. It is difficult to get satisfactory prediction effect by traditional models constructed on the assumption that the training and test data are subject to the same distribution, because the customers usually come from different districts and may be subject to different distributions in reality. This study proposes a feature-selection-based dynamic transfer ensemble (FSDTE) model that aims to introduce transfer learning theory for utilizing the customer data in both the target and related source domains. The model mainly conducts a two-layer feature selection. In the first layer, an initial feature subset is selected by GMDH-type neural network only in the target domain. In the second layer, several appropriate patterns from the source domain to target training set are selected, and some features with higher mutual information between them and the class variable are combined with the initial subset to construct a new feature subset. The selection in the second layer is repeated several times to generate a series of new feature subsets, and then, we train a base classifier in each one. Finally, a best base classifier is selected dynamically for each test pattern. The experimental results in two customer churn prediction datasets show that FSDTE can achieve better performance compared with the traditional churn prediction strategies, as well as three existing transfer learning strategies.", "Mobile data  Behavior similarity  Common behavior analysis  Transfer learning  Principal component analysis  Sequential prediction The proliferation of smart phones has opened up new kinds of data to model human behavior and predict future activity but this prediction can be tempered by the relative sparsity of data. In this paper, we integrate a time-dependent instance transfer mechanism, driven by a hybrid similarity measure, into learning and predicting human behavior. In particular, transfer component analysis (TCA) is utilized for domain adaptation from different data types to overcome data sparsity. The hybrid user similarity measure is developed based on three different characteristics: eigen-behavior, longest common behavior (LCB), and daily common behavior (DCB). Extensive comparisons are made against state-of-the-art time series prediction algorithms using the Nokia Mobile Data Challenge (MDC) dataset and the MIT Reality Mining dataset. We compare the prediction performance given (i) no additional data, (ii) only data from identical behavior from other users, and (iii) data from any type of behavior from other users. Experimental results show that our proposed algorithm significantly improves the performance of behavior prediction. ", "Bots  cross-language learning  editors  feature engineering  transfer learning  users  vandalism  Wikipedia Vandalism, the malicious modification of articles, is a serious problem for open access encyclopedias such as Wikipedia. The use of counter-vandalism bots is changing the way Wikipedia identifies and bans vandals, but their contributions are often not considered nor discussed. In this paper, we propose novel text features capturing the invariants of vandalism across five languages to learn and compare the contributions of bots and users in the task of identifying vandalism. We construct computationally efficient features that highlight the contributions of bots and users, and generalize across languages. We evaluate our proposed features through classification performance on revisions of five Wikipedia languages, totaling over 500 million revisions of over nine million articles. As a comparison, we evaluate these features on the small PAN Wikipedia vandalism data sets, used by previous research, which contain approximately 62,000 revisions. We show differences in the performance of our features on the PAN and the full Wikipedia data set. With the appropriate text features, vandalism bots can be effective across different languages while learning from only one language. Our ultimate aim is to build the next generation of vandalism detection bots based on machine learning approaches that can work effectively across many languages.", "Autonomous robot navigation  Visual place recognition  Domain adaptation  Unsupervised learning  Multiple cues One of the most impressive characteristics of human perception is its domain adaptation capability. Humans can recognize objects and places simply by transferring knowledge from their past experience. Inspired by that, current research in robotics is addressing a great challenge: building robots able to sense and interpret the surrounding world by reusing information previously collected, gathered by other robots or obtained from the web. But, how can a robot automatically understand what is useful among a large amount of information and perform knowledge transfer? In this paper we address the domain adaptation problem in the context of visual place recognition. We consider the scenario where a robot equipped with a monocular camera explores a new environment. In this situation traditional approaches based on supervised learning perform poorly, as no annotated data are provided in the new environment and the models learned from data collected in other places are inappropriate due to the large variability of visual information. To overcome these problems we introduce a novel transfer learning approach. With our algorithm the robot is given only some training data (annotated images collected in different environments by other robots) and is able to decide whether, and how much, this knowledge is useful in the current scenario. At the base of our approach there is a transfer risk measure which quantifies the similarity between the given and the new visual data. To improve the performance, we also extend our framework to take into account multiple visual cues. Our experiments on three publicly available datasets demonstrate the effectiveness of the proposed approach. ", "Detector adaption  Transfer learning  Sparse coding  Pedestrian detection Training a reliable generic pedestrian detector on different scenes is still a very challenging problem. In this paper, we propose a novel transfer learning framework for improving the performance of a generic detector by adapting the detector to a scene specific detector. The main contributions come from 2 aspects: (1) instead of hand-crafted ad-hoc rules, a scene based auxiliary attribute classifier and a position priori map are automatically trained from target scene to collect confident samples  (2) conditional distribution transfer sparse coding is presented to match the conditional distributions of the source and the target samples. Experiments show our approach significantly improves the performance of the generic detector and outperforms the state-of-the-art adapting approaches in benchmark datasets. Comparing with the state-of-the-art methods, the improvements are 6% on the CUHK square pedestrian dataset and 33% on the ETH pedestrian dataset which is considered quite hard because the background is dynamic. ", "Reinforcement learning  transfer learning  task selection  inter-task mappings When transferring knowledge between reinforcement learning agents with different state representations or actions, past knowledge must be efficiently mapped to novel tasks so that it aids learning. The majority of the existing approaches use pre-defined mappings provided by a domain expert. To overcome this limitation and enable autonomous transfer learning, this paper introduces a method for weighting and using multiple inter-task mappings based on a probabilistic framework. Experimental results show that the use of multiple inter-task mappings, accompanied with a probabilistic selection mechanism, can significantly boost the performance of transfer learning relative to 1) learning without transfer and 2) using a single hand-picked mapping. We especially introduce novel tasks for transfer learning in a realistic simulation of the iCub robot, demonstrating the ability of the method to select mappings in complex tasks where human intuition could not be applied to select them. The results verified the efficacy of the proposed approach in a real world and complex environment.", "Action recognition  next-generation cameras  transfer learning  feature borrowing The recent advances in imaging devices have opened the opportunity of better solving the tasks of video content analysis and understanding. Next-generation cameras, such as the depth or binocular cameras, capture diverse information, and complement the conventional 2D RGB cameras. Thus, investigating the yielded multimodal videos generally facilitates the accomplishment of related applications. However, the limitations of the emerging cameras, such as short effective distances, expensive costs, or long response time, degrade their applicability, and currently make these devices not online accessible in practical use. In this paper, we provide an alternative scenario to address this problem, and illustrate it with the task of recognizing human actions. In particular, we aim at improving the accuracy of action recognition in RGB videos with the aid of one additional RGB-D camera. Since RGB-D cameras, such as Kinect, are typically not applicable in a surveillance system due to its short effective distance, we instead offline collect a database, in which not only the RGB videos but also the depth maps and the skeleton data of actions are available jointly. The proposed approach can adapt the interdatabase variations, and activate the borrowing of visual knowledge across different video modalities. Each action to be recognized in RGB representation is then augmented with the borrowed depth and skeleton features. Our approach is comprehensively evaluated on five benchmark data sets of action recognition. The promising results manifest that the borrowed information leads to remarkable boost in recognition accuracy.", " This paper proposes a novel method for training random forests with big data on MapReduce clusters. Random forests are well suited for parallel distributed systems, since they are composed of multiple decision trees and every decision tree can be independently trained by ensemble learning methods. However, naive implementation of random forests on distributed systems easily overfits the training data, yielding poor classification performances. This is because each cluster node can have access to only a small fraction of the training data. The proposed method tackles this problem by introducing the following three steps. (1) Shared forests are built in advance on the master node and shared with all the cluster nodes. (2) With the help of transfer learning, the shared forests are adapted to the training data placed on each cluster node. (3) The adapted forests on every cluster node are returned to the master node, and irrelevant trees yielding poor classification performances are removed to form the final forests. Experimental results show that our proposed method for MapReduce clusters can quickly learn random forests without any sacrifice of classification performance.", " Random Forest, a multi-class classifier based on statistical learning, is widely used in applications because of its high generalization performance due to randomness. However, in applications such as object detection, disparities in the distributions of the training and test samples from the target scene are often inevitable, resulting in degraded performance. In this case, the training samples need to be reacquired for the target scene, typically at a very high human acquisition cost. To solve this problem, transfer learning has been proposed. In this paper, we present data-level transfer learning for a Random Forest using covariate shift. Experimental results show that the proposed method, called Transfer Forest, can adapt to the target domain by transferring training samples from an auxiliary domain.", " Scaling machine learning methods to very large datasets has attracted considerable attention in recent years, thanks to easy access to ubiquitous sensing and data from the web. We study face recognition and show that three distinct properties have surprising effects on the transferability of deep convolutional networks (CNN): (1) The bottleneck of the network serves as an important transfer learning regularizer, and (2) in contrast to the common wisdom, performance saturation may exist in CNN's (as the number of training samples grows)  we propose a solution for alleviating this by replacing the naive random subsampling of the training set with a bootstrapping process. Moreover, (3) we find a link between the representation norm and the ability to discriminate in a target domain, which sheds lights on how such networks represent faces. Based on these discoveries, we are able to improve face recognition accuracy on the widely used LFW benchmark, both in the verification (1:1) and identification (1:N) protocols, and directly compare, for the first time, with the state of the art Commercially-Off-The-Shelf system and show a sizable leap in performance.", " We develop methods for detector learning which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet200 detection with weak labels.", " Metric learning has proved very successful. However, human annotations are necessary. In this paper, we propose an unsupervised method, dubbed Metric Imitation (MI), where metrics over cheap features (target features, TFs) are learned by imitating the standard metrics over more sophisticated, off-the-shelf features (source features, SFs) by transferring view-independent property manifold structures. In particular, MI consists of: 1) quantifying the properties of source metrics as manifold geometry, 2) transferring the manifold from source domain to target domain, and 3) learning a mapping of TFs so that the manifold is approximated as well as possible in the mapped feature domain. MI is useful in at least two scenarios where: 1) TFs are more efficient computationally and in terms of memory than SFs  and 2) SFs contain privileged information, but are not available during testing. For the former, MI is evaluated on image clustering, category-based image retrieval, and instance-based object retrieval, with three SFs and three TFs. For the latter, MI is tested on the task of example-based image super-resolution, where high-resolution patches are taken as SFs and low-resolution patches as TFs. Experiments show that MI is able to provide good metrics while avoiding expensive data labeling efforts and that it achieves state-of-the-art performance for image super-resolution. In addition, manifold transfer is an interesting direction of transfer learning.", "Nonnegative Matrix Factorization  Transfer Learning  Geometrical Distribution  Probability Distribution Transfer visual feature learning (TVFL), which learns compact representations for images such that we can build accurate classifier for target domain by leveraging rich labeled data in the source domain, has attracted increasingly attention recently. Previous methods mainly focus on reducing the distribution difference between domains but ignore the intrinsic hidden semantics in data. In this paper, we put forward a novel method for TVFL, called Distribution Regularized Nonnegative Matrix Factorization (DRNMF). Specifically, we employ Nonnegative Matrix Factorization (NMF) to uncover the intrinsic information in visual data, and regularize it with geometrical distribution, marginal probability distribution and conditional probability distribution. Thus, DRNMF can discover the intrinsic information, preserve the manifold structure and reducing both marginal and conditional probability distribution difference simultaneously, which all perspectives above are important for TVFL. We also propose an effective and efficient algorithm for the optimization of DRNMF and theoretically prove the convergence. Extensive experiments on three types of cross-domain image classification tasks in comparison with several state-of-the-art methods demonstrate the superiority of our DRNMF, which validates its effectiveness.", "Image Classification  Transfer Learning  BoF  CNN Effective image representation plays an important role for image classification and retrieval. Bag-of-Features (BoF) is well known as an effective and robust visual representation. However, on large datasets, convolutional neural networks (CNN) tend to perform much better, aided by the availability of large amounts of training data. In this paper, we propose a bag of Deep Bottleneck Features (DBF) for image classification, effectively combining the strengths of a CNN within a BoF framework. The DBF features, obtained from a previously well-trained CNN, form a compact and low-dimensional representation of the original inputs, effective for even small datasets. We will demonstrate that the resulting BoDBF method has a very powerful and discriminative capability that is generalisable to other image classification tasks.", " Transfer learning is a process in which a system can apply knowledge and skills learned in previous tasks to novel tasks. This technique has emerged as a new framework to enhance the performance of learning methods in machine learning. Surprisingly, transfer learning has not deservedly received the attention from the Genetic Programming research community. In this paper, we propose several transfer learning methods for Genetic Programming (GP). These methods were implemented by transferring a number of good individuals or sub-individuals from the source to the target problem. They were tested on two families of symbolic regression problems. The experimental results showed that transfer learning methods help GP to achieve better training errors. Importantly, the performance of GP on unseen data when implemented with transfer learning was also considerably improved. Furthermore, the impact of transfer learning to GP code bloat was examined that showed that limiting the size of transferred individuals helps to reduce the code growth problem in GP.", " Recent advancement in unsupervised and transfer learning methods of deep learning networks has seen a complete paradigm shift in machine learning. Inspired by the recent evolution of deep learning (DL) networks that demonstrates a proven pathway of addressing challenging dilemmas in various problem domains, we propose a novel DL framework for expression-robust feature acquisition. The framework exploits the contributions of different colour components in different local face regions by recovering the neutral expression from various expressions. Furthermore, the framework rigorously de-noises a face with dynamic expressions in a progressive way thus it is termed as stacked face de-noising auto-encoders (SFDAE). The high-level expression-robust representations that are learnt via this framework will not only yield better reconstruction of neutral expression faces but also boost the performance of the subsequent LDA [1] classifier. The experimental results reveal the superiority of the proposed method to the existing works in terms of its generalization ability and the high recognition accuracy.", " Collaborative clustering is a recent field of Machine Learning that shows similarities with both transfer learning and ensemble learning. It uses two-step approaches where different clustering algorithms first process data individually and then exchange their information and results with a goal of mutual improvement. In this article, we introduce a new collaborative learning approach based on collaborative clustering principles and applied to the Generative Topographic Mapping (GTM) algorithm. Our method consists in applying the GTM algorithm on different data sets where similar clusters can be found (same feature spaces and similar data distributions), and then to use a collaborative framework on the generated maps with the goal of transferring knowledge between them. The proposed approach has been validated on several data sets, and the experimental results have shown very promising performances.", "Domain adaptation  transfer learning  weighted sub-space  manifold  classification This paper describes a method of cross-domain object categorization, using the concept of domain adaptation. Here, a classifier is trained using samples from the source/auxiliary domain and performance is observed on a set of test samples taken from a different domain, termed as the target domain. To overcome the difference between the two domains, we aim to find a sequence of optimally weighted sub-spaces, lying on the geodesic path on Grassmann manifold, such that the instances from both the domains follow similar distributions when projected onto the sub-spaces. Hence, the method models the gradual change of the distribution of data from source to target domain, using a sequence of weighted sub-spaces. Results show that the proposed method of unsupervised domain adaptation provides better classification accuracy than a few state of the art methods.", "transfer learning  embedded assist-control  human adaptive mechatronics  human-machine interface  skill analysis Human Adaptive Mechatronics (HAM) is the research area that covers the design for assisting the human operator in improving its skills. HAM devices are capable to measure/estimate the operator's skiWdexterity, while a real-time embedded assist-controller enhances machine operation, improving the overall human-machine performance. Nowadays, the demand for such devices has particular potential in many activities which involve manual operations. The main contribution of this work is the development of a human adaptive real-time electronic switching controller obtained from a fuzzy clustering inductive learning technique, for improving the operator's proficiency, based on the transfer learning information of an expert driver. Several tests were conducted under a hardware/software driving simulator setup, to prove the effectiveness of the proposed methodology.", " Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects. Inspired by the deep convolutional networks (DCN) on super-resolution [5], we formulate a compact and efficient network for seamless attenuation of different compression artifacts. We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network. Following a similar easy to hard idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low-level vision problems. Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use case (i.e. Twitter).", " In this work we address the problem of indoor scene understanding from RGB-D images. Specifically, we propose to find instances of common furniture classes, their spatial extent, and their pose with respect to generalized class models. To accomplish this, we use a deep, wide, multi-output convolutional neural network (CNN) that predicts class, pose, and location of possible objects simultaneously. To overcome the lack of large annotated RGB-D training sets (especially those with pose), we use an on-the-fly rendering pipeline that generates realistic cluttered room scenes in parallel to training. We then perform transfer learning on the relatively small amount of publicly available annotated RGB-D data, and find that our model is able to successfully annotate even highly challenging real scenes. Importantly, our trained network is able to understand noisy and sparse observations of highly cluttered scenes with a remarkable degree of accuracy, inferring class and pose from a very limited set of cues. Additionally, our neural network is only moderately deep and computes class, pose and position in tandem, so the overall run-time is significantly faster than existing methods, estimating all output parameters simultaneously in parallel.", " Zero-shot learning (ZSL) can be considered as a special case of transfer learning where the source and target domains have different tasks/label spaces and the target domain is unlabelled, providing little guidance for the knowledge transfer. A ZSL method typically assumes that the two domains share a common semantic representation space, where a visual feature vector extracted from an image/video can be projected/embedded using a projection function. Existing approaches learn the projection function from the source domain and apply it without adaptation to the target domain. They are thus based on naive knowledge transfer and the learned projections are prone to the domain shift problem. In this paper a novel ZSL method is proposed based on unsupervised domain adaptation. Specifically, we formulate a novel regularised sparse coding framework which uses the target domain class labels' projections in the semantic space to regularise the learned target domain projection thus effectively overcoming the projection domain shift problem. Extensive experiments on four object and action recognition benchmark datasets show that the proposed ZSL method significantly outperforms the state-of-the-arts.", " How can we reuse existing knowledge, in the form of available datasets, when solving a new and apparently un-related target task from a set of unlabeled data? In this work we make a first contribution to answer this question in the context of image classification. We frame this quest as an active learning problem and use zero-shot classifiers to guide the learning process by linking the new task to the existing classifiers. By revisiting the dual formulation of adaptive SVM, we reveal two basic conditions to choose greedily only the most relevant samples to be annotated. On this basis we propose an effective active learning algorithm which learns the best possible target classification model with minimum human labeling effort. Extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art active learning methodologies, as well as its potential to reuse past datasets with minimal effort for future tasks.", " We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degrees accuracy for large scale outdoor scenes and 0.5m and 5 degrees accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.", " Varied sources of error contribute to the challenge of facial action unit detection. Previous approaches address specific and known sources. However, many sources are unknown. To address the ubiquity of error, we propose a Confident Preserving Machine (CPM) that follows an easy-to-hard classification strategy. During training, CPM learns two confident classifiers. A confident positive classifier separates easily identified positive samples from all else  a confident negative classifier does same for negative samples. During testing, CPM then learns a person-specific classifier using virtual labels provided by confident classifiers. This step is achieved using a quasi-semi-supervised (QSS) approach. Hard samples are typically close to the decision boundary, and the QSS approach disambiguates them using spatio-temporal constraints. To evaluate CPM, we compared it with a baseline single-margin classifier and state-of-the-art semi-supervised learning, transfer learning, and boosting methods in three datasets of spontaneous facial behavior. With few exceptions, CPM outperformed baseline and state-of-the art methods.", " In this article we explore the problem of constructing person-specific models for the detection of facial Action Units (AUs), addressing the problem from the point of view of Transfer Learning and Multi-Task Learning. Our starting point is the fact that some expressions, such as smiles, are very easily elicited, annotated, and automatically detected, while others are much harder to elicit and to annotate. We thus consider a novel problem: all AU models for the target subject are to be learnt using person-specific annotated data for a reference AU (AU12 in our case), and no data or little data regarding the target AU. In order to design such a model, we propose a novel Multi-Task Learning and the associated Transfer Learning framework, in which we consider both relations across subjects and AUs. That is to say, we consider a tensor structure among the tasks. Our approach hinges on learning the latent relations among tasks using one single reference AU, and then transferring these latent relations to other AUs. We show that we are able to effectively make use of the annotated data for AU12 when learning other person-specific AU models, even in the absence of data for the target task. Finally, we show the excellent performance of our method when small amounts of annotated data for the target tasks are made available.", " Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity priors. We establish a connection between slow feature learning and metric learning. Using this connection we define temporal coherence-a criterion which can be used to set hyper-parameters in a principled and automated manner. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labels.", " The problem of transfer learning is considered in the domain of crowd counting. A solution based on Bayesian model adaptation of Gaussian processes is proposed. This is shown to produce intuitive model updates, which are tractable, and lead to an adapted model (predictive distribution) that accounts for all information in both training and adaptation data. The new adaptation procedure achieves significant gains over previous approaches, based on multi-task learning, while requiring much less computation to deploy. This makes it particularly suited for the problem of expanding the capacity of crowd counting camera networks. A large video dataset for the evaluation of adaptation approaches to crowd counting is also introduced. This contains a number of adaptation tasks, involving information transfer across video collected by 1) a single camera under different scene conditions (different times of the day) and 2) video collected from different cameras. Evaluation of the proposed model adaptation procedure in this dataset shows good performance in realistic operating conditions.", "Deep learning  multi-instance learning  multi-task learning  transfer learning  bioinformatics Multi-instance learning studies problems in which labels are assigned to bags that contain multiple instances. In these settings, the relations between instances and labels are usually ambiguous. In contrast, multi-task learning focuses on the output space in which an input sample is associated with multiple labels. In real world, a sample may be associated with multiple labels that are derived from observing multiple aspects of the problem. Thus many real world applications are naturally formulated as multi-instance multi-task (MIMT) problems. A common approach to MIMT is to solve it task-by-task independently under the multi-instance learning framework. On the other hand, convolutional neural networks (CNN) have demonstrated promising performance in single-instance single-label image classification tasks. However, how CNN deals with multi-instance multi-label tasks still remains an open problem. This is mainly due to the complex multiple-to-multiple relations between the input and output space. In this work, we propose a deep leaning model, known as multi-instance multi-task convolutional neural networks (MIMT-CNN), where a number of images representing a multi-task problem is taken as the inputs. Then a shared sub-CNN is connected with each input image to form instance representations. Those sub-CNN outputs are subsequently aggregated as inputs to additional convolutional layers and full connection layers to produce the ultimate multi-label predictions. This CNN model, through transfer learning from other domains, enables transfer of prior knowledge at image level learned from large single-label single-task data sets. The bag level representations in this model are hierarchically abstracted by multiple layers from instance level representations. Experimental results on mouse brain gene expression pattern annotation data show that the proposed MIMT-CNN model achieves superior performance.", " Transfer learning is typically performed between problem instances within the same domain. We consider the problem of transferring across domains. To this effect, we adopt a probabilistic logic approach. First, our approach automatically identifies predicates in the target domain that are similar in their relational structure to predicates in the source domain. Second, it transfers the logic rules and learns the parameters of the transferred rules using target data. Finally, it refines the rules as necessary using theory refinement. Our experimental evidence supports that this transfer method finds models as good or better than those found with state-of-the-art methods, with and without transfer, and in a fraction of the time.", " Sketch recognition is one of the integral components used by law enforcement agencies in solving crime. In recent past, software generated composite sketches are being preferred as they are more consistent and faster to construct than hand drawn sketches. Matching these composite sketches to face photographs is a complex task because the composite sketches are drawn based on the witness description and lack minute details which are present in photographs. This paper presents a novel algorithm for matching composite sketches with photographs using transfer learning with deep learning representation. In the proposed algorithm, first the deep learning architecture based facial representation is learned using large face database of photos and then the representation is updated using small problem-specific training database. Experiments are performed on the extended PRIP database and it is observed that the proposed algorithm outperforms recently proposed approach and a commercial face recognition system.", " Discriminant analysis is an important technique for face recognition because it can extract discriminative features to classify different persons. However, most existing discriminant analysis methods fail to work for single-sample face recognition (SSFR) because there is only a single training sample per person such that the within-class variation of this person cannot be estimated in such scenario. In this paper, we present a new discriminative transfer learning (DTL) approach for SSFR, where discriminant analysis is performed on a multiple-sample generic training set and then transferred into the single-sample gallery set. Specifically, our DTL learns a feature projection to minimize the intra-class variation and maximize the inter-class variation of samples in the training set, and minimize the difference between the generic training set and the gallery set, simultaneously. Experimental results on three face datasets including the FERET, CAS-PEAL-R1, and LFW datasets are presented to show the efficacy of our method.", " Despite the high facial expression recognition accuracy reported on individual databases, cross-database facial expression recognition is still a challenging problem. This is essentially a problem of generalizing a facial expression recognizer trained with data of certain subjects under certain conditions to different subjects and/or different conditions. Such generalization capability is crucial in real-world applications. However, little attention has been focused on this problem in the literature. Transfer learning, a domain adaptation approach, provides effective techniques for transferring knowledge from source (training) data to target (testing) data when they are characterized by different properties. This paper makes the first attempt to apply transferring learning to cross-database facial expression recognition. It proposes a transfer learning based cross-database facial expression recognition approach, in which two training stages are involved: One for learning knowledge from source data, and the other for adapting the learned knowledge to target data. This approach has been implemented based on Gabor features extracted from facial images, regression tree classifiers, the AdaBoosting algorithm, and support vector machines. Evaluation experiments have been done on the JAFFE, FEED, and extended Cohn-Kanade databases. The results demonstrate that using the proposed transferring learning approach the cross-database facial expression recognition accuracy can be improved by more than 20%.", " Collective inference models have recently been used to significantly improve the predictive accuracy of node classifications in network domains. However, these methods have generally assumed a fully labeled network is available for learning. There has been relatively little work on transfer learning methods for collective classification, i.e., to exploit labeled data in one network domain to learn a collective classification model to apply in another network. While there has been some work on transfer learning for link prediction and node classification, the proposed methods focus on developing algorithms to adapt the models without a deep understanding of how the network structure impacts transferability. Here we make the key observation that collective classification models are generally composed of local model templates that are rolled out across a heterogeneous network to construct a larger model for inference. Thus, the transferability of a model could depend on similarity of the local model templates and/or the global structure of the data networks. In this work, we study the performance of basic relational models when learned on one network and transferred to another network to apply collective inference. We show, using both synthetic and real data experiments, that transferability of models depends on both the graph structure and local model parameters. Moreover, we show that a probability calibration process (that removes bias due to propagation errors in collective inference) improves transferability.", "tranfer learning  personalied ranking  collaborative filtering Collaborative filtering techniques aim at recommending products to users based on their historical feedback. And many algorithms focus on personalized ranking problem with implicit feedback due to the one-class nature of many real-world datasets in a variety of services. Most of the existing personalized ranking methods are confined to one domain of data source and the question of how to model users' preferences information across distinct domains is usually be ignored. There are some transfer learning approaches that try to transfer numerical ratings, auxiliary social relations and other information across different domains but they do not address how users' preferences information varies from one domain to another accordingly. And they mainly exploit rating prediction problem rather than personalized ranking problem. In this paper, we propose an algorithm called CroRank to address the question, How to bridge users' preferences information across different domains to promote better personalized ranking performance?. There are two main steps in CroRank, we first present an algorithm called multiple binomial matrix factorization (MBMF) to bridge the gap between items from distinct sources and then we introduce transfer Bayesian personalized ranking (TBPR) to recommend items for each user in the target domain. In CroRank, users' inclinations can transfer from the auxiliary domain to the target domain to provide better personalized ranking results. We compare CroRank to the state-of-the-art non-transfer models to demonstrate the improvements in flexibility and effectiveness.", "video annotation  transfer learning  heterogenous domain adaptation Searching desirable events in uncontrolled videos is a challenging task. Current researches mainly focus on obtaining concepts from numerous labeled videos. But it is time consuming and labor expensive to collect a large amount of required labeled videos to model events under various circumstances. To alleviate the labeling process, we propose to learn models for videos by leveraging abundant Web images which contains a rich source of information with many events taken under various conditions and roughly annotated. However, knowledge from the Web is noisy and diverse, brute force knowledge transfer may hurt the retrieval performance. To address such negative transfer problem, we propose a novel Joint Group Weighting Learning (JGWL) framework to leverage different but related groups of knowledge (source domain) queried from the Web image searching engine to real-world videos (target domain). Under this framework, weights of different groups are learned in a joint optimization framework, and each weight represents how contributive the corresponding image group is to the knowledge transferred to the videos. Moreover, to deal with the feature distribution mismatching between video feature space and image feature space, we build a common feature subspace to bridge these two heterogeneous feature spaces in an unsupervised manner. Experimental results on two challenging video datasets demonstrate that it is effective to use grouped knowledge gained from Web images for video retrieval.", "Chinese deception detection  corpus construction  transfer learning Deception detection is becoming indispensable to a growing number of applications in law enforcement and other government agencies. Recently, many researchers from both speech signal area and machine learning area have already shown that automatically deception detection from speech is promising. While there are a large amount of research works on English deception detection, few efforts have been put on Chinese which is quite different due to the culture divergence. In order to show the full potential of automatically deception detection, in this paper, we first construct the deceptive and non-deceptive Chinese speech corpus which has not been published so far. And then we propose a novel machine learning-based approach to detect deception in the same gender. Several popular machine learning algorithms are applied. Moreover, a transfer learning-based algorithm is applied for cross-gender deception detection. Experimental results show that our approach performs well on real-world corpus. In intra-gender deception detection, our approach can achieve roughly the same accuracy as the traditional method on English corpus. This means our corpus is reasonable and can be used for deception detection research. In cross-gender deception detection, our approach also outperforms the baseline methods.", " Apparent age estimation from face image has attracted more and more attentions as it is favorable in some real-world applications. In this work, we propose an end-to-end learning approach for robust apparent age estimation, named by us AgeNet. Specifically, we address the apparent age estimation problem by fusing two kinds of models, i.e., real-value based regression models and Gaussian label distribution based classification models. For both kind of models, large-scale deep convolutional neural network is adopted to learn informative age representations. Another key feature of the proposed AgeNet is that, to avoid the problem of over-fitting on small apparent age training set, we exploit a general-to-specific transfer learning scheme. Technically, the AgeNet is first pre-trained on a large-scale web-collected face dataset with identity label, and then it is fine-tuned on a large-scale real age dataset with noisy age label. Finally, it is fine-tuned on a small training set with apparent age label. The experimental results on the ChaLearn 2015 Apparent Age Competition demonstrate that our AgeNet achieves the state-of-the-art performance in apparent age estimation.", "heterogeneous feature space  transfer learning  unsupervised learnig  multi-task learning  feature mapping Transfer learning techniques try to transfer knowledge from previous tasks to a new target task with either fewer training data or less training than traditional machine learning techniques. Since transfer learning cares more about relatedness between tasks and their domains, it is useful for handling massive data, which are not labeled, to overcome distribution and feature space gaps, respectively. In this paper, we propose a new task selection algorithm in an unsupervised transfer learning domain, called as Task Selection Machine (TSM). It goes with a key technical problem, i.e., feature mapping for heterogeneous feature spaces. An extended feature method is applied to feature mapping algorithm. Also, TSM training algorithm, which is main contribution for this paper, relies on feature mapping. Meanwhile, the proposed TSM finally meets the unsupervised transfer learning requirements and solves the unsupervised multi-task transfer learning issues conversely.", "transfer learning  deep learning  feature extraction Transfer learning provides an approach to solve target tasks more quickly and effectively by using previously acquired knowledge learned from source tasks. As one category of transfer learning approaches, feature-based transfer learning approaches aim to find a latent feature space shared between source and target domains. The issue is that the sole feature space can't exploit the relationship of source domain and target domain fully. To deal with this issue, this paper proposes a transfer learning method that uses deep learning to extract hierarchical feature spaces, so knowledge of source domain can be exploited and transferred in multiple feature spaces with different levels of abstraction. In the experiment, the effectiveness of transfer learning in multiple feature spaces is compared and this can help us find the optimal feature space for transfer learning.", "transfer learning  e-learning  serious game  learning strategies  assessment strategies  structuring knowledge  learning process  learning by level Learning plays a very important role in the evolution of human knowledge  in teaching area, teachers notice more and more that the learning level is getting down, on the other hand learners are unable to use their knowledge in new situation in an effective way, they claim themselves did not have a good learning or they have misunderstood the course. However, to solve this matter, we believe that the teachers have to transfer their knowledge to the learners in an easy way, which will help the learners to use them in an effective way? In this document we will propose three levels of knowledge structuring that result: the learning strategies to facilitate comprehension and evaluation strategies to make sure of the result outcome. Those levels of structuring and those strategies can be used in digital and classical systems of training.", "object recognition  zero-shot learning  transfer learning We consider the problem of zero-shot recognition of object categories from images. Given a set of object categories (called known classes) with training images, our goal is to learn a system to recognize another non-overlapping set of object categories (called unknown classes) for which there are no training images. Our proposed approach exploits the recent work in natural language processing which has produced vector representations of words. Using the vector representations of object classes, we develop a method for transferring the appearance models from known object classes to unknown object classes. Our experimental results on three benchmark datasets show that our proposed method outperforms other competing approaches.", " Automated quality control of produce such as fruits and vegetables is of great importance to industry. In particular, the ability to evaluate the state of decay for various produce items would allow for efficient sorting of produce such that the freshest items could be more quickly shipped to consumers. Unfortunately, training an accurate classifier for determining how decayed produce is can require a large amount of data. This problem is further exacerbated by the large variety of produce available as different items would exhibit decay in different ways. In this paper, we propose an algorithm that can learn an accurate ranking classifier for sorting produce using only a small amount of data. We achieve this through our proposed transfer learning algorithm that is able to automatically select good preexisting source task training data to supplement insufficient training data in the given target task. We show how much our algorithm improves over standard training on real images of produce items captured at various stages of decay.", "Electrocorticography  Transfer learning  Multiple Kernel Learning Machine learning research is interested in building models based on a training set that can then be applied to new data, whether this unseen data comes from new examples (e.g. new subjects, other tasks) or new features (e.g. different modalities). In this work, we present a simple approach to transfer learning using intracranial EEG (also known as electrocorticographic, ECoG) data from three patients. More specifically, we aimed at detecting numerical processing during naturalistic settings based on a model trained with controlled experimental conditions. Our results showed significant prediction accuracy of numerical events in naturalistic settings when considering a priori knowledge of the target task.", " Fine-grained categorisation has been a challenging problem due to small inter-class variation, large intra-class variation and low number of training images. We propose a learning system which first clusters visually similar classes and then learns deep convolutional neural network features specific to each subset. Experiments on the popular fine-grained Caltech-UCSD bird dataset show that the proposed method outperforms recent fine-grained categorisation methods under the most difficult setting: no bounding boxes are presented at test time. It achieves a mean accuracy of 77.5%, compared to the previous best performance of 73.2%. We also show that progressive transfer learning allows us to first learn domain-generic features (for bird classification) which can then be adapted to specific set of bird classes, yielding improvements in accuracy.", "continuous emotion prediction  deep learning  benchmarking  affective computing Recently, mainly due to the advances of deep learning, the performances in scene and object recognition have been progressing intensively. On the other hand, more subjective recognition tasks, such as emotion prediction, stagnate at moderate levels. In such context, is it possible to make affective computational models benefit from the breakthroughs in deep learning? This paper proposes to introduce the strength of deep learning in the context of emotion prediction in videos. The two main contributions are as follow: (i) a new dataset, composed of 30 movies under Creative Commons licenses, continuously annotated along the induced valence and arousal axes (publicly available) is introduced, for which (ii) the performance of the Convolutional Neural Networks (CNN) through supervised finetuning, the Support Vector Machines for Regression (SVR) and the combination of both (Transfer Learning) are computed and discussed. To the best of our knowledge, it is the first approach in the literature using CNNs to predict dimensional affective scores from videos. The experimental results show that the limited size of the dataset prevents the learning or finetuning of CNN based frameworks but that transfer learning is a promising solution to improve the performance of affective movie content analysis frameworks as long as very large datasets annotated along affective dimensions are not available.", "Brain-computer interface (BCI)  EEG  event-related potentials (ERP)  domain adaptation  transfer learning  single-trial classification  online calibration Rapid serial visual presentation based brain-computer interface (BCI) system relies on single-trial classification of event-related potentials. Because of large individual differences, some labeled subject-specific data are needed to calibrate the classifier for each new subject. This paper proposes an online weighted adaptation regularization (OwAR) algorithm to reduce the online calibration effort, and hence to increase the utility of the BCI system. We show that given the same number of labeled subject-specific training samples, OwAR can significantly improve the online calibration performance. In other words, given a desired classification accuracy, OwAR can significantly reduce the number of labeled subject-specific training samples. Furthermore, we also show that the computational cost of OwAR can be reduced by more than 50% by source domain selection, without a statistically significant sacrifice of classification performance.", "emotion classification  transfer learning  sampling Emotion classification for microblog texts has wide applications such as in social security and business marketing areas. The amount of annotated microblog texts is very limited. In this paper, we therefore study how to utilize annotated data from other domains (source domain) to improve emotion classification on microblog texts (target domain). Transfer learning has been a successful approach for cross domain learning. However, to the best of our knowledge, little attention has been paid for automatically selecting the appropriate samples from the source domain before applying transfer learning. In this paper, we propose an effective framework to sampling available data in the source domain before transfer learning, which we name as Two-Stage Sampling. The improvement of emotion classification on Chinese microblog texts demonstrates the effectiveness of our approach.", "Drowsy driving  EEG  domain adaptation  model fusion  transfer learning Drowsy driving is a pervasive problem among drivers, and is also an important contributor to motor vehicle accidents. It is very important to be able to estimate a driver's drowsiness level online so that preventative actions could be taken to avoid accidents. However, because of large individual differences, it is very challenging to design an estimation algorithm whose parameters fit all subjects. Some subject-specific calibration data must be used to tailor the algorithm for each new subject. This paper proposes a domain adaptation with model fusion (DAMF) online drowsiness estimation approach using EEG signals. By making use of EEG data from other subjects in a transfer learning framework, DAMF requires very little subject-specific calibration data, which significantly increases its utility in practice. We demonstrate using a simulated driving experiment and 15 subjects that DAMF can achieve much better performance than several other approaches.", "emotion recognition  transfer learning  EEG Addressing the structural and functional variability between subjects for robust affective brain-computer interface (aBCI) is challenging but of great importance, since the calibration phase for aBCI is time-consuming. In this paper, we propose a subject transfer framework for electroencephalogram (EEG)-based emotion recognition via component analysis. We compare two state-of-the-art subspace projecting approaches called transfer component analysis (TCA) and kernel principle component analysis (KPCA) for subject transfer. The main idea is to learn a set of transfer components underlying source domain (source subjects) and target domain (target subject). When projected to this subspace, the difference of feature distributions of both domains can be reduced. From the experiments, we show that the two proposed approaches, TCA and KPCA, can achieve an improvement on performance with the best mean accuracies of 71.80% and 79.83%, respectively, in comparison of the baseline of 58.95%. The significant improvement shows the feasibility and efficiency of our approaches for subject transfer emotion recognition from EEG signals.", "Cross-domain recommendation  Transfer learning  Nonparametric pairwise clustering algorithm  Item cold start problem Recommender systems always offer the most suitable goods for users by adopting collaborative filtering or content-based technology based on historical ratings in one domain. However, sometimes we may suffer from item cold start problem in a new domain, especially in Cyber-Physical Systems (CPS). To alleviate this problem, many recommendation models have been proposed to transfer one domain's knowledge to another domain by using transfer learning algorithm. In this paper, we propose a new cross-domain recommendation algorithm which is divided into two stages. In the first stage, we apply the TrAdaBoost algorithm to select some items which are worthy of being recommended to users in the target domain. Then in the second stage, we adopt the nonparametric pairwise clustering algorithm to make a decision whether to recommend an item to a group of users or not. We not only make a classification for the target domain items but also find the recommended or not recommended customer groups for one item through the two stages. Experiments on real world data sets demonstrate that our proposed method performs better than other algorithms for the cross-domain recommendation task.", " In this paper, we propose a Transfer Learning method by Inductive Logic Programing for games. We generate general knowledge from a game, and specify the knowledge so that it is applicable in another game. This is called Transfer Learning. We show the working of Transfer Learning by taking knowledge from Tic-tac-toe and transfer it to Connect4 and Connect5. For Connect4 the number of Heuristic functions we developed is 30  for Connect5 it is 20.", "Stress modelling  Transfer learning  Semi-supervised learning Stress at work is a significant occupational health concern nowadays. Thus, researchers are looking to find comprehensive approaches for improving wellness interventions relevant to stress. Recent studies have been conducted for inferring stress in labour settings  they model stress behaviour based on non-obtrusive data obtained from smartphones. However, if the data for a subject is scarce, a good model cannot be obtained. We propose an approach based on transfer learning for building a model of a subject with scarce data. It is based on the comparison of decision trees to select the closest subject for knowledge transfer. We present an study carried out on 30 employees within two organisations. The results show that the in the case of identifying a  similar subject, the classification accuracy is improved via transfer learning.", "sentiment analysis  machine learning  support vector machine  classification  transfer learning  Polish  opinions A method for sentiment polarity assignment for textual content written in Polish using supervised machine learning approach with transfer learning scheme is proposed in the paper. It has been shown that performing simple natural language processing steps prior to classification, provides inspiring results without redundant computation overhead. The documents containing subjective opinions were classified using N-gram and Bigram language model that is able to encode some of complex word phrases. The experiments carried out on two real datasets taken from different domains proved that learning on one dataset and testing on another, which is commonly called transfer learning, can be effective and may result in very high classification quality.", "Dimension reduction  Sparse coding  Transfer learning  Speech emotion recognition In practice, the training data and testing data are often from different datasets, which have an adverse impact on speech emotion recognition rates. To tackle this problem, in this paper, a novel transfer principal component analysis (TPCA) and sparse coding based speech emotion recognition method is proposed. The TPCA approach is first presented for feature dimension reduction, then the sparse coding algorithm is introduced to learn the robust feature representations for both labeled source and unlabeled target corpora. To evaluate the performance of our proposed method, the experiments are conducted on two public datasets. Experimental results demonstrate that our proposed approach significantly outperforms the automatic recognition method, and obtains better performance than the state-of-the-art method.", "Transfer learning  Computer-aided detection system This paper addresses the issue of fusing datasets coming from different imaging protocols or scanners to boost the performance of computer aided diagnostic system. We present novel contributions in the field of subspace alignment methods that are part of domain adaptation framework. We first introduce a simple approach based on scaling the features of the different distribution and accounting for the class information. We also extend an unsupervised landmark based approach that has been recently developed to the supervised setting. These methods are evaluated in the context of prostate cancer screening based on two patient MRI databases acquired on different scanners. We demonstrate promising performance of the scaling based method when both databases contain similar number of annotated samples, and stable performance of the landmark based method even with unbalanced datasets.", "Brain  Hippocampus  Machine learning  MRI  Transfer learning Image-segmentation techniques based on supervised classification generally perform well on the condition that training and test samples have the same feature distribution. However, if training and test images are acquired with different scanners or scanning parameters, their feature distributions can be very different, which can hurt the performance of such techniques. We propose a feature-space-transformation method to overcome these differences in feature distributions. Our method learns a mapping of the feature values of training voxels to values observed in images from the test scanner. This transformation is learned from unlabeled images of subjects scanned on both the training scanner and the test scanner. We evaluated our method on hippocampus segmentation on 27 images of the Harmonized Hippocampal Protocol (HarP), a heterogeneous dataset consisting of 1.5T and 3T MR images. The results showed that our feature space transformation improved the Dice overlap of segmentations obtained with an SVM classifier from 0.36 to 0.85 when only 10 atlases were used and from 0.79 to 0.85 when around 100 atlases were used.", "EEG  Driving fatigue detection  Transfer learning  Domain adaptation In this paper, we first build up an electroencephalogram (EEG)-based driving fatigue detection system, and then propose a subject transfer framework for this system via component analysis. We apply a subspace projecting approach called transfer component analysis (TCA) for subject transfer. The main idea is to learn a set of transfer components underlying source domain (source subjects) and target domain (target subjects). When projected to this subspace, the difference of feature distributions of both domains can be reduced. Meanwhile, the discriminative information can be preserved. From the experiments, we show that the TCA-based algorithm can achieve a significant improvement on performance with the best mean accuracy of 77.56 %, in comparison of the baseline accuracy of 66.56 %. The improvement shows the feasibility and efficiency of our approach for subject transfer driving fatigue detection from EEG.", "Cross-view action recognition  Transfer learning  Discriminant analysis  Heterogeneous domain adaption We propose an approach of cross-view action recognition, in which the samples from different views are represented by heterogeneous features with different dimensions. Inspired by linear discriminant analysis (LDA), we introduce a discriminative common feature space to bridge the source and target views. Two different projection matrices are learned to respectively map the data from two different views into the common space by simultaneously maximizing the similarity of intra-class samples, minimizing the similarity of inter-class samples, and reducing the mismatch between data distributions of two views. Our method is neither restricted to the corresponding action instances in the two views nor restricted to a specific type of feature. We evaluate our approach on the IXMAS multi-view dataset and the experimental results demonstrate its effectiveness.", "Temporal adaptation  Term smoothing  Text categorization  Transfer learning This paper addresses text categorization problem that training data may be derived from a different time period than test data. We present a method for text categorization that minimizes the impact of temporal effects by using term smoothing and transfer learning techniques. We first used a technique called Temporal-based Term Smoothing (TTS) to replace those time sensitive features with representative terms, then applied boosting based transfer learning algorithm called TrAda-Boost for categorization. The results using a 21-year Japanese Mainichi Newspaper corpus showed that integrating term smoothing and transfer learning improves overall performance, especially it is effective when the creation time period of the test data differs greatly from the training data.", " This paper presents a novel approach for robot instruction for assembly tasks. We consider that robot programming can be made more efficient, precise and intuitive if we leverage the advantages of complementary approaches such as learning from demonstration, learning from feedback and knowledge transfer. Starting from low-level demonstrations of assembly tasks, the system is able to extract a high-level relational plan of the task. A graphical user interface (GUI) allows then the user to iteratively correct the acquired knowledge by refining high-level plans, and low-level geometrical knowledge of the task. This combination leads to a faster programming phase, more precise than just demonstrations, and more intuitive than just through a GUI. A final process allows to reuse high-level task knowledge for similar tasks in a transfer learning fashion. Finally we present a user study illustrating the advantages of this approach.", " Methods from machine learning have successfully been used to improve the performance of control systems in cases when accurate models of the system or the environment are not available. These methods require the use of data generated from physical trials. Transfer Learning (TL) allows for this data to come from a different, similar system. This paper studies a simplified TL scenario with the goal of understanding in which cases a simple, alignment-based transfer of data is possible and beneficial. Two linear, time-invariant (LTI), single-input, single-output systems are tasked to follow the same reference signal. A scalar, LTI transformation is applied to the output from a source system to align with the output from a target system. An upper bound on the 2-norm of the transformation error is derived for a large set of reference signals and is minimized with respect to the transformation scalar. Analysis shows that the minimized error bound is reduced for systems with poles that lie close to each other (that is, for systems with similar response times). This criterion is relaxed for systems with poles that have a larger negative real part (that is, for stable systems with fast response), meaning that poles can be further apart for the same minimized error bound. Additionally, numerical results show that using the reference signal as input to the transformation reduces the minimized bound further.", " Historically, neural nets have learned new things at the cost of forgetting what they already know. This problem is known as 'catastrophic forgetting'. Here, we examine how training a neural net in accordance with latently learned [1] output encodings drastically reduces catastrophic forgetting. Previous approaches to dealing with catastrophic forgetting have tended either to add extra samples to new training sets, modify the training of hidden nodes or model the interaction between short term and long term memory. Our approach is unique in that it both uses transfer learning to mitigate catastrophic forgetting and focuses upon the output nodes of a neural network. This results in a technique that makes it easier rather than harder to learn new tasks while retaining existing knowledge  is architecture independent and trivial to implement on any existing net. Additionally, we examine the use of ternary output codes. Binary codes assign a value to each output bit that may be thought of as either affirmative or negative. Ternary codes allow for the possibility that not every output bit has a meaningful response to every given input. By not forcing each output bit to train for a specific response for each new class, we hope to lessen catastrophic forgetting.", " Convolutional Neural Networks (CNNs) have set the state-of-the-art in many computer vision tasks in recent years. For this type of model, it is common to have millions of parameters to train, commonly requiring large datasets. We investigate a method to transfer learning across different texture classification problems, using CNNs, in order to take advantage of this type of architecture to problems with smaller datasets. We use a Convolutional Neural Network trained on a source dataset (with lots of data) to project the data of a target dataset (with limited data) onto another feature space, and then train a classifier on top of this new representation. Our experiments show that this technique can achieve good results in tasks with small datasets, by leveraging knowledge learned from tasks with larger datasets. Testing the method on the the Brodatz-32 dataset, we achieved an accuracy of 97.04% - superior to models trained with popular texture descriptors, such as Local Binary Patterns and Gabor Filters, and increasing the accuracy by 6 percentage points compared to a CNN trained directly on the Brodatz-32 dataset. We also present a visual analysis of the projected dataset, showing that the data is projected to a space where samples from the same class are clustered together - suggesting that the features learned by the CNN in the source task are relevant for the target task.", " Representation learning has emerged recently as a useful tool in the extraction of features from data. In a range of applications, features learned from data have been shown superior to their hand-crafted counterpart. Many deep learning approaches have taken advantage of such feature extraction. However, further research is needed on how such features can be evaluated for re-use in related applications, hopefully then improving performance on such applications. In this paper, we present a new method for ranking the representations learned by a Restricted Boltzmann Machine, which has been used regularly as a feature learner by deep networks. We show that high-ranking features, according to our method, should capture more information than low-ranking ones. We then apply representation ranking for pruning the network, and propose a new transfer learning algorithm, which uses such features extracted from a trained network to improve learning performance in another network trained on an analogous domain. We show that by transferring a small number of highest scored representations from source domain our method encourages the learning of new knowledge in target domain while preserving most of the information of the source domain during the transfer. This transfer learning is similar to self-taught learning in that it does not use the source domain data during the transfer process.", " We introduce a deep convolutional neural network (DCNN) as feature extraction method in a computer aided diagnosis (CAD) system in order to support diagnosis of diffuse lung diseases (DLD) on high-resolution computed tomography (HRCT) images. DCNN is a kind of multi layer neural network which can automatically extract features expression from the input data, however, it requires large amount of training data. In the field of medical image analysis, the number of acquired data is sometimes insufficient to train the learning system. Overcoming the problem, we apply a kind of transfer learning method into the training of the DCNN. At first, we apply massive natural images, which we can easily collect, for the pre-training. After that, small number of the DLD HRCT image as the labeled data is applied for fine-tuning. We compare DCNNs with training of (i) DLD HRCT images only, (ii) natural images only, and (iii) DLD HRCT images + natural images, and show the result of the case (iii) would be better DCNN feature rather than those of others.", "Kinship verification  Transfer metric learning  Cross domain  Sparse representation Kinship verification between aged parents and their children based on facial images is a challenging problem, due to aging factor which makes their facial similarities less distinct. In this paper, we propose to perform kinship verification in a transfer learning manner, which introduces photos of parents in their earlier ages as intermediate references to facilitate the verification. Child-young parent pairs are regarded as source domain and child-old parent ones are considered as target domain. The transfer learning scheme contains two phases. In the transfer metric learning phase, the extracted locality-constrained sparse features of images are projected into an optimized subspace where the intra-class distances are minimized and the inter-class ones are maximized. In the transfer classifier learning phase, a cross domain classifier is learned by a transfer SVM algorithm. Experimental results on UB KinFace dataset indicate that our method outperforms state-of-the-art methods.", " Object recognition and pose estimation from RGB-D images are important tasks for manipulation robots which can be learned from examples. Creating and annotating datasets for learning is expensive, however. We address this problem with transfer learning from deep convolutional neural networks (CNN) that are pre-trained for image categorization and provide a rich, semantically meaningful feature set. We incorporate depth information, which the CNN was not trained with, by rendering objects from a canonical perspective and colorizing the depth channel according to distance from the object center. We evaluate our approach on the Washington RGB-D Objects dataset, where we find that the generated feature set naturally separates classes and instances well and retains pose manifolds. We outperform state-of-the-art on a number of subtasks and show that our approach can yield superior results when only little training data is available.", "Hedge detection  Cross-domain  Transfer learning The difference of hedge cue distributions in various domains makes the domain-specific detectors difficult to extend to other domains. To make full use of out-of-domain data to adapt to a new domain and minimize annotation costs, we propose a novel cross-domain hedge detection approach called FIMultiSource, which combines instance-based and feature-based transfer learning approaches to make full use of multiple sources. Experiments carried on BioScope, WikiWeasel, and FactBank corpora show that our approach works well for cross-domain uncertainty recognition and always improves the detection performance compared to other state-of-the-art instance-based and feature-based transfer learning approaches.", " A massive amount of data is being produced in a continual stream, in real time, which renders traditional batch processing based data mining and neural network techniques as incapable. In this paper we focus on transfer learning from Spike Train Data, for which traditional techniques often require tasks to be distinctively identified during the training phase. We propose a novel dual network model that demonstrates transfer learning from spike train data without explicit task specification. An implementation of the proposed approach was tested experimentally to evaluate its ability to use previously learned knowledge to improve the learning of new tasks.", "FCM-type Co-clustering  Reinforcement learning  Transfer learning In applying reinforcement learning to continuous space problems, discretization or redefinition of the learning space can be a promising approach. Several methods and algorithms have been introduced to learning agents to respond to this problem. In our previous study, we introduced an FCCM clustering technique into Q-learning (called QL-FCCM) and its transfer learning in the Markov process. Since we could not respond to complicated environments like a non-Markov process, in this study, we propose a method in which an agent updates his Q-table by changing the trade-off ratio, Q-learning and QL-FCCM, based on the damping ratio. We conducted numerical experiments of the single pendulum standing problem and our model resulted in a smooth learning process.", " We study the optimal rates of convergence for estimating a prior distribution over a VC class from a sequence of independent data sets respectively labeled by independent target functions sampled from the prior. We specifically derive upper and lower bounds on the optimal rates under a smoothness condition on the correct prior, with the number of samples per data set equal the VC dimension. These results have implications for the improvements achievable via transfer learning. We additionally extend this setting to real-valued function, where we establish consistency of an estimator for the prior, and discuss an additional application to a preference elicitation problem in algorithmic economics.", "Negative transfer  Transfer learning  Class noise detection Transfer learning has been used as a machine learning method to make good use of available language resources for other resource-scarce languages. However, the cumulative class noise during iterations of transfer learning can lead to negative transfer which can adversely affect performance when more training data is used. In this paper, we propose a novel transfer learning method which can detect negative transfers. This approach detects high quality samples after certain iterations to identify class noise in new transferred training samples and remove them to reduce misclassifications. With the ability to detect bad training samples and remove them, our method can make full use of large unlabeled training data available in the target language. Furthermore, the most important contribution in this paper is the theory of class noise detection. Our new class noise detection method overcame the theoretic flaw of a previous method based on Gaussian distribution. We applied this transfer learning method with negative transfer detection to cross lingual opinion analysis. Evaluation on the NLP&CC 2013 cross-lingual opinion analysis dataset shows that the proposed approach outperforms the state-of-the-art systems.", "Transfer learning  Long text analysis  Short text analysis  Latent semantic analysis Transfer learning has emerged as a new learning technique facilitating an improved learning result of one task by integrating the well learnt knowledge from another related task. While much research has been devoted to develop the transfer learning algorithms in the field of long text analysis, the development of the transfer learning techniques over the short texts still remains challenging. The challenge of short text data analysis arises due to its sparse nature, noise words, syntactical structure and colloquial terminologies used. In this paper, we propose AutoTL(Automatic Transfer Learning), a transfer learning framework in short text analysis with automatic training data selection and no requirement of data priori probability distribution. In addition, AutoTL enables an accurate and effective learning by transferring the knowledge automatically learnt from the online information. Our experimental results confirm the effectiveness and efficiency of our proposed technique.", "Speech analysis  Paralinguistics  Big data  Self-learning In spoken language analysis tasks, one is often faced with comparably small available corpora of only one up to a few hours of speech material mostly annotated with a single phenomenon such as a particular speaker state at a time. In stark contrast to this, engines such as for the recognition of speakers' emotions, sentiment, personality, or pathologies, are often expected to run independent of the speaker, the spoken content, and the acoustic conditions. This lack of large and richly annotated material likely explains to a large degree the headroom left for improvement in accuracy by todays engines. Yet, in the big data era, and with the increasing availability of crowd-sourcing services, and recent advances in weakly supervised learning, new opportunities arise to ease this fact. In this light, this contribution first shows the de-facto standard in terms of data-availability in a broad range of speaker analysis tasks. It then introduces highly efficient 'cooperative' learning strategies basing on the combination of active and semi-supervised alongside transfer learning to best exploit available data in combination with data synthesis. Further, approaches to estimate meaningful confidence measures in this domain are suggested, as they form (part of) the basis of the weakly supervised learning algorithms. In addition, first successful approaches towards holistic speech analysis are presented using deep recurrent rich multi-target learning with partially missing label information. Finally, steps towards needed distribution of processing for big data handling are demonstrated.", " We study the binary transfer learning problem, focusing on how to select sources from a large pool and how to combine them to yield a good performance on a target task. In particular, we consider the transfer learning setting where one does not have direct access to the source data, but rather employs the source hypotheses trained from them. Building on the literature on the best subset selection problem, we propose an efficient algorithm that selects relevant source hypotheses and feature dimensions simultaneously. On three computer vision datasets we achieve state-of-the-art results, substantially outperforming transfer learning and popular feature selection baselines in a small-sample setting. Also, we theoretically prove that, under reasonable assumptions on the source hypotheses, our algorithm can learn effectively from few examples.", "Naive Bayes Nearest Neighbor  Domain adaptation  Transfer learning As of today, object categorization algorithms are not able to achieve the level of robustness and generality necessary to work reliably in the real world. Even the most powerful convolutional neural network we can train fails to perform satisfactorily when trained and tested on data from different databases. This issue, known as domain adaptation and/or dataset bias in the literature, is due to a distribution mismatch between data collections. Methods addressing it go from max-margin classifiers to learning how to modify the features and obtain a more robust representation. Recent work showed that by casting the problem into the image-to-class recognition framework, the domain adaptation problem is significantly alleviated [23]. Here we follow this approach, and show how a very simple, learning free Naive Bayes Nearest Neighbor (NBNN)-based domain adaptation algorithm can significantly alleviate the distribution mismatch among source and target data, especially when the number of classes and the number of sources grow. Experiments on standard benchmarks used in the literature show that our approach (a) is competitive with the current state of the art on small scale problems, and (b) achieves the current state of the art as the number of classes and sources grows, with minimal computational requirements.", " We present in this study a novel approach to predicting EEG epileptic seizures: we accurately model and predict non-ictal cortical activity and use prediction errors as parameters that significantly distinguish ictal from non-ictal activity. We suppress seizure-related activity by modeling EEG signal acquisition as a cocktail party problem and obtaining seizure-related activity using Independent Component Analysis. Following recent studies intricately linking seizure to increased, widespread synchrony, we construct dynamic EEG synchronization graphs in which the electrodes are represented as nodes and the pair-wise correspondences between them are represented by edges. We extract 38 intuitive features from the synchronization graph as well as the original signal. From this, we use a rigorous method of feature selection to determine minimally redundant features that can describe the non-ictal EEG signal maximally. We learn a one-step forecast operator restricted to just these features, using autoregression (AR(1)). We improve this in a novel way by cross-learning common knowledge across patients and recordings using Transfer Learning, and devise a novel transformation to increase the efficiency of transfer learning. We declare imminent seizure based on detecting outliers in our prediction errors using a simple and intuitive method. Our median seizure detection time is 11.04 min prior to the labeled start of the seizure compared to a benchmark of 1.25 min prior, based on previous work on the topic. To the authors' best knowledge this is the first attempt to model seizure prediction in this manner, employing efficient seizure suppression, the use of synchronization graphs and transfer learning, among other novel applications.", "Unsupervised learning  Transfer learning  Deep learning  Scene categorization  Object detection Classifying scenes (e. g. into street, home or leisure) is an important but complicated task nowadays, because images come with variability, ambiguity, and a wide range of illumination or scale conditions. Standard approaches build an intermediate representation of the global image and learn classifiers on it. Recently, it has been proposed to depict an image as an aggregation of its contained objects: the representation on which classifiers are trained is composed of many heterogeneous feature vectors derived from various object detectors. In this paper, we propose to study different approaches to efficiently learn contextual semantics out of these object detections. We use the features provided by Object-Bank [24] (177 different object detectors producing 252 attributes each), and show on several benchmarks for scene categorization that careful combinations, taking into account the structure of the data, allows to greatly improve over original results (from + 5 to + 11 %) while drastically reducing the dimensionality of the representation by 97% (from 44,604 to 1,000). We also show that the uncertainty relative to object detectors hampers the use of external semantic knowledge to improve detectors combination, unlike our unsupervised learning approach.", " Data recorded while learners are interacting with Massive Open Online Courses (MOOC) platforms provide a unique opportunity to build predictive models that can help anticipate future behaviors and develop interventions. But since most of the useful predictive problems are defined for a real-time framework, using knowledge drawn from the past courses becomes crucial. To address this challenge, we designed a set of processes that take advantage of knowledge from both previous courses and previous weeks of the same course to make real time predictions on learners behavior. In particular, we evaluate multiple transfer learning methods. In this article, we present our results for the stopout prediction problem (predicting which learners are likely to stop engaging in the course). We believe this paper is a first step towards addressing the need of transferring knowledge across courses.", "ICD-9-CM  Text classification  Transfer learning  Learning to rank  Document expansion  Icd coding task The task of assigning classification codes to short medical text is a hard text classification problem, especially when the set of possible codes is as big as the ICD-9-CM set. The problem, which has been only partially tamed for a subset of ICD-9-CM, becomes even harder in real world applications, where the labeled data are scarce and noisy. In this paper we first show the ineffectivenesss of current Text Classification algorithms on large datasets, then we present a novel incremental approach to clinical Text Classification, which overcomes the low accuracy problem through the top-K retrieval, exploits Transfer Learning techniques in order to expand a skewed dataset and improves the overall accuracy over time, learning from user selection.", "Multi-task Learning  Machine learning  Blood transfusion  Health care  Regression  Classification It would be desirable before a surgical procedure to have a prediction rule that could accurately estimate the probability of a patient bleeding, need for blood transfusion, and other important outcomes. Such a prediction rule would allow optimal planning, more efficient use of blood bank resources, and identification of high-risk patient cohort for specific perioperative interventions. The goal of this study is to develop an efficient and accurate algorithm that could estimate the risk of multiple outcomes simultaneously. Specifically, a heterogeneous multi-task learning method is proposed for learning outcomes such as perioperative bleeding, intraoperative RBC transfusion, ICU care, and ICU length of stay. Additional outcomes not normally predicted are incorporated in the model for transfer learning and help improve the performance of relevant outcomes. Results for predicting perioperative bleeding and need for blood transfusion for patients undergoing non-cardiac operations from an institutional transfusion datamart show that the proposed method significantly increases AUC and G-Mean by more than 6% and 5% respectively over standard single-task learning methods.", "Deep learning  Transfer learning  Ensemble Transfer learning algorithms typically assume that the training data and the test data come from different distribution. It is better at adapting to learn new tasks and concepts more quickly and accurately by exploiting previously gained knowledge. Deep Transfer Learning (DTL) emerged as a new paradigm in transfer learning in which a deep model offer greater flexibility in extracting high-level features. DTL offers selective layer based transference, and it is problem specific. In this paper, we propose the Ensemble of Deep Transfer Learning (EDTL) methodology to reduce the impact of selective layer based transference and provide optimized framework to work for three major transfer learning cases. Empirical results on character, object and biomedical image recognition tasks achieves that the proposed method indicate statistically significant classification accuracy over the other established transfer learning method.", " We present a Transfer Learning (TL) framework based on Stacked denoising Autoencoder (SDA) for the recognition of immunogold particles. These particles are part of a high-resolution method for the selective localization of biological molecules at the subcellular level only visible through Transmission Electron Miscroscopy (TEM). Four new datasets were acquired encompassing several thousands of immunogold particles. Due to the particles size (for a particular dataset a particle has a radius of 4 pixels in an image of size 4008x2670) the annotation of these datasets is extremely time taking. Thereby, we apply a (TL) approach by reusing the learning model that can be used on other datasets containing particles of different (or similar) sizes. In our experimental study we verified that our (TL) framework outperformed the baseline (not involving TL) approach by more than 20% of accuracy on the recognition of immunogold particles.", "Task-environment  Automation  Intelligence evaluation  Artificial intelligence  Machine learning Evaluation of artificial intelligence (AI) systems is a prerequisite for comparing them on the many dimensions they are intended to perform on. Design of task-environments for this purpose is often ad-hoc, focusing on some limited aspects of the systems under evaluation. Testing on a wide range of tasks and environments would better facilitate comparisons and understanding of a system's performance, but this requires that manipulation of relevant dimensions cause predictable changes in the structure, behavior, and nature of the task-environments. What is needed is a framework that enables easy composition, decomposition, scaling, and configuration of task-environments. Such a framework would not only facilitate evaluation of the performance of current and future AI systems, but go beyond it by allowing evaluation of knowledge acquisition, cognitive growth, lifelong learning, and transfer learning. In this paper we list requirements that we think such a framework should meet to facilitate the evaluation of intelligence, and present preliminary ideas on how this could be realized.", "Collaborative filtering  Neural network  Representation learning Learning from multi-view data is important in many applications. However, traditional multi-view learning algorithms require the availability of the representation from multi-view data in advance, it is hard to apply these methods to recommendation task directly. In fact, the idea of multi-view learning is particularly suitable for alleviating the sparsity challenge faced in various recommender systems by adding additional view to augment traditional view of sparse rating matrix. In this paper, we propose a unified Collaborative Multi-view Learning (CML) framework for recommender systems, which can exploit task adaptive multi-view representation of data automatically. The main idea is to formulate a joint optimization framework, combining the merits of matrix factorization model and transfer learning technique in a multiview framework. Experiments on real-life public datasets show that our model outperforms the compared state-of-the-art baselines.", " We propose a pipeline for transductive transfer learning and demonstrate it in computer vision tasks. In pattern classification, methods for transductive transfer learning (also known as unsupervised domain adaptation) are designed to cope with cases in which one cannot assume that training and test sets are sampled from the same distribution, i.e., they are from different domains. However, some unlabelled samples that belong to the same domain as the test set (i.e. the target domain) are available, enabling the learner to adapt its parameters. We approach this problem by combining three methods that transform the feature space. The first finds a lower dimensional space that is shared between source and target domains. The second uses local transformations applied to each source sample to further increase the similarity between the marginal distributions of the datasets. The third applies one transformation per class label, aiming to increase the similarity between the posterior probability of samples in the source and target sets. We show that this combination leads to an improvement over the state-of-the-art in cross-domain image classification datasets, using raw images or basic features and a simple one-nearest-neighbour classifier.", " Transfer learning has shown promising results in leveraging loosely labeled Web images (source domain) to learn a robust classifier for the unlabeled consumer videos (target domain). Existing transfer learning methods typically apply source domain data to learn a fixed model for predicting target domain data once and for all, ignoring rapidly updating Web data and continuously changes of users requirements. We propose an incremental transfer learning framework, in which heterogeneous knowledge are integrated and incrementally added to update the target classifier during learning process. Under the framework, images (image source domain) queried from Web image search engine and videos (video source domain) from existing action datasets are adopted to provide static information and motion information of the target video, respectively. For the image source domain, images are partitioned into several groups according to their semantic information. And for the video source domain, videos are divided in the same way. Unlike traditional methods which measure relevance between the source group and the whole target domain videos, the group weights in this paper are treated as latent variables for each target domain video and learned automatically according to the probability distribution difference between the individual source group and target domain videos. Experimental results on the two challenging video datasets (i.e., CCV and Kodak) demonstrate the effectiveness of our proposed method.", " A Deep Belief Network is a machine learning approach which can learn hierarchical levels of representations. However, a Deep Belief Network requires large amounts of training examples to learn good representations. Transfer learning is able to improve the performance of learning, especially when the number of training examples is small. This paper studies different transfer learning methods using representational transfer in deep belief networks, and experimental result shows that these methods are able to improve the performance of learning.", "Dataset bias  Domain adaptation  Iterative self-labeling Despite the increasing interest towards domain adaptation and transfer learning techniques to generalize over image collections and overcome their biases, the visual community misses a large scale testbed for cross-dataset analysis. In this paper we discuss the challenges faced when aligning twelve existing image databases in a unique corpus, and we propose two cross-dataset setups that introduce new interesting research questions. Moreover, we report on a first set of experimental domain adaptation tests showing the effectiveness of iterative self-labeling for large scale problems.", "Histograms of Topographical features (HoT)  Spectral regression  Transfer learning  Pain intensity estimation Automatic monitoring for the assessment of pain can significantly improve the psychological comfort of patients. Recently introduced databases with expert annotation opened the way for pain intensity estimation from facial analysis. In this contribution, pivotal face elements are identified using the Histograms of Topographical features (HoT) which are a generalization of the topographical primal sketch. In order to improve the discrimination between different pain intensity values and respectively the generalization with respect to the monitored persons, we transfer data representation from the emotion oriented Cohn-Kanade database to the UNBC McMaster Shoulder Pain database.", "Link Prediction  Signed Social Network  AdaBoost Algorithm In signed social network, the user-generated content and interactions have overtaken the web. Questions of whom and what to trust has become increasingly important. We must have methods which predict the signs of links in the social network to solve this problem. We study signed social networks with positive links (friendship, fan, like, etc) and negative links (opposition, anti-fan, dislike, etc). Specifically, we focus how to effectively predict positive and negative links in newly signed social networks. With SVM model, the small amount of edge sign information in newly signed network is not adequate to train a good classifier. In this paper, we introduce an effective solution to this problem. We present a novel transfer learning framework is called Transfer AdaBoost with SVM (TAS) which extends boosting-based learning algorithms and incorporates properly designed RBFSVM (SVM with the RBF kernel) component classifiers. With our framework, we use explicit topological features and Positive Negative Ratio (PNR) features which are based on decision-making theory. Experimental results on three networks (Epinions, Slashdot and Wiki) demonstrate our method that can improve the prediction accuracy by 40% over baseline methods. Additionally, our method has faster performance time. (C) 2015 The Authors. Published by Elsevier B.V.", "Pedestrians simulation  Transfer learning  Policy Reuse  Vector Quantization  Tile coding In this work, a Multi-agent Reinforcement Learning framework is used to generate simulations of virtual pedestrians groups. The aim is to study the influence of two different learning approaches in the quality of generated simulations. The case of study consists on the simulation of the crossing of two groups of embodied virtual agents inside a narrow corridor. This scenario is a classic experiment inside the pedestrian modeling area, because a collective behavior, specifically the lanes formation, emerges with real pedestrians. The paper studies the influence of different learning algorithms, function approximation approaches, and knowledge transfer mechanisms on performance of learned pedestrian behaviors. Specifically, two different RL-based schemas are analyzed. The first one, Iterative Vector Quantization with Q-Learning (ITVQQL), improves iteratively a state-space generalizer based on vector quantization. The second scheme, named TS, uses tile coding as the generalization method with the Sarsa(lambda) algorithm. Knowledge transfer approach is based on the use of Probabilistic Policy Reuse to incorporate previously acquired knowledge in current learning processes  additionally, value function transfer is also used in the ITVQQL schema to transfer the value function between consecutive iterations. Results demonstrate empirically that our RL framework generates individual behaviors capable of emerging the expected collective behavior as occurred in real pedestrians. This collective behavior appears independently of the learning algorithm and the generalization method used, but depends extremely on whether knowledge transfer was applied or not. In addition, the use of transfer techniques has a remarkable influence in the final performance (measured in number of times that the task was solved) of the learned behaviors.", "transfer learning  deep learning  feature extraction  fuzzy sets Transfer learning provides an approach to solve target tasks more quickly and effectively by using previously-acquired knowledge learned from source tasks. Most of transfer learning approaches extract knowledge of source domain in the given feature space. The issue is that single perspective can't mine the relationship of source domain and target domain fully. To deal with this issue, this paper develops a method using Stacked Denoising Autoencoder (SDA) to extract new feature spaces for source domain and target domain, and define two fuzzy sets to analyse the variation of prediction accuracy of target task in new feature spaces.", "graph learning  probabilistic graph matching  relational data mining  graph mining  collaborative data analysis  distributed processing  attributed graphs  transfer learning Many of the real-world problems, - including human knowledge, communication, biological, and cyber network analysis, - deal with data entities for which the essential information is contained in the relations among those entities. Such data must be modeled and analyzed as graphs, with attributes on both objects and relations encode and differentiate their semantics. Traditional data mining algorithms were originally designed for analyzing discrete objects for which a set of features can be defined, and thus cannot be easily adapted to deal with graph data. This gave rise to the relational data mining field of research, of which graph pattern learning is a key sub-domain [11]. In this paper, we describe a model for learning graph patterns in collaborative distributed manner. Distributed pattern learning is challenging due to dependencies between the nodes and relations in the graph, and variability across graph instances. We present three algorithms that trade-off benefits of parallelization and data aggregation, compare their performance to centralized graph learning, and discuss individual benefits and weaknesses of each model. Presented algorithms are designed for linear speedup in distributed computing environments, and learn graph patterns that are both closer to ground truth and provide higher detection rates than centralized mining algorithm.", "Deep neural network  Transfer learning  Optimization Deep Transfer Learning (DTL) emerged as a new paradigm in machine learning in which a deep model is trained on a source task and the knowledge acquired is then totally or partially transferred to help in solving a target task. Even though DTL offers a greater flexibility in extracting high-level features and enabling feature transference from a source to a target task, the DTL solution might get stuck at local minima leading to performance degradation-negative transference-, similar to what happens in the classical machine learning approach. In this paper, we propose the Source-Target-Source (STS) methodology to reduce the impact of negative transference, by iteratively switching between source and target tasks in the training process. The results show the effectiveness of such approach.", "Machine Vision  Universal feature extraction  Information Theory  Transfer learning In many real world image based pattern recognition tasks, the extraction and usage of task-relevant features are the most crucial part of the diagnosis. In the standard approach, they mostly remain task-specific, although humans who perform such a task always use the same image features, trained in early childhood. It seems that universal feature sets exist, but they are not yet systematically found. In our contribution, we tried to find those universal image feature sets that are valuable for most image related tasks. In our approach, we trained a neural network by natural and non-natural images of objects and background, using a Shannon information-based algorithm and learning constraints. The goal was to extract those features that give the most valuable information for classification of visual objects hand-written digits. This will give a good start and performance increase for all other image learning tasks, implementing a transfer learning approach. As result, in our case we found that we could indeed extract features which are valid in all three kinds of tasks.", "Common decision vector  transfer learning  classification  core vector machine Multiple source transfer learning (MSTL) has been obtaining more and more applications especially from several related source domains to help the learning task on target domain. However, multiple source transfer learning algorithms often deal with the corresponding quadratic programming problems which may suffer a big computational burden caused by the kernel matrix computation. In this paper, a novel common-decision-vector based multiple source transfer classification learning (CDV-MSTL) is proposed which doesn't depend on the intrinsic structure of data. This algorithm is based on the structural risk minimization principle and the SVM like framework, so it has good adaptability and better accuracy. Based on the theory of CVM, CDV-MSTL is extended to its CVM based version which can realize fast training for large scale data. Extensive experiments on synthetic and real-world datasets demonstrate the significant improvement in classification performance obtained by the proposed algorithm over existing MSTL algorithm.", "People counting  transfer learning  correspondence estimation We present a novel two-pass framework for counting the number of people in an environment, where multiple cameras provide different views of the subjects. By exploiting the complementary information captured by the cameras, we can transfer knowledge between the cameras to address the difficulties of people counting and improve the performance. The contribution of this paper is threefold. First, normalizing the perspective of visual features and estimating the size of a crowd are highly correlated tasks. Hence, we treat them as a joint learning problem. The derived counting model is scalable and it provides more accurate results than existing approaches. Second, we introduce an algorithm that matches groups of pedestrians in images captured by different cameras. The results provide a common domain for knowledge transfer, so we can work with multiple cameras without worrying about their differences. Third, the proposed counting system is comprised of a pair of collaborative regressors. The first one determines the people count based on features extracted from intracamera visual information, whereas the second calculates the residual by considering the conflicts between intercamera predictions. The two regressors are elegantly coupled and provide an accurate people counting system. The results of experiments in various settings show that, overall, our approach outperforms comparable baseline methods. The significant performance improvement demonstrates the effectiveness of our two-pass regression framework.", "Preference learning  Collaborative filtering  Heterogeneous implicit feedbacks  Adaptive Bayesian personalized ranking  Transfer learning Implicit feedbacks have recently received much attention in recommendation communities due to their close relationship with real industry problem settings. However, most works only exploit users' homogeneous implicit feedbacks such as users' transaction records from bought activities, and ignore the other type of implicit feedbacks like examination records from browsed activities. The latter are usually more abundant though they are associated with high uncertainty w.r.t. users' true preferences. In this paper, we study a new recommendation problem called heterogeneous implicit feedbacks (HIF), where the fundamental challenge is the uncertainty of the examination records. As a response, we design a novel preference learning algorithm to learn a confidence for each uncertain examination record with the help of transaction records. Specifically, we generalize Bayesian personalized ranking (BPR), a seminal pairwise learning algorithm for homogeneous implicit feedbacks, and learn the confidence adaptively, which is thus called adaptive Bayesian personalized ranking (ABPR). ABPR has the merits of uncertainty reduction on examination records and accurate pairwise preference learning on implicit feedbacks. Experimental results on two public data sets show that ABPR is able to leverage uncertain examination records effectively, and can achieve better recommendation performance than the state-of-the-art algorithm on various ranking-oriented evaluation metrics. ", "transfer learning  transfer subspace learning  electro-optical imaging  vehicle recognition  manifold learning Transfer Subspace Learning has gained recent popularity in the literature for its ability to perform cross-dataset and cross-domain object recognition-enablers for data fusion. The ability to leverage existing data without the need for additional data collections is attractive for Automatic Target Recognition applications. For Automatic Target Recognition (or object assessment) applications, Transfer Subspace Learning is a game changer for dynamic systems, as it enables the incorporation of sparse and dynamically collected data into existing systems that utilize large, dense databases. A baseline Transfer Subspace Learning technique is the Transfer Fisher's Linear Discriminative Analysis, an approach based on Bregman divergence-based regularization. This paper modifies the implementation of the Transfer Fisher's Linear Discriminative Analysis technique by combining it with Manifold Learning and adjusting it to allow for a more systematic search of tuning parameters. Specifically, the Diffusion Map approach is utilized, a Manifold Learning approach based on heat diffusion. The modified technique is then utilized for cross-data and cross-domain electro-optical vehicle recognition.", " Purely, data-driven large scale image classification has been achieved using various feature descriptors like SIFT, HOG etc. Major milestone in this regards is Convolutional Neural Networks (CNN) based methods which learn optimal feature descriptors as filters. Little attention has been given to the use of domain knowledge. Ontology plays an important role in learning to categorize images into abstract classes where there may not be a clear visual connect between category and image, for example identifying image mood - happy, sad and neutral. Our algorithm combines CNN and ontology priors to infer abstract patterns in Indian Monument Images. We use a transfer learning based approach in which, knowledge of domain is transferred to CNN while training (top down transfer) and inference is made using CNN prediction and ontology tree/priors (bottom up transfer). We classify images to categories like Tomb, Fort and Mosque. We demonstrate that our method improves remarkably over logistic classifier and other transfer learning approach. We conclude with a remark on possible applications of the model and note about scaling this to bigger ontology.", " Most existing cross-domain recommendation algorithms focus on modeling ratings, while ignoring review texts. The review text, however, contains rich information, which can be utilized to alleviate data sparsity limitations, and interpret transfer patterns. In this paper, we investigate how to utilize the review text to improve cross-domain collaborative filtering models. The challenge lies in the existence of non-linear properties in some transfer patterns. Given this, we extend previous transfer learning models in collaborative filtering, from linear mapping functions to non-linear ones, and propose a cross-domain recommendation framework with the review text incorporated. Experimental verifications have demonstrated, for new users with sparse feedback, utilizing the review text obtains 10% improvement in the AUC metric, and the non-linear method outperforms the linear ones by 4%", " We establish conditions under which memoryless policies and finite-state controllers that solve one partially observable non-deterministic problem (PONDP) generalize to other problems  namely, problems that have a similar structure and share the same action and observation space. This is relevant to generalized planning where plans that work for many problems are sought, and to transfer learning where knowledge gained in the solution of one problem is to be used on related problems. We use a logical setting where uncertainty is represented by sets of states and the goal is to be achieved with certainty. While this gives us crisp notions of solution policies and generalization, the account also applies to probabilistic PONDs, i.e., Goal POMDPs.", " Recent researches on transfer learning exploit deep structures for discriminative feature representation to tackle cross-domain disparity. However, few of them are able to joint feature learning and knowledge transfer in a unified deep framework. In this paper, we develop a novel approach, called Deep Low-Rank Coding (DLRC), for transfer learning. Specifically, discriminative low-rank coding is achieved in the guidance of an iterative supervised structure term for each single layer. In this way, both marginal and conditional distributions between two domains intend to be mitigated. In addition, a marginalized denoising feature transformation is employed to guarantee the learned single-layer low-rank coding to be robust despite of corruptions or noises. Finally, by stacking multiple layers of low-rank codings, we manage to learn robust cross-domain features from coarse to fine. Experimental results on several benchmarks have demonstrated the effectiveness of our proposed algorithm on facilitating the recognition performance for the target domain.", " Transfer learning has attracted a lot of attention in the past decade. One crucial research issue in transfer learning is how to find a good representation for instances of different domains such that the divergence between domains can be reduced with the new representation. Recently, deep learning has been proposed to learn more robust or higher-level features for transfer learning. However, to the best of our knowledge, most of the previous approaches neither minimize the difference between domains explicitly nor encode label information in learning the representation. In this paper, we propose a supervised representation learning method based on deep autoencoders for transfer learning. The proposed deep autoencoder consists of two encoding layers: an embedding layer and a label encoding layer. In the embedding layer, the distance in distributions of the embedded instances between the source and target domains is minimized in terms of KL-Divergence. In the label encoding layer, label information of the source domain is encoded using a softmax regression model. Extensive experiments conducted on three real-world image datasets demonstrate the effectiveness of our proposed method compared with several state-of-the-art baseline methods.", " In this extended abstract we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by presenting a benchmark set of domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. We conclude with a brief update on the latest ALE developments. All of the software, including the benchmark agents, is publicly available.", " Reinforcement learning algorithms typically require too many 'trial-and-error' experiences before reaching a desirable behaviour. A considerable amount of ongoing research is focused on speeding up this learning process by using external knowledge. We contribute in several ways, proposing novel approaches to transfer learning and learning from demonstration, as well as an ensemble approach to combine knowledge from various sources.", "Transfer learning  Multi-source  Shared subspace of labels Multi source transfer learning focuses on studying the scarcity of samples with labels in target domain, while neglecting the analysis about transferability relationship among multiple source domains. Thus, we propose a method that transforms samples in target domain into multi label samples, with which it is able to analyze the correlations among predicted labels from different sources. We design a method that can extract the shared subspace among labels in multi-sources, and propose a novel multi-source transfer learning method based on multi-label shared subspace. This approach is required when knowledge about multiple sources are available but it is unknown which source is of more transferability. Experiments show that our proposed algorithm can improve the performance of transfer learning method and alleviate dine complexity. ", "Receiver operating characteristic  AUC  Semi-supervised learning  Transfer learning  Semidefinite programming  RankBoost  SVMROC  SSLROC Receiver operating characteristic (ROC) analysis is a standard methodology to evaluate the performance of a binary classification system. The area under the ROC curve (AUC) is a performance metric that summarizes how well a classifier separates two classes. Traditional AUC optimization techniques are supervised learning methods that utilize only labeled data (i.e., the true class is known for all data) to train the classifiers. In this work, inspired by semi-supervised and transductive learning, we propose two new AUC optimization algorithms hereby referred to as semi-supervised learning receiver operating characteristic (SSLROC) algorithms, which utilize unlabeled test samples in classifier training to maximize AUC. Unlabeled samples are incorporated into the AUC optimization process, and their ranking relationships to labeled positive and negative training samples are considered as optimization constraints. The introduced test samples will cause the learned decision boundary in a multidimensional feature space to adapt not only to the distribution of labeled training data, but also to the distribution of unlabeled test data. We formulate the semi-supervised AUC optimization problem as a semi-definite programming problem based on the margin maximization theory. The proposed methods SSLROC1 (1-norm) and SSLROC2 (2-norm) were evaluated using 34 (determined by power analysis) randomly selected datasets from the University of California, Irvine machine learning repository. Wilcoxon signed rank tests showed that the proposed methods achieved significant improvement compared with state-of-the-art methods. The proposed methods were also applied to a CT colonography dataset for colonic polyp classification and showed promising results.(1) Published by Elsevier Ltd.", "Classification  fuzzy systems  generalized hidden-mapping ridge regression (GHRR)  inductive transfer learning  kernel methods  knowledge-leverage  neural networks  regression Inductive transfer learning has attracted increasing attention for the training of effective model in the target domain by leveraging the information in the source domain. However, most transfer learning methods are developed for a specific model, such as the commonly used support vector machine, which makes the methods applicable only to the adopted models. In this regard, the generalized hidden-mapping ridge regression (GHRR) method is introduced in order to train various types of classical intelligence models, including neural networks, fuzzy logical systems and kernel methods. Furthermore, the knowledge-leverage based transfer learning mechanism is integrated with GHRR to realize the inductive transfer learning method called transfer GHRR (TGHRR). Since the information from the induced knowledge is much clearer and more concise than that from the data in the source domain, it is more convenient to control and balance the similarity and difference of data distributions between the source and target domains. The proposed GHRR and TGHRR algorithms have been evaluated experimentally by performing regression and classification on synthetic and real world datasets. The results demonstrate that the performance of TGHRR is competitive with or even superior to existing state-of-the-art inductive transfer learning algorithms.", "Face recognition  cross resolution  transfer learning  co-training  co-transfer learning Face recognition algorithms are generally trained for matching high-resolution images and they perform well for similar resolution test data. However, the performance of such systems degrades when a low-resolution face image captured in unconstrained settings, such as videos from cameras in a surveillance scenario, are matched with high-resolution gallery images. The primary challenge, here, is to extract discriminating features from limited biometric content in low-resolution images and match it to information rich high-resolution face images. The problem of cross-resolution face matching is further alleviated when there is limited labeled positive data for training face recognition algorithms. In this paper, the problem of cross-resolution face matching is addressed where low-resolution images are matched with high-resolution gallery. A co-transfer learning framework is proposed, which is a cross-pollination of transfer learning and co-training paradigms and is applied for cross-resolution face matching. The transfer learning component transfers the knowledge that is learnt while matching high-resolution face images during training to match low-resolution probe images with high-resolution gallery during testing. On the other hand, co-training component facilitates this transfer of knowledge by assigning pseudolabels to unlabeled probe instances in the target domain. Amalgamation of these two paradigms in the proposed ensemble framework enhances the performance of cross-resolution face recognition. Experiments on multiple face databases show the efficacy of the proposed algorithm and compare with some existing algorithms and a commercial system. In addition, several high profile real-world cases have been used to demonstrate the usefulness of the proposed approach in addressing the tough challenges.", "Bag-of-words (BoW) framework  computer vision  deep learning  dictionary learning  hierarchical visual architecture  image categorization  restricted Boltzmann machine (RBM)  sparse feature coding  transfer learning In this paper, we propose a hybrid architecture that combines the image modeling strengths of the bag of words framework with the representational power and adaptability of learning deep architectures. Local gradient-based descriptors, such as SIFT, are encoded via a hierarchical coding scheme composed of spatial aggregating restricted Boltzmann machines (RBM). For each coding layer, we regularize the RBM by encouraging representations to fit both sparse and selective distributions. Supervised fine-tuning is used to enhance the quality of the visual representation for the categorization task. We performed a thorough experimental evaluation using three image categorization data sets. The hierarchical coding scheme achieved competitive categorization accuracies of 79.7% and 86.4% on the Caltech-101 and 15-Scenes data sets, respectively. The visual representations learned are compact and the model's inference is fast, as compared with sparse coding methods. The low-level representations of descriptors that were learned using this method result in generic features that we empirically found to be transferrable between different image data sets. Further analysis reveal the significance of supervised fine-tuning when the architecture has two layers of representations as opposed to a single layer.", "Domain adaptation  semi-supervised learning  transfer learning In real-life problems, the following semi-supervised domain adaptation scenario is often encountered: we have full access to some source data, which is usually very large  the target data distribution is under certain unknown transformation of the source data distribution  meanwhile, only a small fraction of the target instances come with labels. The goal is to learn a prediction model by incorporating information from the source domain that is able to generalize well on the target test instances. We consider an explicit form of transformation functions and especially linear transformations that maps examples from the source to the target domain, and we argue that by proper preprocessing of the data from both source and target domains, the feasible transformation functions can be characterized by a set of rotation matrices. This naturally leads to an optimization formulation under the special orthogonal group constraints. We present an iterative coordinate descent solver that is able to jointly learn the transformation as well as the model parameters, while the geodesic update ensures the manifold constraints are always satisfied. Our framework is sufficiently general to work with a variety of loss functions and prediction problems. Empirical evaluations on synthetic and real-world experiments demonstrate the competitive performance of our method with respect to the state-of-the-art.", "Face recognition  heterogenous data  image representation  transfer learning  low-resolution Face recognition under uncontrolled conditions, e. g., complex backgrounds and variable resolutions, is still challenging in image processing and computer vision. Although many methods have been proved well-performed in the controlled settings, they are usually of weak generality across different data sets. Meanwhile, several properties of the source domain, such as background and the size of subjects, play an important role in determining the final classification results. A transferrable representation learning model is proposed in this paper to enhance the recognition performance. To deeply exploit the discriminant information from the source domain and the target domain, the bioinspired face representation is modeled as structured and approximately stable characterization for the commonality between different domains. The method outputs a grouped boost of the features, and presents a reasonable manner for highlighting and sharing discriminant orientations and scales. Notice that the method can be viewed as a framework, since other feature generation operators and classification metrics can be embedded therein, and then, it can be applied to more general problems, such as low-resolution face recognition, object detection and categorization, and so forth. Experiments on the benchmark databases, including uncontrolled Face Recognition Grand Challenge v2.0 and Labeled Faces in the Wild show the efficacy of the proposed transfer learning algorithm.", "Gait recognition  Covariate conditions  Learning to rank  Transfer learning  Distance learning Gait is a useful biometric because it can operate from a distance and without subject cooperation. However, it is affected by changes in covariate conditions (carrying, clothing, view angle, etc.). Existing methods suffer from lack of training samples, can only cope with changes in a subset of conditions with limited success, and implicitly assume subject cooperation. We propose a novel approach which casts gait recognition as a bipartite ranking problem and leverages training samples from different people and even from different datasets. By exploiting learning to rank, the problem of model over-fitting caused by under-sampled training data is effectively addressed. This makes our approach suitable under a genuine uncooperative setting and robust against changes in any covariate conditions. Extensive experiments demonstrate that our approach drastically outperforms existing methods, achieving up to 14-fold increase in recognition rate under the most difficult uncooperative settings. (C) 2014 Elsevier Ltd. All rights reserved.", "Transfer learning  Domain adaptation  Object localization  Fluoroscopy  Ultrasound The fusion of image data from trans-esophageal echography (TEE) and X-ray fluoroscopy is attracting increasing interest in minimally-invasive treatment of structural heart disease. In order to calculate the needed transformation between both imaging systems, we employ a discriminative learning (DL) based approach to localize the TEE transducer in X-ray images. The successful application of DL methods is strongly dependent on the available training data, which entails three challenges: (1) the transducer can move with six degrees of freedom meaning it requires a large number of images to represent its appearance, (2) manual labeling is time consuming, and (3) manual labeling has inherent errors. This paper proposes to generate the required training data automatically from a single volumetric image of the transducer. In order to adapt this system to real X-ray data, we use unlabeled fluoroscopy images to estimate differences in feature space density and correct covariate shift by instance weighting. Two approaches for instance weighting, probabilistic classification and Kullback-Leibler importance estimation (KLIEP), are evaluated for different stages of the proposed DL pipeline. An analysis on more than 1900 images reveals that our approach reduces detection failures from 7.3% in cross validation on the test set to zero and improves the localization error from 1.5 to 0.8 mm. Due to the automatic generation of training data, the proposed system is highly flexible and can be adapted to any medical device with minimal efforts. ", "decision forests  transfer learning  gesture recognition Decision forests are an increasingly popular tool in computer vision problems. Their advantages include high computational efficiency, state-of-the-art accuracy and multi-class support. In this paper, we present a novel method for transfer learning which uses decision forests, and we apply it to recognize gestures and characters. We introduce two mechanisms into the decision forest framework in order to transfer knowledge from the source tasks to a given target task. The first one is mixed information gain, which is a data-based regularizer. The second one is label propagation, which infers the manifold structure of the feature space. We show that both of them are important to achieve higher accuracy. Our experiments demonstrate improvements over traditional decision forests in the ChaLearn Gesture Challenge and MNIST data set. They also compare favorably against other state-of-the-art classifiers.", "Transfer learning  Wavelet packet decomposition  Short time Fourier transform  Kernel principal component analysis  Electroencephalogram  Epilepsy detection Objective: Intelligent recognition of electroencephalogram (EEG) signals is an important means for epilepsy detection. Almost all conventional intelligent recognition methods assume that the training and testing data of EEG signals have identical distribution. However, this assumption may indeed be invalid for practical applications due to differences in distributions between the training and testing data, making the conventional epilepsy detection algorithms not feasible under such situations. In order to overcome this problem, we proposed a transfer-learning-based intelligent recognition method for epilepsy detection. Methods: We used the large-margin-projected transductive support vector machine method (LMPROJ) to learn the useful knowledge between the training domain and testing domain by calculating the maximal mean discrepancy. The method can effectively learn a model for the testing data with training data of different distributions, thereby relaxing the constraint that the data distribution in the training and testing samples should be identical. Results: The experimental validation is performed over six datasets of electroencephalogram signals with three feature extraction methods. The proposed LMPROJ-based transfer learning method was compared with five conventional classification methods. For the datasets with identical distribution, the performance of these six classification methods was comparable. They all could achieve an accuracy of 90%. However, the LMPROJ method obviously outperformed the five conventional methods for experimental datasets with different distribution between the training and test data. Regardless of the feature extraction method applied, the mean classification accuracy of the proposed method is above 93%, which is greater than that of the other five methods with statistical significance. Conclusion: The proposed transfer-learning-based method has better classification accuracy and adaptability than the conventional methods in classifying EEG signals for epilepsy detection. ", " ", "Transfer learning  reinforcement learning  gaussian processes  continuous states  continuous actions Transfer learning focuses on developing methods to reuse information gathered from a source task in order to improve the learning performance in a related task. In this work, we present a novel approach to transfer knowledge between tasks in a reinforcement learning (RL) framework with continuous states and actions, where the transition and policy functions are approximated by Gaussian processes. The novelty in the proposed approach lies in the idea of transferring information about the hyper-parameters of the state transition function from the source task, which represents qualitative knowledge about the type of transition function that the target task might have, constraining the search space and accelerating the learning process. We performed experiments on relevant tasks for RL, which show a clear improvement in the overall performance when compared to state-of-the-art reinforcement learning and transfer learning algorithms for continuous state and action spaces.", "Computer vision  Gesture recognition  Sign language recognition  RGBD cameras  Kinect  Dataset  Challenge  Machine learning  Transfer learning  One-shot-learning This paper describes the data used in the ChaLearn gesture challenges that took place in 2011/2012, whose results were discussed at the CVPR 2012 and ICPR 2012 conferences. The task can be described as: user-dependent, small vocabulary, fixed camera, one-shot-learning. The data include 54,000 hand and arm gestures recorded with an RGB-D camera. The data are organized into batches of 100 gestures pertaining to a small gesture vocabulary of 8-12 gestures, recorded by the same user. Short continuous sequences of 1-5 randomly selected gestures are recorded. We provide man-made annotations (temporal segmentation into individual gestures, alignment of RGB and depth images, and body part location) and a library of function to preprocess and automatically annotate data. We also provide a subset of batches in which the user's horizontal position is randomly shifted or scaled. We report on the results of the challenge and distribute sample code to facilitate developing new solutions. The data, datacollection software and the gesture vocabularies are downloadable from https://gesture.chalearn.org. We set up a forum for researchers working on these data https://groups.google.com/group/gesturechallenge.", "Transfer learning  Online learning  Knowledge transfer In this paper, we propose a novel machine learning framework called Online Transfer Learning (OTL), which aims to attack an online learning task on a target domain by transferring knowledge from some source domain. We do not assume data in the target domain follows the same distribution as that in the source domain, and the motivation of our work is to enhance a supervised online learning task on a target domain by exploiting the existing knowledge that had been learnt from training data in source domains. OTL is in general very challenging since data in both source and target domains not only can be different in their class distributions, but also can be diverse in their feature representations. As a first attempt to this new research problem, we investigate two different settings of OTL: (i) OTL on homogeneous domains of common feature space, and (ii) OTL across heterogeneous domains of different feature spaces. For each setting, we propose effective OTL algorithms to solve online classification tasks, and show some theoretical bounds of the algorithms. In addition, we also apply the OTL technique to attack the challenging online learning tasks with concept-drifting data streams. Finally, we conduct extensive empirical studies on a comprehensive testbed, in which encouraging results validate the efficacy of our techniques. ", "Pedestrian detection  Transfer learning  Sparse coding  HOG  SVM Pedestrian detection is a fundamental problem in video surveillance and has achieved great progress in recent years. However, training a generic detector performing well in a great variety of scenes has proved to be very difficult. On the other hand, exhausting manual labeling efforts for each specific scene to achieve high accuracy of detection is not acceptable especially for video surveillance applications. To alleviate the manual labeling efforts without scarifying accuracy of detection, we propose a transfer learning framework based on sparse coding for pedestrian detection. In our method, generic detector is used to get the initial target samples, and then several filters are used to select a small part of samples (called as target templates) from the initial target samples which we are very sure about their labels and confidence values. The relevancy between source samples and target templates and the relevancy between target samples and target templates are estimated by sparse coding and later used to calculate the weights for source samples and target samples. By adding the sparse coding-based weights to all these samples during re-training process, we can not only exclude outliers in the source samples, but also tackle the drift problem in the target samples, and thus get a well scene-specific pedestrian detector. Our experiments on two public datasets show that our trained scene-specific pedestrian detector performs well and is comparable with the detector trained on a large number of training samples manually labeled from the target scene.", "Distance metric learning  transfer learning  decomposition  base metric  image classification Distance metric learning (DML) is a critical factor for image analysis and pattern recognition. To learn a robust distance metric for a target task, we need abundant side information (i.e., the similarity/dissimilarity pairwise constraints over the labeled data), which is usually unavailable in practice due to the high labeling cost. This paper considers the transfer learning setting by exploiting the large quantity of side information from certain related, but different source tasks to help with target metric learning (with only a little side information). The state-of-the-art metric learning algorithms usually fail in this setting because the data distributions of the source task and target task are often quite different. We address this problem by assuming that the target distance metric lies in the space spanned by the eigenvectors of the source metrics (or other randomly generated bases). The target metric is represented as a combination of the base metrics, which are computed using the decomposed components of the source metrics (or simply a set of random bases)  we call the proposed method, decomposition-based transfer DML (DTDML). In particular, DTDML learns a sparse combination of the base metrics to construct the target metric by forcing the target metric to be close to an integration of the source metrics. The main advantage of the proposed method compared with existing transfer metric learning approaches is that we directly learn the base metric coefficients instead of the target metric. To this end, far fewer variables need to be learned. We therefore obtain more reliable solutions given the limited side information and the optimization tends to be faster. Experiments on the popular handwritten image (digit, letter) classification and challenge natural image annotation tasks demonstrate the effectiveness of the proposed method.", "Q learning  Extreme learning machine  Continuous space  Multi-source transfer  Boat problem Extreme learning machine (ELM) has advantages of good generalization property, simple structure and convenient calculation. Therefore, an ELM-based Q learning is proposed by using an ELM as a Q-value function approximator, which is suitable for large-scale or continuous space problems. This is the first contribution of this paper. Because the number of ELM hidden layer nodes is equal to that of training samples, large sample size will seriously affect the learning speed. Therefore, a rolling time-window mechanism is introduced into the ELM-based Q learning to reduce the size of training samples of the ELM. In addition, in order to reduce the learning difficulty of new tasks, transfer learning technology is introduced into the ELM-based Q learning. The transfer learning technology can reuse past experience and knowledge to solve current issues. Thus the second contribution is to propose a multi-source transfer ELM-based Q learning (MST-ELMQ), which can take full advantage of valuable information from multiple source tasks and avoid negative transfer resulted from irrelevant information. According to the Bayesian theory, each source task is assigned with a task transfer weight and each source sample is assigned with a sample transfer weight. The task and sample transfer weights determine the number and the manner of transfer samples. Samples with large sample transfer weights are selected from each source task, and assist Q learning agent in quick decision-making for the target task. Simulations results concerning on a boat problem show that MST-ELMQ has better performance than that of Q learning algorithms without or with a single source task, i.e., it can effectively reduce learning difficulty and find an optimal solution with fewer number of training. (C) 2013 Elsevier B.V. All rights reserved.", "Transfer learning  Online Fuzzy Min-Max neural network  Noisy data  Data classification In this paper, we present an empirical analysis on transfer learning using the Fuzzy Min-Max (FMM) neural network with an online learning strategy. Three transfer learning benchmark data sets, i.e., 20 Newsgroups, WiFi Time, and Botswana, are used for evaluation. In addition, the data samples are corrupted with white Gaussian noise up to 50 %, in order to assess the robustness of the online FMM network in handling noisy transfer learning tasks. The results are analyzed and compared with those from other methods. The outcomes indicate that the online FMM network is effective for undertaking transfer learning tasks in noisy environments.", "multiple source  transfer learning  negative transfer  imbalanced distribution Transfer learning has benefited many real-world applications where labeled data are abundant in source domains but scarce in the target domain. As there are usually multiple relevant domains where knowledge can be transferred, multiple source transfer learning (MSTL) has recently attracted much attention. However, we are facing two major challenges when applying MSTL. First, without knowledge about the difference between source and target domains, negative transfer occurs when knowledge is transferred from highly irrelevant sources. Second, existence of imbalanced distributions in classes, where examples in one class dominate, can lead to improper judgement on the source domains' relevance to the target task. Since existing MSTL methods are usually designed to transfer from relevant sources with balanced distributions, they will fail in applications where these two challenges persist. In this article, we propose a novel two-phase framework to effectively transfer knowledge from multiple sources even when there exists irrelevant sources and imbalanced class distributions. First, an effective supervised local weight scheme is proposed to assign a proper weight to each source domain's classifier based on its ability of predicting accurately on each local region of the target domain. The second phase then learns a classifier for the target domain by solving an optimization problem which concerns both training error minimization and consistency with weighted predictions gained from source domains. A theoretical analysis shows that as the number of source domains increases, the probability that the proposed approach has an error greater than a bound is becoming exponentially small. We further extend the proposed approach to an online processing scenario to conduct transfer learning on continuously arriving data. Extensive experiments on disease prediction, spam filtering and intrusion detection datasets demonstrate that: (i) the proposed two-phase approach outperforms existing MSTL approaches due to its ability of tackling negative transfer and imbalanced distribution challenges, and (ii) the proposed online approach achieves comparable performance to the offline scheme. (C) 2014 Wiley Periodicals, Inc.", "transfer learning  multi-view learning  multiple data sources Transfer learning, which aims to help learning tasks in a target domain by leveraging knowledge from auxiliary domains, has been demonstrated to be effective in different applications such as text mining, sentiment analysis, and so on. In addition, in many real-world applications, auxiliary data are described from multiple perspectives and usually carried by multiple sources. For example, to help classify videos on Youtube, which include three perspectives: image, voice and subtitles, one may borrow data from Flickr, Last. FM and Google News. Although any single instance in these domains can only cover a part of the views available on Youtube, the piece of information carried by them may compensate one another. If we can exploit these auxiliary domains in a collective manner, and transfer the knowledge to the target domain, we can improve the target model building from multiple perspectives. In this article, we consider this transfer learning problem as Transfer Learning with Multiple Views and Multiple Sources. As different sources may have different probability distributions and different views may compensate or be inconsistent with each other, merging all data in a simplistic manner will not give an optimal result. Thus, we propose a novel algorithm to leverage knowledge from different views and sources collaboratively, by letting different views from different sources complement each other through a co-training style framework, at the same time, it revises the distribution differences in different domains. We conduct empirical studies on several real-world datasets to show that the proposed approach can improve the classification accuracy by up to 8% against different kinds of state-of-the-art baselines. (C) 2014 Wiley Periodicals, Inc.", "Visual categorization  Image classification  Human action recognition  Event recognition  Transfer learning  Weakly-supervised dictionary learning We address the visual categorization problem and present a method that utilizes weakly labeled data from other visual domains as the auxiliary source data for enhancing the original learning system. The proposed method aims to expand the intra-class diversity of original training data through the collaboration with the source data. In order to bring the original target domain data and the auxiliary source domain data into the same feature space, we introduce a weakly-supervised cross-domain dictionary learning method, which learns a reconstructive, discriminative and domain-adaptive dictionary pair and the corresponding classifier parameters without using any prior information. Such a method operates at a high level, and it can be applied to different cross-domain applications. To build up the auxiliary domain data, we manually collect images from Web pages, and select human actions of specific categories from a different dataset. The proposed method is evaluated for human action recognition, image classification and event recognition tasks on the UCF YouTube dataset, the Caltech101/256 datasets and the Kodak dataset, respectively, achieving outstanding results.", "Action recognition  Lab to real-world  Transfer learning  General Schatten-p norm Much research on human action recognition has been oriented toward the performance gain on lab-collected datasets. Yet real-world videos are more diverse, with more complicated actions and often only a few of them are precisely labeled. Thus, recognizing actions from these videos is a tough mission. The paucity of labeled real-world videos motivates us to borrow strength from other resources. Specifically, considering that many lab datasets are available, we propose to harness lab datasets to facilitate the action recognition in real-world videos given that the lab and real-world datasets are related. As their action categories are usually inconsistent, we design a multi-task learning framework to jointly optimize the classifiers for both sides. The general Schatten -norm is exerted on the two classifiers to explore the shared knowledge between them. In this way, our framework is able to mine the shared knowledge between two datasets even if the two have different action categories, which is a major virtue of our method. The shared knowledge is further used to improve the action recognition in the real-world videos. Extensive experiments are performed on real-world datasets with promising results.", "Transfer learning  Domain adaptation  Low-rank constraint  Subspace learning It is expensive to obtain labeled real-world visual data for use in training of supervised algorithms. Therefore, it is valuable to leverage existing databases of labeled data. However, the data in the source databases is often obtained under conditions that differ from those in the new task. Transfer learning provides techniques for transferring learned knowledge from a source domain to a target domain by finding a mapping between them. In this paper, we discuss a method for projecting both source and target data to a generalized subspace where each target sample can be represented by some combination of source samples. By employing a low-rank constraint during this transfer, the structure of source and target domains are preserved. This approach has three benefits. First, good alignment between the domains is ensured through the use of only relevant data in some subspace of the source domain in reconstructing the data in the target domain. Second, the discriminative power of the source domain is naturally passed on to the target domain. Third, noisy information will be filtered out during knowledge transfer. Extensive experiments on synthetic data, and important computer vision problems such as face recognition application and visual domain adaptation for object recognition demonstrate the superiority of the proposed approach over the existing, well-established methods.", "Transfer learning  Multi-view head pose classification  Varying acquisition conditions  Moving persons Head pose classification from surveillance images acquired with distant, large field-of-view cameras is difficult as faces are captured at low-resolution and have a blurred appearance. Domain adaptation approaches are useful for transferring knowledge from the training (source) to the test (target) data when they have different attributes, minimizing target data labeling efforts in the process. This paper examines the use of transfer learning for efficient multi-view head pose classification with minimal target training data under three challenging situations: (i) where the range of head poses in the source and target images is different, (ii) where source images capture a stationary person while target images capture a moving person whose facial appearance varies under motion due to changing perspective, scale and (iii) a combination of (i) and (ii). On the whole, the presented methods represent novel transfer learning solutions employed in the context of multi-view head pose classification. We demonstrate that the proposed solutions considerably outperform the state-of-the-art through extensive experimental validation. Finally, the DPOSE dataset compiled for benchmarking head pose classification performance with moving persons, and to aid behavioral understanding applications is presented in this work.", "Transfer learning  Wiberg algorithm  Collaborative filtering The sparsity problem is a major bottleneck for the collaborative filtering. Recently, transfer learning methods are introduced in collaborative filtering to alleviate the sparsity problem which aim to use the shared knowledge in related domains to help improve the prediction performance. However, most of the transfer learning methods assume that the user features or item features learned from different data matrices have the same dimensions which is often not met in practice. In this paper, we propose a transfer learning method for collaborative filtering, called Feature Subspace Transfer (FST) to overcome this limitation. In our model, the user feature subspace learned from the auxiliary data is transferred to the target domain. An iterative algorithm is also proposed for solving the optimization problem. Numerical experiments on real-world data show the improvement of our method on alleviating the sparsity problem. ", "Common concept  distinct concept  distribution mismatch  nonnegative matrix trifactorization  triplex transfer learning Transfer learning focuses on the learning scenarios when the test data from target domains and the training data from source domains are drawn from similar but different data distributions with respect to the raw features. Along this line, some recent studies revealed that the high-level concepts, such as word clusters, could help model the differences of data distributions, and thus are more appropriate for classification. In other words, these methods assume that all the data domains have the same set of shared concepts, which are used as the bridge for knowledge transfer. However, in addition to these shared concepts, each domain may have its own distinct concepts. In light of this, we systemically analyze the high-level concepts, and propose a general transfer learning framework based on nonnegative matrix trifactorization, which allows to explore both shared and distinct concepts among all the domains simultaneously. Since this model provides more flexibility in fitting the data, it can lead to better classification accuracy. Moreover, we propose to regularize the manifold structure in the target domains to improve the prediction performances. To solve the proposed optimization problem, we also develop an iterative algorithm and theoretically analyze its convergence properties. Finally, extensive experiments show that the proposed model can outperform the baseline methods with a significant margin. In particular, we show that our method works much better for the more challenging tasks when there are distinct concepts in the data.", "Transfer learning  negative transfer  graph regularization  matrix factorization  text mining  image classification Transfer learning is established as an effective technology to leverage rich labeled data from some source domain to build an accurate classifier for the target domain. The basic assumption is that the input domains may share certain knowledge structure, which can be encoded into common latent factors and extracted by preserving important property of original data, e. g., statistical property and geometric structure. In this paper, we show that different properties of input data can be complementary to each other and exploring them simultaneously can make the learning model robust to the domain difference. We propose a general framework, referred to as Graph Co-Regularized Transfer Learning (GTL), where various matrix factorization models can be incorporated. Specifically, GTL aims to extract common latent factors for knowledge transfer by preserving the statistical property across domains, and simultaneously, refine the latent factors to alleviate negative transfer by preserving the geometric structure in each domain. Based on the framework, we propose two novel methods using NMF and NMTF, respectively. Extensive experiments verify that GTL can significantly outperform state-of-the-art learning methods on several public text and image datasets.", "Multivariate time series  action recognition  online temporal segmentation  saptio-temporal alignment  transfer learning We address the problem of structure learning of human motion in order to recognize actions from a continuous monocular motion sequence of an arbitrary person from an arbitrary viewpoint. Human motion sequences are represented by multivariate time series in the joint-trajectories space. Under this structured time series framework, we first propose Kernelized Temporal Cut (KTC), an extension of previous works on change-point detection by incorporating Hilbert space embedding of distributions, to handle the nonparametric and high dimensionality issues of human motions. Experimental results demonstrate the effectiveness of our approach, which yields realtime segmentation, and produces high action segmentation accuracy. Second, a spatio-temporal manifold framework is proposed to model the latent structure of time series data. Then an efficient spatio-temporal alignment algorithm Dynamic Manifold Warping (DMW) is proposed for multivariate time series to calculate motion similarity between action sequences (segments). Furthermore, by combining the temporal segmentation algorithm and the alignment algorithm, online human action recognition can be performed by associating a few labeled examples from motion capture data. The results on human motion capture data and 3D depth sensor data demonstrate the effectiveness of the proposed approach in automatically segmenting and recognizing motion sequences, and its ability to handle noisy and partially occluded data, in the transfer learning module.", "Transfer learning  Multi-view learning  Domain distance  View consistency  Support vector machine In many real-world applications in the areas of data mining, the distributions of testing data are different from that of training data. And on the other hand, many data are often represented by multiple views which are of importance to learning. However, little work has been done for it. In this paper, we explored to leverage the multi-view information across different domains for knowledge transfer. We proposed a novel transfer learning model which integrates the domain distance and view consistency into a 2-view support vector machine framework, namely DV2S. The objective of DV2S is to find the optimal feature mapping such that under the projections the classification margin is maximized, while both the domain distance and the disagreement between multiple views are minimized simultaneously. Experiments showed that DV2S outperforms a variety of state-of-the-art algorithms.", "Al planning  Action model learning  Transfer learning  Markov logic networks  Web search Applying learning techniques to acquire action models is an area of intense research interest. Most previous work in this area has assumed that there is a significant amount of training data available in a planning domain of interest. However, it is often difficult to acquire sufficient training data to ensure the learnt action models are of high quality. In this paper, we seek to explore a novel algorithm framework, called TRAMP, to learn action models with limited training data in a target domain, via transferring as much of the available information from other domains (called source domains) as possible to help the learning task, assuming action models in source domains can be transferred to the target domain. TRAMP transfers knowledge from source domains by first building structure mappings between source and target domains, and then exploiting extra knowledge from Web search to bridge and transfer knowledge from sources. Specifically, TRAMP first encodes training data with a set of propositions, and formulates the transferred knowledge as a set of weighted formulas. After that it learns action models for the target domain to best explain the set of propositions and the transferred knowledge. We empirically evaluate TRAMP in different settings to see their advantages and disadvantages in six planning domains, including four International Planning Competition (IPC) domains and two synthetic domains. (C) 2014 Published by Elsevier B.V.", "Multi-task reinforcement learning  Feature selection  Abstraction  Potential-based shaping  Transfer learning In multi-task learning, there are roughly two approaches to discovering representations. The first is to discover task relevant representations, i.e., those that compactly represent solutions to particular tasks. The second is to discover domain relevant representations, i.e., those that compactly represent knowledge that remains invariant across many tasks. In this article, we propose a new approach to multi-task learning that captures domain-relevant knowledge by learning potential-based shaping functions, which augment a task's reward function with artificial rewards. We address two key issues that arise when deriving potential functions. The first is what kind of target function the potential function should approximate  we propose three such targets and show empirically that which one is best depends critically on the domain and learning parameters. The second issue is the representation for the potential function. This article introduces the notion of -relevance, the expected relevance of a representation on a sample sequence of tasks, and argues that this is a unifying definition of relevance of which both task and domain relevance are special cases. We prove formally that, under certain assumptions, -relevance converges monotonically to a fixed point as increases, and use this property to derive Feature Selection Through Extrapolation of k-relevance (FS-TEK), a novel feature-selection algorithm. We demonstrate empirically the benefit of FS-TEK on artificial domains.", "Heterogeneous domain adaptation  domain adaptation  transfer learning  augmented features In this paper, we study the heterogeneous domain adaptation (HDA) problem, in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. By introducing two different projection matrices, we first transform the data from two domains into a common subspace such that the similarity between samples across different domains can be measured. We then propose a new feature mapping function for each domain, which augments the transformed samples with their original features and zeros. Existing supervised learning methods (e. g., SVM and SVR) can be readily employed by incorporating our newly proposed augmented feature representations for supervised HDA. As a showcase, we propose a novel method called Heterogeneous Feature Augmentation (HFA) based on SVM. We show that the proposed formulation can be equivalently derived as a standard Multiple Kernel Learning (MKL) problem, which is convex and thus the global solution can be guaranteed. To additionally utilize the unlabeled data in the target domain, we further propose the semi-supervised HFA (SHFA) which can simultaneously learn the target classifier as well as infer the labels of unlabeled target samples. Comprehensive experiments on three different applications clearly demonstrate that our SHFA and HFA outperform the existing HDA methods.", "Transfer learning  adaptation regularization  distribution adaptation  manifold regularization  generalization error Domain transfer learning, which learns a target classifier using labeled data from a different distribution, has shown promising value in knowledge discovery yet still been a challenging problem. Most previous works designed adaptive classifiers by exploring two learning strategies independently: distribution adaptation and label propagation. In this paper, we propose a novel transfer learning framework, referred to as Adaptation Regularization based Transfer Learning (ARTL), to model them in a unified way based on the structural risk minimization principle and the regularization theory. Specifically, ARTL learns the adaptive classifier by simultaneously optimizing the structural risk functional, the joint distribution matching between domains, and the manifold consistency underlying marginal distribution. Based on the framework, we propose two novel methods using Regularized Least Squares (RLS) and Support Vector Machines (SVMs), respectively, and use the Representer theorem in reproducing kernel Hilbert space to derive corresponding solutions. Comprehensive experiments verify that ARTL can significantly outperform state-of-the-art learning methods on several public text and image datasets.", "Expertise development  Decision support system  Dynamic decision making  Dynamic tasks  Laboratory experiment  Facilitation  Task complexity Facilitation, a special kind of decisional aid, is all forms of information, strategies, and heuristics delivered by a facilitator to aid decision makers in dynamic decision environments. It is assumed that facilitation has profound effects on decision making, but these effects are understudied and empirically unproven. By incorporating the three levels of facilitation, pre-task, combined pre-task and in-task, and combined pre-task, in-task, and post-task, in the design of a simulation-based interactive learning environment (ILE), this study provides an empirical, laboratory-experiment-based evaluation of the effectiveness of facilitation on performance in dynamic tasks. We develop and use a comprehensive model consisting of four evaluation criteria: task performance, structural knowledge, heuristics knowledge, and transfer learning. We find that the subjects provided with combined pre-task, in-task, and post-task facilitation performed the best, followed by those provided with combined pretask and in-task facilitation. Contrary to the hypothesis, subjects provided with pre-task facilitation performed poorly. ", "Extreme learning machine  Reduced kernel extreme learning machine  Activity recognition  Support vector machine Activity recognition based on mobile embedded accelerometer is very important for developing human-centric pervasive applications such as healthcare, personalized recommendation and so on. However, the distribution of accelerometer data is heavily affected by varying users. The performance will degrade when the model trained on one person is used to others. To solve this problem, we propose a fast and accurate cross-person activity recognition model, known as TransRKELM (Transfer learning Reduced Kernel Extreme Learning Machine) which uses RKELM (Reduced Kernel Extreme Learning Machine) to realize initial activity recognition model. In the online phase OS-RKELM (Online Sequential Reduced Kernel Extreme Learning Machine) is applied to update the initial model and adapt the recognition model to new device users based on recognition results with high confidence level efficiently. Experimental results show that, the proposed model can adapt the classifier to new device users quickly and obtain good recognition performance. 2014 Elsevier Ltd. All rights reserved.", "Transfer learning  Display advertising  Predictive modeling This paper presents the design of a fully deployed multistage transfer learning system for targeted display advertising, highlighting the important role of problem formulation and the sampling of data from distributions different from that of the target environment. Notably, the machine learning system itself is deployed and has been in continual use for years for thousands of advertising campaigns-in contrast to the more common case where predictive models are built outside the system, curated, and then deployed. In this domain, acquiring sufficient data for training from the ideal sampling distribution is prohibitively expensive. Instead, data are drawn from surrogate distributions and learning tasks, and then transferred to the target task. We present the design of the transfer learning system We then present a detailed experimental evaluation, showing that the different transfer stages indeed each add value. We also present production results across a variety of advertising clients from a variety of industries, illustrating the performance of the system in use. We close the paper with a collection of lessons learned from over half a decade of research and development on this complex, deployed, and intensely used machine learning system.", "Recommender systems  Cross domain  Topic modeling  Latent dirichlet allocation  Transfer learning Due to the scarcity of user interest information in the target domain, recommender systems generally suffer from the sparsity problem. To alleviate this limitation, one natural way is to transfer user interests in other domains to the target domain. However, objects in different domains may be in different media types, which make it very difficult to find the correlations between them. In this paper, we propose a Bayesian hierarchical approach based on Latent Dirichlet Allocation (LDA) to transfer user interests cross domains or media. We model documents (corresponding to media objects) from different domains and user interests in a common topic space, and learn topic distributions for documents and user interests together. Specifically, to learn the model, we combine multi-type media information: media descriptions, user-generated text data and ratings. With this model, recommendation can be done in multiple ways, via predicting ratings, comparing topic distributions of documents and user interests directly and so on. Experiments on two real world datasets demonstrate that our proposed method is effective in addressing the sparsity problem by transferring user interests cross domains. (C) 2013 Elsevier B.V. All rights reserved.", "Dimensionality reduction  Gaussian process  Regression  Transfer learning Dimensionality reduction has been considered as one of the most significant tools for data analysis. In general, supervised information is helpful for dimensionality reduction. However, in typical real applications, supervised information in multiple source tasks may be available, while the data of the target task are unlabeled. An interesting problem of how to guide the dimensionality reduction for the unlabeled target data by exploiting useful knowledge, such as label information, from multiple source tasks arises in such a scenario. In this paper, we propose a new method for dimensionality reduction in the transfer learning setting. Unlike traditional paradigms where the useful knowledge from multiple source tasks is transferred through distance metric, we attempt to learn a more informative mapping function between the original data and the reduced data by Gaussian process that behaves more appropriately than other parametric regression methods due to its less parametric characteristic. In our proposal, we firstly convert the dimensionality reduction problem into integral regression problems in parallel. Gaussian process is then employed to learn the underlying relationship between the original data and the reduced data. Such a relationship can be appropriately transferred to the target task by exploiting the prediction ability of the Gaussian process model and inventing different kinds of regularizers. Extensive experiments on both synthetic and real data sets show the effectiveness of our method.", "Text categorization  Transfer learning  Boosting  Lack of labeled data  Weighting strategy The lack of labeled data is a serious problem which greatly hinders the application of text classification in new domains. In this era of information explosion, dependence of labeled data in traditional classification methods becomes ineffective in emerged new domains. The ideology of transfer learning makes it possible to use labeled identical distribution data of old domains for data mining in new domains. However, previous algorithms and practical application systems did not reach the perfect state. This paper presents a novel complete method for text categorization (TC) in new domains where the labeled data are insufficient. We first present an improved weighting strategy of boosting algorithms family to ensure training data can be used more efficiently. We then introduce boosting ideology with the novel weighting strategy into transfer learning, and a novel text classification algorithm is proposed which has the ability to use labeled data of old domains for new domain classification with a high performance. After the mathematical discussion of the proposed algorithm, we finally deploy a real-world system based on it to evaluate the novel method. Experimental results demonstrate that our method is able to achieve both ideal accuracy and efficiency in TC when dealing with cross-domain problems.", "Data representation  Nonnegative matrix factorization  Cross-domain learning Traditional cross-domain learning methods transfer learning from a source domain to a target domain. In this paper, we propose the multiple-domain learning problem for several equally treated domains. The multiple-domain learning problem assumes that samples from different domains have different distributions, but share the same feature and class label spaces. Each domain could be a target domain, while also be a source domain for other domains. A novel multiple-domain representation method is proposed for the multiple-domain learning problem. This method is based on nonnegative matrix factorization (NMF), and tries to learn a basis matrix and coding vectors for samples, so that the domain distribution mismatch among different domains will be reduced under an extended variation of the maximum mean discrepancy (MMD) criterion. The novel algorithm multiple-domain NMF (MDNMF) was evaluated on two challenging multiple-domain learning problems multiple user spam email detection and multiple-domain glioma diagnosis. The effectiveness of the proposed algorithm is experimentally verified. (C) 2013 Elsevier Ltd. All rights reserved.", "Transfer learning  Expression recognition  Landmark detection When training and testing data are drawn from different distributions, most statistical models need to be retrained using the newly collected data. Transfer learning is a family of algorithms that improves the classifier learning in a target domain of interest by transferring the knowledge from one or multiple source domains, where the data falls in a different distribution. In this paper, we consider a new scenario of transfer learning for two-class classification, where only data samples from one of the two classes (e.g., the negative class) are available in the target domain. We introduce a regression-based one-class transfer learning algorithm to tackle this new problem. In contrast to the traditional discriminative feature selection, which seeks the best classification performance in the training data, we propose a new framework to learn the most transferable discriminative features suitable for our transfer learning. The experiment demonstrates improved performance in the applications of facial expression recognition and facial landmark detection. (C) 2013 Published by Elsevier B.V.", "Attribute learning  latent attribute space  multitask learning  transfer learning  zero-shot learning The rapid development of social media sharing has created a huge demand for automatic media classification and annotation techniques. Attribute learning has emerged as a promising paradigm for bridging the semantic gap and addressing data sparsity via transferring attribute knowledge in object recognition and relatively simple action classification. In this paper, we address the task of attribute learning for understanding multimedia data with sparse and incomplete labels. In particular, we focus on videos of social group activities, which are particularly challenging and topical examples of this task because of their multimodal content and complex and unstructured nature relative to the density of annotations. To solve this problem, we 1) introduce a concept of semilatent attribute space, expressing user-defined and latent attributes in a unified framework, and 2) propose a novel scalable probabilistic topic model for learning multimodal semilatent attributes, which dramatically reduces requirements for an exhaustive accurate attribute ontology and expensive annotation effort. We show that our framework is able to exploit latent attributes to outperform contemporary approaches for addressing a variety of realistic multimedia sparse data learning tasks including: multitask learning, learning with label noise, N-shot transfer learning, and importantly zero-shot learning.", "Pedestrian detection  transfer learning  confidence-encoded SVM  domain adaptation  video surveillance The performance of a generic pedestrian detector may drop significantly when it is applied to a specific scene due to the mismatch between the source training set and samples from the target scene. We propose a new approach of automatically transferring a generic pedestrian detector to a scene-specific detector in static video surveillance without manually labeling samples from the target scene. The proposed transfer learning framework consists of four steps. 1) Through exploring the indegrees from target samples to source samples on a visual affinity graph, the source samples are weighted to match the distribution of target samples. 2) It explores a set of context cues to automatically select samples from the target scene, predicts their labels, and computes confidence scores to guide transfer learning. 3) The confidence scores propagate among target samples according to their underlying visual structures. 4) Target samples with higher confidence scores have larger influence on training scene-specific detectors. All these considerations are formulated under a single objective function called confidence-encoded SVM, which avoids hard thresholding on confidence scores. During test, only the appearance-based detector is used without context cues. The effectiveness is demonstrated through experiments on two video surveillance data sets. Compared with a generic detector, it improves the detection rates by 48 and 36 percent at one false positive per image (FPPI) on the two data sets, respectively. The training process converges after one or two iterations on the data sets in experiments.", "Transfer Learning  Sentiment Analysis  Forensic Analysis  Short Text Messages  SMS  Twitter Emotional Polarity Classification is an important task in Sentiment Analysis area. It is applied in many real problems such as reviews of consumer products and services, financial markets, and forensic analysis. The scientists from the areas of text mining and nature language processing have studied how to solve emotional polarity classification problem. They used a variety of methods, from simple methods (e.g., lexicon-based categorization) to sophisticate methods (e.g., statistical models). However, the problem of statistical models does not work well in a new test set whose distribution is different from training set. Therefore, the accuracy of Emotional Polarity Classification problem is still unstable. In this paper, we propose a novel approach formalism to solve this problem by using adaptation transfer learning. The transfer learning utilizes the labelled data available to solve the related but different problems. We also propose a new method that uses this approach to improve performance. The effectiveness of our approach is verified by the experiment results with two synthesis datasets and three real Twitter datasets.", " The appearance of an attribute can vary considerably from class to class (e.g., a fluffy dog vs. a fluffy towel), making standard class-independent attribute models break down. Yet, training object-specific models for each attribute can be impractical, and defeats the purpose of using attributes to bridge category boundaries. We propose a novel form of transfer learning that addresses this dilemma. We develop a tensor factorization approach which, given a sparse set of class-specific attribute classifiers, can infer new ones for object-attribute pairs unobserved during training. For example, even though the system has no labeled images of striped dogs, it can use its knowledge of other attributes and objects to tailor stripedness to the dog category. With two large-scale datasets, we demonstrate both the need for category-sensitive attributes as well as our method's successful transfer. Our inferred attribute classifiers perform similarly well to those trained with the luxury of labeled class-specific instances, and much better than those restricted to traditional modes of transfer.", " Many prevalent multi-class classification approaches can be unified and generalized by the output coding framework [1, 7] which usually consists of three phases: (1) coding, (2) learning binary classifiers, and (3) decoding. Most of these approaches focus on the first two phases and pre-defined distance function is used for decoding. In this paper, however, we propose to perform learning in coding space for more adaptive decoding, thereby improving overall performance. Ramp loss is exploited for measuring multi-class decoding error. The proposed algorithm has uniform stability. It is insensitive to data noises and scalable with large scale datasets. Generalization error bound and numerical results are given with promising outcomes.", " We consider the intersection of two research fields: transfer learning and statistics on manifolds. In particular, we consider, for manifold-valued data, transfer learning of tangent-space models such as Gaussians distributions, PCA, regression, or classifiers. Though one would hope to simply use ordinary Rn-transfer learning ideas, the manifold structure prevents it. We overcome this by basing our method on inner-product-preserving parallel transport, a well-known tool widely used in other problems of statistics on manifolds in computer vision. At first, this straightforward idea seems to suffer from an obvious shortcoming: Transporting large datasets is prohibitively expensive, hindering scalability. Fortunately, with our approach, we never transport data. Rather, we show how the statistical models themselves can be transported, and prove that for the tangent-space models above, the transport commutes with learning. Consequently, our compact framework, applicable to a large class of manifolds, is not restricted by the size of either the training or test sets. We demonstrate the approach by transferring PCA and logistic-regression models of real-world data involving 3D shapes and image descriptors.", " There has been a lot of work on face modeling, analysis, and landmark detection, with Active Appearance Models being one of the most successful techniques. A major drawback of these models is the large number of detailed annotated training examples needed for learning. Therefore, we present a transfer learning method that is able to learn from related training data using an instance-weighted transfer technique. Our method is derived using a generalization of importance sampling and in contrast to previous work we explicitly try to tackle the transfer already during learning instead of adapting the fitting process. In our studied application of face landmark detection, we efficiently transfer facial expressions from other human individuals and are thus able to learn a precise face Active Appearance Model only from neutral faces of a single individual. Our approach is evaluated on two common face datasets and outperforms previous transfer methods.", " The transfer learning and domain adaptation problems originate from a distribution mismatch between the source and target data distribution. The causes of such mismatch are traditionally considered different. Thus, transfer learning and domain adaptation algorithms are designed to address different issues, and cannot be used in both settings unless substantially modified. Still, one might argue that these problems are just different declinations of learning to learn, i.e. the ability to leverage over prior knowledge when attempting to solve a new task. We propose a learning to learn framework able to leverage over source data regardless of the origin of the distribution mismatch. We consider prior models as experts, and use their output confidence value as features. We use them to build the new target model, combined with the features from the target data through a high-level cue integration scheme. This results in a class of algorithms usable in a plug-and-play fashion over any learning to learn scenario, from binary and multi-class transfer learning to single and multiple source domain adaptation settings. Experiments on several public datasets show that our approach consistently achieves the state of the art.", " Magnetic resonance (MR) images acquired at different field strengths have different intensity appearance and thus cannot be easily combined into a single manifold space. A framework to learn a joint low-dimensional representation of brain MR images, acquired either at 1.5 or 3 Tesla, is proposed. In this manifold subspace, knowledge can be shared and transfered between the two distinct but related datasets. The joint manifold subspace is built using an adaptation of Laplacian eigenmaps (LE) from a data-driven region of interest (ROI). The ROI is learned using sparse regression to perform simultaneous variable selection at multiple levels of alignment to the MNI152 template. Additionally, a stability selection re-sampling scheme is used to reduce sampling bias while learning the ROI. Knowledge about the intrinsic embedding coordinates of different instances, common to both feature spaces, is used to constrain their alignment in the joint manifold. Alzheimer's Disease (AD) classification results obtained with the proposed approach are presented using data from more than 1500 subjects from ADNI-1, ADNI-GO and ADNI-2 datasets. Results calculated using the learned joint manifold in general outperform those obtained in each independent manifold. Accuracies calculated on ADNI-1 are comparable to other state-of-the-art approaches. To our knowledge, classification accuracies have not been reported before on the complete ADNI (-1, -GO and -2) cohort combined.", "feature transfer learning  denoising autoencoders  cross-corpus  domain adaptation  speech emotion recognition The typical inherent mismatch between the test and training corpora and by that between 'target' and 'source' sets usually leads to significant performance downgrades. To cope with this, this study presents a feature transfer learning method using Denoising Autoencoders (DAEs) to build high-order subspaces of the source and target corpora, where features in the source domain are transferred to the target domain by an additional neural network. To exemplify effectiveness of our approach, we select the INTERSPEECH Emotion Challenge's FAU Aibo Emotion Corpus as target corpus and further two publicly available databases as source corpora for extensive and reproducible evaluation. The experimental results show that our method significantly improves over the baseline performance and outperforms today's state-of-the-art domain adaptation methods.", "transfer learning  anatomy registration  automatic evaluation Evaluation of the outcome (end-product) of surgical procedures carried out in virtual reality environments is an essential part of simulation-based surgical training. Automated end-product assessment can be carried out by performance classifiers built from a set of expert performances. When applied to temporal bone surgery simulation, these classifiers can evaluate performance on the bone specimen they were trained on, but they cannot be extended to new specimens. Thus, new expert performances need to be recorded for each new specimen, requiring considerable time commitment from time-poor expert surgeons. To eliminate this need, we propose a transfer learning framework to adapt a classifier built on a single temporal bone specimen to multiple specimens. Once a classifier is trained, we translate each new specimens' features to the original feature space, which allows us to carry out performance evaluation on different specimens using the same classifier. In our experiment, we built a surgical end-product performance classifier from 16 expert trials on a simulated temporal bone specimen. We applied the transfer learning approach to 8 new specimens to obtain machine generated end-products. We also collected end-products for these 8 specimens drilled by a single expert. We then compared the machine generated end-products to those drilled by the expert. The drilled regions generated by transfer learning were similar to those drilled by the expert.", " Learning from few examples is considered a very challenging task where transfer learning proved to be beneficial. Such a learning framework exploits previous experiences and knowledge to compensate for the lack of training data in a novel domain. Knowledge representation plays a vital role in the type and performance of transfer learning approaches, as well as its robustness against negative transfer effect. This aspect is usually not considered in most of the proposed transfer learning methodologies, where the focus is either on the transfer type or on the representation. In this work, we study the use of various high-level semantics in transfer metric learning. We propose a generic transfer metric learning framework, and analyze the effect of different semantic similarity spaces on transfer type and efficiency against negative transfer. Furthermore, we introduce a hierarchical knowledge representation model based on the embedded structure in the attribute semantic space. The evaluation of the framework on challenging transfer settings in the context of action similarity demonstrates the effectiveness of our approach.", "video event recognition  transfer learning Recognizing events in consumer videos is becoming increasingly important because of the enormous growth of consumer videos in recent years. Current researches mainly focus on learning from numerous labeled videos, which is time consuming and labor expensive due to labeling the consumer videos. To alleviate the labeling process, we utilize a large number of loosely labeled Web videos (e.g., from YouTube) for visual event recognition in consumer videos. Web videos are noisy and diverse, so brute force transfer of Web videos to consumer videos may hurt the performance. To address such a negative transfer problem, we propose a novel Multi-Group Adaptation (MGA) framework to divide the training Web videos into several semantic groups and seek the optimal weight of each group. Each weight represents how relative the corresponding group is to the consumer domain. The final classifier for event recognition is learned using the weighted combination of classifiers learned from Web videos and enforced to be smooth on the consumer domain. Comprehensive experiments on three real-world consumer video datasets demonstrate the effectiveness of MGA for event recognition in consumer videos.", " Transfer learning aims to address the problem where we lack the labeled data for training in one domain while utilizing the sufficient training data from other relevant domains. The problem becomes even more challenging when there are no labeled data in the target domain to build the association between two domains, which is more common in real-world scenarios. In this paper, we tackle with the challenge through learning the shared subspace across domains. The subspace is able to capture the intrinsic domain invariant innate characteristics for feature representations. Meanwhile in the learning procedure we train the classifiers in the source domain and predict the labels in the target domain simultaneously. We also incorporate the inherent data structure in the predicted labels to enhance the robustness against the misclassification. Extensive experimental evaluations on the public datasets demonstrate the effectiveness and promise of our method compared with the state-of-the-art transfer learning methods.", " This paper proposes a transfer learning scheme for traffic pattern analysis where the transferred classifier could be trained with a small number of samples. First we make feature descriptors to represent the traffic trajectories so that they should be adequate to transfer and classify the traffic patterns. Then, we use support vector machine (SVM) to learn the feature descriptors of traffic trajectories. The transfer learning scheme is formulated by a convex optimization problem using the geometric relation between target and source patterns. Not only parameters of SVM but also the geometric relation are found at the same time through two step minimization process of the optimization problem. Through experiments on various surveillance videos, the proposed formulation is shown to be valid by investigating the improvement of performance compared to a transfer scheme without the proposed geometric relation as well as SVM without transfer scheme.", "Context-Awareness  Activity Recognition  Transfer Learning  Machine Learning  Dynamic Environment Since a real-life environment may encounter various uncertainties due to its dynamic nature, a smart-home system needs to improve its adaptability in response to the inevitable uncertainties. In this regard, a multi-transfer framework was proposed to keep context models adaptable in order to reduce the efforts in retraining context models in the event of an uncertainty. The framework is used to transfer knowledge from a source domain to a target one by reusing as much information from the source domain. This way, the efforts of training activity models for new users in the target domain can be effectively reduced. This paper presents one instantiation of the framework and its implementation details. The preliminary results show that the effort of training activity models for a new user can be effectively reduced meanwhile maintaining satisfactory performance.", " Studies on the bootstrap problem in evolutionary robotics help lifting the barrier from the way to evolve robots for complex tasks. It remains an open question, though, how to reduce the need for designer knowledge when devising a bootstrapping approach for any particular complex task. Transfer learning may help reducing this need and support the evolution of solutions to complex tasks, through task relatedness. Relying on the commonalities of similar tasks, we introduce a new concept of Family Bootstrapping (FB). FB refers to the creation of biased ancestors that are expected to onset the evolution of a family of solutions not just for one task, but for a set of related robot tasks. A general FB paradigm is outlined and the unique potential of the proposed concept is discussed. To highlight the validity of the FB concept, a simple demonstration case, concerning the evolution of neuro-controllers for a set of robot navigation tasks, is provided. The paper is concluded with some suggestions for future research.", "Brain-Computer Interfaces (BCIs)  Electroencephalography (EEG) signals classification  transfer learning  ensemble methods Reducing calibration time while maintaining good classification accuracy has been one of the most challenging problems in electroencephalography (EEG)-based brain-computer interfaces (BCIs) research during the last years. Most of machine learning approaches that have been attempted to address this issue are based on knowledge transfer between different BCIs users. Assuming that there is a common underlying data generating process, they try to learn a subject-independent classification model from multiple users in order to classify data of future users. In this paper, we propose a novel approach that allows inter-subjects classification of EEG signals without relying on the strong assumptions considered in previous work. It consists of learning a prediction model of a new BCI user through an ensemble of classifiers where base classifiers are trained on data from other users separately and weighted according to the performance of the ensemble on few labeled data of the new user. Evaluation on real EEG data showed that our approach allows achieving good classification accuracy when the size of calibration set is small.", " Retrieving images for an arbitrary user query, provided in textual form, is a challenging problem. A recently proposed method addresses this by constructing a visual classifier with images returned by an internet image search engine, based on the user query, as positive images while using a fixed pool of negative images. However, in practice, not all the images obtained from internet image search are always pertinent to the query  some might contain abstract or artistic representation of the content and some might have artifacts. Such images degrade the performance of on-the-fly constructed classifier We propose a method for improving the performance of on-the-fly classifiers by using transfer learning via attributes. We first map the textual query to a set of known attributes and then use those attributes to prune the set of images downloaded from the internet. This pruning step can be seen as zero-shot learning of the visual classifier for the textual user query, which transfers knowledge from the attribute domain to the query domain. We also use the attributes along with the on-the-fly classifier to score the database images and obtain a hybrid ranking. We show interesting qualitative results and demonstrate by experiments with standard datasets that the proposed method improves upon the baseline on-the-fly classification system.", " Categorizing free-hand human sketches has profound implications in applications such as human computer interaction and image retrieval. The task is non-trivial due to the iconic nature of sketches, signified by large variances in both appearance and structure when compared with photographs. Prior works often utilize off-the-shelf low-level features and assume the availability of a large training set, rendering them sensitive towards abstraction and less scalable to new categories. To overcome this limitation, we propose a transfer learning framework which enables one-shot learning of sketch categories. The framework is based on a novel co-regularized sparse coding model which exploits common/shareable parts among human sketches of seen categories and transfer them to unseen categories. We contribute a new dataset consisting of 7,760 human segmented sketches from 97 object categories. Extensive experiments reveal that the proposed method can classify unseen sketch categories given just one training sample with a 33.04% accuracy, offering a two-fold improvement over baselines.", "Classification  feature transformation  kernel density estimation Density based logistic regression (DLR) is a recently introduced classification technique, that performs a one-to-one non-linear transformation of the original feature space to another feature space based on density estimations. This new feature space is particularly well suited for learning a logistic regression model. Whilst performance gains, good interpretability and time efficiency make DLR attractive, there exist some limitations to its formulation. In this paper, we tackle these limitations and propose several new extensions: 1) A more robust methodology for performing density estimations, 2) A method that can transform two or more features into a single target feature, based on the use of higher order kernel density estimation, 3) Analysis of the utility of DLR for transfer learning scenarios. We evaluate our extensions using several synthetic and publicly available datasets, demonstrating that higher order transformations have the potential to boost prediction performance and that DLR is a promising method for transfer learning.", "Domain Adaptation  Neural Networks  Representation Learning  Transfer Learning  Maximum Mean Discrepancy We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set.", " The main objective of transfer learning is to use the knowledge acquired from a source task in order to boost the learning procedure in a target task. Transfer learning comprises a suitable solution for reinforcement learning algorithms, which often require a considerable amount of training time, especially when dealing with complex tasks. This work proposes an autonomous method for transfer learning in reinforcement learning agents. The proposed method is empirically evaluated in the keepaway and the mountain car domains. The results demonstrate that the proposed method can improve the learning procedure in the target task.", " In this work, we consider a transfer learning approach based on K-means for splice site recognition. We use different representations for the sequences, based on n-gram graphs. In addition, a novel representation based on the secondary structure of the sequences is proposed. We evaluate our approach on genomic sequence data from model organisms of varying evolutionary distance. The first obtained results indicate that the proposed representations are promising for the problem of splice site recognition.", "Improved KL-TSK-FS  Fuzzy systems  Knowledge leverage  Missing data  Fuzzy modeling  Transfer learning In this study, the improved knowledge-leverage based TSK fuzzy system modeling method is proposed in order to overcome the weaknesses of the knowledge-leverage based TSK fuzzy system (TSK-FS) modeling method. In particular, two improved knowledge-leverage strategies have been introduced for the parameter learning of the antecedents and consequents of the TSK-FS constructed in the current scene by transfer learning from the reference scene, respectively. With the improved knowledge-leverage learning abilities, the proposed method has shown the more adaptive modeling effect compared with traditional TSK fuzzy modeling methods and some related methods on the synthetic and real world datasets.", " This paper examines the problem of transfer learning in the context of object recognition in a heterogeneous robot team. We specifically look at the case where robots individually learn object classifiers and must then transfer the resulting learned knowledge to another robot. Recent trends in computer vision and robotics have moved towards feature representation learning, where the underlying feature representation used in classification is learned in a data-driven way. This poses a problem to knowledge transfer, as the underlying representations learned by different robots will differ significantly. In this paper, we present several hypotheses with regard to knowledge transfer in such a scenario, specifically that 1) the transfer of knowledge will be most effective if it involves not just the classifier itself, but the learned feature representations themselves, 2) this is not a problem because given similar scenes and objects, some methods such as sparse coding are able to learn representations that can be successfully used by another robot, and 3) a codebook encoding scheme such as Fisher vectors will result in a smaller reduction in accuracy after transfer even if the receiving robot uses its own learned feature representation. Finally, we contribute an alignment procedure and demonstrate that it can serve to facilitate knowledge transfer even when the underlying feature representations are independently learned by each robot and codebook methods are not used. We test all three of the hypotheses and the alignment procedure on a real-world dataset consisting of two robots viewing the same 12 objects using cameras with differing characteristics.", " Following recent works on HRI for UAVs, we present a gesture recognition system which operates on the video stream recorded from a passive monocular camera installed on a quadcopter. While many challenges must be addressed for building a real- time vision- based gestural interface, in this paper we specifically focus on the problem of user personalization. Different users tend to perform the same gesture with different styles and speed. Thus, a system trained on visual sequences depicting some users may work poorly when data from other people are available. On the other hand, collecting and annotating many user- specific data is time consuming. To avoid these issues, in this paper we propose a personalized gestural interface. We introduce a novel transfer learning algorithm which, exploiting both data downloaded from the web and gestures collected from other users, permits to learn a set of person-specific classifiers. We integrate the proposed gesture recognition module into a HRI system with a flying quadrotor robot. In our system first the UAV localizes a person and individuates her identity. Then, when a user performs a specific gesture, the system recognizes it adopting the associated user-specific classifier and the quadcopter executes the corresponding task. Our experimental evaluation demonstrates that the proposed personalized gesture recognition solution is advantageous with respect to generic ones.", "speech enhancement  deep neural network  transfer learning  multi-lingual  resource-limited language In this paper, we propose a transfer learning approach to adapt a well-trained model obtained with high-resource materials of one language to another target language using a small amount of adaptation data for speech enhancement based on deep neural networks (DNNs). We investigate the performance degradation issues of enhancing noisy Mandarin speech data using DNN models already trained with only English speech materials, and vice versa. By assuming that the hidden layers of the well-trained DNN regression model as a cascade of feature extractors, we hypothesize that the first several layers should be transferable between languages. Our experimental results indicate that even with only about 1 minute of adaptation data from the resource-limited language we can achieve a considerable performance improvement over the DNN model without cross-language transfer learning.", "Transfer learning  online learning  classification  radial basis function network In this paper, a Radial Basis Function Network (RBFN) trained with the Dynamic Decay Adjustment (DDA) algorithm (i.e., RBFNDDA) is deployed as an incremental learning model for tackling transfer learning problems. An online learning strategy is exploited to allow the RBFNDDA model to transfer knowledge from one domain and applied to classification tasks in a different yet related domain. An experimental study is carried out to evaluate the effectiveness of the online RBFNDDA model using a benchmark data set obtained from a public domain. The results are analyzed and compared with those from other methods. The outcomes positively reveal the potentials of the online RBFNDDA model in handling transfer learning tasks.", "SVM  AML  Watershed  Transfer learning It is an effective way to use digital image processing method to assist medical research. This paper presents a hybrid approach based on watershed and transfers learning method to automatically segment, characterize, and classify the particular immature precursor (IP) cells in bone marrow pathological (BMP) images. In segmentation phase, we use adaptive morphological reconstruction to accentuate the cell shapes and use improved marker-controlled watershed to segment the cells. Eleven morphological and statistical features are then extracted from those samples. In classification phase, we use transfer learning method to make use of the assistant sample set and generate a strong SVM classifier. Experimental results show the proposed method has a better performance, and the result lays the foundation for study of the correlations between the IP cells in BMP images and the relapse of acute myeloid leukemia (AML).", "Local Regression Transfer Learning  Importance Reweighting  Personality Prediction Some research has been done to predict users' personality based on their web behaviors. They usually use supervised learning methods to model on training dataset and predict on test dataset. However, when training dataset has different distributions from test dataset, which doesn't meet independently identical distribution condition, traditional supervised learning models may perform not well on test dataset. Thus, we introduce a new regression transfer learning framework to deal with this problem, and propose two local regression instance-transfer methods. We use clustering and k-nearest-neighbor to reweight importance of each training instance to adapt to test dataset distribution, and then train a weighted risk regression model for prediction. We perform experiments on the condition that users dataset are from different genders and from different districts, and the results indicate that our methods can reduce mean square error about 30% to the most compared with non-transfer methods and be better than other transfer method in the whole.", "Risk-Awareness  Memoryless Stochastic Abstract Policies  Transfer Learning In this paper we improve learning performance of a risk-aware robot facing navigation tasks by employing transfer learning  that is, we use information from a previously solved task to accelerate learning in a new task. To do so, we transfer risk-aware memoryless stochastic abstract policies into a new task. We show how to incorporate risk-awareness into robotic navigation tasks, in particular when tasks are modeled as stochastic shortest path problems. We then show how to use a modified policy iteration algorithm, called AbsProb-PI, to obtain risk-neutral and risk-prone memoryless stochastic abstract policies. Finally, we propose a method that combines abstract policies, and show how to use the combined policy in a new navigation task. Experiments validate our proposals and show that one can find effective abstract policies that can improve robot behavior in navigation problems.", " Most existing zero-shot learning approaches exploit transfer learning via an intermediate-level semantic representation such as visual attributes or semantic word vectors. Such a semantic representation is shared between an annotated auxiliary dataset and a target dataset with no annotation. A projection from a low-level feature space to the semantic space is learned from the auxiliary dataset and is applied without adaptation to the target dataset. In this paper we identify an inherent limitation with this approach. That is, due to having disjoint and potentially unrelated classes, the projection functions learned from the auxiliary dataset/domain are biased when applied directly to the target dataset/domain. We call this problem the projection domain shift problem and propose a novel framework, transductive multi-view embedding, to solve it. It is 'transductive' in that unlabelled target data points are explored for projection adaptation, and 'multi-view' in that both low-level feature (view) and multiple semantic representations (views) are embedded to rectify the projection shift. We demonstrate through extensive experiments that our framework (1) rectifies the projection shift between the auxiliary and target domains, (2) exploits the complementarity of multiple semantic representations, (3) achieves state-of-the-art recognition results on image and video benchmark datasets, and (4) enables novel cross-view annotation tasks.", " Modeling the target appearance is critical in many modern visual tracking algorithms. Many tracking-by-detection algorithms formulate the probability of target appearance as exponentially related to the confidence of a classifier output. By contrast, in this paper we directly analyze this probability using Gaussian Processes Regression (GPR), and introduce a latent variable to assist the tracking decision. Our observation model for regression is learnt in a semi-supervised fashion by using both labeled samples from previous frames and the unlabeled samples that are tracking candidates extracted from the current frame. We further divide the labeled samples into two categories: auxiliary samples collected from the very early frames and target samples from most recent frames. The auxiliary samples are dynamically re-weighted by the regression, and the final tracking result is determined by fusing decisions from two individual trackers, one derived from the auxiliary samples and the other from the target samples. All these ingredients together enable our tracker, denoted as TGPR, to alleviate the drifting issue from various aspects. The effectiveness of TGPR is clearly demonstrated by its excellent performances on three recently proposed public benchmarks, involving 161 sequences in total, in comparison with state-of-the-arts.", "Transfer learning  Deep learning  Stacked auto-encoders In this work we explore the idea that, in the presence of a small training set of images, it could be beneficial to use that set itself to obtain a transformed training set (by performing a random rotation on each sample), train a source network using the transformed data, then retrain the source network using the original data. Applying this transfer learning technique to three different types of character data, we achieve average relative improvements between 6% and 16% in the classification test error. Furthermore, we show that it is possible to achieve relative improvements between 8% and 42% in cases where the amount of original training samples is very limited (30 samples per class), by introducing not just one rotation but several random rotations per sample.", "Recommender Systems  Cosine Similarity  Collaborative Filtering  Social Network  Matrix Factorization Traditional recommender systems perform poorly when training data is sparse. During past few years, researchers have proposed several social-based methods to alleviate this sparsity problem. The basic assumption of these social recommender systems is that friends should have similar interests. However, this assumption does not always hold due to the heterogeneity between recommendation domain and social domain. Thus, knowledge transferred from social network often contains noises. To solve this problem, in this paper, we analyze and identify what knowledge is useful during transfer learning process, and develop a method, called Cosine Similarity Regularization (CosSimReg), to transfer only useful information from social domain. CosSimReg is able to minimize the negative effects of noisy data in social network, thus improving the performance. Experiments on two real life datasets demonstrate that CosSimReg performs better than the state-of-the-art approaches.", "Reinforcement learning  state abstraction  function approximation  transfer learning  fuzzy clustering State abstraction and value function approximation are powerful and useful methods for time and memory management in reinforcement learning. In traditional trends, these methods are applied to speed up learning of the current task  however, when we learn multiple similar environments in a general setting, these methods can be applied to improve learning of other tasks. We propose a framework to aggregate the results of state abstraction and function approximation in several tasks of a domain to reuse them in future tasks of that domain. First, we show theoretically how abstraction based on optimal value functions speeds up learning in that same task in the future. In many situations, fuzzy clustering is more natural than hard clustering, since it does not force the states to fully belong to one of the classes. In second part, we examine theoretically and algorithmically how using the knowledge extracted by fuzzy value approximation of a single task improves learning of that same task in the future. In both parts, we show that aggregating states ( hard or fuzzy) preserves the optimal value function in the abstract space with an error bound. Having the support provided by these two parts, we propose new ways to combine the results of abstraction and approximation of different tasks of the domain to infer similarity measures on the state space. Finally, we show empirically that batch learning based on these similarity measures can speed up learning in the future tasks of the setting.", "Brain-computer interface (BCI)  near-infrared spectroscopy (NIRS)  brain signals variability  transfer learning  bipartite graph partitioning One of the major limitations to the use of brain-computer interfaces (BCIs) based on near-infrared spectroscopy (NIRS) in realistic interaction settings is the long calibration time needed before every use in order to train a subject-specific classifier. One way to reduce this calibration time is to use data collected from other users or from previous recording sessions of the same user as a training set. However, brain signals are highly variable and using heterogeneous data to train a single classifier may dramatically deteriorate classification performance. This paper proposes a transfer learning framework in which we model brain signals variability in the feature space using a bipartite graph. The partitioning of this graph into sub-graphs allows creating homogeneous groups of NIRS data sharing similar spatial distributions of explanatory variables which will be used to train multiple prediction models that accurately transfer knowledge between data sets.", "Heterogeneous feature spaces  co-transfer learning  spectral clustering  canonical correlation analysis With the rapid growth of data collection techniques, it is very common that instances in different domains/views share the same set of categories, or one instance is represented in different domains which is called co-occurrence data. For example, the multilingual learning scenario contains documents in different languages, the images in the social media website simultaneously have text descriptions, and etc. In this paper, we address the problem of automatically clustering the instances by making use of the multi-domain information. Especially, the information comes from heterogeneous domains, i.e., the feature spaces in different domains are different. A heterogeneous co-transfer spectral clustering framework is proposed with three main steps. One is to build the relationships across different domains with the aid of co-occurrence data. The next is to construct a joint graph which contains the inter-relationship across different domains and intra-relationship within each domain. The last is to simultaneously group the instances in all domains by applying spectral clustering on the joint graph. A series of experiments on real-world data sets have shown the good performance of the proposed method by comparing with the state-of-the-art methods.", " The problem of learning to mimic a human expert/teacher from training trajectories is called imitation learning. Tomake the process of teaching easier in this setting, we propose to employ transfer learning (where one learns on a source problem and transfers the knowledge to potentially more complex target problems). We consider multi-relational environments such as real-time strategy games and use functional-gradient boosting to capture and transfer the models learned in these environments. Our experiments demonstrate that our learner learns a very good initial model from the simple scenario and effectively transfers the knowledge to the more complex scenario thus achieving a jump start, a steeper learning curve and a higher convergence in performance.", " Vector space word representations have gained big success recently at improving performance across various NLP tasks. However, existing word embeddings learning methods only utilize homo-lingual corpus. Inspired by transfer learning, we propose a novel language transfer method to obtain word embeddings via language transfer. Under this method, in order to obtain word embeddings of one language (target language), we train models on corpus of another different language (source language) instead. And then we use the obtained source language word embeddings to represent target language word embeddings. We evaluate the word embeddings obtained by the proposed method on word similarity tasks across several benchmark datasets. And the results show that our method is surprisingly effective, outperforming competitive baselines by a large margin. Another benefit of our method is that the process of collecting new corpus might be skipped.", "Transfer learning  Missing data  Supported Vector Regression  Knowledge Based The classical regression systems modeling methods only consider the single scene, which has the weakness: partial information missing may weaken the generalization abilities of the regression systems constructed based on this dataset. A regression system with the Knowledge transfer learning abilities, i.e. Knowledge Based e Support Vector Regression (KB-epsilon-SVR for brevity) is proposed based on epsilon-support vector regression. KB-epsilon-SVR can use the current data information sufficiently, and learn from the existing useful historical knowledge effectively, so that remedy the information lack in the current scene. Reinforced current model is obtained through control the similarity between current model and history model in the object function and current model can benefit from history scene when information is missing or insufficient. Experiments show that KB-epsilon-SVR has the better performance and adaptability than the traditional support vector regression methods in scenarios with insufficient data.", "Transductive transfer learning  Large-margin approach  Rademacher complexity  Stochastic gradient descent Transductive transfer learning is one special type of transfer learning problem, in which abundant labeled examples are available in the source domain and only unlabeled examples are available in the target domain. It easily finds applications in spam filtering, microblogging mining, and so on. In this paper, we propose a general framework to solve the problem by mapping the input features in both the source domain and the target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We develop one specific example of the framework, namely latent large-margin transductive transfer learning algorithm, and analyze its theoretic bound of classification loss via Rademacher complexity. We also provide a unified view of several popular transfer learning algorithms under our framework. Experiment results on one synthetic dataset and three application datasets demonstrate the advantages of the proposed algorithm over the other state-of-the-art ones.", "Spectral clustering  Constrained clustering  Transfer learning  Graph partition Constrained clustering has been well-studied for algorithms such as K-means and hierarchical clustering. However, how to satisfy many constraints in these algorithmic settings has been shown to be intractable. One alternative to encode many constraints is to use spectral clustering, which remains a developing area. In this paper, we propose a flexible framework for constrained spectral clustering. In contrast to some previous efforts that implicitly encode Must-Link (ML) and Cannot-Link (CL) constraints by modifying the graph Laplacian or constraining the underlying eigenspace, we present a more natural and principled formulation, which explicitly encodes the constraints as part of a constrained optimization problem. Our method offers several practical advantages: it can encode the degree of belief in ML and CL constraints  it guarantees to lower-bound how well the given constraints are satisfied using a user-specified threshold  it can be solved deterministically in polynomial time through generalized eigendecomposition. Furthermore, by inheriting the objective function from spectral clustering and encoding the constraints explicitly, much of the existing analysis of unconstrained spectral clustering techniques remains valid for our formulation. We validate the effectiveness of our approach by empirical results on both artificial and real datasets. We also demonstrate an innovative use of encoding large number of constraints: transfer learning via constraints.", "transfer learning  deep learning  artificial neural networks Deep architectures have been used in transfer learning applications, with the aim of improving the performance of networks designed for a given problem by reusing knowledge from another problem. In this work we addressed the transfer of knowledge between deep networks used as classifiers of digit and shape images, considering cases where only the set of class labels, or only the data distribution, changed from source to target problem. Our main goal was to study how the performance of knowledge transfer between such problems would be affected by varying the number of layers being retrained and the amount of data used in that retraining. Generally, reusing networks trained for a different label set led to better results than reusing networks trained for a different data distribution. In particular, reusing for less classes a network trained for more classes was beneficial for virtually any amount of training data. In all cases, retraining only one layer to save time consistently led to poorer performance. The results obtained when retraining for upright digits a network trained for rotated digits raise the hypothesis that transfer learning could be used to better deal with image classification problems in which only a small amount of labelled data is available for training.", "audio equalizer  transfer learning  collaborative filtering  personalized item Audio equalizers (EQs) are perhaps the most commonly used tools used in audio production. The SocialEQ project is a web-based personalized audio equalization system that uses an alternative interface paradigm to the standard approach. Here, the user names a desired effect (e.g. make the sound warm) and teaches the tool (e.g. an equalizer) what settings make the sound embody the term. SocialEQ typically requires 25 ratings to properly personalize the equalization settings. In this paper, we present three methods to improve the speed of generating personalized items (audio settings) so users can be provided personalized EQ curves after rating a much smaller number of examples. These methods can be adapted to any situation where collaborative filtering is desirable, the end products created for users are unique and comparable to each other, but prior users did not rate the same set of examples as the current user. Methods are tested on a data set of 1635 user sessions.", "Multi-view  low-rank  common subspace Multi-view data is very popular in real-world applications, as different view-points and various types of sensors help to better represent data when fused across views or modalities. Samples from different views of the same class are less similar than those with the same view but different class. We consider a more general case that prior view information of testing data is inaccessible in multi-view learning. Traditional multi-view learning algorithms were designed to obtain multiple view-specific linear projections and would fail without this prior information available. That was because they assumed the probe and gallery views were known in advance, so the correct view-specific projections were to be applied in order to better learn low-dimensional features. To address this, we propose a Low-Rank Common Subspace (LRCS) for multi-view data analysis, which seeks a common low-rank linear projection to mitigate the semantic gap among different views. The low-rank common projection is able to capture compatible intrinsic information across different views and also well-align the within-class samples from different views. Furthermore, with a low-rank constraint on the view-specific projected data and that transformed by the common subspace, the within-class samples from multiple views would concentrate together. Different from the traditional supervised multi-view algorithms, our LRCS works in a weakly supervised way, where only the view information gets observed. Such a common projection can make our model more flexible when dealing with the problem of lacking prior view information of testing data. Two scenarios of experiments, robust subspace learning and transfer learning, are conducted to evaluate our algorithm. Experimental results on several multi-view datasets reveal that our proposed method outperforms state-of-the-art, even when compared with some supervised learning methods.", " Data Sparsity incurs serious concern in collaborative filtering (CF). This issue is especially critical for newly launched CF applications where observed ratings are too scarce to learn a good model to predict missing values. There could be, however, information from other related domains which are with relatively denser data that can be utilized. This paper proposes a transfer-learning based approach that exploits probabilistic matrix factorization model trained with variational expectation-maximization (VEM) to resolve data sparsity by using information from multiple auxiliary domains. We conduct experiments on several data combination and report significant improvements over state-of-the-art transfer-based models for collaborative filtering. The results also show that our framework is the only solution that can achieve acceptable performance when each user has only one single rating. The code of our model is available at (1).", "Transfer Learning  Deep Learning Transfer learning is a process that allows reusing a learning machine trained on a problem to solve a new problem. Transfer learning studies on shallow architectures show low performance as they are generally based on hand-crafted features obtained from experts. It is therefore interesting to study transference on deep architectures, known to directly extract the features from the input data. A Stacked Denoising Autoencoder (SDA) is a deep model able to represent the hierarchical features needed for solving classification problems. In this paper we study the performance of SDAs trained on one problem and reused to solve a different problem not only with different distribution but also with a different tasks. We propose two different approaches: 1) unsupervised feature transference, and 2) supervised feature transference using deep transfer learning. We show that SDAs using the unsupervised feature transference outperform randomly initialized machines on a new problem. We achieved 7% relative improvement on average error rate and 41% on average computation time to classify typed uppercase letters. In the case of supervised feature transference, we achieved 5.7% relative improvement in the average error rate, by reusing the first and second hidden layer, and 8.5% relative improvement for the average error rate and 54% speed up w.r.t the baseline by reusing all three hidden layers for the same data. We also explore transfer learning between geometrical shapes and canonical shapes, we achieved 7.4% relative improvement on average error rate in case of supervised feature transference approach.", "Single-trial classification  ERP  VEP  EEG  transfer learning  active learning  active transfer learning Single-trial Event-Related Potential (ERP) classification is a key requirement for several types of Brain-Computer Interaction (BCI) technologies. However, strong individual differences make it challenging to develop a generic single-trial ERP classifier that performs well for all subjects. Usually some subject-specific training samples need to be collected in an initial calibration session to customize the classifier. However, if implemented into an actual BCI system, then this calibration process would decrease the utility of the system, potentially decreasing its usability. In this paper we propose a Transfer Learning approach for reducing the amount of subject-specific data in online single-trial ERP classifier calibration, and an Active Transfer Learning approach for offline calibration. By applying these approaches to data from a Visually-Evoked Potential EEG experiment, we demonstrate that they improve the classification performance, given the same number of labeled subject-specific training samples. In other words, these approaches can also attain a desired level of classification accuracy with less labeling effort when compared to a randomly selected training set.", "transfer learning  attributes  multiclass classification  incremental learning  generative model Machine learning is the basis of important advances in artificial intelligence. Unlike the general methods of machine learning, which use the same tasks for training and testing, the method of transfer learning uses different tasks to learn a new task. Among the various transfer learning algorithms in the literature, we focus on the attribute-based transfer learning. This algorithm realizes transfer learning by introducing attributes and transferring the results of training to another task with the common attributes. However, the existing method does not consider the frequency in which each attribute appears in feature vectors (called the observation probability). In this paper, we present a generative model with the observation probability. By the experiments, we show that the proposed method has achieved a higher accuracy rate than the existing method. Moreover, we see that it makes possible the incremental learning that was impossible in the existing method.", "Incremental Learning  Incomplete Data  Transfer Learning  Covering Algorithms  Rules Family Recently strong AI emerged from artificial intelligence due to need for a thinking machine. In this domain, it is necessary to deal with dynamic incomplete data and understanding of how machines make their decision is also important, especially in information system domain. One type of learning called Covering Algorithms (CA) can be used instead of the difficult statistical machine learning methods to produce simple rule with powerful prediction ability. However, although using CA as the base of strong AI is a novel idea, doing so with the current methods available is not possible. Thus, this paper presents a novel CA (RULES-IT) and tests its performance over incomplete data. This algorithm is the first incremental algorithm in its family, and CA as a whole, that transfer rules from different domains and introduce intelligent aspects using simple representation. The performance of RULES-IT will be tested over incomplete and complete data along with other algorithms in the literature. It will be validated using 5-fold cross validation in addition to Friedman with Nemenyi post hoc tests to measure the significance and rank the algorithms.", "Meta-learning  Hierarchical ensembles  Ensemble classifiers  Bagging  Boosting  Stacking  Cascade generalization  Non-stationary environments  Transfer learning  Genetic programming The model selection stage is one of the most difficult in predictive modeling. To select a model with a highest generalization performance involves benchmarking huge number of candidate models or algorithms. Often, a final model is selected without considering potentially high quality candidates just because there are too many possibilities. Improper benchmarking methodology often leads to biased estimates of model generalization performance. Automation of the model selection stage is possible, however the computational complexity is huge especially when ensembles of models and optimization of input features should be also considered. In this paper we show, how to automate model selection process in a way that allows to search for complex hierarchies of ensemble models while maintaining computational tractability. We introduce two-stage learning, meta-learning templates optimized by evolutionary programming with anytime properties to be able to deliver and maintain data-tailored algorithms and models in a reasonable time without human interaction. Co-evolution if inputs together with optimization of templates enabled to solve algorithm selection problem efficiently for variety of datasets.", " Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source/training domain) but only very limited training data for a second task (the target/test domain) that is similar but not identical to the first. Previous work on transfer learning has focused on relatively restricted settings, where specific parts of the model are considered to be carried over between tasks. Recent work on covariate shift focuses on matching the marginal distributions on observations X across domains. Similarly, work on target/conditional shift focuses on matching marginal distributions on labels Y and adjusting conditional distributions P(X vertical bar Y), such that P(X) can be matched across domains. However, covariate shift assumes that the support of test P(X) is contained in the support of training P(X), i.e., the training set is richer than the test set. Target/conditional shift makes a similar assumption for P(Y). Moreover, not much work on transfer learning has considered the case when a few labels in the test domain are available. Also little work has been done when all marginal and conditional distributions are allowed to change while the changes are smooth. In this paper, we consider a general case where both the support and the model change across domains. We transform both X and Y by a location-scale shift to achieve transfer between tasks. Since we allow more flexible transformations, the proposed method yields better results on both synthetic data and real-world data.", " As robots are increasingly integrated into daily life, one of the most important roles they will assume is that of collaboratively helping us perform physical tasks. Be it helping us put together furniture, transporting materials, or assisting with food preparation, a system's ability to assess its (and others') skill level regarding the performance of different tasks is essential to achieving efficient scheduling and collaboration. In this paper, we present preliminary work towards an observation-driven modeling approach allowing an agent to autonomously predict the amount of time required for different agents to complete actions. This approach utilizes insights and observations from the developmental psychology and operations research communities to accurately develop agent-personalized skill proficiency models. We demonstrate our model by evaluating its performance at estimating agent performance in a set of common assembly tasks. Our evaluation measures knowledge-transfer via novel task introduction, as well as extrapolation by predicting future performance given previous experience.", " Learning how to use functional objects is essential for robots that are to carry out household tasks. However, learning every object from scratch would be a very naive and time-consuming approach. In this paper, we propose transfer learning of affordances to reduce the number of exploratory actions needed to learn how to use a new object. Through embodied interaction with the object, the robot discovers the object's similarity to previously learned objects by comparing their shape features and spatial relations between object parts. The robot actively selects object parts along with parameterized actions and evaluates the effects on-line. We demonstrate through real-world experiments with the humanoid robot NAO that our method is able to speed up the use of a new type of garbage can by transferring the affordances learned previously for similar garbage cans.", "Data classification  fuzzy min-max neural network  online learning  transfer learning In this paper, we present an analysis on transfer learning using the Fuzzy Min-Max (FMM) neural network with an online learning strategy. Transfer learning leverages information from the source domain in solving problems in the target domain. Using the online FMM model, the data samples are trained one at a time. In order to evaluate the online FMM model, a transfer learning data set, based on data samples collected from real landmines, is used. The experimental results of FMM are analyzed and compared with those from other methods in the literature. The outcomes indicate that the online FMM model is effective for undertaking transfer learning tasks.", "Cross-site Recommender Systems  Collaborative Filtering  Transfer Learning  RBF Kernel This paper targets at utilizing cross-site ratings to alleviate the data sparse problem for recommender systems. The key issue is how to bridge user features between the targeted site and the auxiliary site. In traditional transfer learning models, a linear mapping function is assumed to map the user feature in the auxiliary site into the targeted site. The limitation lies in that when the real data does not follow the linear property, such models will fail to work. Therefore, the motivation of this paper is to identify whether the rating prediction performance in recommender systems can be improved by considering the nonlinear transformation. As a primary study, we propose a nonlinear transfer learning model, and utilize the radial basis function (RBF) kernel to map user features of multiple sites. Through empirical analysis in a real-world cross-site dataset, we demonstrate that by utilizing the nonlinear mapping function RBF kernel, the rating prediction performance is consistently better than previous transfer learning models at a significant scale. It indicates that the nonlinear property does exist in real recommender systems, which has been ignored previously.", "boosting  transfer learning  distribution measure  facial expression recognition In the machine learning community, most algorithms proposed, particularly for inductive learning, are based entirely on one crucial assumption: that the training and test data points are drawn or generated from the exact same distribution. If this condition is not fully satisfied, most learning algorithms or models are corrupted. In this paper, we propose a new instance based transductive transfer learning method based on Boosting framework by using a distribution measure approach. There follows a detailed description of this distribution measure approach. Subsequently, we describe our boosting transfer learning method in detail and report its performance in facial expression recognition tasks.", " Large-scale, multi-agent systems are too complex for optimal control strategies to be known at design time and as a result good strategies must be learned at runtime. Learning in such systems, particularly those with multiple objectives, takes a considerable amount of time because of the size of the environment and dependencies between goals. Transfer Learning (TL) has been shown to reduce learning time in single-agent, single-objective applications. It is the process of sharing knowledge between two learning tasks called the source and target. The source is required to have been completed prior to the target task. This work proposes extending TL to multi-agent, multi-objective applications. To achieve this, an on-line version of TL called Parallel Transfer Learning (PTL) is presented. The issues involved in extending this algorithm to a multi-objective form are discussed. The effectiveness of this approach is evaluated in a smart grid scenario. When using PTL in this scenario learning is significantly accelerated. PTL achieves comparable performance to the base line in one third of the time.", " In this article, we focus on time-continuous predictions of emotion in music and speech, and the transfer of learning from one domain to the other. First, we compare the use of Recurrent Neural Networks (RNN) with standard hidden units (Simple Recurrent Network - SRN) and Long-Short Term Memory (LSTM) blocks for intra-domain acoustic emotion recognition. We show that LSTM networks outperform SRN, and we explain, in average, 74%/59% (music) and 42%/29% (speech) of the variance in Arousal/Valence. Next, we evaluate whether cross-domain predictions of emotion are a viable option for acoustic emotion recognition, and we test the use of Transfer Learning (TL) for feature space adaptation. In average, our models are able to explain 70%/43% (music) and 28%/11% (speech) of the variance in Arousal/Valence. Overall, results indicate a good cross-domain generalization performance, particularly for the model trained on speech and tested on music without pre-encoding of the input features. To our best knowledge, this is the first demonstration of cross-modal time-continuous predictions of emotion in the acoustic domain.", " Domain transfer learning aims to learn an effective classifier for a target domain, where only a few labeled samples are available, with the help of many labeled samples from a source domain. The source and target domain samples usually share the same features and class label space, but have significantly different In these experiments error of the classifier distributions. Nonnegative Matrix Factorization (NMF) has been studied and applied widely as a powerful data representation method. However, NMF is limited to single domain learning problem. It can not be directly used in domain transfer learning problem due to the significant differences between the distributions of the source and target domains. In this paper, we extend the NMF method to domain transfer learning problem. The Maximum Mean Discrepancy (MMD) criteria is employed to reduce the mismatch of source and target domain distributions in the coding vector space. Moreover, we also learn a classifier in the coding vector space to directly utilize the class labels from both the two domains. We construct an unified objective function for the learning of both NMF parameters and classifier parameters, which is optimized alternately in an iterative algorithm. The proposed algorithm is evaluated on two challenging domain transfer tasks, and the encouraging experimental results show its advantage over state-of-the-art domain transfer learning algorithms.", " In this paper we propose a new unsupervised transfer learning approach which aims at finding a partition of unlabeled data in target domain using the knowledge obtained from clustering a source domain unlabeled data. The key idea behind our method is that finding partitions in different feature's subspaces of a source task can help to obtain a more accurate partition in a target one. From the set of source partitions we select only.. nearest neighbors using some measure of similarity. Finally, multi-layer non-negative matrix factorization is performed to obtain a partition of objects in target domain. Experimental results show high potential and effectiveness of the proposed technique.", "Bayesian inference  classification  Gaussian process (GP)  multitask learning (MTL)  semisupervised learning (SSL)  transfer learning We present a probabilistic framework for transferring learning across tasks and between labeled and unlabeled data. The approach is based on Gaussian process (GP) prediction and incorporates both the geometry of the data and the similarity between tasks within a GP covariance, allowing Bayesian prediction in a natural way. We discuss the transfer of learning in a multitask scenario in the two cases where the underlying geometry is assumed to be the same across tasks and where different tasks are assumed to have independent geometric structures. We demonstrate the method on a number of real datasets, indicating that the semisupervised multitask approach can result in very significant improvements in performance when very few labeled training examples are available.", "Hyperspectral images  Transfer learning  Target detection  Segmentation Target detection has been of great interest in hyperspectral image analysis. Feature extraction from target samples and counterpart backgrounds consist the key to the problem. Traditional target detection methods depend on comparatively fixed feature for all the pixels under observation. For example, RX employs the same distance measurement for all the pixels. However, the best separation results usually come from certain targets and backgrounds. Theoretically, they are the purest targets and backgrounds pixels, or the constructive endmembers in the subspace model. So using those most representative pixels feature to train a concentrated subspace is expected to enhance the separability between targets and backgrounds. Meanwhile, applying the discriminative information from these training data to the large testing data which are not in the same feature space and with different data distributions is a challenge. Here, the idea of transfer learning from interactive annotation technique in video is employed. Based on the transfer learning frame, several points are taken into consideration and the proposed method is named as an unsupervised transfer learning based target detection (UTLD) method. Firstly, the extreme target and background pixels are generated from robust outlier detection, providing the input for target samples and background samples in transfer learning. Secondly, pixels are calculated from the root points in a segmentation method with the purpose to preserve the most distribution feature of the backgrounds after reduced dimension. Thirdly, sparse constraint is imposed into the transfer learning procedure. With this constraint, a simpler and more concentrated subspace with clear physical meaning can be constructed. Extensive experiments reveal the performance is comparable to the state-of-art target detection methods. (c) 2013 Elsevier B.V. All rights reserved.", "Learning taxonomy  Learning syntactic parse tree  Transfer learning  Syntactic generalization  Search relevance We apply a paradigm of transfer learning to build a taxonomy of entities intended to improve search engine relevance in a vertical domain. The taxonomy construction process starts from the seed entities and mines available source domains for new entities associated with these seed entities. New entities are formed by applying the machine learning of syntactic parse trees (their generalizations) to the search results for existing entities to form commonalities between them. These commonality expressions then form parameters of existing entities, and are turned into new entities at the next learning iteration. To match natural language expressions between source and target domains, we use syntactic generalization, an operation which finds a set of maximal common sub-trees of constituency parse trees of these expressions. Taxonomy and syntactic generalization are applied to relevance improvement in search and text similarity assessment. We conduct an evaluation of the search relevance improvement in vertical and horizontal domains and observe significant contribution of the learned taxonomy in the former, and a noticeable contribution of a hybrid system in the latter domain. We also perform industrial evaluation of taxonomy and syntactic generalization-based text relevance assessment and conclude that a proposed algorithm for automated taxonomy learning is suitable for integration into industrial systems. The proposed algorithm is implemented as a component of Apache OpenNLP project. (C) 2013 Elsevier Ltd. All rights reserved.", "Classifier selection  domain adaptation  ordinal regression  sentiment analysis  source sample selection bias  transfer learning Designing a classifier in the absence of labeled data is becoming a common encounter as the acquisition of informative labels is often difficult or expensive, particularly on new uncharted target domains. The feasibility of attaining a reliable classifier for the task of interest is embarked by some in transfer learning, where label information from relevant source domains is considered for complimenting the design process. The core challenge arising from such endeavors, however, is the induction of source sample selection bias, such that the trained classifier has the tendency of steering toward the distribution of the source domain. In addition, this bias is deemed to become more severe on data involving multiple classes. Considering this cue, our interest in this paper is to address such a challenge in the target domain, where ordinal labeled data are unavailable. In contrast to the previous works, we propose a transfer ordinal label learning paradigm to predict the ordinal labels of target unlabeled data by spanning the feasible solution space with ensemble of ordinal classifiers from the multiple relevant source domains. Specifically, the maximum margin criterion is considered here for the construction of the target classifier from an ensemble of source ordinal classifiers. Theoretical analysis and extensive empirical studies on real-world data sets are presented to study the benefits of the proposed method.", "Cottonmaturity estimation  Transfer learning  Feature-based domain adaptation This paper describes a proposed machine vision system developed to acquire longitudinal images of complete cotton fibers and then estimate their average maturity using image and pattern analysis. Maturity is important to the cotton industry because it relates to fiber's propensity to break when submitted to mechanical stress and it influences the quality of the goods produced from it (yarns and fabrics). The proposed system is novel because it estimates maturity indirectly from fibers' longitudinal views using auxiliary training data generated from fibers' cross-sectional views. It uses the transfer learning framework to reconcile the distribution differences between the two views before traditional machine learning algorithms are applied to learn a suitable model for cotton fiber maturity imaged longitudinally. In addition, the proposed system is more descriptive than commercially available systems currently used in the cotton industry because it estimates not only the average maturity of a complete cotton fiber, but also the maturity variations along the fiber from end to end. Validation studies performed show that the transfer learning approach is a practical and promising way to train our system.", " We consider the problem of designing local reinforcement learning rules for artificial neural network (ANN) controllers. Motivated by the universal approximation properties of ANNs, we adopt an ANN representation for the learning rules, which are optimized using evolutionary algorithms. We evaluate the ANN rules in partially observable versions of four tasks: the mountain car, the acrobot, the cart pole balancing, and the nonstationary mountain car. For testing whether such evolved ANN-based learning rules perform satisfactorily, we compare their performance with the performance of SARSA(lambda) with tile coding, when the latter is provided with either full or partial state information. The comparison shows that the evolved rules perform much better than SARSA(lambda) with partial state information and are comparable to the one with full state information, while in the case of the nonstationary environment, the evolved rule is much more adaptive. It is therefore clear that the proposed approach can be particularly effective in both partially observable and nonstationary environments. Moreover, it could potentially be utilized toward creating more general rules that can be applied in multiple domains and transfer learning scenarios.", "Facial expression  Invariant representation  Delaunay tessellation  Manifold of expressions  Piecewise affine warping  Transfer learning Facial expressions analysis plays an important part in emotion detection. However, having an automatic and non-intrusive system to detect blended facial expression is still a challenging problem, especially when the subject is unknown to the system. Here, we propose a method that adapts to the morphology of the subject and that is based on a new invariant representation of facial expressions. In our system, one expression is defined by its relative position to 8 other expressions. As the mode of representation is relative, we show that the resulting expression space is person-independent. The 8 expressions are synthesized for each unknown subject from plausible distortions. Recognition tasks are performed in this space with a basic algorithm. The experiments have been performed on 22 different blended expressions and on either known or unknown subjects. The recognition results on known subjects demonstrate that the representation is robust to the type of data (shape and/or texture information) and to the dimensionality of the expression space. The recognition results on 22 expressions of unknown subjects show that a dimensionality of the expression space of 4 is enough to outperform traditional methods based on active appearance models and accurately describe an expression. (c) 2013 Elsevier Inc. All rights reserved.", "Metric learning  k-nearest neighbors classification  nearest class mean classification  large scale image classification  transfer learning  zero-shot learning  image retrieval We study large-scale image classification methods that can incorporate new classes and training images continuously over time at negligible cost. To this end, we consider two distance-based classifiers, the k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers, and introduce a new metric learning approach for the latter. We also introduce an extension of the NCM classifier to allow for richer class representations. Experiments on the ImageNet 2010 challenge dataset, which contains over 10(6) training images of 1,000 classes, show that, surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier. Moreover, the NCM performance is comparable to that of linear SVMs which obtain current state-of-the-art performance. Experimentally, we study the generalization performance to classes that were not used to learn the metrics. Using a metric learned on 1,000 classes, we show results for the ImageNet-10K dataset which contains 10,000 classes, and obtain performance that is competitive with the current state-of-the-art while being orders of magnitude faster. Furthermore, we show how a zero-shot class prior based on the ImageNet hierarchy can improve performance when few training images are available.", "Transfer learning  Expression recognition  Action unit recognition A key assumption of traditional machine learning approach is that the test data are draw from the same distribution as the training data. However, this assumption does not hold in many real-world scenarios. For example, in facial expression recognition, the appearance of an expression may vary significantly for different people. As a result, previous work has shown that learning from adequate person-specific data can improve the expression recognition performance over the one from generic data. However, person-specific data is typically very sparse in real-world applications due to the difficulties of data collection and labeling, and learning from sparse data may suffer from serious over-fitting. In this paper, we propose to learn a person-specific model through transfer learning. By transferring the informative knowledge from other people, it allows us to learn an accurate model for a new subject with only a small amount of person-specific data. We conduct extensive experiments to compare different person-specific models for facial expression and action unit (AU) recognition, and show that transfer learning significantly improves the recognition performance with a small amount of training data. (C) 2013 Published by Elsevier B.V.", "Semi-supervised classifier  Hybrid discriminative and generative model  Transfer learning  Text classification Developing methods for designing good classifiers from labeled samples whose distribution is different from that of test samples is an important and challenging research issue in the fields of machine learning and its application. This paper focuses on designing semi-supervised classifiers with a high generalization ability by using unlabeled samples drawn by the same distribution as the test samples and presents a semi-supervised learning method based on a hybrid discriminative and generative model. Although JESS-CM is one of the most successful semi-supervised classifier design frameworks based on a hybrid approach, it has an overfitting problem in the task setting that we consider in this paper. We propose an objective function that utilizes both labeled and unlabeled samples for the discriminative training of hybrid classifiers and then expect the objective function to mitigate the overfitting problem. We show the effect of the objective function by theoretical analysis and empirical evaluation. Our experimental results for text classification using four typical benchmark test collections confirmed that with our task setting in most cases, the proposed method outperformed the JESS-CM framework. We also confirmed experimentally that the proposed method was useful for obtaining better performance when classifying data samples into either known or unknown classes, which were included in given labeled samples or not, respectively.", "Computer vision  Human pose estimation  Knowledge transfer learning  Attribute model  Supervised Latent Dirichlet Allocation  Bag of features Because of the diversity of human poses and appearances of the same pose, it is difficult to collect comprehensive training samples of all kinds of human poses and maintain the probability distribution of the training samples with the same as that of testing. Therefore, this paper learns prior knowledge from the known human pose in the collected samples to infer characteristics of new human poses. This way realizing zero-shot and one-shot learning overcomes the above two difficulties. Finally, a novel human pose estimation based on knowledge transfer learning is proposed in this paper. In the process of our human pose estimation method, first, an attribute-based representation model of the human pose is built based on our proposed body-pose-attribute hierarchical framework. Under this model, an image would be divided into disjoint regions, and an attribute is extracted from each region. Thus, an attribute bags can be used to represent a specific human pose, then the attribute bag of a new human pose can be effectively transferred from the prior knowledge obtained from the known human poses. Second, an attribute parameter model called supervised LDA (SLDA) is built, and the Gibbs sampling algorithm is exploited to infer and to learn several parameters of the model for predicting the attributes of the test target samples. The experimental results from the subset of H3D dataset and VOC2011 dataset have shown that the proposed method is feasible and effective even if the training sample set is small. (C) 2012 Elsevier B.V. All rights reserved.", "Machine learning  Activity recognition  Transfer learning  Smart environments Many intelligent systems that focus on the needs of a human require information about the activities being performed by the human. At the core of this capability is activity recognition, which is a challenging and well-researched problem. Activity recognition algorithms require substantial amounts of labeled training data yet need to perform well under very diverse circumstances. As a result, researchers have been designing methods to identify and utilize subtle connections between activity recognition datasets, or to perform transfer-based activity recognition. In this paper, we survey the literature to highlight recent advances in transfer learning for activity recognition. We characterize existing approaches to transfer-based activity recognition by sensor modality, by differences between source and target environments, by data availability, and by type of information that is transferred. Finally, we present some grand challenges for the community to consider as this field is further developed.", "Supervised transfer learning  Cross-task classification  Noise handling Transfer learning is a widely investigated learning paradigm that is initially proposed to reuse informative knowledge from related domains, as supervised information in the target domain is scarce while it is sufficiently available in the multiple source domains. One of the challenging issues in transfer learning is how to handle the distribution differences between the source domains and the target domain. Most studies in the research field implicitly assume that data distributions from the source domains and the target domain are similar in a well-designed feature space. However, it is often the case that label assignments for data in the source domains and the target domain are significantly different. Therefore, in reality even if the distribution difference between a source domain and a target domain is reduced, the knowledge from multiple source domains is not well transferred to the target domain unless the label information is carefully considered. In addition, noisy data often emerge in real world applications. Therefore, considering how to handle noisy data in the transfer learning setting is a challenging problem, as noisy data inevitably cause a side effect during the knowledge transfer. Due to the above reasons, in this paper, we are motivated to propose a robust framework against noise in the transfer learning setting. We also explicitly consider the difference in data distributions and label assignments among multiple source domains and the target domain. Experimental results on one synthetic data set, three UCI data sets and one real world text data set in different noise levels demonstrate the effectiveness of our method.", "Fuzzy modeling  fuzzy systems  knowledge leverage  Mamdani-Larsen fuzzy model  missing data  reduced set density estimator (RSDE)  transfer learning The classical fuzzy system modeling methods only consider the current scene where the training data are assumed fully collectable. However, if the available data from that scene are insufficient, the fuzzy systems trained will suffer from weak generalization for the modeling task in this scene. In order to overcome this problem, a fuzzy system with knowledge-leverage capability, which is known as a knowledge-leverage-based fuzzy system (KL-FS), is proposed in this paper. The KL-FS not only makes full use of the data from the current scene in the learning procedure but can effectively make leverage on the existing knowledge from the reference scene, e. g., the parameters of a fuzzy system obtained from a reference scene, as well. Specifically, a knowledge-leverage-based Mamdani-Larsen-type fuzzy system (KL-ML-FS) is proposed by using the reduced set density estimation technique integrating with the corresponding knowledge-leverage mechanism. The new fuzzy system modeling technique has been verified by experiments on synthetic and real-world datasets, where KL-ML-FS has better performance and adaptability than the traditional fuzzy modeling methods in scenarios with insufficient data.", "Fuzzy modeling  fuzzy systems (FS)  knowledge leverage (KL)  missing data  transfer learning Classical fuzzy system modeling methods consider only the current scene where the training data are assumed to be fully collectable. However, if the data available from the current scene are insufficient, the fuzzy systems trained by using the incomplete datasets will suffer from weak generalization capability for the prediction in the scene. In order to overcome this problem, a knowledge-leverage-based fuzzy system (KL-FS) is studied in this paper from the perspective of transfer learning. The KL-FS intends to not only make full use of the data from the current scene in the learning procedure, but also effectively leverage the existing knowledge from the reference scenes. Specifically, a knowledge-leverage-based Takagi-Sugeno-Kang-type Fuzzy System (KL-TSK-FS) is proposed by integrating the corresponding knowledge-leverage mechanism. The new fuzzy system modeling technique is evaluated through experiments on synthetic and real-world datasets. The results demonstrate that KL-TSK-FS has better performance and adaptability than the traditional fuzzy modeling methods in scenes with insufficient data.", "Transfer learning  Non-negative matrix factorization  Sparse regularization  Graph regularization In real-world applications, we often have to deal with some high-dimensional, sparse, noisy, and non-independent identically distributed data. In this paper, we aim to handle this kind of complex data in a transfer learning framework, and propose a robust non-negative matrix factorization via joint sparse and graph regularization model for transfer learning. First, we employ robust non-negative matrix factorization via sparse regularization model (RSNMF) to handle source domain data and then learn a meaningful matrix, which contains much common information between source domain and target domain data. Second, we treat this learned matrix as a bridge and transfer it to target domain. Target domain data are reconstructed by our robust non-negative matrix factorization via joint sparse and graph regularization model (RSGNMF). Third, we employ feature selection technique on new sparse represented target data. Fourth, we provide novel efficient iterative algorithms for RSNMF model and RSGNMF model and also give rigorous convergence and correctness analysis separately. Finally, experimental results on both text and image data sets demonstrate that our REGTL model outperforms existing start-of-art methods.", "Time series  cascade classification  transfer learning  active learning  automatic classification  land-cover maps  remote sensing Image classification usually requires the availability of reliable reference data collected for the considered image to train supervised classifiers. Unfortunately when time series of images are considered, this is seldom possible because of the costs associated with reference data collection. In most of the applications it is realistic to have reference data available for one or few images of a time series acquired on the area of interest. In this paper, we present a novel system for automatically classifying image time series that takes advantage of image(s) with an associated reference information (i.e., the source domain) to classify image(s) for which reference information is not available (i.e., the target domain). The proposed system exploits the already available knowledge on the source domain and, when possible, integrates it with a minimum amount of new labeled data for the target domain. In addition, it is able to handle possible significant differences between statistical distributions of the source and target domains. Here, the method is presented in the context of classification of remote sensing image time series, where ground reference data collection is a highly critical and demanding task. Experimental results show the effectiveness of the proposed technique. The method can work on multimodal (e.g., multispectral) images.", "Transfer learning  Instance-based transfer learning  Source data selection  Bagging Instance-based transfer is an important paradigm for transfer learning, where data from related tasks (source data) are combined with the data for the current learning task (target data) to train a learner for the current (target) task. However, in most application scenarios, the benefit of the source data is unclear. The source may contain both helpful and harmful instances to the target learning. Simply combining the source with the target data may result in performance deterioration (negative transfer). Selecting the instances from the source data that will benefit the target task is a key step for instance-based transfer learning. Most existing instance-based transfer methods lack such selection or mix source selection with the training for the target task. This leads to problems as the training may use source data harmful to the target. We propose a simple yet effective method for instance-based transfer learning in environments where the usefulness of the sources are unclear. The method employs a double-selection process, based on bootstrapping, to reduce the impact of irrelevant/harmful data in the source. Experiment results show that in most cases, our method produces more improvements through transfer than TrBagg (Kamishima et al., 2009) and TrAdaBoost (Dai et al., 2009). Our method can also deal with a wider range of transfer learning scenarios. (C) 2013 Elsevier B.V. All rights reserved.", "Transfer learning  Latent Dirichlet allocation  Nonparametric Bayesian inference  Sparsity  Small sample size  Topic models In many domains data items are represented by vectors of counts  count data arises, for example, in bioinformatics or analysis of text documents represented as word count vectors. However, often the amount of data available from an interesting data source is too small to model the data source well. When several data sets are available from related sources, exploiting their similarities by transfer learning can improve the resulting models compared to modeling sources independently. We introduce a Bayesian generative transfer learning model which represents similarity across document collections by sparse sharing of latent topics controlled by an Indian buffet process. Unlike a prominent previous model, hierarchical Dirichlet process (HDP) based multi-task learning, our model decouples topic sharing probability from topic strength, making sharing of low-strength topics easier. In experiments, our model outperforms the HDP approach both on synthetic data and in first of the two case studies on text collections, and achieves similar performance as the HDP approach in the second case study. (C) 2013 Elsevier B.V. All rights reserved.", "Reinforcement learning  Linked multicomponent robotic systems  Transfer learning  Hose transportation Transfer learning is a hierarchical approach to reinforcement learning of complex tasks modeled as Markov Decision Processes. The learning results on the source task are used as the starting point for the learning on the target task. In this paper we deal with a hierarchy of constrained systems, where the source task is an under-constrained system, hence called the Partially Constrained Model (PCM). Constraints in the framework of reinforcement learning are dealt with by state-action veto policies. We propose a theoretical background for the hierarchy of training refinements, showing that the effective action repertoires learnt on the PCM are maximal, and that the PCM-optimal policy gives maximal state value functions. We apply the approach to learn the control of Linked Multicomponent Robotic Systems using Reinforcement Learning. The paradigmatic example is the transportation of a hose. The system has strong physical constraints and a large state space. Learning experiments in the target task are realized over an accurate but computationally expensive simulation of the hose dynamics. The PCM is obtained simplifying the hose model. Learning results of the PCM Transfer Learning show an spectacular improvement over conventional Q-learning on the target task. (C) 2012 Elsevier B.V. All rights reserved.", "Inductive transfer learning  Minimum description length principle  Feature-based transfer Transfer learning provides a solution in real applications of how to learn a target task where a large amount of auxiliary data from source domains are given. Despite numerous research studies on this topic, few of them have a solid theoretical framework and are parameter-free. In this paper, we propose an Extended Minimum Description Length Principle (EMDLP) for feature-based inductive transfer learning, in which both the source and the target data sets contain class labels and relevant features are transferred from the source domain to the target one. Unlike conventional methods, our encoding measure is based on a theoretical background and has no parameter. To obtain useful features to be used in the target task, we design an enhanced encoding length by adopting a code book that stores useful information obtained from the source task. With the code book that builds connections between the source and the target tasks, our EMDLP is able to evaluate the inferiority of the results of transfer learning with the add sum of the code lengths of five components: those of the corresponding two hypotheses, the two data sets with the help of the hypotheses, and the set of the transferred features. The proposed method inherits the nice property of the MDLP that elaborately evaluates the hypotheses and balances the simplicity of the hypotheses and the goodness-of-the-fit to the data. Extensive experiments using both synthetic and real data sets show that the proposed method provides a better performance in terms of the classification accuracy and is robust against noise.", "Reinforcement Learning  Transfer learning  Model-based Reinforcement Learning The main objective of transfer learning is to reuse knowledge acquired in a previous learned task, in order to enhance the learning procedure in a new and more complex task. Transfer learning comprises a suitable solution for speeding up the learning procedure in Reinforcement Learning tasks. This work proposes a novel method for transferring models to Reinforcement Learning agents. The models of the transition and reward functions of a source task, will be transferred to a relevant but different target task. The learning algorithm of the target task's agent takes a hybrid approach, implementing both model-free and model-based learning, in order to fully exploit the presence of a source task model. Moreover, a novel method is proposed for transferring models of potential-based reward shaping functions. The empirical evaluation, of the proposed approaches, demonstrated significant results and performance improvements in the 3D Mountain Car and Server Job Scheduling tasks, by successfully using the models generated from their corresponding source tasks. (C) 2012 Elsevier B.V. All rights reserved.", " Divergence estimators based on direct approximation of density ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high fluctuation, divergence estimation is a challenging task in practice. In this letter, we use relative divergences for distribution comparison, which involves approximation of relative density ratios. Since relative density ratios are always smoother than corresponding ordinary density ratios, our proposed method is favorable in terms of nonparametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overfits even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach.", "Transfer learning  Collaborative filtering  Missing ratings A major challenge for collaborative filtering (CF) techniques in recommender systems is the data sparsity that is caused by missing and noisy ratings. This problem is even more serious for CF domains where the ratings are expressed numerically, e.g. as 5-star grades. We assume the 5-star ratings are unordered bins instead of ordinal relative preferences. We observe that, while we may lack the information in numerical ratings, we sometimes have additional auxiliary data in the form of binary ratings. This is especially true given that users can easily express themselves with their preferences expressed as likes or dislikes for items. In this paper, we explore how to use these binary auxiliary preference data to help reduce the impact of data sparsity for CF domains expressed in numerical ratings. We solve this problem by transferring the rating knowledge from some auxiliary data source in binary form (that is, likes or dislikes), to a target numerical rating matrix. In particular, our solution is to model both the numerical ratings and ratings expressed as like or dislike in a principled way. We present a novel framework of Transfer by Collective Factorization (TCF), in which we construct a shared latent space collectively and learn the data-dependent effect separately. A major advantage of the TCF approach over the previous bilinear method of collective matrix factorization is that we are able to capture the data-dependent effect when sharing the data-independent knowledge. This allows us to increase the overall quality of knowledge transfer. We present extensive experimental results to demonstrate the effectiveness of TCF at various sparsity levels, and show improvements of our approach as compared to several state-of-the-art methods. (C) 2013 Elsevier B.V. All rights reserved.", "Feature generation  heterogeneous data  transfer learning In many applications, it is very expensive or time consuming to obtain a lot of labeled examples. One practically important problem is: can the labeled data from other related sources help predict the target task, even if they have 1) different feature spaces (e. g., image versus text data), 2) different data distributions, and 3) different output spaces? This paper proposes a solution and discusses the conditions where this is highly likely to produce better results. It first unifies the feature spaces of the target and source data sets by spectral embedding, even when they are with completely different feature spaces. The principle is to devise an optimization objective that preserves the original structure of the data, while at the same time, maximizes the similarity between the two. A linear projection model, as well as a nonlinear approach are derived on the basis of this principle with closed forms. Second, a judicious sample selection strategy is applied to select only those related source examples. At last, a Bayesian-based approach is applied to model the relationship between different output spaces. The three steps can bridge related heterogeneous sources in order to learn the target task. Among the 20 experiment data sets, for example, the images with wavelet-transformed-based features are used to predict another set of images whose features are constructed from color-histogram space  documents are used to help image classification, etc. By using these extracted examples from heterogeneous sources, the models can reduce the error rate by as much as 50 percent, compared with the methods using only the examples from the target task.", "Imitation learning  Inverse reinforcement learning  Transfer learning  Bootstrapping We consider the problem of imitation learning when the examples, provided by an expert human, are scarce. Apprenticeship learning via inverse reinforcement learning provides an efficient tool for generalizing the examples, based on the assumption that the expert's policy maximizes a value function, which is a linear combination of state and action features. Most apprenticeship learning algorithms use only simple empirical averages of the features in the demonstrations as a statistics of the expert's policy. However, this method is efficient only when the number of examples is sufficiently large to cover most of the states, or the dynamics of the system is nearly deterministic. In this paper, we show that the quality of the learned policies is sensitive to the error in estimating the averages of the features when the dynamics of the system is stochastic. To reduce this error, we introduce two new approaches for bootstrapping the demonstrations by assuming that the expert is near-optimal and the dynamics of the system is known. In the first approach, the expert's examples are used to learn a reward function and to generate furthermore examples from the corresponding optimal policy. The second approach uses a transfer technique, known as graph homomorphism, in order to generalize the expert's actions to unvisited regions of the state space. Empirical results on simulated robot navigation problems show that our approach is able to learn sufficiently good policies from a significantly small number of examples. (C) 2012 Elsevier B.V. All rights reserved.", "Transfer learning  Multi-task learning  Active learning  Statistical learning theory  Bayesian learning  Sample complexity We explore a transfer learning setting, in which a finite sequence of target concepts are sampled independently with an unknown distribution from a known family. We study the total number of labeled examples required to learn all targets to an arbitrary specified expected accuracy, focusing on the asymptotics in the number of tasks and the desired accuracy. Our primary interest is formally understanding the fundamental benefits of transfer learning, compared to learning each target independently from the others. Our approach to the transfer problem is general, in the sense that it can be used with a variety of learning protocols. As a particularly interesting application, we study in detail the benefits of transfer for self-verifying active learning  in this setting, we find that the number of labeled examples required for learning with transfer is often significantly smaller than that required for learning each target independently.", "Pedestrian detection  Scene change  Manifold learning  Transfer learning Most of the existing methods for pedestrian detection work well, only when the following assumption is satisfied: the features extracted from the training dataset and the testing dataset have very similar distributions in the feature space. However, in practice, this assumption does not hold because of the scene complexity and variation. In this paper, a new method is proposed for detecting pedestrians in various scenes based on the transfer learning technique. Our proposed method employs the following two strategies for improving the pedestrian detection performance. First, a new sample screening method based on manifold learning is proposed. The basic idea is to choose samples from the training set, which may be similar to the samples from the unseen scene, and then merge the selected samples into the unseen set. Second, a new classification model based on transfer learning is proposed. The advantage of the classification model is that only a small number of samples need to be used from the unseen scenes. Most of the training samples are still obtained from the training scene, which take up to 90% of the entire training samples. Compared to the traditional pedestrian detection methods, the proposed algorithm can adapt to different scenes for detecting pedestrians. Experiments on two pedestrian detection benchmark datasets, DC and NICTA, showed that the method can obtain better performance as compared to other previous methods. (C) 2012 Elsevier B.V. All rights reserved.", "transfer learning  behavioural genetics  artificial neural networks  heritability  genetic algorithms We explore the use of Artificial Neural Networks (ANNs) as computational models capable of sharing, retaining and reusing knowledge when they are combined via Behavioural Genetic principles. In behavioural genetics, the performance and the variability in performance (in case of population studies) stems from structure (intrinsic factors or genes) and environment (training dataset). We simulate the effects of genetic influences via variations in the neuro-computational parameters of the ANNs, and the effects of environmental influences via a filter applied to the training set. Our approach uses the twin method to disentangle genetic and environmental influences on performance, capturing transfer effects via changes to the heritability measure. Our model captures the wide range of variability exhibited by population members as they are trained on five different tasks. Preliminary experiments produced encouraging results as to the utility of this method. Results provide a foundation for future work in using a computational framework to capture population-level variability, optimising performance on multiple tasks, and establishing a relationship between selective pressure on cognitive skills and the change in the heritability of these skills across generations.", " Detecting text in scene images is very challenging due to complex backgrounds, various fonts and different illumination conditions. Without prior knowledge, a detector previously trained using lots of samples still perform badly on a test image because of the disparities in distributions between the training samples and the testing ones. In this paper, we propose to adapt a pre-trained generic scene text detector towards new scenes by transfer learning. In particular, we choose cascade Adaboost as the detector style and try to re-weight pre-selected features according to their abilities to classify high confidence samples. The proposed adaptation mechanism has been evaluated on ICDAR 2011 scene text detection competition dataset and the encouraging experiments results can be compared with the latest published algorithms.", "maximum mean discrepancy embedding  local patches based maximum mean discrepancy  local domain adaptation feature extraction method In this paper, we propose a novel measure: Local Patches Based Maximum Mean Discrepancy (LPMMD). Based on the above measure, we also propose a novel feature extraction method: A Local Domain Adaptation Feature Extraction Method (LDAFE), which not only fulfills the transfer learning task, but also has a certain local learning capability. The LDAFE can complete traditional feature extraction as well as domain adaptation learning in two domains whose distributions are different but relative, thus indicating its better robustness and adaptation. Tests show the above-proposed advantages of the LPMMD criterion and the LDAFE method.", "object recognition  remote sensing  image processing  machine learning The deviation of an object's real data distribution from the known training data distribution would lead to low reliability of object recognition. To tackle this problem for Remote Sensing (RS) images, we propose a novel object recognition method based on transfer learning. The feature vectors of an object are first extracted by a joint Local Binary Pattern (LBP). The transfer learning is then employed to find the common parameter set among feature spaces of the object under different distributions. Through extensive experiments, it has been shown that a significant improvement on the accuracy is has been brought by the proposed novel method.", " In the perspective of life long learning, a robot may face different, but related situations. Being able to exploit the knowledge acquired during a first learning phase may be critical in order to solve more complex tasks. This is the transfer learning problem. This problem is addressed here in the case of direct policy search algorithms. No discrete states, nor actions are defined a priori. A policy is described by a controller that computes orders to be sent to the motors out of sensor values. Both motor and sensor values can be continuous. The proposed approach relies on population based direct policy search algorithms, i.e. evolutionary algorithms. It exploits the numerous behaviors that are generated during the search. When learning on the source task, a knowledge base is built. The knowledge base aims at identifying the most salient behaviors segments with regards to the considered task. Afterwards, the knowledge base is exploited on a target task, with a reward shaping approach: besides its reward on the task, a policy is credited with a reward computed from the knowledge base. The rationale behind this approach is to automatically detect the stepping stones, i. e. the behavior segments that have lead to a reward in the source task before the policy is efficient enough to get the reward on the target task. The approach is tested in simulation with a neuroevolution approach and on ball collecting tasks.", "Region classification  Conformal framework  Transfer learning This paper proposes to consider the region classification task in the context of instance-transfer learning. The proposed solution consists of the conformal algorithm that employs a nonconformity function learned by the Transfer AdaBoost algorithm. The experiments showed that our approach results in valid class regions. In addition the conditions when instance transfer can improve learning are empirically derived.", " In the detection of human from image using statistical learning methods, the labor cost of collecting training samples and the time cost for retraining to match the target scene are major issues. One method to reduce the work involved in sample collection is transfer learning based on boosting. However, if there is a large change between the auxiliary scene and target scene, it is difficult to apply the transfer learning. We therefore propose a hybrid transfer learning method in which two feature spaces are prepared, one of feature obtained by transfer and another of full feature search that is the same as retraining. The feature space is selectively switched on the basis of the defined training efficiency. The proposed method improving accuracy up to 8.35% compared to conventional transfer learning while accelerating training time by 3.2 times faster compared to retraining.", "speech emotion recognition  transfer learning  sparse autoencoder  deep neural networks In speech emotion recognition, training and test data used for system development usually tend to fit each other perfectly, but further 'similar' data may be available. Transfer learning helps to exploit such similar data for training despite the inherent dissimilarities in order to boost a recogniser's performance. In this context, this paper presents a sparse autoencoder method for feature transfer learning for speech emotion recognition. In our proposed method, a common emotion-specific mapping rule is learnt from a small set of labelled data in a target domain. Then, newly reconstructed data are obtained by applying this rule on the emotion-specific data in a different domain. The experimental results evaluated on six standard databases show that our approach significantly improves the performance relative to learning each source domain independently.", "P2P  transfer learning  traffic flow identification With the rapid development of Internet, a large number of peer networks (Peer-to-Peer) applications rise and are widely used. Because of this, it is more difficult for network operators to manage and monitor their networks in a proper way. To identify the peer networks applications generating the traffic traveling through networks is necessary and if we can identify them sooner, we control them better. In this work, we use the machine learning-based classification method to identify the classes of the flows. According to previous work, we choose transfer learning algorithm to classify the traffic, and improve classified results. Finally we compare and evaluate the classification results in terms of the two metrics such as true positive ratio and time expense. Our experiments show that the machine learning algorithm is an efficient algorithm for traffic identification and is able to build a quick identification system.", "Network  Transfer learning This paper addresses the problem of transferring useful knowledge from a source network to predict node labels in a newly formed target network. While existing transfer learning research has primarily focused on vector-based data, in which the instances are assumed to be independent and identically distributed, how to effectively transfer knowledge across different information networks has not been well studied, mainly because networks may have their distinct node features and link relationships between nodes. In this paper, we propose a new transfer learning algorithm that attempts to transfer common latent structure features across the source and target networks. The proposed algorithm discovers these latent features by constructing label propagation matrices in the source and target networks, and mapping them into a shared latent feature space. The latent features capture common structure patterns shared by two networks, and serve as domain-independent features to be transferred between networks. Together with domain-dependent node features, we thereafter propose an iterative classification algorithm that leverages label correlations to predict node labels in the target network. Experiments on real-world networks demonstrate that our proposed algorithm can successfully achieve knowledge transfer between networks to help improve the accuracy of classifying nodes in the target network.", "multiclass classification  transfer learning  PLSA A new transfer learning method is presented in this paper, addressing a particularly hard transfer learning problem: the case where the target domain shares only a subset of its classes with the source domain and only unlabeled data are provided for the target domain. This is a situation that occurs frequently in real-world applications, such as the multiclass document classification problems that motivated our work. The proposed approach is a transfer learning variant of the Probabilistic Latent Semantic Analysis (PLSA) model [1] that we name TL-PLSA. Unlike most approaches in the literature, TL-PLSA captures both the difference of the domains and the commonalities of the class sets, given no labelled data from the target domain. We perform experiments over three different datasets and show the difficulty of the task, as well as the promising results that we obtained with the new method.", " Bayesian network structure learning algorithms with limited data are being used in domains such as systems biology and neuroscience to gain insight into the underlying processes that produce observed data. Learning reliable networks from limited data is difficult, therefore transfer learning can improve the robustness of learned networks by leveraging data from related tasks. Existing transfer learning algorithms for Bayesian network structure learning give a single maximum a posteriori estimate of network models. Yet, many other models may be equally likely, and so a more informative result is provided by Bayesian structure discovery. Bayesian structure discovery algorithms estimate posterior probabilities of structural features, such as edges. We present transfer learning for Bayesian structure discovery which allows us to explore the shared and unique structural features among related tasks. Efficient computation requires that our transfer learning objective factors into local calculations, which we prove is given by a broad class of transfer biases. Theoretically, we show the efficiency of our approach. Empirically, we show that compared to single task learning, transfer learning is better able to positively identify true edges. We apply the method to whole-brain neuroimaging data.", " Active learning, transfer learning, and related techniques are unified by a core theme: efficient and effective use of available data. Active learning offers scalable solutions for building effective supervised learning models while minimizing annotation effort. Transfer learning utilizes existing labeled data from one task to help learn related tasks for which limited labeled data are available. There has been limited research, however, on how to combine these two techniques. In this paper, we present a simple and principled transfer active learning framework that leverages pre-existing labeled data from related tasks to improve the performance of an active learner. We derive an intuitive bound on the generalization error for the classifiers learned by this algorithm that provides insight into the algorithm's behavior and the problem in general. We provide experimental results using several well-known transfer learning data sets that confirm our theoretical analysis. What is more, our results suggest that this approach represents a promising solution to a specific weakness of active learning algorithms: cold starts with zero labeled data.", " DNA copy number variations (CNVs) are prevalent in all types of tumors. It is still a challenge to study how CNVs play a role in driving tumorgenic mechanisms that are either universal or specific in different cancer types. To address the problem, we introduce a transfer learning framework to discover common CNVs shared across different tumor types as well as CNVs specific to each tumor type from genome-wide CNV data measured by arrayCGH and SNP genotyping array. The proposed model, namely Transfer Learning with Fused LASSO (TLFL), detects latent CNV components from multiple CNV datasets of different tumor types to distinguish the CNVs that are common across the datasets and those that are specific in each dataset. Both the common and type-specific CNVs are detected as latent components in matrix factorization coupled with fused LASSO on adjacent CNV probe features. TLFL considers the common latent components underlying the multiple datasets to transfer knowledge across different tumor types. In simulations and experiments on real cancer CNV datasets, TLFL detected better latent components that can be used as features to improve classification of patient samples in each individual dataset compared with the model without the knowledge transfer. In cross-dataset analysis on bladder cancer and cross-domain analysis on breast cancer and ovarian cancer, TLFL also learned latent CNV components that are both predictive of tumor stages and correlate with known cancer genes.", " On-the-fly learning systems are necessary for the deployment of general purpose robots. New training examples for such systems are often supplied by mentor interactions. Due to the cost of acquiring such examples, it is desirable to reduce the number of necessary interactions. Transfer learning has been shown to improve classification results for classes with small numbers of training examples by pooling knowledge from related classes. Standard practice in these works is to assume that the relationship between the transfer target and related classes is already known. In this work, we explore how previously learned categories, or related groupings of classes, can be used to transfer knowledge to novel classes without explicitly known relationships to them. We demonstrate an algorithm for determining the category membership of a novel class, focusing on the difficult case when few training examples are available. We show that classifiers trained via this method outperform classifiers optimized to learn the novel class individually when evaluated on both synthetic and real-world datasets.", " Many applications require autonomous agents to achieve quick responses to task instances drawn from a rich family of qualitatively-related tasks. We address the setting where the tasks share a state-action space and have the same qualitative objective but differ in dynamics. We adopt a transfer learning approach where common structure in previously-learnt policies, in the form of shared subtasks, is exploited to accelerate learning in subsequent ones. We use a probabilistic mixture model to describe regions in state space which are common to successful trajectories in different instances. Then, we extract policy fragments from previously-learnt policies that are specialised to these regions. These policy fragments are options, whose initiation and termination sets are automatically extracted from data by the mixture model. In novel task instances, these options are used in an SMDP learning process and option learning repeats over the resulting policy library. The utility of this method is demonstrated through experiments in a standard navigation environment and then in the RoboCup simulated soccer domain with opponent teams of different skill.", " As researchers are striving for developing robotic systems able to move into the 'the wild', the interest towards novel learning paradigms for domain adaptation has increased. In the specific application of semantic place recognition from cameras, supervised learning algorithms are typically adopted. However, once learning has been performed, if the robot is moved to another location, the acquired knowledge may be not useful, as the novel scenario can be very different from the old one. The obvious solution would be to retrain the model updating the robot internal representation of the environment. Unfortunately this procedure involves a very time consuming data-labeling effort at the human side. To avoid these issues, in this paper we propose a novel transfer learning approach for place categorization from visual cues. With our method the robot is able to decide automatically if and how much its internal knowledge is useful in the novel scenario. Differently from previous approaches, we consider the situation where the old and the novel scenario may differ significantly (not only the visual room appearance changes but also different room categories are present). Importantly, our approach does not require labeling from a human operator. We also propose a strategy for improving the performance of the proposed method by fusing two complementary visual cues. Our extensive experimental evaluation demonstrates the advantages of our approach on several sequences from publicly available datasets.", " Learning about activities and object affordances from human demonstration are important cognitive capabilities for robots functioning in human environments, for example, being able to classify objects and knowing how to grasp them for different tasks. To achieve such capabilities, we propose a Labeled Multi-modal Latent Dirichlet Allocation (LM-LDA), which is a generative classifier trained with two different data cues, for instance, one cue can be traditional visual observation and another cue can be contextual information. The novel aspects of the LM-LDA classifier, compared to other methods for encoding contextual information are that, I) even with only one of the cues present at execution time, the classification will be better than single cue classification since cue correlations are encoded in the model, II) one of the cues (e. g., common grasps for the observed object class) can be inferred from the other cue (e. g., the appearance of the observed object). This makes the method suitable for robot online and transfer learning  a capability highly desirable in cognitive robotic applications. Our experiments show a clear improvement for classification and a reasonable inference of the missing data.", " Since the seminal work of Thrun [17], the learning to learn paradigm has been defined as the ability of an agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks of learning over multiple tasks, there is a need for multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Extensive experiments on subsets of publicly available datasets prove the effectiveness of our approach.", " Over the last years, link discovery frameworks have been employed successfully to create links between knowledge bases. Consequently, repositories of high-quality link specifications have been created and made available on the Web. The basic question underlying this work is the following: Can the specifications in these repositories be reused to ease the detection of link specifications between unlinked knowledge bases? In this paper, we address this question by presenting a formal transfer learning framework that allows detecting existing specifications that can be used as templates for specifying links between previously unlinked knowledge bases. We discuss both the advantages and the limitations of such an approach for determining link specifications. We evaluate our approach on a variety of link specifications from several domains and show that the detection of accurate link specifications for use as templates can be achieved with high reliability.", "Human Action Recognition  Transfer Learning  Ubiquitous Computing  Computer Vision  Video Analysis In order to increase the success rate of a human action recognition system trained with limited labelled video sequences, we propose an approach which combines an efficient use of the scarce data and a transfer learning improvement. The efficient use of data is implemented using the Fuzzy Observation Hidden Markov Model so as to outperform the constraints of the classical approaches when training with small datasets. Additionally, we use a transfer learning procedure that takes advantage of the fact that some human body poses are shared among actions and then key poses can be trained from external sources. Thanks to this method we have improved the recognition performance in new action classes introduced in the target domain. In order to confirm the usefulness of the approach we have tested the performance using the IXMAS dataset as target domain and the ViHASi dataset as source domain.", "Transfer Learning  Gaussian Process  MDLP In real applications, labeled instances are often deficient which makes the classification problem on the target task difficult. To solve this problem, transfer learning techniques are introduced to make use of existing knowledge from the source data sets to the target data set. However, due to the discrepancy of distributions between tasks, directly transferring knowledge will possibly lead to degenerated performance which is also called negative trasnfer. In this paper, we adopted the Gaussian process to alleviate this problem by directly evaluating the distribution differences, with the parameter-free Minimum Description Length Principle (MDLP) for encoding. The proposed method inherits the good property of solid theoretical foundation as well as noise-tolerance. Extensive experiments results show the effectiveness of our method.", " Many successful methods for biomedical image segmentation are based on supervised learning, where a segmentation algorithm is trained based on manually labeled training data. For supervised-learning algorithms to perform well, this training data has to be representative for the target data. In practice however, due to differences between scanners such representative training data is often not available. We therefore present a segmentation algorithm in which labeled training data does not necessarily need to be representative for the target data, which allows for the use of training data from different studies than the target data. The algorithm assigns an importance weight to all training images, in such a way that the Kullback-Leibler divergence between the resulting distribution of the training data and the distribution of the target data is minimized. In a set of experiments on MRI brain-tissue segmentation with training and target data from four substantially different studies our method improved mean classification errors with up to 25% compared to common supervised-learning approaches.", " Effective prediction of conversion of mild cognitive impairment (MCI) to Alzheimer's disease (AD) is important for early diagnosis of AD, as well as for evaluating AD risk pre-symptomatically. Different from most traditional methods for MCI conversion prediction, in this paper, we propose a novel sparse multimodal manifold-regularized transfer learning classification ((SMTLC)-T-2) method, which can simultaneously use other related classification tasks (e.g., AD vs. normal controls (NC) classification) and also the unlabeled data for improving the MCI conversion prediction. Our proposed method includes two key components: (1) a criterion based on the maximum mean discrepancy (MMD) for eliminating the negative effect related to the distribution differences between the auxiliary (i.e., AD/NC) and the target (i.e., MCI converters/MCI non-converters) domains, and (2) a sparse semi-supervised manifold-regularized least squares classification method for utilization of unlabeled data. Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database show that the proposed method can significantly improve the classification performance between MCI converters and MCI non-converters, compared with the state-of-the-art methods.", "face recognition  machine learning  biometrics  image processing Biometrics recognition aims to identify and predict new personal identities based on their existing knowledge. As the use of multiple biometric traits of the individual may enables more information to be used for recognition, it has been proved that multi-biometrics can produce higher accuracy than single biometrics. However, a common problem with traditional machine learning is that the training and test data should be in the same feature space, and have the same underlying distribution. If the distributions and features are different between training and future data, the model performance often drops. In this paper, we propose a transfer learning method for face recognition on bimodal biometrics. The training and test samples of bimodal biometric images are composed of the visible light face images and the infrared face images. Our algorithm transfers the knowledge across feature spaces, relaxing the assumption of same feature space as well as same underlying distribution by automatically learning a mapping between two different but somewhat similar face images. According to the experiments in the face images, the results show that the accuracy of face recognition has been greatly improved by the proposed method compared with the other previous methods. It demonstrates the effectiveness and robustness of our method.", " We study the use of domain adaptation and transfer learning techniques as part of a framework for adaptive object detection. Unlike recent applications of domain adaptation work in computer vision, which generally focus on image classification, we explore the problem of extreme class imbalance present when performing domain adaptation for object detection. The main difficulty caused by this imbalance is that test images contain millions or billions of negative image subwindows but just a few image subwindows containing positive instances, which makes it difficult to adapt to changes in the positive classes present new domains by simple techniques such as random sampling. We propose an initial approach to addressing this problem and apply our technique to vehicle detection in a challenging urban surveillance dataset, demonstrating the performance of our approach with various amounts of supervision, including the fully unsupervised case.", " In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.", "Nonnegative shared subspace learning  Transfer learning  Auxiliary sources  Multi-task clustering Joint modeling of related data sources has the potential to improve various data mining tasks such as transfer learning, multitask clustering, information retrieval etc. However, diversity among various data sources might outweigh the advantages of the joint modeling, and thus may result in performance degradations. To this end, we propose a regularized shared subspace learning framework, which can exploit the mutual strengths of related data sources while being immune to the effects of the variabilities of each source. This is achieved by further imposing a mutual orthogonality constraint on the constituent subspaces which segregates the common patterns from the source specific patterns, and thus, avoids performance degradations. Our approach is rooted in nonnegative matrix factorization and extends it further to enable joint analysis of related data sources. Experiments performed using three real world data sets for both retrieval and clustering applications demonstrate the benefits of regularization and validate the effectiveness of the model. Our proposed solution provides a formal framework appropriate for jointly analyzing related data sources and therefore, it is applicable to a wider context in data mining.", "speaker verification  text-dependent  transfer learning Recently we have investigated the use of state-of-the-art text independent and text-dependent speaker verification algorithms for a text-dependent user authentication task and obtained satisfactory results mainly by using a fair amount of text-dependent development data. In our study, best results were obtained using the NAP framework rather than using the more advanced JFA and i-vector-based frameworks. In this work we investigate the ability to build high accuracy i-vector-based systems by leveraging widely available conversational data. We explore various techniques for transforming conversational sessions in such a way that attributes which are more relevant to the text-dependent task are enhanced. Using these techniques we managed to reduce verification error significantly.", " In this paper we investigate the use of the Transfer Learning (TL) framework to extract the commonalities across a set of subjects and also to learn the way each individual instantiates these commonalities to model idiosyncrasy. To implement this we apply three variants of Multi Task Learning, namely: Regularized Multi Task Learning (RMTL), Multi Task Feature Learning (MTFL) and Composite Multi Task Feature Learning (CMTFL). Two datasets are used  the first is a set of point based facial expressions with annotated discrete levels of pain. The second consists of full body motion capture data taken from subjects diagnosed with chronic lower back pain. A synchronized electromyographic signal from the lumbar paraspinal muscles is taken as a pain-related behavioural indicator. We compare our approaches with Ridge Regression which is a comparable model without the Transfer Learning property  as well as with a subtractive method for removing idiosyncrasy. The TL based methods show statistically significant improvements in correlation coefficients between predicted model outcomes and the target values compared to baseline models. In particular RMTL consistently outperforms all other methods  a paired t-test between RMTL and the best performing baseline method returned a maximum p-value of 2.3 x 10(-4).", " In this paper, we propose a method to improve the results of clustering in a target domain, using significant information from an auxiliary (source) domain dataset. The applicability of this method concerns the field of transfer learning (or domain adaptation), where the performance of a task (say, classification using clustering) in one domain is improved using knowledge obtained from a similar domain. We propose two unsupervised methods of cross-domain clustering and show results on two different categories of benchmark datasets, both having difference in density distributions over the pair of domains. In the first method, we propose an iterative framework, where the clustering in the target domain is influenced by the clusters formed in the source domain and vice-versa. Similarity/dissimilarity measures have been appropriately formulated using Euclidean distance and Bregman Divergence, for cross-domain clustering. In the second method, we perform clustering in the target domain by estimating local density computed using a non-parametric (NP) density estimator (due to less number of samples). Prior to clustering, the NP-density scattering in the target domain is modified using information of cluster density distribution in source domain. Results shown on real-world datasets suggest that the proposed methods of cross-domain clustering are comparable to the recent start-of-the-art work.", "Manifold Alignment  Indoor Localization  Transfer Learning  Hidden Markov Model In received signal strength fingerprints based indoor localization systems, the radio map built by labeled wireless fingerprints is easily outdated over time, while recalibrating the overall radio map is time consuming. To avoid the tedious task, we propose to employ manifold alignment to label the current radio map from outdated radio map, with the constraint of the Hidden Markov Model trained by trajectories of the received signal strength readings. Manifold alignment can align the low-dimensional manifold structures of two different data sets and transfer knowledge across them. Transition matrix generated by Hidden Markov Model provides addition local weights to the alignment process. The proposed algorithms are tested in a real world ZigBee environment. Experiment results show that our method outperforms state-of-the-art transfer learning algorithms.", " Transfer learning is established as an effective technology in computer vision for leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robust for substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.", " Data sparsity has been a thorny issue for manifold-based image synthesis, and in this paper we address this critical problem by leveraging ideas from transfer learning. Specifically, we propose methods based on generating auxiliary data in the form of synthetic samples using transformations of the original sparse samples. To incorporate the auxiliary data, we propose a weighted data synthesis method, which adaptively selects from the generated samples for inclusion during the manifold learning process via a weighted iterative algorithm. To demonstrate the feasibility of the proposed method, we apply it to the problem of face image synthesis from sparse samples. Compared with existing methods, the proposed method shows encouraging results with good performance improvements.", " Regression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives: (1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively. (2) Rather than learning from only labelled data, the abundant unlabelled data are exploited. (3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.", " Face verification involves determining whether a pair of facial images belongs to the same or different subjects. This problem can prove to be quite challenging in many important applications where labeled training data is scarce, e.g., family album photo organization software. Herein we propose a principled transfer learning approach for merging plentiful source-domain data with limited samples from some target domain of interest to create a classifier that ideally performs nearly as well as if rich target-domain data were present. Based upon a surprisingly simple generative Bayesian model, our approach combines a KL-divergence-based regularizer/prior with a robust likelihood function leading to a scalable implementation via the EM algorithm. As justification for our design choices, we later use principles from convex analysis to recast our algorithm as an equivalent structured rank minimization problem leading to a number of interesting insights related to solution structure and feature-transform invariance. These insights help to both explain the effectiveness of our algorithm as well as elucidate a wide variety of related Bayesian approaches. Experimental testing with challenging datasets validate the utility of the proposed algorithm.", " Robot manipulation tasks require on robot models. When exact physical parameters of the robot are not available, learning robot models from data becomes an appealing alternative. Most learning approaches are formulated in a supervised learning framework and are based on clearly defined training sets. We propose a method that improves the learning process by using additional data obtained from other experiments of the robot or even from experiments with different robot architectures. Incorporating experiences from other experiments requires transfer learning that has been used with success in machine learning. The proposed method can be used for arbitrary robot model, together with any type of learning algorithm. Experimental results indicate that task transfer between different robot architectures is a sound concept. Furthermore, clear improvement is gained on forward kinematics model learning in a task- space control task.", " Although existing online tracking algorithms can solve the problems of scene illumination changes, partial or full object occlusions, and pose variation, there are still two weaknesses, inadequacy of training data and drift problem. Considering these, Compressive Tracking algorithm (CT) [1] extracts features from compressed domain, and classified object and background via a naive Bayes classier with online update. To further solve the problems of drift and inadequacy of training data, we introduce transfer learning into CT to take full advantage of prior information and propose a self-traininglike transfer learning algorithm. It selects training samples from samples collection to update classifier by the conduction of the classifier constructed at first frame. Eventually we introduce self-training-like transfer learning algorithm into CT to construct a novel tracking algorithm called Transfer Learning based Compressive Tracking (TLCT). Experimental results on 17 publicly available challenging sequences have shown the effectiveness and robustness of our algorithm.", " Learning Convolutional Neural Networks (CNN) is commonly carried out by plain supervised gradient descent. With sufficient training data, this leads to very competitive results for visual recognition tasks when starting from a random initialization. When the amount of labeled data is limited, CNNs reveal their strong dependence on large amounts of training data. However, recent results have shown that a well chosen optimization starting point can be beneficial for convergence to a good generalizing minimum. This starting point was mostly found using unsupervised feature learning techniques such as sparse coding or transfer learning from related recognition tasks. In this work, we compare these two approaches against a simple patch based initialization scheme and a random initialization of the weights. We show that pre-training helps to train CNNs from few samples and that the correct choice of the initialization scheme can push the network's performance by up to 41% compared to random initialization.", " Traditional machine learning works well under the assumption that the training data and test data are in the same distribution. However, in many real-world applications, this assumption does not hold. The research of knowledge transfer has received considerable interest recently in Natural Language Processing to improve the domain adaptation of machine learning. In this paper, we present a novel transfer learning framework called TPTSVM (Transfer Progressive Transductive Support Vector Machine), which combines transfer learning and semi-supervised learning. TPTSVM makes use of the limited labeled data in target domain to leverage a large amount of labeled data in source domain and queries the most confident instances in target domain. Experiments on two data sets show that TPTSVM algorithm always improves the classification performance compared to other state-of-the-art transfer learning approaches or semi-supervised approaches. Furthermore, our algorithm could be extended to multiple source domains easily.", "gait complexity  non-stationary process  transfer learning This paper proposes a probabilistic method for measuring and characterizing the complexity of gait samples represented by the well-known Gait Energy Image (GEI). The term complexity is here used to denote how singular (far from normality) a GEI is and how challenging might be its classification. We believe that a robust complexity index computed on a new gait sample could help to judge the reliability of the prediction given by a classifier. Experiments to assess the validity of the proposed measure and how it correlates with gait recognition effectiveness were conducted on two public databases covering both indoor and outdoor scenarios and variable covariate conditions.", " Transportation mode recognition plays an important role in discovering life patterns from people's physical behavior. Learning knowledge from mobile sensing data enables transportation mode recognition on mobile phone. However, existing transportation mode recognition methods are mostly based on fixed recognition models, which do not consider the diversities in different users and their transportation context. In this paper, an online sequential extreme learning machine based transfer learning method (TransELM) is proposed to recognize various transportation modes. TransELM is mainly comprised of three steps: firstly, an initial ELM classifier is trained on the labeled training data from the source domain  secondly, the mean and standard deviation are calculated as multi-class trustable intervals in source domain, and then the partially trustable samples are effectively extracted from the target domain  thirdly, the trustable samples are integrated, where an incremental OSELM method is employed to update the original ELM classifier. Experimental results show that TransELM obtains higher accuracy than the traditional ELM classifier in real world transportation mode recognition problems.", " The objective of this work is to estimate upper body pose for signers in TV broadcasts. Given suitable training data, the pose is estimated using a random forest body joint detector. However, obtaining such training data can be costly. The novelty of this paper is a method of transfer learning which is able to harness existing training data and use it for new domains. Our contributions are: (i) a method for adapting existing training data to generate new training data by synthesis for signers with different appearances, and (ii) a method for personalising training data. As a case study we show how the appearance of the arms for different clothing, specifically short and long sleeved clothes, can be modelled to obtain person-specific trackers. We demonstrate that the transfer learning and person specific trackers significantly improve pose estimation performance.", "hemophilia  missense mutation  factor IX  SVM There have been reported a variety of defects in the factor IX gene, which is responsible for hemophilia B, and these are summarized in the hemophilia B database. We analyzed amino acid changing mutations, or missense mutations in the database described with factor IX activity values. We have carried out several kinds of theoretical studies to predict the effect of a missense mutations in Factor IX gene. In this paper, we report results of the analysis using Support Vector Machine. We applied the method of transfer learning, which uses the knowledge of some domain to predict the properties of other domains. As a training set, we use mutations of one of seven regions, and test the obtained parameters by the prediction of mutations in remaining regions.", "Object detection  Weakly supervised learning  Transfer learning  Conditional random fields Learning a new object class from cluttered training images is very challenging when the location of object instances is unknown, i.e. in a weakly supervised setting. Many previous works require objects covering a large portion of the images. We present a novel approach that can cope with extensive clutter as well as large scale and appearance variations between object instances. To make this possible we exploit generic knowledge learned beforehand from images of other classes for which location annotation is available. Generic knowledge facilitates learning any new class from weakly supervised images, because it reduces the uncertainty in the location of its object instances. We propose a conditional random field that starts from generic knowledge and then progressively adapts to the new class. Our approach simultaneously localizes object instances while learning an appearance model specific for the class. We demonstrate this on several datasets, including the very challenging Pascal VOC 2007. Furthermore, our method allows training any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations.", "Machine learning  Transfer learning  Domain adaptation  Good similarity functions In this paper, we address the problem of domain adaptation for binary classification. This problem arises when the distributions generating the source learning data and target test data are somewhat different. From a theoretical standpoint, a classifier has better generalization guarantees when the two domain marginal distributions of the input space are close. Classical approaches try mainly to build new projection spaces or to reweight the source data with the objective of moving closer the two distributions. We study an original direction based on a recent framework introduced by Balcan et al. enabling one to learn linear classifiers in an explicit projection space based on a similarity function, not necessarily symmetric nor positive semi-definite. We propose a well-founded general method for learning a low-error classifier on target data, which is effective with the help of an iterative procedure compatible with Balcan et al.'s framework. A reweighting scheme of the similarity function is then introduced in order to move closer the distributions in a new projection space. The hyperparameters and the reweighting quality are controlled by a reverse validation procedure. Our approach is based on a linear programming formulation and shows good adaptation performances with very sparse models. We first consider the challenging unsupervised case where no target label is accessible, which can be helpful when no manual annotation is possible. We also propose a generalization to the semi-supervised case allowing us to consider some few target labels when available. Finally, we evaluate our method on a synthetic problem and on a real image annotation task.", "Statistical generative models  cross-domain learning  distinction and commonality  classification The distribution difference among multiple domains has been exploited for cross-domain text categorization in recent years. Along this line, we show two new observations in this study. First, the data distribution difference is often due to the fact that different domains use different index words to express the same concept. Second, the association between the conceptual feature and the document class can be stable across domains. These two observations actually indicate the distinction and commonality across domains. Inspired by the above observations, we propose a generative statistical model, named Collaborative Dual-PLSA (CD-PLSA), to simultaneously capture both the domain distinction and commonality among multiple domains. Different from Probabilistic Latent Semantic Analysis (PLSA) with only one latent variable, the proposed model has two latent factors y and z, corresponding to word concept and document class, respectively. The shared commonality intertwines with the distinctions over multiple domains, and is also used as the bridge for knowledge transformation. An Expectation Maximization (EM) algorithm is developed to solve the CD-PLSA model, and further its distributed version is exploited to avoid uploading all the raw data to a centralized location and help to mitigate privacy concerns. After the training phase with all the data from multiple domains we propose to refine the immediate outputs using only the corresponding local data. In summary, we propose a two-phase method for cross-domain text classification, the first phase for collaborative training with all the data, and the second step for local refinement. Finally, we conduct extensive experiments over hundreds of classification tasks with multiple source domains and multiple target domains to validate the superiority of the proposed method over existing state-of-the-art methods of supervised and transfer learning. It is noted to mention that as shown by the experimental results CD-PLSA for the collaborative training is more tolerant of distribution differences, and the local refinement also gains significant improvement in terms of classification accuracy.", "Vehicle routing problem  Transfer learning  Genetic algorithms Vehicle routing problem is a transportation optimization problem, in transporting stuffs from, depot(s) to receivers via vehicle(s) having limited capacity. There are a lot of different problem types in VRP literature with different problem parameters. No VRP method uses past solutions to solve current problems more quickly and to find better solution with less computation. Instead, most of the methods in literature evaluate the changes as a new problem and try to solve the new problem using specialized heuristics. We developed a method which uses past vehicle routes to make new routes quickly for frequently changing conditions, and we achieved good performance improvements over classical methods.", "Transfer learning  Subspace learning  Sparse regularization  MMD  Bregman divergence In this paper, we propose a general framework for transfer learning, referred to as transfer sparse subspace learning (TSSL). This framework is suitable for different assumptions on the divergence measures of the data distributions, such as maximum mean discrepancy, Bregman divergence, and K-L divergence. We introduce an effective sparse regularization to the proposed transfer subspace learning framework, which can reduce time and space cost obviously, and more importantly, which can avoid or at least reduce over-fitting problem. We give different solutions to the problems based on different distribution distance estimation criteria, and convergence analysis is also given. Comprehensive experiments on the text data sets and the face image data sets demonstrate that TSSL-based methods outperform existing transfer learning methods.", "Semi-supervised projection clustering  Transferred centroid regularization  Domain adaptation  Pairwise constraint  Transfer learning We propose a novel method, called Semi-supervised Projection Clustering in Transfer Learning (SPCTL), where multiple source domains and one target domain are assumed. Traditional semi-supervised projection clustering methods hold the assumption that the data and pairwise constraints are all drawn from the same domain. However, many related data sets with different distributions are available in real applications. The traditional methods thus can not be directly extended to such a scenario. One major challenging issue is how to exploit constraint knowledge from multiple source domains and transfer it to the target domain where all the data are unlabeled. To handle this difficulty, we are motivated to construct a common subspace where the difference in distributions among domains can be reduced. We also invent a transferred centroid regularization, which acts as a bridge to transfer the constraint knowledge to the target domain, to formulate this geometric structure formed by the centroids from different domains. Extensive experiments on both synthetic and benchmark data sets show the effectiveness of our method.", "Transfer learning  Neural networks  Base transfer  Mapping function A machine learning framework which uses unlabeled data from a related task domain in supervised classification tasks is described. The unlabeled data come from related domains, which share the same class labels or generative distribution as the labeled data. Patterns in the unlabeled data are learned via a neural network and transferred to the target domain from where the labeled data are generated, so as to improve the performance of the supervised learning task. We call this approach self-taught transfer learning from unlabeled data. We introduce a general-purpose feature learning algorithm producing features that retain information from the unlabeled data. Information preservation assures that the features obtained will be useful for improving the classification performance of the supervised tasks.", "Knowledge transfer  transfer learning  feature extraction  sparse coding  low-quality data Effectively utilizing readily available auxiliary data to improve predictive performance on new modeling tasks is a key problem in data mining. In this research, the goal is to transfer knowledge between sources of data, particularly when ground-truth information for the new modeling task is scarce or is expensive to collect where leveraging any auxiliary sources of data becomes a necessity. Toward seamless knowledge transfer among tasks, effective representation of the data is a critical but yet not fully explored research area for the data engineer and data miner. Here, we present a technique based on the idea of sparse coding, which essentially attempts to find an embedding for the data by assigning feature values based on subspace cluster membership. We modify the idea of sparse coding by focusing the identification of shared clusters between data when source and target data may have different distributions. In our paper, we point out cases where a direct application of sparse coding will lead to a failure of knowledge transfer. We then present the details of our extension to sparse coding, by incorporating distribution distance estimates for the embedded data, and show that the proposed algorithm can overcome the shortcomings of the sparse coding algorithm on synthetic data and achieve improved predictive performance on a real world chemical toxicity transfer learning task.", "Gaussian processes  Multi-task learning  Transfer learning  Negative transfer Multi-task learning, learning of a set of tasks together, can improve performance in the individual learning tasks. Gaussian process models have been applied to learning a set of tasks on different data sets, by constructing joint priors for functions underlying the tasks. In these previous Gaussian process models, the setting has been symmetric in the sense that all the tasks have been assumed to be equally important, whereas in settings such as transfer learning the goal is asymmetric, to enhance performance in a target task given the other tasks. We propose a focused Gaussian process model which introduces an explaining away model for each of the additional tasks to model their non-related variation, in order to focus the transfer to the task-of-interest. This focusing helps reduce the key problem of negative transfer, which may cause performance to even decrease if the tasks are not related closely enough. In experiments, our model improves performance compared to single-task learning, symmetric multi-task learning using hierarchical Dirichlet processes, transfer learning based on predictive structure learning, and symmetric multi-task learning with Gaussian processes.", "Event recognition  transfer learning  domain adaptation  cross-domain learning  adaptive MKL  aligned space-time pyramid matching We propose a visual event recognition framework for consumer videos by leveraging a large amount of loosely labeled web videos (e.g., from YouTube). Observing that consumer videos generally contain large intraclass variations within the same type of events, we first propose a new method, called Aligned Space-Time Pyramid Matching (ASTPM), to measure the distance between any two video clips. Second, we propose a new transfer learning method, referred to as Adaptive Multiple Kernel Learning (A-MKL), in order to 1) fuse the information from multiple pyramid levels and features (i.e., space-time features and static SIFT features) and 2) cope with the considerable variation in feature distributions between videos from two domains (i.e., web video domain and consumer video domain). For each pyramid level and each type of local features, we first train a set of SVM classifiers based on the combined training set from two domains by using multiple base kernels from different kernel types and parameters, which are then fused with equal weights to obtain a prelearned average classifier. In A-MKL, for each event class we learn an adapted target classifier based on multiple base kernels and the prelearned average classifiers from this event class or all the event classes by minimizing both the structural risk functional and the mismatch between data distributions of two domains. Extensive experiments demonstrate the effectiveness of our proposed framework that requires only a small number of labeled consumer videos by leveraging web data. We also conduct an in-depth investigation on various aspects of the proposed method A-MKL, such as the analysis on the combination coefficients on the prelearned classifiers, the convergence of the learning algorithm, and the performance variation by using different proportions of labeled consumer videos. Moreover, we show that A-MKL using the prelearned classifiers from all the event classes leads to better performance when compared with A-MKL using the prelearned classifiers only from each individual event class.", "Parametric programming  Solution path  Weighted support vector machines An instance-weighted variant of the support vector machine (SVM) has attracted considerable attention recently since they are useful in various machine learning tasks such as non-stationary data analysis, heteroscedastic data modeling, transfer learning, learning to rank, and transduction. An important challenge in these scenarios is to overcome the computational bottleneck-instance weights often change dynamically or adaptively, and thus the weighted SVM solutions must be repeatedly computed. In this paper, we develop an algorithm that can efficiently and exactly update the weighted SVM solutions for arbitrary change of instance weights. Technically, this contribution can be regarded as an extension of the conventional solution-path algorithm for a single regularization parameter to multiple instance-weight parameters. However, this extension gives rise to a significant problem that breakpoints (at which the solution path turns) have to be identified in high-dimensional space. To facilitate this, we introduce a parametric representation of instance weights. We also provide a geometric interpretation in weight space using a notion of critical region: a polyhedron in which the current affine solution remains to be optimal. Then we find breakpoints at intersections of the solution path and boundaries of polyhedrons. Through extensive experiments on various practical applications, we demonstrate the usefulness of the proposed algorithm.", "Data stream classification  Transfer learning  Semi-supervised learning  Relational k-means  Concept drifting Data stream classification has drawn increasing attention from the data mining community in recent years. Relevant applications include network traffic monitoring, sensor network data analysis, Web click stream mining, power consumption measurement, dynamic tracing of stock fluctuations, to name a few. Data stream classification in such real-world applications is typically subject to three major challenges: concept drifting, large volumes, and partial labeling. As a result, training examples in data streams can be very diverse and it is very hard to learn accurate models with efficiency. In this paper, we propose a novel framework that first categorizes diverse training examples into four types and assign learning priorities to them. Then, we derive four learning cases based on the proportion and priority of the different types of training examples. Finally, for each learning case, we employ one of the four SVM-based training models: classical SVM, semi-supervised SVM, transfer semi-supervised SVM, and relational k-means transfer semi-supervised SVM. We perform comprehensive experiments on real-world data streams that validate the utility of our approach. (c) 2012 Elsevier B.V. All rights reserved.", "Machine learning  Transfer learning  Unsupervised learning  Metric learning  Kernel learning  Unlabeled data  Challenge  Competition We organized a challenge in Unsupervised and Transfer Learning: the UTL challenge (https://clopinetcom/ul). We made available large datasets from various application domains: handwriting recognition, image recognition, video processing, text processing, and ecology. The goal was to learn data representations that capture regularities of an input space for re-use across tasks. The representations were evaluated on supervised learning target tasks unknown to the participants. The first phase of the challenge was dedicated to unsupervised transfer learning (the competitors were given only unlabeled data). The second phase was dedicated to cross-task transfer learning (the competitors were provided with a limited amount of labeled data from source tasks, distinct from the target tasks). The analysis indicates that learned data representations yield significantly better results than those obtained with original data or data preprocessed with standard normalizations and functional transforms. (C) 2012 Elsevier Ltd. All rights reserved.", "Dimensionality reduction  Mixed kernel  Canonical Correlation Analysis  Model selection In this paper, we propose a novel method named Mixed Kernel CCA (MKCCA) to achieve easy yet accurate implementation of dimensionality reduction. MKCCA consists of two major steps. First, the high dimensional data space is mapped into the reproducing kernel Hilbert space (RKHS) rather than the Hilbert space, with a mixture of kernels, i.e. a linear combination between a local kernel and a global kernel. Meanwhile, a uniform design for experiments with mixtures is also introduced for model selection. Second, in the new RKHS, Kernel CCA is further improved by performing Principal Component Analysis (PCA) followed by CCA for effective dimensionality reduction. We prove that MKCCA can actually be decomposed into two separate components, i.e. PCA and CCA, which can be used to better remove noises and tackle the issue of trivial learning existing in CCA or traditional Kernel CCA. After this, the proposed MKCCA can be implemented in multiple types of learning, such as multi-view learning, supervised learning, semi-supervised learning, and transfer learning, with the reduced data. We show its superiority over existing methods in different types of learning by extensive experimental results. (C) 2012 Elsevier Ltd. All rights reserved.", "Object recognition  object tracking  sparse coding  transfer learning  visual prior Visual prior from generic real-world images can be learned and transferred for representing objects in a scene. Motivated by this, we propose an algorithm that transfers visual prior learned offline for online object tracking. From a collection of real-world images, we learn an overcomplete dictionary to represent visual prior. The prior knowledge of objects is generic, and the training image set does not necessarily contain any observation of the target object. During the tracking process, the learned visual prior is transferred to construct an object representation by sparse coding and multiscale max pooling. With this representation, a linear classifier is learned online to distinguish the target from the background and to account for the target and background appearance variations over time. Tracking is then carried out within a Bayesian inference framework, in which the learned classifier is used to construct the observation model and a particle filter is used to estimate the tracking result sequentially. Experiments on a variety of challenging sequences with comparisons to several state-of-the-art methods demonstrate that more robust object tracking can be achieved by transferring visual prior.", "Human motion  Gait analysis  3D human pose tracking  Transfer learning  Gender recognition  Human attributes It is well known that biological motion conveys a wealth of socially meaningful information. From even a brief exposure, biological motion cues enable the recognition of familiar people, and the inference of attributes such as gender, age, mental state, actions and intentions. In this paper we show that from the output of a video-based 3D human tracking algorithm we can infer physical attributes (e.g., gender and weight) and aspects of mental state (e.g., happiness or sadness). In particular, with 3D articulated tracking we avoid the need for view-based models, specific camera viewpoints, and constrained domains. The task is useful for man-machine communication, and it provides a natural benchmark for evaluating the performance of 3D pose tracking methods (vs. conventional Euclidean joint error metrics). We show results on a large corpus of motion capture data and on the output of a simple 3D pose tracker applied to videos of people walking. (C) 2012 Elsevier Inc. All rights reserved.", "Opinion mining  Sentimental classification  Boosting  Transfer learning  Transfer learning with multiple sources  Multiple source domains Transfer learning aims at adapting a classifier trained on one domain with adequate labeled samples to a new domain where samples are from a different distribution and have no class labels. In this paper, we explore the transfer learning problems with multiple data sources and present a novel boosting algorithm, SharedBoost. This novel algorithm is capable of applying for very high dimensional data such as in text mining where the feature dimension is beyond several ten thousands. The experimental results illustrate that the SharedBoost algorithm significantly outperforms the traditional methods which transfer knowledge with supervised learning techniques. Besides, SharedBoost also provides much better classification accuracy and more stable performance than some other typical transfer learning methods such as the structural correspondence learning (SCL) and the structural learning in the multiple sources transfer learning problems. (C) 2011 Elsevier B.V. All rights reserved.", "EEG classification  Transfer learning  Ensemble learning  Sparse representation This paper proposes a subject transfer framework for EEG classification. It aims to improve the classification performance when the training set of the target subject (namely user) is small owing to the need to reduce the calibration session. Our framework pursues improvement not only at the feature extraction stage, but also at the classification stage. At the feature extraction stage, we first obtain a candidate filter set for each subject through a previously proposed feature extraction method. Then, we design different criterions to learn two sparse subsets of the candidate filter set, which are called the robust filter bank and adaptive filter bank, respectively. Given robust and adaptive filter banks, at the classification step, we learn classifiers corresponding to these filter banks and employ a two-level ensemble strategy to dynamically and locally combine their outcomes to reach a single decision output. The proposed framework, as validated by experimental results, can achieve positive knowledge transfer for improving the performance of EEG classification. (c) 2011 Elsevier B.V. All rights reserved.", "Context modeling  object detection  transfer learning Context is critical for reducing the uncertainty in object detection. However, context modeling is challenging because there are often many different types of contextual information coexisting with different degrees of relevance to the detection of target object(s) in different images. It is therefore crucial to devise a context model to automatically quantify and select the most effective contextual information for assisting in detecting the target object. Nevertheless, the diversity of contextual information means that learning a robust context model requires a larger training set than learning the target object appearance model, which may not be available in practice. In this work, a novel context modeling framework is proposed without the need for any prior scene segmentation or context annotation. We formulate a polar geometric context descriptor for representing multiple types of contextual information. In order to quantify context, we propose a new maximum margin context (MMC) model to evaluate and measure the usefulness of contextual information directly and explicitly through a discriminant context inference method. Furthermore, to address the problem of context learning with limited data, we exploit the idea of transfer learning based on the observation that although two categories of objects can have very different visual appearance, there can be similarity in their context and/or the way contextual information helps to distinguish target objects from nontarget objects. To that end, two novel context transfer learning models are proposed which utilize training samples from source object classes to improve the learning of the context model for a target object class based on a joint maximum margin learning framework. Experiments are carried out on PASCAL VOC2005 and VOC2007 data sets, a luggage detection data set extracted from the i-LIDS data set, and a vehicle detection data set extracted from outdoor surveillance footage. Our results validate the effectiveness of the proposed models for quantifying and transferring contextual information, and demonstrate that they outperform related alternative context models.", "transfer learning  meta-generalising  multi-task learning  Gaussian processes  mixture of experts We propose a novel model for meta-generalisation, that is, performing prediction on novel tasks based on information from multiple different but related tasks. The model is based on two coupled Gaussian processes with structured covariance function  one model performs predictions by learning a constrained covariance function encapsulating the relations between the various training tasks, while the second model determines the similarity of new tasks to previously seen tasks. We demonstrate empirically on several real and synthetic data sets both the strengths of the approach and its limitations due to the distributional assumptions underpinning it.", "Cross-domain learning  domain adaptation  transfer learning  support vector machine  multiple kernel learning Cross-domain learning methods have shown promising results by leveraging labeled patterns from the auxiliary domain to learn a robust classifier for the target domain which has only a limited number of labeled samples. To cope with the considerable change between feature distributions of different domains, we propose a new cross-domain kernel learning framework into which many existing kernel methods can be readily incorporated. Our framework, referred to as Domain Transfer Multiple Kernel Learning (DTMKL), simultaneously learns a kernel function and a robust classifier by minimizing both the structural risk functional and the distribution mismatch between the labeled and unlabeled samples from the auxiliary and target domains. Under the DTMKL framework, we also propose two novel methods by using SVM and prelearned classifiers, respectively. Comprehensive experiments on three domain adaptation data sets (i.e., TRECVID, 20 Newsgroups, and email spam data sets) demonstrate that DTMKL-based methods outperform existing cross-domain learning and multiple kernel learning methods.", "Facial Expression Representation  Transfer learning  Fusion Techniques  Fuzzy Inference System  Context in Emotion Recognition This paper presents a multimodal fuzzy inference system for emotion detection. The system extracts and merges visual, acoustic and context relevant features. The experiments have been performed as part of the AVEC 2012 challenge. Facial expressions play an important role in emotion detection. However, having an automatic system to detect facial emotional expressions on unknown subjects is still a challenging problem. Here, we propose a method that adapts to the morphology of the subject and that is based on an invariant representation of facial expressions. Our method relies on 8 key expressions of emotions of the subject. In our system, each image of a video sequence is defined by its relative position to these 8 expressions. These 8 expressions are synthesized for each subject from plausible distortions learnt on other subjects and transferred on the neutral face of the subject. Expression recognition in a video sequence is performed in this space with a basic intensity-area detector. The emotion is described in the 4 dimensions : valence, arousal, power and expectancy. The results show that the duration of high intensity smile is an expression that is meaningful for continuous valence detection and can also be used to improve arousal detection. The main variations in power and expectancy are given by context data.", "transfer learning  ensemble learning  machine learning The lack of labeled training data is a common issue in many machine learning applications. Semi-supervised learning addresses this issue by self-labeling unlabelled examples. Transfer learning tackles it from a different way: borrow labeled examples from a different but related domain (source domain) by assigning weights to those examples based on their suitability on the new domain (target domain). However, it is quite challenging to figure out the suitability. In this paper, we propose a different way for utilizing the labeled examples from source domain. That is, we use them only for labelling the unlabelled examples in the target domain. In this self-labelling, we use the idea of Tri-training. We call our new algorithm: TriTransfer. In TriTransfer, three initial classifiers are generated from the source data and the originally labeled data in the target domain, and an unlabeled example is labeled and added to the labeled data for a classifier if other two classifiers agree on its label. After an expanded labeled data set is obtained, we re-train the classifier. We repeat this process until no more change can be made. At the end, the final classifier, which is a weighted combination of the three classifiers, is output. We conduct an extensive empirical study on 34 UCI datasets, which shows that TriTransfer performs better than the state-of-art algorithms TransferBoost, Tritraining, and NaiveBayes.", "Rare class  transfer learning  class imbalance  AdaBoost  Weighted Majority Algorithm  healthcare Small datasets pose a tremendous challenge in machine learning due to the few available training examples compounded with the relative rarity of certain labels which can potentially impede the development of a representative hypothesis. We define Rare Datasets as ones with low samples/features ratio and a skewed label distribution. Since a generalized training model can not be theoretically guaranteed, a method to leverage similar data is needed. We propose the first algorithm that utilizes transfer learning for the label space, present theoretical verification of our method and demonstrate the effectiveness of our framework with several real-world experiments. In addition, we formally describe what constitutes a Rare Dataset and present a detailed characterization of related methods.", "surgical model  preoperative model  support vector machines  transfer learning Preoperative models to assess surgical mortality are important clinical tools in determining optimal patient care. The traditional approach to develop these models has been primarily centralized, i.e., it uses surgical case records aggregated across multiple hospitals. While this approach of pooling greatly increases the data size, the resulting models fail to reflect individual variations across hospitals in terms of patients and the delivery of care. We hypothesize that this process can be improved through adapting the multi-hospital data model to an individual hospital. This approach simultaneously leverages the large multi-hospital data and the patient-and-case mix at individual hospitals. We explore transfer learning to refine surgical models for individual hospitals in the framework of support vector machine by using data from both the National Surgical Quality Improvement Program and a single hospital. Our results show that transferring models trained on multi-hospital data to an individual hospital significantly improves discrimination for surgical mortality at the individual provider level.", "relationships between attributes  Markov Logic Network  small sample  first-order predicate  regression problem In practical engineering, small-scale data sets are usually sparse and contaminated by noise. It is difficult to guarantee a competitive generalization performance of regression model from such a data set. However, what is worth mentioning is that there are often a lot of incomplete relationships between attributes in practical engineering. The involvement of the relationships might be significant in improving the generalization performance of machine learning. So in this paper, we propose a transfer learning method based on the incomplete relationships between attributes, in which the incomplete relationships is reasoned to get complete relationships, and the complete relationships are then transferred to the regression learning to improve the generalization performance of the regression model. Finally the proposed method was applied to least squares support vector machine (LSSVM) and was evaluated on benchmark data sets. The experiment results show that the transfer learning can improve the generalization performance and prediction accuracy of the regression model.", "transfer learning  neural network  feature transfer The degree of abundance of labeled training data is an important factor in determining the performance of supervised machine learning systems. However, in some applications, labeled data are either costly to collect or easily outdated, resulting in poor generalization of trained machine learners. Nonetheless, there are often related domains where large corpuses of labeled data can be easily obtained. Therefore, we propose a new transfer learning algorithm to adapt a neural network trained on such a related domain to the target domain by grafting additional nodes onto its hidden layer. This neural grafting method is capable of transfering the knowledge embedded in the structures of the trained neural network to the problem in the target domain. Experiments on synthesized and real data sets show that grafted networks achieve good performance with very small amounts of data from the target domain. Compared with existing transfer learning techniques, the proposed neural grafting is easy to tune and computationally simple, with superior performance.", "low-rank  transfer learning  domain adaptation One of the most important challenges in machine learning is performing effective learning when there are limited training data available. However, there is an important case when there are sufficient training data coming from other domains (source). Transfer learning aims at finding ways to transfer knowledge learned from a source domain to a target domain by handling the subtle differences between the source and target. In this paper, we propose a novel framework to solve the aforementioned knowledge transfer problem via low-rank representation constraints. This is achieved by finding an optimal subspace where each datum in the target domain can be linearly represented by the corresponding subspace in the source domain. Extensive experiments on several databases, i.e., Yale B, CMU PIE, UB KinFace databases validate the effectiveness of the proposed approach and show the superiority to the existing, well-established methods.", "Photo quality assessment  geo-context  transfer learning  social media Automatic photo quality assessment emerged as a hot topic in recent years for its potential in numerous applications. Most existing approaches to photo quality assessment have predominantly focused on image content itself, while ignoring various contexts such as the associated geo-location and timestamp. However, such a universal aesthetic assessment model may not work well with significantly different contexts, since the photography rules are always scene and context dependent. In real cases, professional photographers use different photography knowledge when shooting various scenes in different places. Motivated by this observation, we leverage the geo-context information associated with photos for visual quality assessment. Specifically, we propose in this paper a Scene-Dependent Aesthetic Model (SDAM) to assess photo quality, by jointly leveraging the geo-context and visual content. Geo-contextual leveraged searching is performed to obtain relevant images with similar content to discover the scene-dependent photography principles for accurate photo quality assessment. To overcome the problem that in many cases the number of the contextually searched images is insufficient for learning the SDAM, we adopt transfer learning to utilize auxiliary photos within the same scene category from other locations for learning photography rules. Extensive experiments shows that the proposed SDAM scheme indeed improves the photo quality assessment accuracy via leveraging photo geo-contexts, compared with traditional universal aesthetic models.", "Relevance  search engine  learning to rank  transfer learning  super-features  convergence analysis  experimental evaluation  Algorithms  Experimentation  Theory In learning to rank, both the quality and quantity of the training data have significant impacts on the performance of the learned ranking functions. However, in many applications, there are usually not sufficient labeled training data for the construction of an accurate ranking model. It is therefore desirable to leverage existing training data from other tasks when learning the ranking function for a particular task, an important problem which we tackle in this article utilizing a boosting framework with transfer learning. In particular, we propose to adaptively learn transferable representations called super-features from the training data of both the target task and the auxiliary task. Those super-features and the coefficients for combining them are learned in an iterative stage-wise fashion. Unlike previous transfer learning methods, the super-features can be adaptively learned by weak learners from the data. Therefore, the proposed framework is sufficiently flexible to deal with complicated common structures among different learning tasks. We evaluate the performance of the proposed transfer learning method for two datasets from the Letor collection and one dataset collected from a commercial search engine, and we also compare our methods with several existing transfer learning methods. Our results demonstrate that the proposed method can enhance the ranking functions of the target tasks utilizing the training data from the auxiliary tasks.", "Extraction of conceptual relation  concept graph  template matching In view of the low efficiency of depending on one extracting method, this paper proposes a blending extracting method based on the combination of statistics, regulations and managing nature language. By employing template construction to extract conceptual relations, this method adopts transfer learning to obtain concept pairs and by using the advantages of concept graph in knowledge representation, matches templates through conjoining the Hownet in order to gain template set and extract conceptual relations. The experimental results show that this method can raise the accuracy rate in relation extraction.", "transfer learning  similarity measure  machine learning Most traditional machine learning methods make an assumption that the distribution of the training dataset is the same as the applied domain. Transfer learning omits this assumption and is able to transfer knowledge between different domains. It is a promising method to make machine learning technology become more practical. However, negative transfer can hurt the performance of the model, therefore, it should be avoided. In this paper, we focus on how to select a good knowledge source when there are multiple labelled datasets available. A method to estimate the divergence between two labelled datasets is given. In addition, we also provide a method to decide the mappings between features in different datasets. The experimental results show that the divergence estimated by our method is highly related to the performance of the model.", "Algorithms  Metric learning  transfer learning  multitask learning  semi-supervised learning Distance metric learning plays a very crucial role in many data mining algorithms because the performance of an algorithm relies heavily on choosing a good metric. However, the labeled data available in many applications is scarce, and hence the metrics learned are often unsatisfactory In this article, we consider a transfer-learning setting in which some related source tasks with labeled data are available to help the learning of the target task. We first propose a convex formulation for multitask metric learning by modeling the task relationships in the form of a task covariance matrix. Then we regard transfer learning as a special case of multitask learning and adapt the formulation of multitask metric learning to the transfer-learning setting for our method, called transfer metric learning (TML). In TML, we learn the metric and the task covariances between the source tasks and the target task under a unified convex formulation. To solve the convex optimization problem, we use an alternating method in which each subproblem has an efficient solution. Moreover, in many applications, some unlabeled data is also available in the target task, and so we propose a semi-supervised extension of TML called STML to further improve the generalization performance by exploiting the unlabeled data based on the manifold assumption. Experimental results on some commonly used transfer-learning applications demonstrate the effectiveness of our method.", "Music genre classification  Self-taught learning  non-negative matrix factorization  Transfer learning Availability of large amounts of raw unlabeled data has sparked the recent surge in semi-supervised learning research. In most works, however, it is assumed that labeled and unlabeled data come from the same distribution. This restriction is removed in the self-taught learning approach where unlabeled data can be different, but nevertheless have similar structure. First, a representation is learned from the unlabeled data via non-negative matrix factorization (NMF) and then it is applied to the labeled data used for classification. In this work, we implemented this method for the music genre classification task using two different databases: one as unlabeled data pool and the other for supervised classifier training. Music pieces come from 10 and 6 genres for each database respectively, while only one genre is common for both of them. Results from wide variety of experimental settings show that the self-taught learning method improves the classification rate when the amount of labeled data is small and, more interestingly, that consistent improvement can be achieved for a wide range of unlabeled data sizes.", " Solving the person re-identification problem has become important for understanding peoples behaviours in a multicamera network of non-overlapping views. In this work, we address the problem of re-identification from a set-based verification perspective. More specifically, we have a small set of target people on a watch list (a set) and we aim to verify whether a query image of a person is on this watch list. This differs from the existing person re-identification problem in that the probe is verified against a small set of known people but requires much higher degree of verification accuracy with very limited sampling data for each candidate in the set. That is, rather than recognising everybody in the scene, we consider identifying a small set of target people against non-target people when there is only a limited number of target training samples and a large number of unlabelled (unknown) non-target samples available. To this end, we formulate a transfer learning framework for mining discriminant information from non-target people data to solve the watch list set verification problem. Based on the proposed approach, we introduce the concepts of multishot and one-shot verifications. We also design new criteria for evaluating the performance of the proposed transfer learning method against the i-LIDS and ETHZ data sets.", " The performance of a generic pedestrian detector may drop significantly when it is applied to a specific scene due to mismatch between the source dataset used to train the detector and samples in the target scene. In this paper, we investigate how to automatically train a scene-specific pedestrian detector starting with a generic detector in video surveillance without further manually labeling any samples under a novel transfer learning framework. It tackles the problem from three aspects. (1) With a graphical representation and through exploring the indegrees from target samples to source samples, the source samples are properly re-weighted. The indegrees detect the boundary between the distributions of the source dataset and the target dataset. The re-weighted source dataset better matches the target scene. (2) It takes the context information from motions, scene structures and scene geometry as the confidence scores of samples from the target scene to guide transfer learning. (3) The confidence scores propagate among samples on a graph according to the underlying visual structures of samples. All these considerations are formulated under a single objective function called Confidence-Encoded SVM. At the test stage, only the appearance-based detector is used without the context cues. The effectiveness of the proposed framework is demonstrated through experiments on two video surveillance datasets. Compared with a generic pedestrian detector, it significantly improves the detection rate by 48% and 36% at one false positive per image on the two datasets respectively.", " We analyze transfer learning with Deep Neural Networks (DNN) on various character recognition tasks. DNN trained on digits are perfectly capable of recognizing uppercase letters with minimal retraining. They are on par with DNN fully trained on uppercase letters, but train much faster. DNN trained on Chinese characters easily recognize uppercase Latin letters. Learning Chinese characters is accelerated by first pretraining a DNN on a small subset of all classes and then continuing to train on all classes. Furthermore, pretrained nets consistently outperform randomly initialized nets on new tasks with few labeled data.", " In this paper, we propose a study on the use of weighted topological learning and matrix factorization methods to transform the representation space of a sparse dataset in order to increase the quality of learning, and adapt it to the case of transfer learning. The matrix factorization allows us to find latent variables, weighted topological learning is used to detect the most relevant among them. New data representation is based on their projections on the weighted topological model. Each object in the dataset is described by a new representation consisting of the distances of this object to all components of the topological model (prototypes). For transfer learning, we propose a new method where the representation of data is done in the same way as in the first phase, but using a pruned topological model. This pruning is performed after labeling the units of the topological model using the labels available for transfer. The experiments are presented as a part of an International Challenge [1] where we have obtained promising results (5th rank).", "Transfer learning  dataset selection  distance measures  debt valuation  prediction  supervised learning Machine learning and data mining algorithms usually assume that the training and future data have the same distribution and come from the same feature space. However, in majority of real-world problems, this is not true. In case of Debt portfolio appraisal we have sufficient training data only in another domain of interest, namely in other portfolios. Therefore, only knowledge transfer from these portfolios in inference for new one is possible. In the paper we propose transfer learning and learning based on similarity methods, basing on similarity between training and testing datasets. The proposed approach is examined in real domain debt portfolio valuation.", " A key question in machine perception is how to adaptively build upon existing capabilities so as to permit novel functionalities. Implicit in this are the notions of anomaly detection and learning transfer. A perceptual system must firstly determine at what point the existing learned model ceases to apply, and secondly, what aspects of the existing model can be brought to hear on the newly-defined learning domain. Anomalies must thus be distinguished from mere outliers, i.e. cases in which the learned model has failed to produce a clear response  it is also necessary to distinguish novel (but meaningful) input from misclassification error within the existing models. We thus apply a methodology of anomaly detection based on comparing the outputs of strong and weak classifiers [10] to the problem of detecting the rule-incongruence involved in the transition from singles to doubles tennis videos. We then demonstrate how the detected anomalies can be used to transfer learning from one (initially known) rule-governed structure to another. Our ultimate aim, building on existing annotation technology, is to construct an adaptive system for court-based sport video annotation.", " Within the context of detection of incongruent events, an often overlooked aspect is how a system should react to the detection. The set of all the possible actions is certainly conditioned by the task at hand, and by the embodiment of the artificial cognitive system under consideration. Still, we argue that a desirable action that does not depend from these factors is to update the internal model and learn the new detected event. This paper proposes a recent transfer learning algorithm as the way to address this issue. A notable feature of the proposed model is its capability to learn from small samples, even a single one. This is very desirable in this context, as we cannot expect to have too many samples to learn from, given the very nature of incongruent events. We also show that one of the internal parameters of the algorithm makes it possible to quantitatively measure incongruence of detected events. Experiments on two different datasets support our claim.", "traffic scene surveillance  object classification  transfer learning Object classification in traffic scene is of vital importance to intelligent traffic surveillance. In real applications, the shooting view changes frequently in different scenes, which leads to sharp accuracy decrease since source and target domain samples do not follow the same distribution anymore. On the other hand, manual labeling training samples is time and labor consuming. Transfer learning approaches are to utilize the knowledge learnt from source view for target object classification. In this paper, we propose a hybrid transfer learning mechanism combining two single transfer approaches to gap the divergence of different domain distributions. An instance-based transfer approach is implemented to label target samples that represent target domain distribution best. And a feature-based transfer framework is to learn a strong classifier for target domain with both labeled source and target domain samples. Experimental results indicate that our approach outperforms traditional machine learning and single transfer learning methods.", " This paper describes an active transfer learning technique for multi-view head-pose classification. We combine transfer learning with active learning, where an active learner asks the domain expert to label the few most informative target samples for transfer learning. Employing adaptive multiple-kernel learning for head-pose classification from four low-resolution views, we show how active sampling enables more efficient learning with few examples. Experimental results confirm that active transfer learning produces 10% higher pose-classification accuracy over several competing transfer learning approaches.", " Object classification is of vital importance to intelligent traffic surveillance. A big challenge is that shooting view changes in different scenes, which leads to sharp accuracy decrease since training and test samples do not follow the same distribution anymore. On the other hand, manual labeling training samples is time and labor consuming. We propose a feature-based transfer learning framework to gap the divergence of different domain distributions with scarce target view samples. Source view samples, following a different but relevant distribution, could be utilized to learn what a good classifier is like by structure learning. At the same time, small amount of target view samples could make a great contribution to reflect the target distribution. Experimental results indicate that our method outperforms traditional approaches when target samples are too scarce to build a strong classifier.", "particle swarm optimization  transfer learning  similar image search  remote sensing Remote sensing using satellite monitored sensor data is one of the most important methods for global environmental monitoring. Similar image search is an important problem in the satellite image data analysis. The similar image search extracts local area images from a given global image. The similar image search using shape features can be used to analyze the cloud type, the volcanic activity, the change in vegetation and etc. However, the similar image search in satellite image data requires the fast computation infrastructure and search method due to the huge image data. In previous research, we proposed a variant of particle swarm optimization that globally searches using particle groups in dynamically changed problem space. In this paper, our PSO was applied to the similar image search problem based on Transfer learning concept. The transfer learning is a meta learning methods that uses the knowledge and data in an domain in order to solve the problem in the another domain. In this experiment, we compared the accuracy and the calculation time among the different transfer learning conditions in order to investigate the possibility of knowledge transfer in similar image search.", " We aim to accelerate learning processes in reinforcement learning by transfer learning. Its concept is that knowledge to solve similar tasks accelerates a learning process of a target task. We have proposed that the basic transfer method based on forbidden rule set that is a set of rules which cause to immediately failure of a target task. However, the basic method works poorly for the Same Transition Model, which has same state transition probability and different goal. In this article, we propose an effective transfer learning method in same transition model. In detail, it consists of two strategies: (1) approaching to the goal for the selected source task quickly, and (2) exploring states around the goal preferentially.", "Gait recognition  Learning to rank  Transfer learning The advantage of gait over other biometrics such as face or fingerprint is that it can operate from a distance and without subject cooperation. However, this also makes gait subject to changes in various covariate conditions including carrying, clothing, surface and view angle. Existing approaches attempt to address these condition changes by feature selection, feature transformation or discriminant subspace learning. However, they suffer from lack of training samples from each subject, can only cope with changes in a subset of conditions with limited success, and are based on the invalid assumption that the covariate conditions are known a priori. They are thus unable to perform gait recognition under a genuine uncooperative setting. We propose a novel approach which casts gait recognition as a bipartite ranking problem and leverages training samples from different classes/people and even from different datasets. This makes our approach suitable for recognition under a genuine uncooperative setting and robust against any covariate types, as demonstrated by our extensive experiments.", " Transfer learning can counter the heavy-tailed nature of the distribution of training examples over object classes. Here, we study transfer learning for object class detection. Starting from the intuition that what makes a good detector should manifest itself in the form of repeatable statistics over existing good detectors, we design a low-level feature model that can be used as a prior for learning new object class models from scarce training data. Our priors are structured, capturing dependencies both on the level of individual features and spatially neighboring pairs of features. We confirm experimentally the connection between the information captured by our priors and good detectors as well as the connection to transfer learning from sources of different quality. We give an in-depth analysis of our priors on a subset of the challenging PASCAL VOC 2007 data set and demonstrate improved average performance over all 20 classes, achieved without manual intervention.", " We present a novel transfer learning approach to cross-camera action recognition. Inspired by canonical correlation analysis (CCA), we first extract the spatio-temporal visual words from videos captured at different views, and derive a correlation subspace as a joint representation for different bag-of-words models at different views. Different from prior CCA-based approaches which simply train standard classifiers such as SVM in the resulting subspace, we explore the domain transfer ability of CCA in the correlation subspace, in which each dimension has a different capability in correlating source and target data. In our work, we propose a novel SVM with a correlation regularizer which incorporates such ability into the design of the SVM. Experiments on the IXMAS dataset verify the effectiveness of our method, which is shown to outperform state-of-the-art transfer learning approaches without taking such domain transfer ability into consideration.", " In recent studies of Alzheimer's disease (AD), it has increasing attentions in identifying mild cognitive impairment (MCI) converters (MCI-C) from MCI non-converters (MCI-NC). Note that MCI is a prodromal stage of AD, with possibility to convert to AD. Most traditional methods for MCI conversion prediction learn information only from MCI subjects (including MCI-C and MCI-NC), not from other related subjects, e.g., AD and normal controls (NC), which can actually aid the classification between MCI-C and MCI-NC. In this paper, we propose a novel domain-transfer learning method for MCI conversion prediction. Different from most existing methods, we classify MCI-C and MCI-NC with aid from the domain knowledge learned with AD and NC subjects as auxiliary domain to further improve the classification performance. Our method contains two key components: (1) the cross-domain kernel learning for transferring auxiliary domain knowledge, and (2) the adapted support vector machine (SVM) decision function construction for cross-domain and auxiliary domain knowledge fusion. Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database show that the proposed method can significantly improve the classification performance between MCI-C and MCI-NC, with aid of domain knowledge learned from AD and NC subjects.", "Meta-analysis  fMRI  multiple comparison  machine learning Typical cohorts in brain imaging studies are not large enough for systematic testing of all the information contained in the images. To build testable working hypotheses, investigators thus rely on analysis of previous work, sometimes formalized in a so-called meta-analysis. In brain imaging, this approach underlies the specification of regions of interest (ROIs) that are usually selected on the basis of the coordinates of previously detected effects. In this paper, we propose to use a database of images, rather than coordinates, and frame the problem as transfer learning: learning a discriminant model on a reference task to apply it to a different but related new task. To facilitate statistical analysis of small cohorts, we use a sparse discriminant model that selects predictive voxels on the reference task and thus provides a principled procedure to define ROIs. The benefits of our approach are twofold. First it uses the reference database for prediction, i.e. to provide potential biomarkers in a clinical setting. Second it increases statistical power on the new task. We demonstrate on a set of 18 pairs of functional MRI experimental conditions that our approach gives good prediction. In addition, on a specific transfer situation involving different scanners at different locations, we show that voxel selection based on transfer learning leads to higher detection power on small cohorts.", "Reinforcement Learning  Knowledge Transfer  Dimension Reduction  Exploration Policy In this paper we introduce a budgeted knowledge transfer algorithm for non-homogeneous reinforcement learning agents. Here the source and the target agents are completely identical except in their state representations. The algorithm uses functional space (Q-value space) as the transfer-learning media. In this method, the target agent's functional points (Q-values) are estimated in an automatically selected lower-dimension subspace in order to accelerate knowledge transfer. The target agent searches that subspace using an exploration policy and selects actions accordingly during the period of its knowledge transfer in order to facilitate gaining an appropriate estimate of its Q-table. We show both analytically and empirically that this method decreases the required learning budget for the target agent.", "PAC Theory  Rare Class  Transfer Learning  Class Imbalance  AdaBoost  Weighted Majority Algorithm The exponential growth of data dimensions presents an obstacle in informatics as data miners try to construct ever greater training sets to overcome the theoretical limitations of statistical learning theory. Machine learning models require a minimum set of samples within each label to develop a representative hypothesis. To overcome these bounds, we developed an algorithm that can extract samples from an auxiliary domain to augment the training set. Our work exploits concepts from the Transfer Learning and Imbalanced Learning domains to expand the training set and permit standard models to be applied. We present theoretical verification of our method and demonstrate the effectiveness of our framework with experimental results on real-world data.", "Multiple outlooks learning  multiple views learning  transfer learning  support vector machines Multiple Outlooks Learning (MOL) has recently received considerable attentions in machine learning. While traditional classification models often assume patterns are living in a fixed-dimensional vector space, MOL focuses on the tasks involving multiple representations or outlooks (e. g., biometrics based on face, fingerprint and iris)  samples belonging to different outlooks may have varying feature dimensionalities and distributions. Current MOL methods attempted to first map each outlook heuristically to a common space, where samples from all the outlooks are assumed to share the same dimensionality and distribution after mapping. Traditional off-the-shelf classifiers can then be applied in the common space. The performance of these approaches is however often limited due to the independence of mapping functions learning and classifier learning. Different from existing approaches, in this paper, we proposed a novel MOL framework capable of learning jointly the mapping functions and the classifier in the common latent space. In particular, we coupled our novel framework with Support Vector Machines (SVM) and proposed a new model called MOL-SVM. MOL-SVM only needs to solve a sequence of standard linear SVM problems and converges rather rapidly within only a few steps. A series of experiments on the 20 newsgroups dataset demonstrated that our proposed model can consistently outperform the other competitive approaches.", "Transfer learning  Multi-source learning  Multi-view learning  Adaboost  Supervised learning Transfer learning, which is one of the most important research directions in machine learning, has been studied in various fields in recent years. In this paper, we combine the theories of multi-source and multi-view learning into transfer learning and propose a new algorithm named Multi-source Transfer Learning with Multi-view Adaboost (MsTL-MvAdaboost). Different from many previous works on transfer learning, in this algorithm, we not only use the labeled data from several source tasks to help learn one target task, but also consider how to transfer them in different views synchronously. We regard all the source and target tasks as a collection of several constituent views and each of these tasks can be learned from different views. Experimental results also validate the effectiveness of our proposed approach.", "Genetic algorithms  human computer interaction  weighted support vector machine  surrogate model  transfer learning Interactive genetic algorithms (IGAs) are effective methods of tackling optimization problems involving qualitative indices by incorporating a user's evaluations into traditional genetic algorithms. The problem of user fatigue resulting from the user's evaluations, however, has a negative influence on the performance of these algorithms. Substituting the user's evaluations with various surrogate models is beneficial to alleviate user fatigue. Previous studies, however, have not taken full advantage of information provided by samples obtained earlier when constructing or updating these models. We focus on the issue of user fatigue in this study, and present a novel method of effectively alleviating user fatigue by substituting the user's evaluations with a weighted support vector machine (WSVM) and by incorporating it with the mechanism of transfer learning. The proposed method is applied to the fashion evolutionary design system and compared with previous effective IGAs. The experimental results confirm the advantage of the proposed method in both alleviating user fatigue and improving the precision of the surrogate model.", " Most existing approaches to training object detectors rely on fully supervised learning, which requires the tedious manual annotation of object location in a training set. Recently there has been an increasing interest in developing weakly supervised approach to detector training where the object location is not manually annotated but automatically determined based on binary (weak) labels indicating if a training image contains the object. This is a challenging problem because each image can contain many candidate object locations which partially overlaps the object of interest. Existing approaches focus on how to best utilise the binary labels for object location annotation. In this paper we propose to solve this problem from a very different perspective by casting it as a transfer learning problem. Specifically, we formulate a novel transfer learning based on learning to rank, which effectively transfers a model for automatic annotation of object location from an auxiliary dataset to a target dataset with completely unrelated object categories. We show that our approach outperforms existing state-of-the-art weakly supervised approach to annotating objects in the challenging VOC dataset.", " Open ended learning is a dynamic process based on the continuous analysis of new data, guided by past experience. On one side it is helpful to take advantage of prior knowledge when only few information on a new task is available (transfer learning). On the other, it is important to continuously update an existing model so to exploit the new incoming data, especially if their informative content is very different from what is already known (online learning). Until today these two aspects of the learning process have been tackled separately. In this paper we propose an algorithm that takes the best of both worlds: we consider a sequential learning setting, and we exploit the potentiality of knowledge transfer with a computationally cheap solution. At the same time, by relying on past experience we boost online learning to predict reliably on future problems. A theoretical analysis, coupled with extensive experiments, show that our approach performs well in terms of the online number of training mistakes, as well as in terms of performance on separate test sets.", "particle swarm optimization  transfer learning  similar image search  remote sensing Remote sensing of the earth surface using satellite monitored sensor data is one of the most important methods for global environmental monitoring. For satellite monitored sensor data, MODIS (Moderate Resolution Imaging Spectoradiometer) satellite data is actively used for the remote sensing data. In remote sensing fields, similar image search which extracts local area images from a given global map image is often required. Similar image search is important because physical changes to the earth's surface caused by human or nature can be monitored. However, long calculation time is required for similar image search in MODIS data due to the very large search space. In our previous research, an effective result was yielded using genetic algorithm on the similar image search from the satellite image. Based on this result, we proposed a particle swarm optimization based search method that globally searches for the problem space using particle groups.", "Cross domain learning  machine learning  semi-supervised learning  transfer learning Semi-Supervised Learning (SSL) traditionally makes use of unlabeled samples(1) by including them into the training set through an automated labeling process. Such a primitive Semi-Supervised Learning (pSSL) approach suffers from a number of disadvantages including false labeling and incapable of utilizing out-of-domain samples. In this paper, we propose a formative Semi-Supervised Learning (fSSL) framework which explores hidden features between labeled and unlabeled samples to achieve semi-supervised learning. fSSL regards that both labeled and unlabeled samples are generated from some hidden concepts with labeling information partially observable for some samples. The key of the fSSL is to recover the hidden concepts, and take them as new features to link labeled and unlabeled samples for semi-supervised learning. Because unlabeled samples are only used to generate new features, but not to be explicitly included in the training set like pSSL does, fSSL overcomes the inherent disadvantages of the traditional pSSL methods, especially for samples not within the same domain as the labeled instances. Experimental results and comparisons demonstrate that fSSL significantly outperforms pSSL-based methods for both within-domain and cross-domain semi-supervised learning.", "classification  large margin  maximum likelihood Many popular linear classifiers, such as logistic regression, boosting, or SVM, are trained by optimizing a margin-based risk function. Traditionally, these risk functions are computed based on a labeled data set. We develop a novel technique for estimating such risks using only unlabeled data and the marginal label distribution. We prove that the proposed risk estimator is consistent on high-dimensional data sets and demonstrate it on synthetic and real-world data. In particular, we show how the estimate is used for evaluating classifiers in transfer learning, and for training classifiers with no labeled data whatsoever.", "Transfer learning  semisupervised learning  text classification We propose a framework for improving classifier performance by effectively using auxiliary samples. The auxiliary samples are labeled not in terms of the target taxonomy according to which we wish to classify samples, but according to classification schemes or taxonomies that are different from the target taxonomy. Our method finds a classifier by minimizing a weighted error over the target and auxiliary samples. The weights are defined so that the weighted error approximates the expected error when samples are classified into the target taxonomy. Experiments using synthetic and text data show that our method significantly improves the classifier performance in most cases compared to conventional data augmentation methods.", "Dimensionality reduction  Latent variable model  Transfer learning  Bregman divergence Latent variable models are powerful dimensionality reduction approaches in machine learning and pattern recognition. However, this kind of methods only works well under a necessary and strict assumption that the training samples and testing samples are independent and identically distributed. When the samples come from different domains, the distribution of the testing dataset will not be identical with the training dataset. Therefore, the performance of latent variable models will be degraded for the reason that the parameters of the training model do not suit for the testing dataset. This case limits the generalization and application of the traditional latent variable models. To handle this issue, a transfer learning framework for latent variable model is proposed which can utilize the distance (or divergence) of the two datasets to modify the parameters of the obtained latent variable model. So we do not need to rebuild the model and only adjust the parameters according to the divergence, which will adopt different datasets. Experimental results on several real datasets demonstrate the advantages of the proposed framework. (C) 2010 Elsevier Ltd. All rights reserved.", "Feature relevance networks  indoor location estimation  transfer learning We present a new machine learning framework for indoor location estimation. In many cases, locations could be easily estimated using various traditional positioning methods and conventional machine learning approaches based on signalling devices, e. g., access points (APs). When there exist environmental changes, however, such traditional methods cannot be employed due to data distribution change. In order to circumvent this difficulty, we introduce feature relevance network-based method, which focuses on interrelatedness among features. Feature relevance networks are connected graphs representing concurrency of the signalling devices such as APs. In the newly created relevance network, a test instance and the prototype of a location are expanded until convergence. The expansion cost corresponds to distance between the test instance and the prototype. Unlike other methods, our model is nonparametric making no assumptions about signal distributions. The proposed method is applied to the 2007 IEEE International Conference on Data Mining Data Mining Contest Task #2 (transfer learning), which is a typical example situation where the training and test datasets have been gathered during different periods. Using the proposed method, we accomplish the estimation accuracy of 0.3238, which is better than the best result of the contest.", "Sentiment analysis  Opinion mining  Information retrieval  Data mining In recent years, Structural Correspondence Learning (SCL) is regarded as one of the most promising techniques for transfer learning. The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features. However, SCL model treats each feature as well as each instance by an equivalent-weight strategy. From the perspective of feature, this strategy fails to overcome the adverse influence of high-frequency domain-specific (HFDS) features: they occupy a relative large portion of weight in classification model, while hardly carry corresponding sentiment information. From the other perspective, the equivalent-weight strategy of SCL model does not take into account the labels (positive or negative) of labeled instance and the labels of pivot features: positive pivot features tend to occur more frequently in positive instances and vice versa. To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances. More specifically, W-SCL assigns a smaller weight to HFDS features and assigns a larger weight to instances with the same label as the involved pivot feature. The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features. (C) 2011 Elsevier Ltd. All rights reserved.", "Transfer learning  Text classification  Feature selection  Feature subspace Transfer learning aims to solve the problem that the training data from a source domain and the test data from a target domain follow different distributions. The feature-based method and the case-based method have been widely used in transfer learning. In this paper we propose a knowledge transfer method based on feature representation mapping from the source domain to the target domain. We first construct a new feature subspace, then build a feature representation mapping function and re-weight the source domain and the target domain data to minimize the distance between different distributions. As a result, with the new feature representations in this subspace, we can apply standard machine learning methods to train classifier models in the source domain for use in the target domain. Importantly, different from many previously proposed methods, we combine the feature-based method and the case-based method to construct the knowledge transfer model for solving text classification problems. The experimental results show that our algorithm greatly improves the classification performance over the traditional learning algorithms. (C) 2011 Elsevier Ltd. All rights reserved.", " Transfer learning aims to solve new learning problems by extracting and making use of the common knowledge found in related domains. A key element of transfer learning is to identify structured knowledge to enable the knowledge transfer. Structured knowledge comes in different forms, depending on the nature of the learning problem and characteristics of the domains. In this article, we describe three of our recent works on transfer learning in a progressively more sophisticated order of the structured knowledge being transferred. We show that optimization methods and techniques inspired by the concerns of data reuse can be applied to extract and transfer deep structural knowledge between a variety of source and target problems. In our examples, this knowledge spans explicit data labels, model parameters, relations between data clusters, and relational action descriptions.", "Reinforcement learning  Actor-critic method  Transfer learning In this paper, we aim to accelerate learning processes in actor-critic method. We proposed the effective transfer learning method, which reduces training cycles by using information acquired from source tasks. The proposed method consists of two ideas, the method to select a policy to transfer, and the transfer method considering the characteristic of each actor-critic parameter set. The selection method aims to reduce redundant trial and error that are used in the selection phase and the training phase. We introduce the forbidden rule set, which are detected easily in the training phase, and concordance rate that measures an effectiveness of a source policy. The transfer method aims to merge a selected source policy to the target policy without negative transfers. it transfers only reliable action preferences and state values that implies preferred actions. We show the effectiveness of the proposed method by simple experiments. Agents found effective policies from, the database, and finished their training with less or same episodes than the original actor-critic method.", "Boosting  covariate shift  detector adaptiveness  object detection  transfer learning In object detection, disparities in distributions between the training samples and the test ones are often inevitable, resulting in degraded performance for application scenarios. In this paper, we focus on the disparities caused by viewpoint and scene changes and propose an efficient solution to these particular cases by adapting generic detectors, assuming boosting style. A pretrained boosting-style detector encodes a priori knowledge in the form of selected features and weak classifier weighting. Towards adaptiveness, the selected features are shifted to the most discriminative locations and scales to compensate for the possible appearance variations. Moreover, the weighting coefficients are further adapted with covariate boost, which maximally utilizes the related training data to enrich the limited new examples. Extensive experiments validate the proposed adaptation mechanism towards viewpoint and scene adaptiveness and show encouraging improvement on detection accuracy over state-of-the-art methods.", " Transfer learning has recently gained popularity due to the development of algorithms that can successfully generalize information across multiple tasks. This article focuses on transfer in the context of reinforcement learning domains, a general learning framework where an agent acts in an environment to maximize a reward signal. The goals of this article are to (1) familiarize readers with the transfer learning problem in reinforcement learning domains, (2) explain why the problem is both interesting and difficult, (3) present a selection of existing techniques that demonstrate different solutions, and (4) provide representative open problems in the hope of encouraging additional research in this exciting area.", " Sequential decision tasks present many opportunities for the study of transfer learning. A principal one among them is the existence of multiple domains that share the same underlying causal structure for actions. We describe an approach that exploits this shared causal structure to discover a hierarchical task structure in a source domain, which in turn speeds up learning of task execution knowledge in a new target domain. Our approach is theoretically justified and compares favorably to manually designed task hierarchies in learning efficiency in the target domain. We demonstrate that causally motivated task hierarchies transfer more robustly than other kinds of detailed knowledge that depend on the idiosyncrasies of the source domain and are hence less transferable.", " Case-based reasoning (CBR) is a problem-solving process in which a new problem is solved by retrieving a similar situation and reusing its solution. Transfer learning occurs when, after gaining experience from learning how to solve source problems, the same learner exploits this experience to improve performance and learning on target problems. In transfer learning, the differences between the source and target problems characterize the transfer distance. CBR can support transfer learning methods in multiple ways. We illustrate how CBR and transfer learning interact and characterize three approaches for using CBR in transfer learning: (1) as a transfer learning method, (2) for problem learning, and (3) to transfer knowledge between sets of problems. We describe examples of these approaches from our own and related work and discuss applicable transfer distances for each. We close with conclusions and directions for future research applying CBR to transfer learning.", " We report on a series of transfer learning experiments in game domains, in which we use structural analogy from one learned game to speed learning of another related game. We find that a major benefit of analogy is that it reduces the extent to which the source domain must be generalized before transfer. We describe two techniques in particular, minimal ascension and metamapping, that enable analogies to be drawn even when comparing descriptions using different relational vocabularies. Evidence for the effectiveness of these techniques is provided by a large-scale external evaluation, involving a substantial number of novel distant analogs.", " This article relates transfer learning to other AI research areas, envisions several new types of transfer learning capabilities, and suggests several new research directions to achieve them.", "Dimensionality reduction  domain adaptation  Hilbert space embedding of distributions  transfer learning Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.", "Object categorization  Randomized trees  Few examples  Interclass transfer  Transfer learning The human visual system is often able to learn to recognize difficult object categories from only a single view whereas automatic object recognition with few training examples is still a challenging task This is mainly due to the human ability to transfer knowledge from related classes Therefore an extension to Randomized Decision Trees is introduced for learning with very few examples by exploiting interclass relationships The approach consists of a maximum a posteriori estimation of classifier parameters using a prior distribution learned from similar object categories Experiments on binary and multiclass classification tasks show significant performance gains (C) 2010 Elsevier B V All rights reserved", " The main objective of transfer learning is to reuse knowledge acquired in a previous learned task, in order to enhance the learning procedure in a new and more complex task. Transfer learning comprises a suitable solution for speeding up the learning procedure in Reinforcement Learning tasks. In this work, we propose a novel method for transferring models to a hybrid reinforcement learning agent. The models of the transition and reward functions of a source task, will be transferred to a relevant but different target task. The learning algorithm of the target task's agent takes a hybrid approach, implementing both model-free and model-based learning, in order to fully exploit the presence of a model. The empirical evaluation, of the proposed approach, demonstrated significant results and performance improvements in the 3D Mountain Car task, by successfully using the models generated from the standard 2D Mountain Car.", " Recent advances in biomedical imaging have enabled the analysis of many different cell types. Learning-based cell detectors tend to be specific to a particular imaging protocol and cell type. For a new dataset, a tedious re-training process is required. In this paper, we present a novel method of training a cell detector on new datasets with minimal effort. First, we combine the classification rules extracted from existing data with the training samples of new data using transfer learning. Second, a global parameter is incorporated to refine the ranking of the classification rules. We demonstrate that our method achieves the same performance as previous approaches with only 10% of the training effort.", "Affective computing  arousal classification  individual differences  nearest neighbors classification  transfer learning Although psychophysiological and affective computing approaches may increase facility for development of the next generation of human-computer systems, the data resulting from research studies in affective computing include large individual differences. As a result, it is important that the data gleaned from an affective computing system be tailored for each individual user by re-tuning it using user-specific training examples. Given the often time-consuming and/or expensive nature of efforts to obtain such training examples, there is a need to either 1) minimize the number of user-specific training examples required  or 2) to maximize the learning performance through the incorporation of auxiliary training examples from other subjects. In [11] we have demonstrated an active class selection approach for the first purpose. Herein we use transfer learning to improve the learning performance by combining user-specific training examples with auxiliary training examples from other subjects, which are similar but not exactly the same as the user-specific training examples. We report results from an arousal classification application to demonstrate the effectiveness of transfer learning in a Virtual Reality Stroop Task designed to elicit varying levels of arousal.", "Transfer learning  domain adaptation  rule discovery Traditional classification methods in machine learning assume that training data and testing data should share the same feature space and have the same data distribution. In real world applications, however, this assumption often does not hold. If there are very few labeled instances in the target domain for training, it is time-consuming to label them manually. In this case, a source domain which has semantic relationships with the target domain but has the different feature space or distribution can be used to assist the classification. In this paper, we propose a new method using rules to help the domain adaptation, which can well represent the knowledge relationships between source domain and target domain. In this algorithm we first discover term-term rules according to the term relationships in target domain to build the knowledge bridge, then we reconstruct the source domain using these rules and get a better classifier to improve the cross-domain classification performance. We conduct several cross-domain data sets and demonstrate that the proposed method is easy to understand and it has a better performance compared to state-of-art transfer algorithms.", " This paper investigates the application of transductive transfer learning methods for action classification. The application scenario is that of off-line video annotation for retrieval. We show that if a classification system can analyze the unlabeled test data in order to adapt its models, a significant performance improvement can be achieved. We applied it for action classification in tennis games for train and test videos of different nature. Actions are described using HOG3D features and for transfer we used a method based on feature re-weighting and a novel method based on feature translation and scaling.", " We are organizing a competition on gesture recognition. This challenge is part of a series of challenges on the theme of unsupervised and transfer learning. The goal is to push the state of the art in algorithms capable of learning data representations, which may be re-used from task to task, using unlabeled data and/or labeled data from similar domains. In this challenge, the competitors will obtain for training a large database of videos of gestures from various gesture lexicons (religious emblems, sports referee signals, marshalling signals to guide vehicles or machineries, diving signals to communicate under water, signs from sign languages for the deaf and signs accompanying narratives of hearing people, etc.). They will then be tested on gestures from different domains and different gesture vocabularies, unknown in advance. The final test will be carried out in a live competition in which the systems will be demonstrated at the site of a conference during the summer 2012. We are holding a milestone event at the HCI workshop held in conjunction with ICCV 2011, where the organizers will demonstrate baseline systems and explain the competition protocol to encourage participation. Up to five competitors will demonstrate systems under development.", " One of the great open challenges in visual recognition is the ability to cope with unexpected stimuli. In this work, we present a technique to interpret detected anomalies and update the existing knowledge of normal situations. The addressed context is the analysis of human behavior in indoor surveillance scenarios, where new activities might need to be learned, once the system is already in operation. Our approach is based on human tracking with multiple activity trackers. The main contribution is to integrate a learning stage, where labeled and unlabeled information is collected and analyzed. To this end we develop a new multiclass version of transfer learning which requires minimal human interaction but still provides semantic labels of the new classes. The activity model is then updated with the new activities. Experiments show promising results.", "Machine learning  Transfer learning  Part-based modeling  Support vector machine Transfer learning, serving as one of the most popular theory in machine learning, has attracted a lot of attention recently. In this paper, we propose a new learning strategy called part-based transfer learning (pbTL), which is a process of parameter transfer. Dissimilar to many traditional works, we consider how to transfer the information from one task to another in the form of parts. We regard all the complex tasks as a collection of constituent parts and every task can be divided into several parts respectively. It means transfer learning between two complex tasks can be accomplished by sub-transfer learning tasks between their parts. Through developing it in this hierarchical fasion, we can reach a better outcome. Experiments on synthetic data with support vector machines (SVMs) validate the effectiveness of the proposed learning framework.", "KNN  data distribution  transfer learning  classification It is interesting and helpful to use the labeled data of some tasks to improve the classification performance of another task. This paper focuses on this issue and proposes an algorithm named SSDT (Synthetic Source Data Transfer). As the number of the training data influences the classification performance greatly, we create some synthetic training data using the source data and combine them with the target data to train a classifier. The classifier is applied to the target data, and experimental results show that SSDT improves the performance obviously.", " Because manual image annotation is both expensive and labor intensive, in practice we often do not have sufficient labeled images to train an effective classifier for the new image classification tasks. Although multiple labeled image data sets are publicly available for a number of computer vision tasks, a simple mixture of them cannot achieve good performance due to the heterogeneous properties and structures between different data sets. In this paper, we propose a novel nonnegative matrix tri-factorization based transfer learning framework, called as Dyadic Knowledge Transfer (DKT) approach, to transfer cross-domain image knowledge for the new computer vision tasks, such as classifications. An efficient iterative algorithm to solve the proposed optimization problem is introduced. We perform the proposed approach on two benchmark image data sets to simulate the real world cross-domain image classification tasks. Promising experimental results demonstrate the effectiveness of the proposed approach.", " Most feature selection methods for object tracking assume that the labeled samples obtained in the next frames follow the similar distribution with the samples in the previous frame. However, this assumption is not true in some scenarios. As a result, the selected features are not suitable for tracking and the drift problem happens. In this paper, we consider data's distribution in tracking from a new perspective. We classify the samples into three categories: auxiliary samples (samples in the previous frames), target samples (collected in the current frame) and unlabeled samples (obtained in the next frame). To make the best use of them for tracking, we propose a novel semi-supervised transfer learning approach. Specifically, we assume only target samples follow the same distribution as the unlabeled samples and develop a novel semi-supervised CovBoost method. It could utilize auxiliary samples and unlabeled samples effectively when training the best strong classifier for tracking. Furthermore, we develop a new online updating algorithm for semi-supervised CovBoost, making our tracker handle with significant variations of the tracked target and background successfully. We demonstrate the excellent performance of the proposed tracker on several challenging test videos.", " Our objective is transfer training of a discriminatively trained object category detector, in order to reduce the number of training images required. To this end we propose three transfer learning formulations where a template learnt previously for other categories is used to regularize the training of a new category. All the formulations result in convex optimization problems. Experiments (on PASCAL VOC) demonstrate significant performance gains by transfer learning from one class to another (e.g. motorbike to bicycle), including one-shot learning, specialization from class to a subordinate class (e.g. from quadruped to horse) and transfer using multiple components. In the case of multiple training samples it is shown that a detection performance approaching that of the state of the art can be achieved with substantially fewer training samples.", " In this paper, we propose a novel framework of style transfer matrix (STM) learning to reduce the writing style variation in handwriting recognition. After writer-specific style transfer learning, the data of different writers is projected onto a style-free space, where a writer independent classifier can yield high accuracy. We combine STM learning with a specific nearest prototype classifier: learning vector quantization (LVQ) with discriminative feature extraction (DFE), where both the prototypes and the subspace transformation matrix are learned via online discriminative learning. To adapt the basic classifier (trained with writer-independent data) to particular writers, we first propose two supervised models, one based on incremental learning and the other based on supervised STM learning. To overcome the lack of labeled samples for particular writers, we propose an unsupervised model to learn the STM using the self-taught strategy (also known as self-training). Experiments on a large-scale Chinese online handwriting database demonstrate that STM learning can reduce recognition errors significantly, and the unsupervised adaptation model performs even better than the supervised models.", " We propose a novel unsupervised transfer learning framework that utilises Unlabelled auxiliary data to quantify and select the most relevant transferrable knowledge for recognising a target object class from the background given very limited training target samples. Unlike existing transfer learning techniques, our method does not assume that auxiliary data are labelled, nor the relationships between target and auxiliary classes are known a priori. Our unsupervised transfer learning is formulated by a novel kernel adaptation transfer (KAT) learning framework, which aims to (a) extract general knowledge about how more structured objects are visually distinctive from cluttered background regardless object class, and (b) more importantly, perform selective transfer of knowledge extracted from the auxiliary data to minimise negative knowledge transfer suffered by existing methods. The effectiveness and efficiency of the proposed approach is demonstrated by performing one-class object recognition (object vs. background) task using the Caltech256 dataset.", " By extracting local spatial-temporal features from videos, many recently proposed approaches for action recognition achieve promising performance. The Bag-of-Words (BoW) model is commonly used in the approaches to obtain the video level representations. However, BoW model roughly assigns each feature vector to its closest visual word, therefore inevitably causing nontrivial quantization errors and impairing further improvements on classification rates. To obtain a more accurate and discriminative representation, in this paper, we propose an approach for action recognition by encoding local 3D spatial-temporal gradient features within the sparse coding framework. In so doing, each local spatial-temporal feature is transformed to a linear combination of a few atoms in a trained dictionary. In addition, we also investigate the construction of the dictionary under the guidance of transfer learning. We collect a large set of diverse video clips of sport games and movies, from which a set of universal atoms composed of the dictionary are learned by an online learning strategy. We test our approach on KTH dataset and UCF sports dataset. Experimental results demonstrate that our approach outperforms the state-of-art techniques on KTH dataset and achieves the comparable performance on UCF sports dataset.", " Effectively utilizing readily available auxiliary data to improve predictive performance on new modeling tasks is a key problem in data mining. In this research the goal is to transfer knowledge between sources of data, particularly when ground truth information for the new modeling task is scarce or is expensive to collect where leveraging any auxiliary sources of data becomes a necessity. Towards seamless knowledge transfer among tasks, effective representation of the data is a critical but yet not fully explored research area for the data engineer and data miner. Here we present a technique based on the idea of sparse coding, which essentially attempts to find an embedding for the data by assigning feature values based on subspace cluster membership. We modify the idea of sparse coding by focusing the identification of shared clusters between data when source and target data may have different distributions. In our paper, we point out cases where a direct application of sparse coding will lead to a failure of knowledge transfer. We then present the details of our extension to sparse coding, by incorporating distribution distance estimates for the embedded data, and show that the proposed algorithm can overcome the shortcomings of the sparse coding algorithm on synthetic data and achieve improved predictive performance on a real world chemical toxicity transfer learning task.", "Transfer Learning  Fuzzy Neural Network  Fuzzy Sets  Long Term Prediction  Bank Failure Prediction machine learning algorithms, which have been considered as robust methods in different computational fields, assume that the training and test data are drawn from the same distribution. This assumption may be violated in many real world applications like bank failure prediction because training and test data may come from different time periods or domains. An efficient novel algorithm known as Fuzzy Refinement (FR) is proposed in this paper to solve this problem and improve the performance. The algorithm utilizes the fuzzy system and similarity concept to modify the instances' labels in target domain which was initially predicted by shift-unaware Fuzzy Neural Network (FNN) proposed by [1]. The experiments are performed using bank failure financial data of United States to evaluate the algorithm performance. The results address a significant improvement in the predictive accuracy of FNN due to applying the proposed algorithm.", "transfer learning  adaboost  multi-view learning  classification Transfer learning, serving as one of the most important research directions in machine learning, has been studied in various fields in recent years. In this paper, we integrate the theory of multi-view learning into transfer learning and propose a new algorithm named Multi-View Transfer Learning with Adaboost (MV-TLAdaboost). Different from many previous works on transfer learning, we not only focus on using the labeled data from one task to help to learn another task, but also consider how to transfer them in different views synchronously. We regard both the source and target task as a collection of several constituent views and each of these two tasks can be learned from every views at the same time. Moreover, this kind of multi-view transfer learning is implemented with adaboost algorithm. Furthermore, we analyze the effectiveness and feasibility of MV-TLAdaboost. Experimental results also validate the effectiveness of our proposed approach.", "dimensionality reduction  Fisher discriminant analysis  transfer learning  brain-computer interface In transfer learning scenarios, previous discriminative dimensionality reduction methods tend to perform poorly owing to the difference between source and target distributions. In such cases, it is unsuitable to only consider discrimination in the low-dimensional source latent space since this would generalize badly to target domains. In this paper, we propose a new dimensionality reduction method for transfer learning scenarios, which is called transferable discriminative dimensionality reduction (TDDR). By resolving an objective function that encourages the separation of the domain-merged data and penalizes the distance between source and target distributions, we can find a low-dimensional latent space which guarantees not only the discrimination of projected samples, but also the transferability to enable later classification or regression models constructed in the source domain to generalize well to the target domain. In the experiments, we firstly analyze the perspective of transfer learning in brain-computer interface (BCI) research and then test TDDR on two real datasets from BCI applications. The experimental results show that the TDDR method can learn a low-dimensional latent feature space where the source models can perform well in the target domain.", "survey  collaborative filtering  transfer learning  cross-domain  recommender systems Cross-domain collaborative filtering (CF) is an emerging research topic in recommender systems. It aims to alleviate the sparsity problem in individual CF domains by transferring knowledge among related domains. In this paper, we will give a brief survey of the pilot studies in this research line in two dimensions: Collaborative Filtering Domains and Knowledge Transfer Styles. Some possible extensions for cross-domain CF will be discussed in the end.", "Machine Learning  Transfer Learning  Graph-based Model  Spectral Clustering Traditional data mining and machine learning technologies may fail when the training data and the testing data are drawn from different feature spaces and different distributions. Transfer learning, which uses the data from source domain and target domain, can tackle this problem. In this paper, we propose an improved Graph-based Model for Transfer learning (GM-Transfer). We construct a tripartite graph to represent the transfer learning problem and model the relations between the source domain data and the target domain data more efficiently. By learning the informational graph, the knowledge from the source domain data can be transferred to help improve the learning efficiency on the target domain data. Experiments show the effectiveness of our algorithm.", " Moving objects classification in traffic scene videos is a hot topic in recent years. It has significant meaning to intelligent traffic system by classifying moving traffic objects into pedestrians, motor vehicles, non-motor vehicles etc.. Traditional machine learning approaches make the assumption that source scene objects and target scene objects share same distributions, which does not hold for most occasions. Under this circumstance, large amount of manual labeling for target scene data is needed, which is time and labor consuming. In this paper, we introduce TrAdaBoost, a transfer learning algorithm, to bridge the gap between source and target scene. During training procedure, TrAdaBoost makes full use of the source scene data that is most similar to the target scene data so that only small number of labeled target scene data could help improve the performance significantly. The features used for classification are Histogram of Oriented Gradient features of the appearance based instances. The experiment results show the outstanding performance of the transfer learning method comparing with traditional machine learning algorithm.", " Transfer learning might be a promising approach to boost the learning of non-player characters' behaviors by exploiting some existing knowledge available from a different game. In this paper, we investigate how to transfer driving behaviors from The Open Racing Car Simulator (TORCS) to VDrift, which are two well known open-source racing games featuring rather different physics engines and game dynamics. We focus on a neuroevolution learning framework based on NEAT and compare three different methods of transfer learning: (i) transfer of the learned behaviors  (ii) transfer of the learning process  (iii) transfer of both the behaviors and the process. Our experimental analysis suggests that all the proposed methods of transfer learning might be effectively applied to boost the learning of driving behaviors in VDrift by exploiting the knowledge previously learned in TORCS. In particular, transferring both learned behaviors and learning process appears to be the best trade-off between the final performance and the computational cost.", " The vast majority of transfer learning methods proposed in the visual recognition domain over the last years addresses the problem of object category detection, assuming a strong control over the priors from which transfer is done. This is a strict condition, as it concretely limits the use of this type of approach in several settings: for instance, it does not allow in general to use off-the-shelf models as priors. Moreover, the lack of a multiclass formulation for most of the existing transfer learning algorithms prevents using them for object categorization problems, where their use might be beneficial, especially when the number of categories grows and it becomes harder to get enough annotated data for training standard learning methods. This paper presents a multiclass transfer learning algorithm that allows to take advantage of priors built over different features and with different learning methods than the one used for learning the new task. We use the priors as experts, and transfer their outputs to the new incoming samples as additional information. We cast the learning problem within the Multi Kernel Learning framework. The resulting formulation solves efficiently a joint optimization problem that determines from where and how much to transfer, with a principled multiclass formulation. Extensive experiments illustrate the value of this approach.", "Transfer Learning  Policy Homomorphism  Reinforcement Learning A life-long learning agent must have the ability to learn new tasks, adapt the policies of already learned tasks, and extract and reuse knowledge from previous tasks for future use. To do the latter, it needs methods that can autonomously identify, categorize and generalize control and representational knowledge. This paper presents a novel approach to achieve this by combining the policy homomorphism framework with a utility criterion to autonomously identify task types, categorize situation-specific policy instances into these types, and generalize the policies into a single abstract policy for each identified task type. The capabilities of this approach to identify, categorize, and generalize skills, as well as the potential benefit of reuse of the abstracted policies for the learning of new tasks is demonstrated in a grid world domain.", " An instance-weighted variant of the support vector machine (SVM) has attracted considerable attention recently since they are useful in various machine learning tasks such as non-stationary data analysis, heteroscedastic data modeling, transfer learning, learning to rank, and transduction. An important challenge in these scenarios is to overcome the computational bottleneck-instance weights often change dynamically or adaptively, and thus the weighted SVM solutions must be repeatedly computed. In this paper, we develop an algorithm that can efficiently and exactly up-date the weighted SVM solutions for arbitrary change of instance weights. Technically, this contribution can be regarded as an extension of the conventional solution-path algorithm for a single regularization parameter to multiple instance-weight parameters. However, this extension gives rise to a significant problem that breakpoints (at which the solution path turns) have to be identified in high-dimensional space. To facilitate this,we introduce a parametric representation of instance weights which allows us to find the breakpoints in high-dimensional space easily. Despite its simplicity, our parametrization covers various important machine learning tasks and it widens the applicability of the solution-path algorithm. Through extensive experiments on various practical applications, we demonstrate the usefulness of the proposed algorithm", " Classifying new unseen object classes has become a popular topic of research in the computer-vision and robotics community. Coping with this problem requires determining the attributes shared among objects and transferring them for use in classifying unseen object classes. Nevertheless, most current state-of-the-art methods require a fully offline training process and take a very long time for the batch training process, which renders them inapplicable for use in online applications such as robotics. This study proposes a novel online and incremental approach for learning and transferring the learned attributes in order to classify another disjoint set of image classes. Among three methods proposed in this paper, a method combining those favorable features of a self-organizing incremental neural network (SOINN) and a support vector machine (SVM) achieves the best performance. This method, called the Alt-SOINN-SVM, can run online incrementally, similar to an SOINN, and perform accurate classification, similar to an SVM. An evaluation was performed with 50 classes of an animal with an attributes dataset (>30,000 images). The results shows that despite the great reduction in both learning time (92.25% reduction) and classification time (99.87% reduction), and possessing the ability for incremental learning on gradually obtained samples, the proposed method offers reasonably good accuracy for classification. Furthermore, the proposed methods are applicable to use with the increasing number of attribute which improves the accuracy gradually and incrementally.", " We organized a data mining challenge in unsupervised and transfer learning (the UTL challenge), in collaboration with the DARPA Deep Learning program. The goal of this year's challenge was to learn good data representations that can be re-used across tasks by building models that capture regularities of the input space. The representations provided by the participants were evaluated by the organizers on supervised learning target tasks, which were unknown to the participants. In a first phase of the challenge, the competitors were given only unlabeled data to learn their data representation. In a second phase of the challenge, the competitors were also provided with a limited amount of labeled data from source tasks, distinct from the target tasks. We made available large datasets from various application domains: handwriting recognition, image recognition, video processing, text processing, and ecology. The results indicate that learned data representation yield results significantly better than what can be achieved with raw data or data preprocessed with standard normalizations and functional transforms. The UTL challenge is part of the IJCNN 2011 competition program(1). The website of the challenge remains open for submission of new methods beyond the termination of the challenge as a resource for students and researchers(2).", " A neural net can learn to discriminate among a set of classes without explicitly training to do so. It does not even need exposure to any instances of those classes. The learning occurs while the net is being trained to discriminate among a set of related classes. This form of transfer learning is referred to as 'Latent Learning' by psychologists, because until specifically elicited, the knowledge remains latent. Evidence that latent learning has occurred lies in the existence of consistent, unique responses to the unseen classes. Standard supervised learning can improve the accuracy of those responses with exceedingly small sets of labeled images. In this paper, we use a convolutional neural net (CNN) to demonstrate not only a method of determining a net's latent responses, but also simple ways to optimize latent learning. Additionally, we take advantage of the fact that CNN's are deep nets in order to show how the latently learned accuracy of the CNN may be greatly improved by allowing only its output layer to train. We compare our results both to those obtained with standard backpropagation training of the CNN on small datasets without any transfer learning and to a related set of current published results.", "Algorithms  Classification  feature selection  transfer learning  spatial data mining  planetary and space science Counting craters in remotely sensed images is the only tool that provides relative dating of remote planetary surfaces. Surveying craters requires counting a large amount of small subkilometer craters, which calls for highly efficient automatic crater detection. In this article, we present an integrated framework on autodetection of subkilometer craters with boosting and transfer learning. The framework contains three key components. First, we utilize mathematical morphology to efficiently identify crater candidates, the regions of an image that can potentially contain craters. Only those regions occupying relatively small portions of the original image are the subjects of further processing. Second, we extract and select image texture features, in combination with supervised boosting ensemble learning algorithms, to accurately classify crater candidates into craters and noncraters. Third, we integrate transfer learning into boosting, to enhance detection performance in the regions where surface morphology differs from what is characterized by the training set. Our framework is evaluated on a large test image of 37, 500 x 56, 250 m(2) on Mars, which exhibits a heavily cratered Martian terrain characterized by nonuniform surface morphology. Empirical studies demonstrate that the proposed crater detection framework can achieve an F1 score above 0.85, a significant improvement over the other crater detection algorithms.", "Active class selection  active learning  affective computing  arousal classification  nearest neighbors classification  transfer learning Active class selection (ACS) studies how to optimally select the classes to obtain training examples so that a good classifier can be constructed from a small number of training examples. It is very useful in situations where the class labels need to be determined before the training examples and features can be obtained. For example, in many emotion classification problems, the emotion (class label) needs to be specified before the corresponding responses can be generated and recorded. However, there has been very limited research on ACS, and to the best knowledge of the authors, ACS has not been introduced to the affective computing community. In this paper, we compare two ACS approaches in an arousal classification application. Experimental results using a kNN classifier show that one of them almost always results in higher classification accuracy than a uniform sampling approach. We expect that ACS, together with transfer learning, will greatly reduce the data acquisition effort to customize an affective computing system.", " Activity videos are widespread on the Internet but current video search is limited to text tags due to limitations in recognition systems. One of the main reasons for this limitation is the wide variety of activities users could query. Thus codifying knowledge for all queries becomes problematic. Relevance Feedback (RP) is a retrieval framework that addresses this issue via interactive feedback with the user during the search session. An added benefit is that RF can also learn the subjective component of a user's search preferences. However for good retrieval performance, RF may require a large amount of user feedback for activity search. We address this issue by introducing Transfer Learning (TL) into RF. With TL, we can use auxiliary data from known classification problems different from the user's target query to decrease the needed amount of user feedback. We address key issues in integrating RF and TL and demonstrate improved performance on the challenging YouTube Action Dataset(1).", "Multi-agent reinforcement learning  Agent coordination  Transfer learning Transfer learning leverages an agent's experience in a source task in order to improve its performance in a related target task. Recently, this technique has received attention in reinforcement learning settings. Training a reinforcement learning agent on a suitable source task allows the agent to reuse this experience to significantly improve performance on more complex target problems. Currently, reinforcement learning transfer approaches focus almost exclusively on speeding up learning in single agent systems. In this paper we investigate the potential of applying transfer learning to the problem of agent coordination in multi-agent systems. The idea underlying our approach is that agents can determine how to deal with the presence of other agents in a relatively simple training setting. By then generalizing this knowledge, the agents can use this experience to speed up learning in more complex multi-agent learning tasks.", "Transfer learning  AdaBoost  TrAdaBoost  Weighted Majority Algorithm Instance-based transfer learning methods utilize labeled examples from one domain to improve learning performance in another domain via knowledge transfer. Boosting-based transfer learning algorithms are a subset of such methods and have been applied successfully within the transfer learning community. In this paper, we address some of the weaknesses of such algorithms and extend the most popular transfer boosting algorithm, TrAdaBoost. We incorporate a dynamic factor into TrAdaBoost to make it meet its intended design of incorporating the advantages of both AdaBoost and the Weighted Majority Algorithm. We theoretically and empirically analyze the effect of this important factor on the boosting performance of TrAdaBoost and we apply it as a correction factor that significantly improves the classification performance. Our experimental results on several real-world datasets demonstrate the effectiveness of our framework in obtaining better classification results.", "Gaussian processes  multi-task learning  asymmetric setting  negative transfer Given a learning task for a data set, learning it together with related tasks (data sets) can improve performance. Gaussian process models have been applied to such multi-task learning scenarios, based on joint priors for functions underlying the tasks. In previous Gaussian process approaches, all tasks have been assumed to be of equal importance, whereas in transfer learning the goal is asymmetric: to enhance performance on a target task given all other tasks. In both settings, transfer learning and joint modelling, negative transfer is a key problem: performance may actually decrease if the tasks are not related closely enough. In this paper, we propose a Gaussian process model for the asymmetric setting, which learns to explain away non-related variation in the additional tasks, in order to focus on improving performance on the target task. In experiments, our model improves performance compared to single-task learning, symmetric multi-task learning using hierarchical Dirichlet processes, and transfer learning based on predictive structure learning.", "transfer learning  multitask learning  regularization The success of regularized risk minimization approaches to classification with linear models depends crucially on the selection of a regularization term that matches with the learning task at hand. If the necessary domain expertise is rare or hard to formalize, it may be difficult to find a good regularizer. On the other hand, if plenty of related or similar data is available, it is a natural approach to adjust the regularizer for the new learning problem based on the characteristics of the related data. In this paper, we study the problem of obtaining good parameter values for a l(2)-style regularizer with feature weights. We analytically investigate a moment-based method to obtain good values and give uniform convergence bounds for the prediction error on the target learning task. An empirical study shows that the approach can improve predictive accuracy considerably in the application domain of text classification.", " Transfer learning techniques have witnessed a significant development in real applications where the knowledge from previous tasks are required to reduce the high cost of inquiring the labeled information for the target task. However, how to avoid negative transfer which happens due to different distributions of tasks in heterogeneous environment is still a open problem. In order to handle this kind of issue, we propose a Compact Coding method for Hyperplane Classifiers (CCHC) under a two-level framework in inductive transfer learning setting. Unlike traditional methods, we measure the similarities among tasks from the macro level perspective through minimum encoding. Particularly speaking, the degree of the similarity is represented by the relevant code length of the class boundary of each source task with respect to the target task. In addition, informative parts of the source tasks are adaptively selected in the micro level viewpoint to make the choice of the specific source task more accurate. Extensive experiments show the effectiveness of our algorithm in terms of the classification accuracy in both UCI and text data sets.", " Common assumption in most machine learning algorithms is that, labeled (source) data and unlabeled (target) data are sampled from the same distribution. However, many real world tasks violate this assumption: in temporal domains, feature distributions may vary over time, clinical studies may have sampling bias, or sometimes sufficient labeled data for the domain of interest does not exist, and labeled data from a related domain must be utilized. In such settings, knowing in which dimensions source and target data vary is extremely important to reduce the distance between domains and accurately transfer knowledge. In this paper, we present a novel method to identify variant and invariant features between two datasets. Our contribution is two fold: First, we present a novel transfer learning approach for domain adaptation, and second, we formalize the problem of finding differently distributed features as a convex optimization problem. Experimental studies on synthetic and benchmark real world datasets show that our approach outperform other transfer learning approaches, and it aids the prediction accuracy significantly.", "Good Similarity Functions  Transfer Learning  Domain Adaptation  image Classification Similarity functions are widely used in many machine learning or pattern recognition tasks. We consider here a recent framework for binary classification, proposed by Balcan et al., allowing to learn in a potentially non geometrical space based on good similarity functions. This framework is a generalization of the notion of kernels used in support vector machines in the sense that allows one to use similarity functions that do not need to be positive semi-definite nor symmetric. The similarities are then used to define an explicit projection space where a linear classifier with good generalization properties can be learned. In this paper, we propose to study experimentally the usefulness of similarity based projection spaces for transfer learning issues. More precisely, we consider the problem of domain adaptation where the distributions generating learning data and test data are somewhat different. We stand in the case where no information on the test labels is available. We show that a simple renormalization of a good similarity function taking into account the test data allows us to learn classifiers more performing on the target distribution for difficult adaptation problems. Moreover, this normalization always helps to improve the model when we try to regularize the similarity based projection space in order to move closer the two distributions. We provide experiments on a toy problem and on a real image annotation task.", "Good Similarity Functions  Transfer Learning  Domain Adaptation  Image Classification Similarity functions are widely used in many machine learning or pattern recognition tasks. We consider here a recent framework for binary classification, proposed by Balcan et al., allowing to learn in a potentially non geometrical space based on good similarity functions. This framework is a generalization of the notion of kernels used in support vector machines in the sense that allows one to use similarity functions that do not need to be positive semi-definite nor symmetric. The similarities are then used to define an explicit projection space where a linear classifier with good generalization properties can be learned. In this paper, we propose to study experimentally the usefulness of similarity based projection spaces for transfer learning issues. More precisely, we consider the problem of domain adaptation where the distributions generating learning data and test data are somewhat different. We stand in the case where no information on the test labels is available. We show that a simple renormalization of a good similarity function taking into account the test data allows us to learn classifiers more performing on the target distribution for difficult adaptation problems. Moreover, this normalization always helps to improve the model when we try to regularize the similarity based projection space in order to move closer the two distributions. We provide experiments on a toy problem and on a real image annotation task.", "Abstraction  knowledge transfer  reinforcement learning  transfer learning  robot navigation In this article we investigate the role of abstraction principles for knowledge transfer in agent control learning tasks. We analyze abstraction from a formal point of view and characterize three distinct facets: aspectualization, coarsening, and conceptual classification. The taxonomy we develop allows us to interrelate existing approaches to abstraction, leading to a code of practice for designing knowledge representations that support knowledge transfer. We detail how aspectualization can be utilized to achieve knowledge transfer in reinforcement learning. We propose the use of so-called structure space aspectualizable knowledge representations that explicate structural properties of the state space and present a posteriori structure space aspectualization (APSST) as a method to extract generally sensible behavior from a learned policy. This new policy can be used for knowledge transfer to support learning new tasks in different environments. Finally, we present a case study that demonstrates transfer of generally sensible navigation skills from simple simulation to a real-world robotic platform.", "Transfer learning  Genetic algorithms  Learning to learn Transfer learning is a method which aims to improve related tasks performance. Transfer learning tries to use information gained from related tasks solutions to improve performance of learning strategy. Transfer learning addresses the problem of how to utilize plenty of labeled data in a source domain to solve related but different problems in a target domain, even when the training and testing problems have different distributions or features (Pan, Kwok, & Yang, 2008). In this paper we have used transfer learning to improve performance of genetic algorithms. (C) 2010 Elsevier Ltd. All rights reserved.", "Transfer learning  survey  machine learning  data mining A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.", "Concept drift  Transfer learning  Prior estimation Data stream classification is a hot topic in data mining research. The great challenge is that the class priors may evolve along the data sequence. Algorithms have been proposed to estimate the dynamic class priors and adjust the classifier accordingly. However, the existing algorithms do not perform well on prior estimation due to the lack of samples from the target distribution. Sample size has great effects in parameter estimation and small-sample effects greatly contaminate the estimation performance. In this paper, we propose a novel parameter estimation method called transfer estimation. Transfer estimation makes use of samples not only from the target distribution but also from similar distributions. We apply this new estimation method to the existing algorithms and obtain an improved algorithm. Experiments on both synthetic and real data sets show that the improved algorithm outperforms the existing algorithms on both class prior estimation and classification. (C) 2010 Elsevier Ltd. All rights reserved.", "Boosting  Multi-task learning  Inductive transfer learning  Multiple tasks  Text classification Boosting has become one of the state-of-the-art techniques in many supervised learning and semi-supervised learning applications. In this paper, we develop a novel boosting algorithm, MTBoost, for multi-task learning problem. Many previous multi-task learning algorithms can only solve the problem in low or moderate dimensional space. However, the MTBoost algorithm is capable of working for very high dimensional data such as in text mining where the feature number is beyond several 10,000. The experimental results illustrate that the MTBoost algorithm provides significantly better classification performance than supervised single task learning algorithms. Moreover. MTBoost outperforms some other typical multi-task learning methods. (C) 2010 Elsevier B.V. All rights reserved.", "Reinforcement learning  Transfer learning  Policy Reuse Policy Reuse is a reinforcement learning technique that efficiently learns a new policy by using past similar learned policies. The Policy Reuse learner improves its exploration by probabilistically including the exploitation of those past policies. Policy Reuse was introduced, and its effectiveness was previously demonstrated, in problems with different reward functions in the same state and action spaces. In this article, we contribute Policy Reuse as transfer learning among different domains. We introduce extended Markov Decision Processes (MDPs) to include domains and tasks, where domains have different state and action spaces, and tasks are problems with different rewards within a domain. We show how Policy Reuse can be applied among domains by defining and using a mapping between their state and action spaces. We use several domains, as versions of a simulated RoboCup Keepaway problem, where we show that Policy Reuse can be used as a mechanism of transfer learning significantly outperforming a basic policy learner. (C) 2010 Elsevier B.V. All rights reserved.", "Data mining  transfer learning  cross-domain  text classification  Wikipedia A major problem of classification learning is the lack of ground-truth labeled data. It is usually expensive to label new data instances for training a model. To solve this problem, domain adaptation in transfer learning has been proposed to classify target domain data by using some other source domain data, even when the data may have different distributions. However, domain adaptation may not work well when the differences between the source and target domains are large. In this paper, we design a novel transfer learning approach, called BIG (Bridging Information Gap), to effectively extract useful knowledge in a worldwide knowledge base, which is then used to link the source and target domains for improving the classification performance. BIG works when the source and target domains share the same feature space but different underlying data distributions. Using the auxiliary source data, we can extract a bridge that allows cross-domain text classification problems to be solved using standard semisupervised learning algorithms. A major contribution of our work is that with BIG, a large amount of worldwide knowledge can be easily adapted and used for learning in the target domain. We conduct experiments on several real-world cross-domain text classification tasks and demonstrate that our proposed approach can outperform several existing domain adaptation approaches significantly.", "Transfer learning  Topographic subspace model  Sparse representation  Image classification  Image retrieval In this paper, we propose a topographic subspace learning algorithm, named key-coding learning, which utilizes irrelevant unlabeled auxiliary data to facilitate image classification and retrieval tasks. It is worth noticing that we do not need to assume the auxiliary data follows the same class labels or generative distribution as the target training data. Firstly, the subspace model is learnt from enormous scale- and rotation-invariant SURF descriptors extracted from auxiliary and training images, which makes model insensitive to geometric and photometric image transformation. Then the bases of model are pooled by clustering to generate topographic basis banks. We provide insights to show that the topographic model is highly biologically plausible in simulating the complex cells in the visual cortex. Finally we generate the succinct sparse representations by mapping target data into this topographic model. Due to the capability of transferring knowledge, the proposed topographic subspace model can effectively address insufficient training data problem for image classification and is also helpful for generating discriminative features for image retrieval. Intensive experiments are conducted on three image datasets to evaluate the performance of our proposed model, the experimental results are encouraging and promising. (C) 2010 Elsevier B.V. All rights reserved.", "Domain adaptation  transfer learning  semi-supervised learning  support vector machines  accuracy assessment  validation strategy This paper addresses pattern classification in the framework of domain adaptation by considering methods that solve problems in which training data are assumed to be available only for a source domain different (even if related) from the target domain of (unlabeled) test data. Two main novel contributions are proposed: 1) a domain adaptation support vector machine (DASVM) technique which extends the formulation of support vector machines (SVMs) to the domain adaptation framework and 2) a circular indirect accuracy assessment strategy for validating the learning of domain adaptation classifiers when no true labels for the target-domain instances are available. Experimental results, obtained on a series of two-dimensional toy problems and on two real data sets related to brain computer interface and remote sensing applications, confirmed the effectiveness and the reliability of both the DASVM technique and the proposed circular validation strategy.", "transfer learning  task transfer  evolutionary computation  neuroevolution  indirect encoding An important goal for machine learning is to transfer knowledge between tasks. For example, learning to play RoboCup Keepaway should contribute to learning the full game of RoboCup soccer. Previous approaches to transfer in Keepaway have focused on transforming the original representation to fit the new task. In contrast, this paper explores the idea that transfer is most effective if the representation is designed to be the same even across different tasks. To demonstrate this point, a bird's eye view (BEV) representation is introduced that can represent different tasks on the same two-dimensional map. For example, both the 3 vs. 2 and 4 vs. 3 Keepaway tasks can be represented on the same BEV. Yet the problem is that a raw two-dimensional map is high-dimensional and unstructured. This paper shows how this problem is addressed naturally by an idea from evolutionary computation called indirect encoding, which compresses the representation by exploiting its geometry. The result is that the BEV learns a Keepaway policy that transfers without further learning or manipulation. It also facilitates transferring knowledge learned in a different domain, Knight Joust, into Keepaway. Finally, the indirect encoding of the BEV means that its geometry can be changed without altering the solution. Thus static representations facilitate several kinds of transfer.", "Online learning  Domain adaptation  Classifier combination  Transfer learning  Multi-task learning State-of-the-art statistical NLP systems for a variety of tasks learn from labeled training data that is often domain specific. However, there may be multiple domains or sources of interest on which the system must perform. For example, a spam filtering system must give high quality predictions for many users, each of whom receives emails from different sources and may make slightly different decisions about what is or is not spam. Rather than learning separate models for each domain, we explore systems that learn across multiple domains. We develop a new multi-domain online learning framework based on parameter combination from multiple classifiers. Our algorithms draw from multi-task learning and domain adaptation to adapt multiple source domain classifiers to a new target domain, learn across multiple similar domains, and learn across a large number of disparate domains. We evaluate our algorithms on two popular NLP domain adaptation tasks: sentiment classification and spam filtering.", "Domain adaptation  Transfer learning  Learning theory  Sample-selection bias Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. Often, however, we have plentiful labeled training data from a source domain but wish to learn a classifier which performs well on a target domain with a different distribution and little or no labeled training data. In this work we investigate two questions. First, under what conditions can a classifier trained from source data be expected to perform well on target data? Second, given a small amount of labeled target data, how should we combine it during training with the large amount of labeled source data to achieve the lowest target error at test time? We address the first question by bounding a classifier's target error in terms of its source error and the divergence between the two domains. We give a classifier-induced divergence measure that can be estimated from finite, unlabeled samples from the domains. Under the assumption that there exists some hypothesis that performs well in both domains, we show that this quantity together with the empirical source error characterize the target error of a source-trained classifier. We answer the second question by bounding the target error of a model which minimizes a convex combination of the empirical source and target errors. Previous theoretical work has considered minimizing just the source error, just the target error, or weighting instances from the two domains equally. We show how to choose the optimal combination of source and target error as a function of the divergence, the sample sizes of both domains, and the complexity of the hypothesis class. The resulting bound generalizes the previously studied cases and is always at least as tight as a bound which considers minimizing only the target error or an equal weighting of source and target errors.", "Inductive transfer  Bayesian networks  Structure learning  Parameter learning In several domains it is common to have data from different, but closely related problems. For instance, in manufacturing, many products follow the same industrial process but with different conditions  or in industrial diagnosis, where there is equipment with similar specifications. In these cases it is common to have plenty of data for some scenarios but very little for others. In order to learn accurate models for rare cases, it is desirable to use data and knowledge from similar cases  a technique known as transfer learning. In this paper we propose an inductive transfer learning method for Bayesian networks, that considers both structure and parameter learning. For structure learning we use conditional independence tests, by combining measures from the target task with those obtained from one or more auxiliary tasks, using a novel weighted sum of the conditional independence measures. For parameter learning, we propose two variants of the linear pool for probability aggregation, combining the probability estimates from the target task with those from the auxiliary tasks. To validate our approach, we used three Bayesian networks models that are commonly used for evaluating learning techniques, and generated variants of each model by changing the structure as well as the parameters. We then learned one of the variants with a small dataset and combined it with information from the other variants. The experimental results show a significant improvement in terms of structure and parameters when we transfer knowledge from similar tasks. We also evaluated the method with real-world data from a manufacturing process considering several products, obtaining an improvement in terms of log-likelihood between the data and the model when we do transfer learning from related products.", " Transfer learning allows leveraging the knowledge of source domains, available a priori, to help training a classifier for a target domain, where the available data is scarce. The effectiveness of the transfer is affected by the relationship between source and target. Rather than improving the learning, brute force leveraging of a source poorly related to the target may decrease the classifier performance. One strategy to reduce this negative transfer is to import knowledge from multiple sources to increase the chance of finding one source closely related to the target. This work extends the boosting framework for transferring knowledge from multiple sources. Two new algorithms, MultiSource-TrAdaBoost, and TaskTrAdaBoost, are introduced, analyzed, and applied for object category recognition and specific object detection. The experiments demonstrate their improved performance by greatly reducing the negative transfer as the number of sources increases. TaskTrAdaBoost is a fast algorithm enabling rapid retraining over new targets.", " For object category recognition to scale beyond a small number of classes, it is important that algorithms be able to learn from a small amount of labeled data per additional class. One-shot recognition aims to apply the knowledge gained from a set of categories with plentiful data to categories for which only a single exemplar is available for each. As with earlier efforts motivated by transfer learning, we seek an internal representation for the domain that generalizes across classes. However, in contrast to existing work, we formulate the problem in a fundamentally new manner by optimizing the internal representation for the one-shot task using the notion of micro-sets. A micro-set is a sample of data that contains only a single instance of each category, sampled from the pool of available data, which serves as a mechanism to force the learned representation to explicitly address the variability and noise inherent in the one-shot recognition task. We optimize our learned domain features so that they minimize an expected loss over micro-sets drawn from the training set and show that these features generalize effectively to previously unseen categories. We detail a discriminative approach for optimizing one-shot recognition using micro-sets and present experiments on the Animals with Attributes and Caltech-101 datasets that demonstrate the benefits of our formulation.", " Psychologists define latent learning as learning that occurs without task-specific reinforcement and is not demonstrated until needed. Since this knowledge is acquired while mastering some other task(s), it is a form of transfer learning. We utilize latent learning to enable to deep neural net to distinguish among a set of handwritten numerals. The accuracies obtained are compared to those achievable with a simplistic 'group-mean' classification technique, which is explained later in this paper. The deep neural net architecture used was a Lc-Net 5 [3] convolutional neural net with only minor differences in the output layer.", " Transfer learning is a new learning paradigm, in which, besides the training data for the targeted learning task, data that are related to the task (often under a different distribution) are also employed to help train a better learner. For example, out-dated data can be used as such related data. In this paper, we propose a new transfer learning framework for training neural network (NN) ensembles. The framework has two key features: 1) it uses the well-known negative correlation learning to train an ensemble of diverse neural networks from the related data, fully discovering the knowledge in the data  and 2) a penalized incremental learning scheme is used to adapt the neural networks obtained from negative correlation learning to the training data for the targeted learning task. The adaptation is guided by reference neural networks that measure the relatedness between the training and the related data. Experiments on benchmark data sets show that our framework can achieve classification accuracy competitive to existing ensemble transfer learning methods such as TrAdaBoost [1] and TrBagg [2]. We discuss some characteristics of our framework observed in the experiment and the scenarios under which the framework may have superior performance.", " In this paper we survey the basics of reinforcement learning, generalization and abstraction. We start with an introduction to the fundamentals of reinforcement learning and motivate the necessity for generalization and abstraction. Next we summarize the most important techniques available to achieve both generalization and abstraction in reinforcement learning. We discuss basic function approximation techniques and delve into hierarchical, relational and transfer learning. All concepts and techniques are illustrated with examples.", "Kernel Learning  Maximum Mean Discrepancy When training and testing data are drawn from different distributions, the performance of the classification model will be low. Such a problem usually comes from sample selection bias or transfer learning scenarios. In this paper, we propose a novel multiple kernel learning framework improved by Maximum Mean Discrepancy (MMD) to solve the problem. This new model not only utilizes the capacity of kernel learning to construct a nonlinear hyperplane which maximizes the separation margin, but also reduces the distribution discrepancy between training and testing data simultaneously, which is measured by MMD. This approach is formulated as a bi-objective optimization problem. Then an efficient optimization algorithm based on gradient descent and quadratic programming [13] is adopted to solve it. Extensive experiments on UCI and text datasets show that the proposed model outperforms traditional multiple kernel learning model in sample selection bias and transfer learning scenarios.", "transductive transfer learning  classification  multiple sources  logistic regression Recent years have witnessed the increasing interest in transfer learning. And transdactive transfer learning from multiple source domains is one of the important topics in transfer learning. In this paper, we also address this issue. However, a new method, namely TTLRM (Transductive Transfer based on Logistic Regression from Multi-sources) is proposed to address transductive transfer learning from multiple sources to one target domain. In term of logistic regression, TTLRM estimates the data distribution difference in different domains to adjust the weights of instances, and then builds a model using these re-weighted data. This is beneficial to adapt to the target domain. Experimental results demonstrate that our method outperforms the traditional supervised learning methods and some transfer learning methods.", "Transfer learning  Support vector machine  Principal curve One of the basic ideas of present transfer learning algorithms is to find a common underlying representation among multiple related tasks as a bridge of transferring knowledge. Different with most transfer learning algorithms which are designed to solve classification problems, a new algorithm is proposed in this paper to solve multiple regression tasks. First, based on self-consistency of principal curves, this algorithm utilizes non-parametric approach to find the principal curve passing through data sets of all tasks. We treat this curve as common-across-tasks representation. Second, the importance of every sample in target task is determined by computing the deviation from the principal curve and finally the weighted support vector regression is used to obtain a regression model. We simulate multiple related regression tasks using noisy Sine data sets with various intensities and report experiments which demonstrate that the proposed algorithm can draw the useful information of multiple tasks and dramatically improve the performance relative to learning target task independently. Furthermore, we replace principal curve with support vector regression with model selection to find common representation and show the comparative results of these two algorithms.", "Multi-agent systems  reinforcement learning  transfer learning A major challenge in multi-agent reinforcement learning remains dealing with the large state spaces typically associated with realistic multi-agent systems. As the state space grows, agent policies become increasingly complex and learning slows down. Currently, advanced single-agent techniques are already very capable of learning optimal policies in large unknown environments. When multiple agents are present however, we are challenged by an increase of the state-action space, exponential in the number of agents, even though these agents do not always interfere with each other and thus their presence should not always be included in the state information of the other agent. A solution to this problem lies in the use of generalized learning automata (GLA). In this paper we will first demonstrate how GLA can help take the correct actions in large unknown multi-agent environments. Furthermore we introduce a framework capable of dealing with this issue of observing other agents. We also present an implementation of our framework, called 2observe which we apply to some gridworld problems. Finally, we demonstrate that our approach is capable of transferring its knowledge to new agents entering the environment.", " Within the past few decades, many researches on intelligent sensory evaluation for product design mainly focused on improving machine learning to build single classification model, such as neural networks, decision tree and etc. Because labeling the product research data is expensive, training data often has characteristics of high-dimensional, imbalance and small sample. Traditional machine learning is difficult to construct single high-quality model, and can't reasonable use the knowledge learned from the different-distribution data in isomorphism space. In this paper, we present a novel modeling method named CFS-TrDataBoostIM. Experiments demonstrate the effectiveness of this method on sensory evaluation model of tobacco blends. We show that this method can reduce dimensions of features, and decrease the complexity of the model and learning error of minority class. In comparison with other ensemble learning and transfer learning algorithms, the results prove that our approach transfers the knowledge from the sensory evaluation data of tobacco leaves as auxiliary training data, and the sensory evaluation model of tobacco blends produces high predictive accuracy.", "incremental SVM  car detection  constraint training  incremental re-training  transfer learning In image classification problems, changes in imaging conditions such as lighting, camera position, etc. can strongly affect the performance of trained support vector machine (SVM) classifiers. For instance, SVMs trained using images obtained during daylight can perform poorly when used to classify images taken at night. In this paper, we investigate the use of incremental learning to efficiently adapt SVMs to classify the same class of images taken under different imaging conditions. A two-stage algorithm to adapt SVM classifiers was developed and applied to the car detection problem when imaging conditions changed such as changes in camera location and for the classification of car images obtained during day and night times. A significant improvement in the classification performance was achieved with re-trained SVMs as compared to that of the original SVMs without adaptation.", " We study the problem of multimodal dimensionality reduction assuming that data samples can be missing at training time, and not all data modalities may be present at application time. Maximum covariance analysis, as a generalization of PGA, has many desirable properties, but its application to practical problems is limited by its need for perfectly paired data. We overcome this limitation by a latent variable approach that allows working with weakly paired data and is still able to efficiently process large datasets using standard numerical routines. The resulting weakly paired maximum covariance analysis often finds better representations than alternative methods, as we show in two exemplary tasks: texture discrimination and transfer learning.", " This paper studies the one-shot and zero-shot learning problems, where each object category has only one training example or has no training example at all. We approach this problem by transferring knowledge from known categories (a.k.a source categories) to new categories (a.k.a target categories) via object attributes. Object attributes are high level descriptions of object categories, such as color, texture, shape, etc. Since they represent common properties across different categories, they can be used to transfer knowledge from source categories to target categories effectively. Based on this insight, we propose an attribute-based transfer learning framework in this paper. We first build a generative attribute model to learn the probabilistic distributions of image features for each attribute, which we consider as attribute priors. These attribute priors can be used to (1) classify unseen images of target categories (zero-shot learning), or (2) facilitate learning classifiers for target categories when there is only one training examples per target category (one-shot learning). We demonstrate the effectiveness of the proposed approaches using the Animal with Attributes data set and show state-of-the-art performance in both zero-shot and one-shot learning tests.", " Quantitative structure-activity relationships (QSARs) are regression models relating chemical structure to biological activity. Such models allow to make predictions for toxicologically or pharmacologically relevant endpoints, which constitute the target outcomes of trials or experiments. The task is often tackled by instance-based methods (like k-nearest neighbors), which are all based on the notion of chemical (dis) similarity. Our starting point is the observation by Raymond and Willett that the two big families of chemical distance measures, fingerprint-based and maximum common subgaph based measures, provide orthogonal information about chemical similarity. The paper presents a novel method for finding suitable combinations of them, called adapted transfer, which adapts a distance measure learned on another, related dataset to a given dataset. Adapted transfer thus combines distance learning and transfer learning in a novel manner. In a set of experiments, we compare adapted transfer with distance learning on the target dataset itself and inductive transfer without adaptations. In our experiments, we visualize the performance of the methods by learning curves (i.e., depending on training set size) and present a quantitative comparison for 10% and 100% of the maximum training set size.", " Transfer learning refers to reusing the knowledge gained while solving a task, to solve a related task more efficiently. Much of the prior work on transfer learning, assumes that identical robots were involved in both the tasks. In this work we focus on transfer learning across heterogeneous robots while solving the same task. The action capabilities of the robots are different and are unknown to each other. The actions of one robot cannot be mimicked by another even if known. Such situations arise in multi-robot systems. The objective then is to speed-up the learning of one robot, i.e., reduce its initial exploration, using very minimal knowledge from a different robot. We propose a framework in which the knowledge transfer is effected through a pseudo reward function generated from the trajectories followed by a different robot while solving the same task. The framework can effectively be used even with a single trajectory. We extend the framework to enable the robot to learn an equivalence between certain sequences of its actions and certain sequences of actions of the other robot. These are then used to learn faster on subsequent tasks. We empirically validate the framework in a rooms world domain.", "model-based  afterstates  transfer learning  relational reinforcement learning  Markov decision processes Transfer Learning refers to learning of knowledge in one domain that can be applied to a different domain, in this paper, we view transfer learning as generalization of knowledge in a richer representation language that includes multiple subdomains as parts of the same superdomain. We employ relational templates of different specificity to learn pieces of additive value functions. We show significant transfer of learned knowledge across different subdomains of a real-time strategy game by generalizing the value function using relational templates.", "Machine learning  meta learning  task similarity  task space In theory, learning is not possible over all tasks in general. In practice, the tasks for which learning is desired exhibit significant regularity, which makes learning practical. For the most effective learning, it is valuable to understand the nature of this regularity and how it manifests in the tasks where learning is applied. This research presents the DICES distance metric for finding similarity between learning tasks. With this distance metric, a collection of learning tasks can be given a distance matrix. This distance matrix can be used for visualizing the relationships between learning tasks and searching through task space for tasks which are similar in nature. Examples of task visualization are given, and other possible applications of this tool are touched upon. Such applications include learning algorithm selection, transfer learning, and analysis of empirical results.", "Locally Weighted Learning  Transfer Learning  Adaptive  Multi-model Locally weighted learning (LWL), which is an effectual and flexible method for prediction problems, is widely used in many regression scenarios. The training data samples, referring to the history experience knowledge base, are required to help do regression by new queries. However, sometimes, the knowledge base tends to be helpless due to the lake of information, such as inadequate training data. In such cases, traditional locally weighted learning will be powerless due to less history or inappropriate experience if there are not an adaptive mechanism or other learning methods like knowledge transfer to assistant. In this paper, we propose an adaptive transfer learning mechanism to assistant LWL to do prediction. As there are many auxiliary training sets, we assign different optimal local models to take each training set as the learning basic, and combine those models into an integrated one adaptively to give the final prediction value by allocating weights for each model dynamically with the feedback prediction error. Importantly, this learning process is assigned for multi-domain knowledge bases transference and multi-locally-weighted-model integration. Moreover, we also give an analysis about how the selection of additional training domains affects the regression result. Experimental studies are based on climate data which contains the monthly average of global land air temperature from 1901 to 2002 on grids divided by 0.5 latitude and 0.5 longitude. Knowledge transfer is taken out from neighbor grids to a center. The results show that our mechanism performs much better than traditional LWL.", " We propose a novel method, called Semi-supervised Projection Clustering in Transfer Learning (SPCTL), where multiple source domains and one target domain are assumed. Traditional semi-supervised projection clustering methods hold the assumption that the data and pairwise constraints are all drawn from the same domain. However, many related data sets with different distributions are available in real applications. The traditional methods thus can not be directly extended to such a scenario. One major challenging issue is how to exploit constraint knowledge from multiple source domains and transfer it to the target domain where all the data are unlabeled. To handle this difficulty, we are motivated to construct a common subspace where the difference in distributions among domains can be reduced. We also invent a transferred centroid regularization, which acts as a bridge to transfer the constraint knowledge to the target domain, to formulate this geometric structure formed by the centroids from different domains. Extensive experiments on both synthetic and practical data sets show the effectiveness of our method.", " One solution to the lack of label problem is to exploit transfer learning, whereby one acquires knowledge from source-domains to improve the learning performance in the target-domain. The main challenge is that the source and target domains may have different distributions. An open problem is how to select the available models (including algorithms and parameters) and importantly, abundance of source-domain data, through statistically reliable methods, thus making transfer learning practical and easy-to-use for real-world applications. To address this challenge, one needs to take into account the difference in both marginal and conditional distributions in the same time, but not just one of them. In this paper, we formulate a new criterion to overcome double distribution shift and present a practical approach Transfer Cross Validation (TrCV) to select both models and data in a cross validation framework, optimized for transfer learning. The idea is to use density ratio weighting to overcome the difference in marginal distributions and propose a reverse validation procedure to quantify how well a model approximates the true conditional distribution of target-domain. The usefulness of TrCV is demonstrated on different cross-domain tasks, including wine quality evaluation, web-user ranking and text categorization. The experiment results show that the proposed method outperforms both traditional cross-validation and one state-of-the-art method which only considers marginal distribution shift. The software and datasets are available from the authors.", " Data sparsity is a major problem for collaborative filtering (CF) techniques in recommender systems, especially for new users and items. We observe that, while our target data are sparse for CF systems, related and relatively dense auxiliary data may already exist in some other more mature application domains. In this paper, we address the data sparsity problem in a target domain by transferring knowledge about both users and items from auxiliary data sources. We observe that in different domains the user feedbacks are often heterogeneous such as ratings vs. clicks. Our solution is to integrate both user and item knowledge in auxiliary data sources through a principled matrix-based transfer learning framework that takes into account the data heterogeneity. In particular, we discover the principle coordinates of both users and items in the auxiliary data matrices, and transfer them to the target domain in order to reduce the effect of data sparsity. We describe our method, which is known as coordinate system transfer or CST, and demonstrate its effectiveness in alleviating the data sparsity problem in collaborative filtering. We show that our proposed method can significantly outperform several state-of-the-art solutions for this problem.", " People constantly apply acquired knowledge to new learning tasks, but machines almost never do. Research on transfer learning attempts to address this dissimilarity. Working within this area, we report on a procedure that learns and transfers constraints in the context of inductive process modeling, which we review. After discussing the role of constraints in model induction, we describe the learning method, MISC, and introduce our metrics for assessing the cost and benefit of transferred knowledge. The reported results suggest that cross-domain transfer is beneficial in the scenarios that we investigated, lending further evidence that this strategy is a broadly effective means for increasing the efficiency of learning systems. We conclude by discussing the aspects of inductive process modeling that encourage effective transfer, by reviewing related strategies, and by describing future research plans for constraint induction and transfer learning.", " Transfer learning aims at reusing the knowledge in some source tasks to improve the learning of a target task. Many transfer learning methods assume that the source tasks and the target task be related, even though many tasks are not related in reality. However, when two tasks are unrelated, the knowledge extracted from a source task may not help, and even hurt, the performance of a target task. Thus, how to avoid negative transfer and then ensure a safe transfer of knowledge is crucial in transfer learning. In this paper, we propose an Adaptive Transfer learning algorithm based on Gaussian Processes (AT-GP), which can be used to adapt the transfer learning schemes by automatically estimating the similarity between a source and a target task. The main contribution of our work is that we propose a new semi-parametric transfer kernel for transfer learning from a Bayesian perspective, and propose to learn the model with respect to the target task, rather than all tasks as in multi-task learning. We can formulate the transfer learning problem as a unified Gaussian Process (GP) model. The adaptive transfer ability of our approach is verified on both synthetic and real-world datasets.", " Building an intelligent agent, which simulates human-level learning appropriate for learning math, science, or a second language, could potentially benefit both education in understanding human learning, and artificial intelligence in creating human-level intelligence. Recently, we have proposed an efficient approach to acquiring procedural knowledge using transfer learning. However, it operated as a separate module. In this paper, we describe how to integrate this module into a machine-learning agent, SimStudent, that learns procedural knowledge from examples and through problem solving. We illustrate this method in the domain of algebra, after which we consider directions for future research in this area.", "Transfer learning  Analogical reasoning  Model formulation  Case-based reasoning Transfer learning is the ability to apply previously learned knowledge to new problems or domains. In qualitative reasoning, model formulation is the process of moving from the unruly, broad set of concepts used in everyday life to a concise, formal vocabulary of abstractions, assumptions, causal relationships, and models that support problem-solving. Approaching transfer learning from a model formulation perspective, we found that analogy with examples can be used to learn how to solve AP Physics style problems. We call this process analogical model formulation and implement it in the Companion cognitive architecture. A Companion begins with some basic mathematical skills, a broad common sense ontology, and some qualitative mechanics, but no equations. The Companion uses worked solutions, explanations of example problems at the level of detail appearing in textbooks, to learn what equations are relevant, how to use them, and the assumptions necessary to solve physics problems. We present an experiment, conducted by the Educational Testing Service, demonstrating that analogical model formulation enables a Companion to learn to solve AP Physics style problems. Across six different variations of relationships between base and target problems, or transfer levels, a Companion exhibited a 63% improvement in initial performance. While already a significant result, we describe an in-depth analysis of this experiment to pinpoint the causes of failures. Interestingly, the sources of failures were primarily due to errors in the externally generated problem and worked solution representations as well as some domain-specific problem-solving strategies, not analogical model formulation. To verify this, we describe a second experiment which was performed after fixing these problems. In this second experiment, a Companion achieved a 95.8% improvement in initial performance due to transfer, which is nearly perfect. We know of no other problem-solving experiments which demonstrate performance of analogical learning over systematic variations of relationships between problems at this scale. (C) 2009 Elsevier B.V. All rights reserved.", "Analogy  Representation mapping  Cognitive architectures  Transfer  Transfer learning  Agent architectures In this paper, we present an approach to transfer that involves analogical mapping of symbols across different domains. We relate this mechanism to ICARUS, a theory of the human cognitive architecture. Our system can transfer skills across domains hypothesizing maps between representations, improving performance in novel domains. Unlike previous approaches to analogical transfer, our method uses an explanatory analysis that compares how well a new domain theory explains previous solutions under different mapping hypotheses. We present experimental evidence that the new mechanism improves transfer over ICARUS' basic learning processes. Moreover, we argue that the same features which distinguish ICARUS from other architectures support representation mapping in a natural way and operate synergistically with it. These features enable our analogy system to translate a map among concepts into a map between skills, and to support transfer even if two domains are only partially analogous. We also discuss our system's relation to other work on analogy and outline directions for future research. (C) 2009 Elsevier B.V. All rights reserved.", "covariate shift  discriminative learning  transfer learning We address classification problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution-problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classifier for covariate shift. The optimization problem is convex under certain conditions  our findings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam filtering, text classification, and landmine detection.", "transfer learning  reinforcement learning  multi-task learning The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.", "Learning to learn  Transfer learning  Kernel methods  Generalization If regression tasks are sampled from a distribution, then the expected error for a future task can be estimated by the average empirical errors on the data of a finite sample of tasks, uniformly over a class of regularizing or pre-processing transformations. The bound is dimension free, justifies optimization of the pre-processing feature-map and explains the circumstances under which learning-to-learn is preferable to single task learning.", "inductive  error  neural network  Imbalance  regroup A new inductive transfer-learning algorithm called NEDRT is presented in this paper in order to improve the classification accuracy of a domain task by using the knowledge learned from labeled data generated from a different domain NEDRT introduces a novel error function for a constructed neural network by summing a weighted squared difference between the real output and the neural network output for each instance of label training data from the source domain and the target domain Each weight could be regarded as an instance's contribution degree to transfer, The source data set is partitioned Into different sunsets to minimize the imbalance between the target data and source data, and each subset is combined with the target data to form a new training data set These newly obtained training data sets are used to construct classifiers for the target task Experimental results of knowledge transfer on UCI data sets and text data sets show that NEDRT performs well", " Many everyday human skills can be framed in terms of performing some task subject to constraints imposed l y the environment. Constraints are usually unobservable and frequently change between contexts. In this paper, we resent a novel approach for learning (unconstrained) control policies from movement data, where observations are recorded under different constraint settings. Our approach seamlessly integrates unconstrained and constrained observations by pert wining hybrid optimisation of two risk functionals. The first a novel risk functional that makes a meaningful comparison between the estimated policy and constrained observations. The second is the standard risk, used to reduce the expected error under impoverished sets of constraints. We demonstrate our approach on systems of varying complexity, and illustrate its utility for transfer learning of a car washing task from human motion capture data.", "Machine Learning  Kernel Methods  Learning the Kernels  Support Vector Machine (SVM)  Transfer Learning In this paper, we propose an approach to learn the kernel which uses transferred knowledge from unlabeled data to cope with situations where training examples are scarce. In our approach, unlabeled data has been used to construct an optimized kernel that better generalizes on the target dataset. For the proposed kernel learning algorithm, Fisher Discriminant Analysis (FDA) is used in conjunction with Maximum Mean Discrepancy (MMD) test of statistics to optimize a base kernel using labeled and unlabeled data. Thereafter  the constructed kernel from both labeled and unlabeled datasets is used in SVM to evaluate the results which proved to increase prediction accuracy.", " A central goal of transfer learning is to enable learning when training data from the domain of interest is limited. Yet, work on transfer across relational domains has so far focused on the case where there is a significant amount of target data. This paper bridges this gap by studying transfer when the amount of target data is minimal and consists of information about just a handful of entities. In the extreme case, only a single entity is known. We present the SR2LR algorithm that finds an effective mapping of predicates from a source model to the target domain in this setting and thus renders preexisting knowledge useful to the target task. We demonstrate SR2LR's effectiveness in three benchmark relational domains on social interactions and study its behavior as information about an increasing number of entities becomes available.", " In machine learning problems, labeled data are often in short supply. One of the feasible solution for this problem is transfer learning. It can make use of the labeled data from other domain to discriminate those unlabeled data in the target domain. In this paper, we propose a transfer learning framework based on similarity matrix approximation to tackle such problems. Two practical algorithms are proposed, which are the label propagation and the similarity propagation. In these methods, we build a hybrid graph based on all available data. Then the information is transferred cross domains through alternatively constructing the similarity matrix for different part of the graph. Among all related methods, similarity propagation approach can make maximum use of all available similarity information across domains. This leads to more efficient transfer and better learning result. The experiment on real world text mining applications demonstrates the promise and effectiveness of our algorithms.", " We propose a probabilistic transfer learning model that uses task-level features to control the task mixture selection in a hierarchical Bayesian model. These task-level features, although rarely used in existing approaches, can provide additional information to model complex task distributions and allow effective transfer to new tasks especially when only limited number of data are available. To estimate the model parameters, we develop an empirical Bayes method based on variational approximation techniques. Our experiments on information retrieval show that the proposed model achieves significantly better performance compared with other transfer learning methods.", "Multi-task learning  transfer learning  information entropy Multi-task learning utilizes labeled data from other similar tasks and can achieve efficient knowledge-sharing between tasks. In this paper, a novel information-theoretic multi-task learning model, i.e. IBMTL is proposed. The key idea of IBMTL is to minimize the loss mutual information during the classification, while constrain the Kullback Leibler divergence between multiple tasks to some maximal level. The basic trade-off is between maximize the relevant information while minimize the dissimilarity between multiple tasks. The IBMTL algorithm is compared with TrAdaBoost which extends AdaBoost for transfer learning. The experiments were conducted on two data sets for transfer learning, Email spam-filtering data set and sentiment classification data set. The experimental results demonstrate that IBMTL outperforms TrAdaBoost.", "Transfer Learning  Data Edit  Semi-supervised Learning We often face the situation where very limited labeled data are available to learn an effective classifier in target domain while there exist large amounts of labeled data with similar feature or distribution in certain relevant domains. Transfer learning aims at improving the performance of a learner in target domain given labeled data, in one, or more Source domains. In this paper, we present an algorithm to learn effective classifier without or a few labeled data on target, domain, given some labeled data with same features and similar distribution in source domain. Our algorithm uses the data edit technique to approach distribution from the source domain to the target domain by removing mismatched examples in source domain and adding matched examples in target domain. Experimental results on email classification problem have confirmed the, effectiveness of the proposed algorithm.", " AI planning requires action models to be given in advance. However, it is both time consuming and tedious for a human to encode the action models by hand using it formal language such its PDDL, its a result, learning action models is important for AI planning. On the other hand, the data being used to learn action models are often limited in planning domains. which makes the learning task very difficult. In this paper, we present a new algorithm to learn action models from plan traces by transferring useful information from other domains whose action models are already known. We present,l method of building it metric to measure the shared information and transfer this information according to this metric. The larger the metric is, the bigger the information is transferred. In the experiment result, we show that our proposed algorithm is effective.", " Transfer learning is a new machine learning and data mining framework that allows the training and test data to come from different distributions or feature spaces. We can find many novel applications of machine learning and data mining where transfer learning is necessary. While much has been done in transfer learning in text classification and reinforcement learning, there has been a lack of documented success stories of novel applications of transfer learning in other areas. In this invited article, I will argue that transfer learning is in fact quite ubiquitous in many real world applications. In this article, I will illustrate this point through an overview of a broad spectrum of applications of transfer learning that range from collaborative filtering to sensor based location estimation and logical action model learning for AI planning. I will also discuss some potential future directions of transfer learning.", " In recent years, transfer learning has attracted much attention in multimedia In this paper. we propose an efficient transfer dimensionality reduction algorithm called transfer discriminative Logmaps (TDL) TDL. finds a common feature so that 1) the quadratic distance between the distribution of the training set and that of the testing set is minimized and 2) specific knowledge of the training samples can be conveniently delivered to or shared with the testing samples Drawing on this common feature in the representation space. our objective is to develop a linear subspace in which discriminative and geometric information can be exploited TDL adopts the margin maximization to identify discriminative information between different classes. while Logmaps is used to preserve the local-global geodesic distance as well as the direction information Experiments carried out on both synthetic and real-word image datasets show the effectiveness of TDL for cross-domain face recognition and web image annotation", "Transfer learning  graphical models  student modeling We describe a novel framework developed for transfer learning within reinforcement learning (RL) problems. Then we exhibit how this framework can be extended to intelligent tutoring systems (ITS). We compose an algorithm that automatically constructs a graphical representation based on the transfer framework. We evaluate this on a real-world ITS example and show that the model constructed by our approach performs better than previously published results. We propose that transfer learning is a useful and related area to explore for furthering intelligent tutoring systems.", " Positive transfer learning (TL) occurs when, after gaining experience from learning how to solve a (Source) task, the same learner can exploit this experience to improve performance and/or learning on a different (target) task. TL methods are typically complex, and case-based reasoning can support them in multiple ways. We introduce a method for recognizing intent in a source task, and then applying that knowledge to improve the performance of a a case-based reinforcement learner in a target task. We report on its ability to significantly outperform baseline approaches for a control task in a simulated game of American football. We also compare our approach to an alternative approach where source and target task learning occur concurrently, and discuss the tradeoffs between them.", "Concept Discovery  Online Learning  Transfer Learning The capacity to apply knowledge in a context different than the one in which it was learned has become crucial within the area of autonomous agents. This paper specifically addresses the issue of transfer of knowledge acquired through online learning in partially observable environments. We investigate the discovery of relevant abstract concepts which help the transfer of knowledge in the context of an environment characterized by its 2D geographical configuration. The architecture proposed is tested in a simple grid-world environment where two agents duel each other. Results show that an agent's performances are improved through learning, including when it is tested on it map it has not yet seen.", "Feature generation  Knowledge transfer  Classification  Correlation One important problem in machine learning is how to extract knowledge from prior experience, then transfer and apply this knowledge in new learning tasks. To address this problem, transfer learning leverages information from (supervised) learning on related tasks to facilitate the current learning task. Self-taught learning uses information extracted from (unsupervised) learning on related data. In this paper, we propose a new method for knowledge extraction, transfer and application in classification. We consider document classification where we mine correlation relationships among the words from a set of documents and compile a collection of correlation relationships as prior knowledge. This knowledge is then applied to generate new features for classifying documents in classes/types different from the ones which we obtain the correlation relationships from. Our experiment results show that the correlation-based knowledge transfer helps to reduce classification errors.", "reinforcement learning  markov decision processes  assignment problem  coordination  transfer learning We describe a system that successfully transfers value function knowledge across multiple subdomains of real-time strategy games in the context of multiagent reinforcement learning. First, we implement an assignment-based decomposition architecture, which decomposes the problem of coordinating multiple agents into the two levels of task assignment and task execution. Second, a hybrid model-based approach allows us to use simple deterministic action models while relying on sampling for the opponents' actions. Third, value functions based on parameterized relational templates enable transfer across sub-domains with different numbers of agents.", "Information Retrieval  Transfer Learning  Kullback-leibler Divergence  Markov Network Along with the development of internet, a lot of new data appear in the web every day. To construct a retrieval model to adapt the new data quickly and to retrieval the new documents accurately is becoming an important research topic. In this paper, we put forward a new retrieval model by incorporating the theory of transfer learning with Markov Network. Firstly, compare term spaces network of old dataset and new (target) dataset, and the distance between data sets is measured using the Kullback-Leibler divergence. Moreover, KL-divergence is used to decide the trade-off parameter in retrieval formula. Then we transfer the useful prior knowledge of old dataset to the new (target) dataset, and finally implement the retrieval process on the target dataset. Experiments on multiple datasets indicate that our new approach outperforms other methods. Furthermore, we perform several T-tests to demonstrate the improvements are statistically significant.", " The design of NPC(Non-Player Character) is an analytic process. It is relying on assumptions of human game players behavior In practice, however different PCs(Player Characters) often exhibit variable behavior making them difficult to predicate and complicating the design process. In this paper we describe an approach for team AI planning and learning. This approach is based on procedural knowledge and a layered multi-agent architecture. We implement real-time transfer learning and adaptive mechanism for the team of NPCs. The team can react to the human player with the tactical awareness of seasoned team behavior Results indicate that the approach of using the hybrid of transfer learning and adaptive mechanism can improve NPCs' overall performance in real-time.", "Smart Environments  Data Mining  Transfer Learning  Activity Mapping Most commonly-used techniques in smart environments such as ADL recognition are designed and tested for a specific space and a specific person  therefore learning in each environmental situation is treated as a separate context. In this paper, we try to develop a method for recognizing and transferring learned knowledge of activities between different residents. Our method is able to map activities despite intra-subject variability and inter-subject variability, by using a discontinuous mining method and a similarity measurement method. At the end, we will provide the results of our experiments on real data obtained from a smart apartment.", " When labeled examples are limited and difficult to obtain. transfer learning employs knowledge from a source domain to improve learning accuracy in the target domain. However, the assumption made by existing approaches, that the marginal and conditional probabilities are directly related between source and target domains, has limited applicability in either the original space or its linear transformations. To solve this problem, we propose an adaptive kernel approach that maps the marginal distribution of target-domain and source-domain data into a common kernel space. and utilize a sample selection strategy to draw conditional probabilities between the two domains closer. We formally show that under the kernel-mapping space, the difference in distributions between the two domains is bounded  and the prediction error of the proposed approach can also be bounded. Experimental results demonstrate that the proposed method outperforms both traditional inductive classifiers and the state-of-the-art boosting-based transfer algorithms on most domains, including text categorization and web page ratings. In particular, it can achieve around 10% higher accuracy than other approaches for the text categorization problem. The source code and datasets are available from the authors.", " Transfer algorithms allow the rise of knowledge previously learned on related tasks to speed-tip learning of the current task. Recently, Many complex reinforcement learning problems have been successfully solved by efficient transfer learners. However, most of these algorithms suffer front a severe flaw: they are implicitly tuned to transfer knowledge between tasks having a given degree of similarity. In other words, if the previous task is very dissimilar (resp. nearly identical) to the current task, then the transfer process might slow down the learning (resp. might be far from optimal speed-up). In this paper, we address this specific issue by explicitly optimizing the transfer rate between tasks and answer to the question : can the transfer rate be accurately optimized, and at. what cost  ?. We show that this optimization problem is related to the continuum bandit problem. We then propose a generic adaptive transfer method (AdaTran), which allows to extend several existing transfer learning algorithms to optimize the transfer rate. Finally, we run several experiments validating our approach.", " This Paper presents a novel feature selection method for classification of high dimensional data, such as those produced by microarrays. It, includes,a partial supervision to smoothly favor the selection of some dimensions (genes) Oil a new dataset, to be classified. The dimensions to he favored axe previously selected from similar datasets in large microarray databases, hence performing inductive transfer learning at the feature level. This technique relies on a, feature selection method embedded within a regularized linear model estimation. A practical approximation of this technique reduces to linear SVM learning with iterative input rescaling. The, scaling factors depend on the selected dimensions from the related datasets. The final selection may depart from those whenever necessary to optimize the classification objective. Experiments on several microarray datasets show that the proposed method both improves the selected gene lists stability, with respect to sampling variation, as well as the classification performances.", " Most existing transfer learning techniques are limited to problems of knowledge transfer across tasks sharing the same set, of class labels. In this paper, however, we relax this constraint and propose.-L spectral-based solution that aims at unveiling the intrinsic structure of the data and generating a partition of the target data, by transferring the eigenspace that well separates the source data. Furthermore, a clustering-based KL divergence is proposed to automatically adjust how much to transfer. We evaluate the, proposed model on text and image datasets where class categories of the source and target data, are explicitly different  e.g., 3-classes transfer to 2-classes, and show that  the proposed approach improves other baselines by an average of 1.0% in accuracy. The source code and datasets are available from the authors.", " The basis assumption that training and test data drawn from the same distribution is often violated in reality. In this paper, we propose one common solution to cover various scenarios of learning under different but related distributions in a single framework. Explicit examples include (a) sample selection bias between training and testing data, (b) transfer learning or no labeled data, in target domain, and (c) noisy or uncertain training data. The main motivation is that one could ideally solve as many problems as possible with a single approach. The proposed solution extends graph transduction using the maximum margin principle over unlabeled data. The error of the proposed method is bounded under reasonable assumptions even when the training and testing distributions axe different. Experiment results demonstrate that the proposed method improves the traditional graph transduction by as Much as 15% in accuracy and AUC in all common situations of distribution difference. Most importantly, it outperforms, by up to 10% in accuracy, several state-of-art approaches proposed to solve specific category of distribution difference, i.e, BRSD [1] for sample selection bias, CDSC [2] for transfer learning, etc. The main claim is that the adaptive graph transduction is a general and competitive method to solve distribution differences implicitly without knowing and worrying about the exact type. These at least include sample selection bias, transfer learning, uncertainty mining, as well as those alike that are still not studied yet. The source code and datasets are available from the authors.", " To achieve good generalization in supervised learning, the training and testing examples are usually required to be drawn from the same source distribution. However, in many cases, this identical distribution assumption might be violated when a task from one new domain(target domain) comes, while there are only labeled data from a similar old domain (auxiliary domain). Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and derive a maximum-margin formulation of unsupervised transfer learning. Two alternative solutions are proposed to solve the problem. Experimental results on many real data sets demonstrate the effectiveness and the potential of the proposed methods.", " To achieve good generalization in supervised learning, the training and testing examples are usually required to be drawn from the same source distribution. However, in many cases, this identical distribution assumption might be violated when a task from one new domain(target domain) comes, while there are only labeled data from a similar old domain(auxiliary domain). Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data, points and derive a maximum-margin formulation of unsupervised transfer learning. Two alternative solutions are proposed to solve the problem. Experimental results on many real data. sets demonstrate the effectiveness and the potential of the proposed methods."]}}; }
plotInterface = buildViz(1000,600,null,null,false,false,false,false,false,true,false,false,true,0.1,false,undefined,undefined,getDataAndInfo(),true,false,null,null,null,null,true,false,true,false,null,null,10,null,null,null,false,true,true,undefined,null);
</script>
