FN Clarivate Analytics Web of Science
VR 1.0
PT S
AU Isin, A
   Sharif, T
AF Isin, Ali
   Sharif, Tazeen
BE Aliev, RA
   Kacprzyk, J
   Pedrycz, W
   Jamshidi, M
   Sadikoglu, FM
TI Deep Learning for Lung Lesion Detection
SO 13TH INTERNATIONAL CONFERENCE ON THEORY AND APPLICATION OF FUZZY SYSTEMS
   AND SOFT COMPUTING - ICAFS-2018
SE Advances in Intelligent Systems and Computing
CT 13th International Conference on Application of Fuzzy Systems and Soft
   Computing (ICAFS)
CY AUG 27-28, 2018
CL Warsaw, POLAND
DE Deep learning; Lung lesion detection; Biomedical image processing;
   Transfer learning
AB As the most fatal cancer type, early diagnosis of the lung cancer plays an important role for the survival of the patients. Diagnosis of the lung cancer involves screening the patients initially by Computed Tomography (CT) for the presence of lung lesions. This procedure requires expert radiologists which need to go over very large numbers of image slices manually in order to detect and diagnose lung lesions. Unfortunately this is a very time consuming process and its performance is very dependent on the performing radiologist. Thus assisting the radiologists by developing an automated computer aided detection (CAD) system is an interesting research goal. In this regard, as the aim of this paper a pre-trained AlexNet (deep learning) framework is transferred to develop and implement a robust CAD system for the classification of lung images depending on whether they bear a lung lesion or not. High performances of 98.72% sensitivity, 98.35% specificity and 98.48% accuracy are reported as a result.
CR DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Isin A, 2017, J INSTRUM, V12, DOI 10.1088/1748-0221/12/03/C03089
   Isin A, 2017, PROCEDIA COMPUT SCI, V120, P268, DOI 10.1016/j.procs.2017.11.238
   Isin A, 2016, PROCEDIA COMPUT SCI, V102, P317, DOI 10.1016/j.procs.2016.09.407
   Krizhevsky A., 2016, P ADV NEUR INF PROC, P1097
   Reeves AP, 2009, IEEE ENG MED BIO, P3715, DOI 10.1109/IEMBS.2009.5334807
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Wang G, 2016, IEEE ACCESS, V4, P8914, DOI 10.1109/ACCESS.2016.2624938
NR 8
TC 0
Z9 0
SN 2194-5357
EI 2194-5365
BN 978-3-030-04164-9
PY 2019
VL 896
BP 799
EP 806
DI 10.1007/978-3-030-04164-9_105
ER

PT S
AU Hussain, M
   Bird, JJ
   Faria, DR
AF Hussain, Mahbub
   Bird, Jordan J.
   Faria, Diego R.
BE Lotfi, A
   Bouchachia, H
   Gegov, A
   Langensiepen, C
   McGinnity, M
TI A Study on CNN Transfer Learning for Image Classification
SO ADVANCES IN COMPUTATIONAL INTELLIGENCE SYSTEMS (UKCI)
SE Advances in Intelligent Systems and Computing
CT 18th UK Workshop on Computational Intelligence (UKCI)
CY SEP 05-07, 2018
CL Nottingham Trent Univ, Nottingham, ENGLAND
HO Nottingham Trent Univ
AB Many image classification models have been introduced to help tackle the foremost issue of recognition accuracy. Image classification is one of the core problems in Computer Vision field with a large variety of practical applications. Examples include: object recognition for robotic manipulation, pedestrian or obstacle detection for autonomous vehicles, among others. A lot of attention has been associated with Machine Learning, specifically neural networks such as the Convolutional Neural Network (CNN) winning image classification competitions. This work proposes the study and investigation of such a CNN architecture model (i.e. Inception-v3) to establish whether it would work best in terms of accuracy and efficiency with new image datasets via Transfer Learning. The retrained model is evaluated, and the results are compared to some state-of-the-art approaches.
CR Arun PV, 2013, GEOD CARTOGR, V62, P33, DOI 10.2478/geocart-2013-0005
   Gao Y., 2018, COMPUT AIDED CIV INF
   Larsen-Freeman D, 2013, LANG LEARN, V63, P107, DOI 10.1111/j.1467-9922.2012.00740.x
   Lin M., 2013, NETWORK NETWOR UNPUB
   Mathworks.com, 2018, MATHWORKS
   Nguyen N., 2016, COMPUTATIONAL COLLEC
   Rohrer B, 2016, CONVOLUTIONAL NEURAL
   Szegedy C., 2016, IEEE CVPR 2016 COMPU
   Szegedy C, 2015, GOING DEEPER CONVOLU
   TensorFlow, 2018, IM REC
   Vision Caltech, 2018, COMP VIS
   Wu J., 2015, CNN FOR DUMMIES
NR 12
TC 0
Z9 0
SN 2194-5357
EI 2194-5365
BN 978-3-319-97982-3; 978-3-319-97981-6
PY 2019
VL 840
BP 191
EP 202
DI 10.1007/978-3-319-97982-3_16
ER

PT S
AU Quinonez, Y
   Zatarain, O
   Lizarraga, C
   Peraza, J
AF Quinonez, Yadira
   Zatarain, Oscar
   Lizarraga, Carmen
   Peraza, Juan
BE Mejia, J
   Munoz, M
   Rocha, A
   Pena, A
   PerezCisneros, M
TI Using Convolutional Neural Networks to Recognition of Dolphin Images
SO TRENDS AND APPLICATIONS IN SOFTWARE ENGINEERING (CIMPS 2018)
SE Advances in Intelligent Systems and Computing
CT 7th International Conference on Software Process Improvement (CIMPS)
CY OCT 17-19, 2018
CL CUCEI, Guadalajara, MEXICO
HO CUCEI
DE Convolutional neural networks; Machine learning; Deep learning;
   TensorFlow; Inception V3; Inception ResNet V2
AB Classification of specific objects through Convolutional Neural Networks (CNN) has become an interesting research line in the area from information processing and machine learning, main idea is training a image dataset to perform the classifying a given pattern. In this work, a new dataset with 2504 images was introduced, the method used to train the networks was transfer learning to recognition of dolphin images. For this purpose, two models were used: Inception V3 and Inception ResNet V2 to train on TensorFlow platform with different images, corresponding to the four main classes: dolphin, dolphin_pod, open_sea, and seabirds. The paper ends with a critical discussion of the experimental results.
CR Alshahrani S, 2016, LECT NOTES COMPUT SC, V9612, P343, DOI 10.1007/978-3-319-41754-7_33
   [Anonymous], 2011, 1 ERKAM GURESEN 2 GU, V3, P426
   Atallah RR, 2018, IEEE ACCESS, V6, P28290, DOI 10.1109/ACCESS.2018.2836924
   Awad M., 2015, EFFICIENT LEARNING M, P167
   Bradbury Gwyneth, 2013, Computer Analysis of Images and Patterns. 15th International Conference, CAIP 2013. Proceedings: LNCS 8048, P209, DOI 10.1007/978-3-642-40246-3_26
   Chen Q, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P204, DOI 10.1109/SITIS.2016.40
   Cheng J, 2018, FRONT INFORM TECH EL, V19, P64, DOI 10.1631/FITEE.1700789
   Choi WG, 2014, LECT NOTES COMPUT SC, V8692, P417, DOI 10.1007/978-3-319-10593-2_28
   Goswami Tilottama, 2018, Microelectronics, Electromagnetics and Telecommunications. Proceedings of ICMEET 2017. LNEE 471, P475, DOI 10.1007/978-981-10-7329-8_48
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Habibi A. H., 2017, GUIDE CONVOLUTIONAL, P85
   He K, 2015, ARXIV151203385
   He X., 2018, DEEP LEARNING NATURA, P289
   Heck L, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P597, DOI 10.1109/GlobalSIP.2014.7032187
   Huang FL, 2016, 2016 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P704, DOI 10.1109/CompComm.2016.7924793
   Larasati R, 2017, PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON SMART CITIES, AUTOMATION & INTELLIGENT COMPUTING SYSTEMS (ICON-SONICS 2017), P99, DOI 10.1109/ICON-SONICS.2017.8267829
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Lu WS, 2017, IEEE PAC RIM CONF CO
   Miyajima R, 2017, IEEE MULTIMEDIA, V24, P91
   Moriya Shun, 2018, 2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC). Proceedings, P153, DOI 10.1109/COMPSAC.2018.10220
   Ouarda W, 2014, 2014 6TH INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION (SOCPAR), P89, DOI 10.1109/SOCPAR.2014.7007987
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Sustika R, 2017, 2017 2ND INTERNATIONAL CONFERENCES ON INFORMATION TECHNOLOGY, INFORMATION SYSTEMS AND ELECTRICAL ENGINEERING (ICITISEE), P17, DOI 10.1109/ICITISEE.2017.8285488
   Szegedy C., 2015, ARXIV151200567
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wu Q, 2017, 2017 CHINESE AUTOMATION CONGRESS (CAC), P6522, DOI 10.1109/CAC.2017.8243952
   Yamashita R., 2018, INSIGHTS IMAGING, P1
NR 28
TC 0
Z9 0
SN 2194-5357
EI 2194-5365
BN 978-3-030-01171-0; 978-3-030-01170-3
PY 2019
VL 865
BP 236
EP 245
DI 10.1007/978-3-030-01171-0_22
ER

PT S
AU Comert, Z
   Kocamaz, AF
AF Comert, Zafer
   Kocamaz, Adnan Fatih
BE Silhavy, R
TI Fetal Hypoxia Detection Based on Deep Convolutional Neural Network with
   Transfer Learning Approach
SO SOFTWARE ENGINEERING AND ALGORITHMS IN INTELLIGENT SYSTEMS
SE Advances in Intelligent Systems and Computing
CT 7th Computer Science On-Line Conference (CSOC)
CY APR, 2018
CL ELECTR NETWORK
DE Biomedical signal processing; Fetal monitoring; Deep convolutional
   neural network; Classification
ID HEART-RATE CLASSIFICATION; SUPPORT VECTOR MACHINE; RATE-VARIABILITY;
   CARDIOTOCOGRAPHY
AB Electronic fetal monitoring (EFM) device which is used to record Fetal Heart Rate (FHR) and Uterine Contraction (UC) signals simultaneously is one of the significant tools in terms of the present obstetric clinical applications. In clinical practice, EFM traces are routinely evaluated with visual inspection by observers. For this reason, such a subjective interpretation has been caused various conflicts among observers to arise. Although the existing of international guidelines for ensuring more consistent assessment, the automated FHR analysis has been adopted as the most promising solution. In this study, an innovative approach based on deep convolutional neural network (DCNN) is proposed to classify FHR signals as normal and abnormal. The proposed method composes of three stages. FHR signals are passed through a set of preprocessing procedures in order to ensure more meaningful input images, firstly. Then, a visual representation of time-frequency information, spectrograms are obtained with the help of the Short Time Fourier Transform (STFT). Finally, DCNN method is utilized to classify FHR signals. To this end, the colored spectrograms images are used to train the network. In order to evaluate the proposed model, we conducted extensive experiments on the open CTU-UHB database considering the area under the receiver operating characteristic curve and other several performance metrics derived from the confusion matrix. Consequently, we achieved encouraging results.
CR Ayres-De-Campos D, 2015, INT J GYNECOL OBSTET, V131, P13, DOI 10.1016/j.ijgo.2015.06.020
   Bernardes J, 1998, INT J GYNECOL OBSTET, V62, P141, DOI 10.1016/S0020-7292(98)00079-4
   Brown R, 2014, MED HYPOTHESES, V83, P410, DOI 10.1016/j.mehy.2014.07.009
   Bursa M, 2017, LECT NOTES COMPUT SC, V10443, P100, DOI 10.1007/978-3-319-64265-9_9
   Cesarelli M, 2007, COMPUT BIOL MED, V37, P663, DOI 10.1016/j.compbiomed.2006.06.003
   Chudacek V, 2014, BMC PREGNANCY CHILDB, V14, DOI 10.1186/1471-2393-14-16
   Comert Z, 2017, ACTA PHYS POL A, V132, P451, DOI 10.12693/APhysPolA.132.451
   Comert Z., 2016, INT J COMPUT APPL, V156, P26, DOI DOI 10.5120/IJCA2016912417.
   Comert Z., 2017, INT C ARTIF INTEL DA, P1, DOI [10.1109/IDAP.2017.8090210, DOI 10.1109/IDAP.2017.8090210]
   Comert Z., 2017, J SCI TECHNOL, V7, P93
   Comert Z., 2017, 25 SIGN PROC COMM AP, P1, DOI [10.1109/SIU.2017, DOI 10.1109/SIU.2017.7960397]
   Comert Z., 2016, 24 SIGN PROC COMM AP
   Comert Z., 2016, INT ART INT DAT PROC, P569, DOI [10.13140.CRG.2.2.23901.00489, DOI 10.13140/RG.2.2.23901.00489]
   Czabanski R, 2012, EXPERT SYST APPL, V39, P11846, DOI 10.1016/j.eswa.2012.01.196
   Garabedian C, 2017, J GYNECOL OBSTET HUM, V46, P131, DOI 10.1016/j.jogoh.2016.11.002
   GROOME LJ, 1994, EARLY HUM DEV, V38, P1, DOI 10.1016/0378-3782(94)90045-0
   Kahaner D., 1989, NUMERICAL METHODS SO
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Magenes G, 2004, P ANN INT IEEE EMBS, V26, P462
   Monteiro-Santos J, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19120688
   Murray H, 2017, BEST PRACT RES CL OB, V38, P2, DOI 10.1016/j.bpobgyn.2016.10.008
   NAWAB SH, 1983, IEEE T ACOUST SPEECH, V31, P986, DOI 10.1109/TASSP.1983.1164162
   Palomaki O, 2006, J PERINAT MED, V34, P298, DOI 10.1515/JPM.2006.057
   Romano M., 2014, 13 MED C MED BIOL EN, P651, DOI [10.1007/978-3-319-00846-2_161, DOI 10.1007/978-3-319-00846-2_161]
   Romano M, 2016, COMPUT MATH METHOD M, DOI 10.1155/2016/9585431
   Sahin H, 2015, APPL SOFT COMPUT, V33, P231, DOI 10.1016/j.asoc.2015.04.038
   Spilka Jiri, 2013, Information Technology in Bio- and Medical Informatics. 4th International Conference (ITBAM 2013). Proceedings: LNCS 8060, P47, DOI 10.1007/978-3-642-40093-3_4
   Spilka J, 2017, IEEE J BIOMED HEALTH, V21, P664, DOI 10.1109/JBHI.2016.2546312
   Subha V., 2015, J COMMUN TECHNOL EL, V2, P13
   Tongsong T, 2005, J OBSTET GYNAECOL RE, V31, P68, DOI 10.1111/j.1447-0756.2005.00243.x
   vanGeijn HP, 1996, BAILLIERE CLIN OB GY, V10, P185, DOI 10.1016/S0950-3552(96)80033-2
   Warrick P, 2005, IEEE IJCNN, P2400
   Yu YH, 2017, INFORMATION, V8, DOI 10.3390/info8030091
NR 34
TC 1
Z9 1
SN 2194-5357
EI 2194-5365
BN 978-3-319-91186-1; 978-3-319-91185-4
PY 2019
VL 763
BP 239
EP 248
DI 10.1007/978-3-319-91186-1_25
ER

PT J
AU Crosswhite, N
   Byrne, J
   Stauffer, C
   Parkhi, O
   Cao, Q
   Zisserman, A
AF Crosswhite, Nate
   Byrne, Jeffrey
   Stauffer, Chris
   Parkhi, Omkar
   Cao, Qiong
   Zisserman, Andrew
TI Template adaptation for face verification and identification
SO IMAGE AND VISION COMPUTING
CT 12th IEEE International Conference on Automatic Face and Gesture
   Recognition (FG)
CY MAY 30-JUN 03, 2017
CL Washington, DC
DE Face recognition; Biometrics; Face verification
AB Face recognition performance evaluation has traditionally focused on one-to-one verification, popularized by the Labeled Faces in the Wild data set [1] for imagery and the YouTubeFaces data set [2] for videos. In contrast, the newly released IJB-A face recognition data set [3] unifies evaluation of one-to-many face identification with one-to-one face verification over templates, or sets of imagery and videos for a subject. In this paper, we study the problem of template adaptation, a form of transfer learning to the set of media in a template. Extensive performance evaluations on IJB-A show a surprising result, that perhaps the simplest method of template adaptation, combining deep convolutional network features with template specific linear SVMs, outperforms the state-of-the-art by a wide margin. We study the effects of template size, negative set construction and classifier fusion on performance, then compare template adaptation to convolutional networks with metric learning, 2D and 3D alignment. Our unexpected conclusion is that these other methods, when combined with template adaptation, all achieve nearly the same top performance on IJB-A for template-based face verification and identification. (C) 2018 Elsevier B.V. All rights reserved.
CR AbdAlmageed W., 2016, WACV
   Chatfield K., 2015, INT J MULTIMED INF R
   Chen D., 2012, ECCV
   Chen J., 2015, ICCV WORKSH CHALEARN
   Chen V. PJ., 2016, WACV
   Crosswhite N, 2016, TEMPLATE ADAPTATION
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Grother P., 2014, 8009 NIST
   Hayat M., 2017, CVPR
   Hu G., 2015, ICCV WORKSH CHALEARN
   Huang GB., 2007, 0749 U MASS
   Kazemi V., 2014, CVPR
   Klare B. F., 2015, CVPR
   Kobayashi T., 2015, CVPR
   Learned-Miller E., 2015, ADV FACE DETECTION F
   Lu C., 2015, AAAI
   Malisiewicz T., 2011, ICCV
   Maze B., 2018, 11 IAPR INT C BIOM
   Parkhi O. M., 2014, CVPR
   Parkhi O. M., 2015, BMVC
   Phillips J., 2015, BTAS
   Ranjan  R., 2017, L2 CONSTRAINED SOFTM
   RoyChowdry A., 2016, WACV
   Sankaranarayanan S., 2016, TRIPLET SIMILARITY E
   Schroff F., 2015, CVPR
   Simonyan  K., 2015, ICLR
   Sun Y, 2015, CVPR
   Sun Y., 2014, DEEPID3 FACE RECOGNI
   Szegedy C., 2015, CVPR
   Taigman Y., 2014, CVPR
   Taigman Y., 2015, CVPR
   Wan L., 2013, ICML
   Whitelam C., 2017, CVPR WORKSH BIOM
   Wolf L., 2011, CVPR
   Wolf L, 2011, PAMI, V33
   Wolf L, 2009, ICCV
   Xiong  L., 2017, GOOD PRACTICE TOP PE
   Yi D, 2014, LEARNING FACE REPRES
   Zinkevich M., 2011, NIPS
NR 39
TC 0
Z9 0
SN 0262-8856
EI 1872-8138
PD NOV
PY 2018
VL 79
BP 35
EP 48
DI 10.1016/j.imavis.2018.09.002
ER

PT J
AU Singaravel, S
   Suykens, J
   Geyer, P
AF Singaravel, Sundaravelpandian
   Suykens, Johan
   Geyer, Philipp
TI Deep-learning neural-network architectures and methods: Using component
   based models in building-design energy prediction
SO ADVANCED ENGINEERING INFORMATICS
CT 24th EG-ICE International Workshop on Intelligent Computing in
   Engineering (EG-ICE)
CY JUL 10-12, 2017
CL Nottingham, ENGLAND
DE Performance gap; Sustainability; Building performance simulation;
   Transfer learning; Multi-task learning; LSTM
ID COOLING-LOAD PREDICTION; RESIDENTIAL BUILDINGS; CONSUMPTION;
   PERFORMANCE; OPTIMIZATION; REGRESSION; FRAMEWORK; VS.
AB Increasing sustainability requirements make evaluating different design options for identifying energy-efficient design ever more important. These requirements demand simulation models that are not only accurate but also fast. Machine Learning (ML) enables effective mimicry of Building Performance Simulation (BPS) while generating results much faster than BPS. Component-Based Machine Learning (CBML) enhances the capabilities of the monolithic ML model. Extending monolithic ML approach, the paper presents deep-learning architectures, component development methods and evaluates their suitability for space exploration in building design. Results indicate that deep learning increases the performance of models over simple artificial neural network models. Methods such as transfer learning and Multi-Task Learning make the component development process more efficient. Testing the deep-learning model on 201 new design cases indicates that its cooling energy prediction (R-2: 0.983) is similar to BPS, while errors for heating energy predictions (R-2: 0.848) are higher than BPS. Higher heating energy prediction error can be resolved by collecting heating data using better design space sampling methods that cover the heating demand distribution effectively. Given that the accuracy of the deep-learning model for heating predictions can be increased, the major advantage of deep-learning models over BPS is their high computation speed. BPS required 1145 s to simulate 201 design cases. Using the deep-learning model, similar results can be obtained in 0.9 s. High computation speed makes deep-learning models suitable for design space exploration.
CR Amasyali K, 2018, RENEW SUST ENERG REV, V81, P1192, DOI 10.1016/j.rser.2017.04.095
   Ascione F, 2017, ENERG BUILDINGS, V146, P200, DOI 10.1016/j.enbuild.2017.04.069
   Augenbroe G, 2002, BUILD ENVIRON, V37, P891, DOI 10.1016/S0360-1323(02)00041-0
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chary Aarthi, 2017, Expert Rev Clin Pharmacol, P1, DOI 10.1080/17512433.2017.1377610
   Cheng MY, 2014, APPL SOFT COMPUT, V22, P178, DOI 10.1016/j.asoc.2014.05.015
   Chollet F., 2015, KERAS
   Cohen N., 2015, EXPRESSIVE POWER DEE
   de Souza CB, 2012, AUTOMAT CONSTR, V22, P112, DOI 10.1016/j.autcon.2011.09.008
   de Wilde P, 2014, AUTOMAT CONSTR, V41, P40, DOI 10.1016/j.autcon.2014.02.009
   Dong B, 2005, ENERG BUILDINGS, V37, P545, DOI 10.1016/j.enbuild.2004.09.009
   Edwards RE, 2012, ENERG BUILDINGS, V49, P591, DOI 10.1016/j.enbuild.2012.03.010
   Eisenhower B, 2012, ENERG BUILDINGS, V47, P292, DOI 10.1016/j.enbuild.2011.12.001
   Fan C, 2017, APPL ENERG, V195, P222, DOI 10.1016/j.apenergy.2017.03.064
   Geyer P., COMPONENT BASE UNPUB
   Goodfellow I., 2016, DEEP LEARN
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Horsey H., 2016, ACEEE SUMMER STUDY E
   Hou ZJ, 2006, APPL ENERG, V83, P1033, DOI 10.1016/j.apenergy.2005.08.006
   Kiros R., 2014, P 31 INT C MACH LEAR, P595
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Magalhaes SMC, 2017, ENERG BUILDINGS, V151, P332, DOI 10.1016/j.enbuild.2017.06.076
   Magnier L, 2010, BUILD ENVIRON, V45, P739, DOI 10.1016/j.buildenv.2009.08.016
   Menezes AC, 2012, APPL ENERG, V97, P355, DOI 10.1016/j.apenergy.2011.11.075
   Ngiam  J., 2011, IEEE INT C MACH LEAR, P689
   Paudel S, 2017, ENERG BUILDINGS, V138, P240, DOI 10.1016/j.enbuild.2016.11.009
   Ruder S., 2017, OVERVIEW MULTITASK L
   Sak H, 2014, INTERSPEECH, P338
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Singaravel S., 2016, INT WORKSH EUR GROUP, P23
   Singaravel S., 2017, EG ICE, P260
   Singaravel S., 2017, P 15 IBPSA C, P2617
   Sundermeyer M, 2015, IEEE-ACM T AUDIO SPE, V23, P517, DOI 10.1109/TASLP.2015.2400218
   Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194
   Sutskever I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tsanas A, 2012, ENERG BUILDINGS, V49, P560, DOI 10.1016/j.enbuild.2012.03.003
   U. of W. -M. S. E. Laboratory, 1975, TRNSYS TRANS SIM PRO
   van Dronkelaar C., 2016, FRONTIERS MECH ENG, V1, P1, DOI DOI 10.3389/FMECH.2015.00017
   Van Gelder L, 2014, SIMUL MODEL PRACT TH, V49, P245, DOI 10.1016/j.simpat.2014.10.004
   Yu Z, 2010, ENERG BUILDINGS, V42, P1637, DOI 10.1016/j.enbuild.2010.04.006
   Zapata-Lancaster G, 2016, ARCHIT ENG DES MANAG, V12, P279, DOI 10.1080/17452007.2016.1178627
   Zhang F, 2016, ENERG BUILDINGS, V126, P94, DOI 10.1016/j.enbuild.2016.05.028
NR 43
TC 3
Z9 3
SN 1474-0346
EI 1873-5320
PD OCT
PY 2018
VL 38
BP 81
EP 90
DI 10.1016/j.aei.2018.06.004
ER

PT J
AU Dodge, S
   Mounsef, J
   Karam, L
AF Dodge, Samuel
   Mounsef, Jinane
   Karam, Lina
TI Unconstrained ear recognition using deep neural networks
SO IET BIOMETRICS
CT Conference on Unconstrained Ear Recognition Challenge (UERC) held in
   conjunction with the International Joint Conference on Biometrics (IJCB)
CY OCT 01-04, 2017
CL Denver, CO
DE neural nets; ear; feature extraction; learning (artificial
   intelligence); image classification; shallow classifier; deep
   learning-based averaging ensemble; DNNs; feature extractor; transfer
   learning; deep neural networks; combined AWE plus CVLE dataset;
   unconstrained ear recognition datasets; feature-extraction models;
   training dataset
ID DISCRIMINANT-ANALYSIS; FEATURES; SHAPE
AB The authors perform unconstrained ear recognition using transfer learning with deep neural networks (DNNs). First, they show how existing DNNs can be used as a feature extractor. The extracted features are used by a shallow classifier to perform ear recognition. Performance can be improved by augmenting the training dataset with small image transformations. Next, they compare the performance of the feature-extraction models with fine-tuned networks. However, because the datasets are limited in size, a fine-tuned network tends to over-fit. They propose a deep learning-based averaging ensemble to reduce the effect of over-fitting. Performance results are provided on unconstrained ear recognition datasets, the AWE and CVLE datasets as well as a combined AWE + CVLE dataset. They show that their ensemble results in the best recognition performance on these datasets as compared to DNN feature-extraction based models and single fine-tuned models.
CR Ahmad A., 2017, INT JOINT C BIOM OCT
   Anwar AS, 2015, PROCEDIA COMPUT SCI, V65, P529, DOI 10.1016/j.procs.2015.09.126
   Benzaoui A., 2016, INT C ADV ASP SOFTW, P1
   Burge M., 1996, BIOMETRICS, P273
   Burge M., 1997, WORKSH AUSTR ASS PAT, P275
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang K, 2003, IEEE T PATTERN ANAL, V25, P1160, DOI 10.1109/TPAMI.2003.1227990
   Chatfield K., 2014, BRIT MACH VIS C
   Choras M., 2005, ELECT LETT COMPUTER, V5, P51
   Choras M, 2006, ISDA 2006: SIXTH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS DESIGN AND APPLICATIONS, VOL 2, P451
   Emersic Z., 2017, INT JOINT C BIOM OCT
   Emersic Z, 2017, IEEE INT CONF AUTOMA, P987, DOI 10.1109/FG.2017.123
   Emersic Z, 2017, NEUROCOMPUTING, V255, P26, DOI 10.1016/j.neucom.2016.08.139
   Erhan D., 2009, 1341 U MONTR
   Fabate A., 2006, INT C PATTERN RECOGN, V4, P437
   Glorot X., 2010, JLMR P TRACK, V9, P249, DOI DOI 10.1.1/207.2059
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Howland P, 2004, IEEE T PATTERN ANAL, V26, P995, DOI 10.1109/TPAMI.2004.46
   Hurley DJ, 2005, COMPUT VIS IMAGE UND, V98, P491, DOI 10.1016/j.cviu.2004.11.001
   Iandola F. N., 2016, ARXIV160207360
   Iannarelli A. V., 1989, EAR IDENTIFICATION
   Jacob L, 2014, ADV INTELL SYST, V264, P1, DOI 10.1007/978-3-319-04960-1_1
   Kisku DR, 2009, 2009 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTATIONAL TOOLS FOR ENGINEERING APPLICATIONS, P380, DOI 10.1109/ACTEA.2009.5227958
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kumar A., 2007, SPIE DEF SEC S
   Kumar A, 2013, PATTERN RECOGN, V46, P73, DOI 10.1016/j.patcog.2012.06.020
   Galdamez PL, 2017, J APPL LOGIC, V24, P62, DOI 10.1016/j.jal.2016.11.014
   Mamta, 2013, EXPERT SYST APPL, V40, P6478, DOI 10.1016/j.eswa.2013.05.020
   Moreno B., 1999, Proceedings IEEE 33rd Annual 1999 International Carnahan Conference on Security Technology (Cat. No.99CH36303), P469, DOI 10.1109/CCST.1999.797956
   Mu Z., 2009, USTB EAR IMAGE DATAB
   Mu ZC, 2004, LECT NOTES COMPUT SC, V3338, P663
   Murukesh C, 2012, PROCEDIA ENGINEER, V38, P771, DOI 10.1016/j.proeng.2012.06.097
   Omara I, 2017, 2017 16TH IEEE/ACIS INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE (ICIS 2017), P341
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sana A, 2007, PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION, P46
   Sforza C, 2009, FORENSIC SCI INT, V187, DOI 10.1016/j.forsciint.2009.02.019
   Simonyan K., 2014, INT C LEARN REPR
   Tariq A., 2011, Proceedings of the 2011 International Conference on Computer Science and Network Technology (ICCSNT), P50, DOI 10.1109/ICCSNT.2011.6181906
   Tian L, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P437, DOI 10.1109/CISP-BMEI.2016.7852751
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Yu W, 2008, IEEE INT S PAR DISTR, P1
   Zhang H.J., 2005, IEEE INT C MACHINE L, P4511
   Zhao HL, 2009, 2009 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND INFORMATION TECHNOLOGY, VOL 2, P228, DOI 10.1109/ICCSIT.2009.5234392
NR 44
TC 1
Z9 1
SN 2047-4938
EI 2047-4946
PD MAY
PY 2018
VL 7
IS 3
SI SI
BP 207
EP 214
DI 10.1049/iet-bmt.2017.0208
ER

PT B
AU Han, XY
   Peng, QK
   Zhu, ZB
AF Han, Xuyue
   Peng, Qinke
   Zhu, Zhibo
GP IEEE
TI Popularity Prediction of News Event Based on ELM and Transfer Learning
SO 2018 13TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA)
CT 13th World Congress on Intelligent Control and Automation (WCICA)
CY JUL 04-08, 2018
CL Changsha, PEOPLES R CHINA
ID ELECTION
AB With the popularization of the Internet, the impact of news events on the society is growing. Research on the evolution of news event popularity could help netizens to grasp the overall development of news events. At present, the timeliness of the popularity prediction method for news event is prevalently weak and the data set is insufficient. This paper captures the real-time data from Baidu news search engine, and builds the predictive model of news event popularity based on ELM (Extreme Learning Machine). According to the similarity between news events and the prediction error of news event popularity, we propose the ELM-based prediction method by introducing the transfer learning strategy which considers the normalization of peak values of the popularity series. Our method is beneficial for establishing the prediction model under the non-independent distribution with missing data in the modeling process. The experimental results show that our method can predict the popularity of news event timely and effectively.
CR Dai W, 2008, NIPS
   Grigorievskiy A, 2014, NEURAL NETWORKS, V51, P50, DOI 10.1016/j.neunet.2013.12.002
   Hong Jiaming, 2011, COMPUTER RES DEV, V48, P1823
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Hyun KD, 2015, INFORM COMMUN SOC, V18, P766, DOI 10.1080/1369118X.2014.994543
   Kaleel SB, 2015, J COMPUT SCI-NETH, V6, P47, DOI 10.1016/j.jocs.2014.11.004
   Le Zhang, 2013, COMPUTER DIGITAL ENG, V41, P772
   Lee FLF, 2010, INT J PRESS/POLIT, V15, P462, DOI 10.1177/1940161210375463
   Li Q, 2014, INFORM SCIENCES, V278, P826, DOI 10.1016/j.ins.2014.03.096
   Li XD, 2014, KNOWL-BASED SYST, V69, P14, DOI 10.1016/j.knosys.2014.04.022
   Liu JQ, 2014, LECT NOTES COMPUT SC, V8597, P3, DOI 10.1007/978-3-319-11538-2_1
   Lu J, 2015, KNOWL-BASED SYST, V80, P14, DOI 10.1016/j.knosys.2015.01.010
   [梅灿华 Mei Canhua], 2011, [计算机研究与发展, Journal of Computer Research and Development], V48, P1722
   Nardis Y, 2015, INT J PRESS/POLIT, V20, P45, DOI 10.1177/1940161214556710
   Nassirtoussi AK, 2015, EXPERT SYST APPL, V42, P306, DOI 10.1016/j.eswa.2014.08.004
   Nuij W, 2014, IEEE T KNOWL DATA EN, V26, P823, DOI 10.1109/TKDE.2013.133
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pereira EN, 2016, INDEP J MANAG PROD, V7, P252, DOI 10.14807/ijmp.v7i1.400
   Rojas I, 2016, CONTRIBUTIONS STAT
   Schemer C, 2012, J COMMUN, V62, P739, DOI 10.1111/j.1460-2466.2012.01672.x
   Singh R, 2007, PROC WRLD ACAD SCI E, V26, P361
   Tatar A, 2014, SOC NETW ANAL MIN, V4, DOI 10.1007/s13278-014-0174-8
   Wu B, 2015, INT J INFORM MANAGE, V35, P702, DOI 10.1016/j.ijinfomgt.2015.07.003
   Xu Min, 2014, Control and Decision, V29, P141
   Yang Deqing, 2010, J COMPUTER RES DEV, V47
   Yang YM, 1999, IEEE INTELL SYST APP, V14, P32, DOI 10.1109/5254.784083
   Zhang P, 2013, 2013 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM), P1427
   Zhang XM, 2015, NEUROCOMPUTING, V149, P1469, DOI 10.1016/j.neucom.2014.08.045
   Zhang XM, 2014, LECT NOTES COMPUT SC, V8485, P484, DOI 10.1007/978-3-319-08010-9_53
NR 29
TC 0
Z9 0
BN 978-1-5386-7345-4
PY 2018
BP 329
EP 334
ER

PT S
AU Alsabahi, YAL
   Fan, L
   Feng, XY
AF Alsabahi, Y. A. L.
   Fan, Lei
   Feng, Xiaoyi
GP IEEE
TI Image Classification Method in DR Image Based on Transfer Learning
SO 2018 EIGHTH INTERNATIONAL CONFERENCE ON IMAGE PROCESSING THEORY, TOOLS
   AND APPLICATIONS (IPTA)
SE International Conference on Image Processing Theory Tools and
   Applications
CT 8th International Conference on Image Processing Theory, Tools and
   Applications (IPTA)
CY NOV 07-10, 2018
CL Xian, PEOPLES R CHINA
DE Transfer Learning; DR images; medical image; CAD
ID TUBERCULOSIS
AB Until now many cancer cases have been discovered in their early stages based on Computer Aided Diagnosis (CAD) system. There are many methods in the medical image processing field have been proposed to address this issue, and the result of these methods was deficient. Further, the application of AI in DR images is not widespread in hospitals. The classification process in the DR image is more difficult than other types of images. In this paper, we use transfer learning which is based on Inception V3 model to classify the DR images. We used the weight of Inception V3 model which was trained in the ImageNet dataset, and fine-tuning in our own dataset. Comparing to other proposed methods, our result had a higher accuracy.
CR Abadi Martin, 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Becker AS, 2017, INVEST RADIOL, V52, P434, DOI 10.1097/RLI.0000000000000358
   Chauhan R. P, 2016, INT J ADV TECHNOLOGY, V3, P217
   Dai W, 2009, P 26 ANN INT C MACH, P193, DOI DOI 10.1145/1553374.1553399
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   DeSantis CE, 2017, CA-CANCER J CLIN, V67, P439, DOI 10.3322/caac.21412
   Duncan JS, 2000, IEEE T PATTERN ANAL, V22, P85, DOI 10.1109/34.824822
   Fan L, 2017, 2017 INTERNATIONAL CONFERENCE ON THE FRONTIERS AND ADVANCES IN DATA SCIENCE (FADS), P7, DOI 10.1109/FADS.2017.8253184
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Jaeger S, 2013, QUANT IMAGING MED SU, V3, P89, DOI 10.3978/j.issn.2223-4292.2013.04.03
   Lakhani P, 2017, RADIOLOGY, V284, P574, DOI 10.1148/radiol.2017162326
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu JC, 2018, PROC SPIE, V10574, DOI 10.1117/12.2293125
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Thomas MA, 2004, J DIGIT IMAGING, V17, P189, DOI 10.1007/s10278-004-1000-z
   Veta M, 2014, IEEE T BIO-MED ENG, V61, P1400, DOI 10.1109/TBME.2014.2303852
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wu J., 2018, J ELECTRON IMAGING, V27, P1
   Xia XL, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P783, DOI 10.1109/ICIVC.2017.7984661
NR 20
TC 0
Z9 0
SN 2154-512X
BN 978-1-5386-6428-5
PY 2018
BP 195
EP 198
ER

PT B
AU Suharjito
   Gunawan, H
   Thiracitta, N
   Nugroho, A
AF Suharjito
   Gunawan, Herman
   Thiracitta, Narada
   Nugroho, Ariadi
GP IEEE
TI Sign Language Recognition Using Modified Convolutional Neural Network
   Model
SO 2018 INDONESIAN ASSOCIATION FOR PATTERN RECOGNITION INTERNATIONAL
   CONFERENCE (INAPR)
CT 1st International Conference of the
   Indonesian-Association-for-Pattern-Recognition (INAPR)
CY SEP 07-08, 2018
CL Bina Nusantara Univ, Jakarta, INDONESIA
HO Bina Nusantara Univ
DE deep learning; convolutional neural network; recognition; comparation;
   sign language
AB Sign Language is an interesting topic and similar to Action Recognition. Especially along with the great development of Deep Learning. Video-based Sign Language Recognition is our concern because we want to recognize a sign not only by the shape but also by the action the signer does. The problem is sign language is very complex and vary. The variation of sign language is making the system harder to recognize all the words accurately. Many researchers have been researching Sign Language Recognition for a long time. So many methods had been used to find out which one is the best method. Because of similarity between Sign Language Recognition and Action Recognition, we are trying to implement one of the top-tier models in Action Recognition which is i3d inception this model is also a new Action Recognition model with very high accuracy. So we can know is it possible to adopt Action Recognition behavior into Sign Language Recognition. The goal of this paper is to implement the i3d inception model to Sign Language Recognition with transfer learning method. From the test we've been done, we got 100% accuracy on training with 10 words and 10 signers with 100 classes but the validation accuracy is pretty low. This model is too overfit.
CR Christoph Feichtenhofer R.P.W., 2016, ADV NEURAL INFORM PR, P3468
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Escalera S., 2014, P EUR C COMP VIS WOR, P459
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Jaoa Carriera A. Z., 2018, COMP VIS PATT REC CV, P4724
   Kay W., 2017, COMPUTER VISION PATT, P1
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Ma WY, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON SMART CITY/SOCIALCOM/SUSTAINCOM (SMARTCITY), P1, DOI [10.1109/SmartCity.2015.38, 10.1109/NEBEC.2015.7117114]
   Pigou L., 2014, WORKSH EUR C COMP VI, p[572, 572]
   Ronchetti F., 2016, 22 C ARG CIENC COMP, P794
   Soomro K., 2012, CORR, P1
   Suharjito, 2017, 2 INT C COMP SCI COM
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
NR 14
TC 0
Z9 0
BN 978-1-5386-9422-0
PY 2018
BP 1
EP 5
ER

PT B
AU Liawatimena, S
   Heryadi, Y
   Lukas
   Trisetyarso, A
   Wibowo, A
   Abbas, BS
   Barlian, E
AF Liawatimena, Suryadiputra
   Heryadi, Yaya
   Lukas
   Trisetyarso, Agung
   Wibowo, Antoni
   Abbas, Bahtiar Saleh
   Barlian, Erland
GP IEEE
TI A Fish Classification on Images using Transfer Learning and Matlab
SO 2018 INDONESIAN ASSOCIATION FOR PATTERN RECOGNITION INTERNATIONAL
   CONFERENCE (INAPR)
CT 1st International Conference of the
   Indonesian-Association-for-Pattern-Recognition (INAPR)
CY SEP 07-08, 2018
CL Bina Nusantara Univ, Jakarta, INDONESIA
HO Bina Nusantara Univ
DE Image classification; Machine learning; Marine animals
AB The Ministry of Marine Affairs and Fisheries (MMAF) carry out the responsibilities and features associated with the policy of marine and fisheries. MMAF carry out the responsibilities and features associated with marine and fisheries policy. One of their duties and functions are organized marine and fishery statistics, in accordance to Law of The Republic of Indonesia Number sixteen of 1997 regarding Statistics. The main problem is the number of enumerators at each Basis Landing Of Fish. This paper proposed a fish classification on fish images using transfer learning and Matlab as the first stage of tackling the problem. FishNet is a modification from AlexNet to classify Katsuwonus Pelamis (Skipjack tuna or Cakalang), Euthynnus Affinis (Tongkol) and Coryphaena Hippurus (Mahi-mahi) that caught by fishermen. There are 15.120 images of 3 type of fishes, 5.040 for each fish. Data is split into 70: 30 for training and validation set. The training process is done using Matlab 2018a on Windows 7 operating system in a notebook with single CPU i7 with 8 GB RAM for 124 minutes. The validation accuracy is 99.63%.
CR Al-Smadi M., 2013, AFRICAN J COMPUTING, V3, P199, DOI DOI 10.1146/ANNUREV.ANTHR0.34.081804.120613
   [Anonymous], 2018, MACHINE LEARNING ARM
   Brownlee  J., 2018, GENTLE INTRO TRANSFE
   Canziani  A., 2016, ANAL DEEP NEURAL NET
   Dettmers  T., 2018, FULL HARDWARE GUIDE
   Han JW, 2018, IEEE SIGNAL PROC MAG, V35, P84, DOI 10.1109/MSP.2017.2749125
   Hof  R., 2018, DEEP LEARNING MIT TE
   Hu J, 2012, COMPUT ELECTRON AGR, V88, P133, DOI 10.1016/j.compag.2012.07.008
   John Lu Z., 2010, JR STAT SOC A, V173, P693, DOI [10. 1111/j. 1467-985X. 2010. 00646_6. x, DOI 10.1111/J.1467-985X.2010.00646_6.X, 10.1111/j.1467-985X.2010.00646_6.x]
   Jung  A., 2017, IMGAUG IMGAUG 0 2 6
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lee  A., 2016, COMP DEEP NEURAL NET
   R. A. P. Publication, 2009, FISHING FLEET ACEH P
   Rathi  D., 2018, ARXIV180510106V1CSCV
   Wayongkere, MELAUT 40 MIL DAPAT
   Widodo A. A., 2016, INDONES FISH RES J, V22, P43
   Yang  Y., 2018, PRIOR NOTIFICATION I, P158
   Yin  D., 2018, GRADIENT DIVERSITY K
   Yuniarta S, 2017, FISH RES, V193, P173, DOI 10.1016/j.fishres.2017.04.009
NR 19
TC 0
Z9 0
BN 978-1-5386-9422-0
PY 2018
BP 108
EP 112
ER

PT S
AU Norouzifard, M
   Nemati, A
   GholamHosseini, H
   Klette, R
   Nouri-Mahdavi, K
   Yousefi, S
AF Norouzifard, Mohammad
   Nemati, Ali
   GholamHosseini, Hamid
   Klette, Reinhard
   Nouri-Mahdavi, Kouros
   Yousefi, Siamak
GP IEEE
TI Automated Glaucoma Diagnosis Using Deep and Transfer Learning: Proposal
   of a System for Clinical Testing
SO 2018 INTERNATIONAL CONFERENCE ON IMAGE AND VISION COMPUTING NEW ZEALAND
   (IVCNZ)
SE International Conference on Image and Vision Computing New Zealand
CT International Conference on Image and Vision Computing New Zealand
   (IVCNZ)
CY NOV 19-21, 2018
CL Auckland, NEW ZEALAND
DE Glaucoma diagnosis; Deep learning; Image classification; Transfer
   learning; VGG19; InceptionResNet-V2
ID PERIMETRY; SAP
AB We developed a deep learning algorithm for identifying glaucoma on optic nerve head (ONH) photographs. We applied transfer learning to overcome overfitting on the small training sample size that we employed. The transfer learning framework that was previously trained on large datasets such as ImageNet, uses the initial parameters and makes the approach applicable to small sample sizes. We then classified the input ONH photographs as "normal" or "glaucoma".
   The proposed approach achieved a validation accuracy of 92.3% on a dataset of 277 ONH photographs from normal eyes and 170 ONH photographs from eyes with glaucoma. In order to re-test the accuracy and generalizability of the proposed approach, we re-tested the algorithm using an independent dataset of 30 ONH photographs. The re-test accuracy was 80.0% on average.
CR Al-Bander B, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10040087
   Al-Bander B, 2017, INT MULTICONF SYST, P207, DOI 10.1109/SSD.2017.8166974
   [Anonymous], 2015, LATEST STATS GLANCE
   [Anonymous], 2017, INCEPTIONRESNET V2 N
   [Anonymous], 2017, VGG19 NETWORK
   Bowd C, 2000, Semin Ophthalmol, V15, P194, DOI 10.3109/08820530009037871
   Budai  A., 2013, INT J BIOMEDICAL IMA, V20
   Chen XY, 2015, IEEE ENG MED BIO, P715, DOI 10.1109/EMBC.2015.7318462
   Chen XY, 2015, LECT NOTES COMPUT SC, V9351, P669, DOI 10.1007/978-3-319-24574-4_80
   Choi JY, 2017, PLOS ONE, V12, DOI [10.1371/journal.pone.0187338, 10.1371/journal.pone.0187336]
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Johnson  C., 2014, CLIN GLAUCOMA CARE, P117
   Johnson CA, 2002, AM J OPHTHALMOL, V134, P177, DOI 10.1016/S0002-9394(02)01577-5
   Jonas JB, 1996, SURV OPHTHALMOL, V40, P369, DOI 10.1016/S0039-6257(96)80065-8
   Kingman S, 2004, B WORLD HEALTH ORGAN, V82, P887
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Li F, 2018, BMC MED IMAGING, V18, DOI 10.1186/s12880-018-0273-5
   Lim G, 2015, PROC INT C TOOLS ART, P162, DOI 10.1109/ICTAI.2015.36
   Nayak J, 2009, J MED SYST, V33, P337, DOI 10.1007/s10916-008-9195-z
   Nomoto H, 2009, J GLAUCOMA, V18, P165, DOI 10.1097/IJG.0b013e318179f7ca
   Raghavendra U, 2018, INFORM SCIENCES, V441, P41, DOI 10.1016/j.ins.2018.01.051
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Szegedy C., 2017, AAAI, P4278
   Thakur N, 2018, BIOMED SIGNAL PROCES, V42, P162, DOI 10.1016/j.bspc.2018.01.014
   Tham YC, 2014, OPHTHALMOLOGY, V121, P2081, DOI 10.1016/j.ophtha.2014.05.013
   Wahab  H., 2014, IM PROC THEOR TOOLS, P1
   Westcott MC, 1997, BRIT J OPHTHALMOL, V81, P452, DOI 10.1136/bjo.81.6.452
NR 27
TC 0
Z9 0
SN 2151-2191
BN 978-1-7281-0125-5
PY 2018
ER

PT S
AU Singh, S
   Ho-Shon, K
   Karimi, S
   Hamey, L
AF Singh, Sonit
   Ho-Shon, Kevin
   Karimi, Sarvnaz
   Hamey, Len
GP IEEE
TI Modality Classification and Concept Detection in Medical Images using
   Deep Transfer Learning
SO 2018 INTERNATIONAL CONFERENCE ON IMAGE AND VISION COMPUTING NEW ZEALAND
   (IVCNZ)
SE International Conference on Image and Vision Computing New Zealand
CT International Conference on Image and Vision Computing New Zealand
   (IVCNZ)
CY NOV 19-21, 2018
CL Auckland, NEW ZEALAND
DE Medical Image Analysis; Modality Classification; Medical Image
   Classification; Concept Detection; Multi-Label learning; Convolutional
   Neural Networks; Deep Learning
ID RADIOLOGY; FEATURES
AB Medical image classification and concept detection are two important tasks for efficient and robust medical retrieval systems and also help with downstream tasks such as knowledge discovery, medical report generation, medical question answering, and clinical decision making. We investigate the effectiveness of transfer learning on the modality classification task using state-of-the-art deep convolutional neural networks pretrained on generic images. We also compare the performance of the traditional pipeline of handcrafted features with multi-label learning algorithms with end-to-end deep learning features for the concept detection task. Experimental results on the modality classification task show that transfer learning can leverage the patterns learned from large training data to the medical domain where little labeled data is available. Moreover, results on the concept detection task show that the deep learning approach provides better and more powerful feature representations compared to handcrafted feature extraction methods. The results on both tasks suggest that deep transfer learning methods are effective in the medical domain where data is scarce.
CR Arias J, 2016, COMPUT VIS IMAGE UND, V151, P61, DOI 10.1016/j.cviu.2016.04.002
   Benites F, 2015, 2015 IEEE International Conference on Data Mining Workshop (ICDMW), P847, DOI 10.1109/ICDMW.2015.14
   Chollet F., 2015, KERAS
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Demner-Fushman  D., 2012, J COMPUTING SCI ENG, V6, P166
   Demner-Fushman D, 2016, J AM MED INFORM ASSN, V23, P304, DOI 10.1093/jamia/ocv080
   Dimitrovski I, 2015, COMPUT MED IMAG GRAP, V39, P14, DOI 10.1016/j.compmedimag.2014.06.005
   Duda R. O., 1973, PATTERN CLASSIFICATI
   Erickson BJ, 2018, J AM COLL RADIOL, V15, P521, DOI 10.1016/j.jacr.2017.12.027
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Garcia Seco de Herrera  A., 2015, WORKING NOTES CLEF 2
   Gibaja  E., WILEY INTERDISCIPLIN, V4, P411
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hosmer Jr. D. W., 2005, APPL LOGISTIC REGRES
   Howard A. G., 2017, ABS170404861 CORR
   Junghwan  C., 2015, 151106348V2 CORR
   Kim HJ, 2017, INT C CONTROL DECISI, P1, DOI 10.1109/CoDIT.2017.8102557
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lan RS, 2018, MULTIMED TOOLS APPL, V77, P10853, DOI 10.1007/s11042-017-5341-2
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Meier C, 2013, AM J PREV MED, V44, pS5, DOI 10.1016/j.amepre.2012.09.018
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1007/BF00116251
   Rajpurkar  P., 2017, ABS171105225 CORR
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Read J, 2009, LECT NOTES ARTIF INT, V5782, P254
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Szegedy  C., 2017, INCEPTION V4 INCEPTI
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang JS, 2014, INT J AUTOM COMPUT, V11, P72, DOI 10.1007/s11633-014-0767-8
   Wang LM, 2017, SCI REP-UK, V7, DOI 10.1038/srep41545
   Webb G. I., 2010, NAIVE BAYES, P713
   Yu  Y., 2017, INFORMATION, V8
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
NR 42
TC 0
Z9 0
SN 2151-2191
BN 978-1-7281-0125-5
PY 2018
ER

PT B
AU Sumon, SA
   Chowdhury, J
   Debnath, S
   Mohammed, N
   Momen, S
AF Sumon, Shakil Ahmed
   Chowdhury, Joydip
   Debnath, Sujit
   Mohammed, Nabeel
   Momen, Sifat
GP IEEE
TI Bangla Short Speech Commands Recognition Using Convolutional Neural
   Networks
SO 2018 INTERNATIONAL CONFERENCE ON BANGLA SPEECH AND LANGUAGE PROCESSING
   (ICBSLP)
CT International Conference on Bangla Speech and Language Processing
   (ICBSLP)
CY SEP 21-22, 2018
CL Sylhet, BANGLADESH
DE Automatic Speech Recognition; Bangla Speech Recognition; Short Speech
   Commands; MFCC; Transfer learning; Convolutional neural network
AB Despite being one of the most widely spoken languages of the world, no significant efforts have been made in Bangla speech recognition. Speech recognition is a difficult task, particularly if the demand is to do so in noisy real-life conditions. In this study, Bangla short speech commands data set has been reported, where all the samples are taken in the real-life setting. Three different convolutional neural network (CNN) architectures have been designed to recognize those short speech commands. Mel-frequency cepstral coefficients (MFCC) features have been extracted from the audio files in one approach whereas only the raw audio files have been used in another CNN architecture. Lastly, a pre-trained model which is trained on a large English short speech commands data set has been fine-tuned by retraining on Bangla data set. Experimental results reveal that the MFCC model shows better accuracy in recognizing Bangla short speech commands where, surprisingly, the model predicting on raw audio data is very competitive. The models have shown proficiency in identifying single syllable words but encounter difficulties in recognizing multi-syllable commands.
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Ali M. A., 2013, AUTOMATIC SPEECH REC
   Deng L, 2014, INT CONF ACOUST SPEE
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   Hossain M. A., 2013, IMPLEMENTATION BACK
   Jaitly  N., 2012, P INTERSPEECH 2012
   Juangand B. H., 2005, AUTOMATIC SPEECH REC
   Muhammad Ghulam, 2009, Proceedings of the 2009 12th International Conference on Computer and Information Technology (ICCIT 2009), P379, DOI 10.1109/ICCIT.2009.5407267
   Paul AK, 2009, ICAPR 2009: SEVENTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION, PROCEEDINGS, P171, DOI 10.1109/ICAPR.2009.80
   Piczak KJ, 2015, IEEE INT WORKS MACH
   Qian YM, 2016, IEEE-ACM T AUDIO SPE, V24, P2263, DOI 10.1109/TASLP.2016.2602884
NR 11
TC 0
Z9 0
BN 978-1-5386-8207-4
PY 2018
ER

PT B
AU Zunair, H
   Mohammed, N
   Momen, S
AF Zunair, Hasib
   Mohammed, Nabeel
   Momen, Sifat
GP IEEE
TI Unconventional Wisdom: A New Transfer Learning Approach Applied to
   Bengali Numeral Classification
SO 2018 INTERNATIONAL CONFERENCE ON BANGLA SPEECH AND LANGUAGE PROCESSING
   (ICBSLP)
CT International Conference on Bangla Speech and Language Processing
   (ICBSLP)
CY SEP 21-22, 2018
CL Sylhet, BANGLADESH
DE bengali digit classification; deep learning; convolution neural
   networks; transfer learning; data augmentation; keras; Numtadb
AB In this modern age, natural language processing (NLP) is evolving due to advances in the field of deep learning and its access to huge amount of data and computation power. Recently a lot of attention has been given to OCR for Bangla, the 5th most widely spoken language in the world. This paper reports on certain rather unconventional transfer learning approaches used to attain 6th place in the Kaggle Numta competition, where the challenge was to classify images of isolated Bangla numerals. The best result reported in this paper is an accuracy of 97.09% on the NumtaDB Bengali handwritten digit datasets test set, which was obtained by freezing intermediate layers. The unconventional approach used in this paper produces better results than conventional transfer learning while taking less epochs and having almost half the number of trainable parameters.
CR Alam S., 2018, NUMTADB ASSEMBLED BE, DOI [10.13140/RG.2.2.33418.36800, DOI 10.13140/RG.2.2.33418.36800]
   Alex K., 2012, IMAGENET CLASSIFICAT
   Bengio Y., 2012, J MACHINE LEARNING R, P17
   Christian S., 2012, JMLR WORKSHOP C P, V27, P17
   Geoffrey H., 2015, DISTILLING KNOWLEDGE
   Gopalakrishnan K, 2017, CONSTR BUILD MATER, V157, P322, DOI [10.1016/j.conbuildmat2017.09.110, 10.1016/j.conbuildmat.2017.09.110]
   Jason Y., 2012, TRANSFERABLE ARE FEA
   Karen S., 2015, VERY DEEP CONVOLUTIO
   Mithun B., 2017, DATA IN BRIEF
   Saha Sourajit, 2018, Procedia Computer Science, V132, P1760, DOI 10.1016/j.procs.2018.05.151
   Sharif SMA, 2016, INT C COMP ELEC ENG, P463, DOI 10.1109/ICECE.2016.7853957
   Sharif S. M. A., 2018, P INT C COMP COMM SY, P403
   Shopon Md, 2016, 2016 International Workshop on Computational Intelligence (IWCI), P64, DOI 10.1109/IWCI.2016.7860340
   Shopon M., 2017, IM VIS PATT REC ICIV, P1
NR 14
TC 0
Z9 0
BN 978-1-5386-8207-4
PY 2018
ER

PT S
AU Liu, SH
   Shang, Y
   Han, JZ
   Wang, X
   Gao, HC
   Liu, DQ
AF Liu, Shaohua
   Shang, Yan
   Han, Jizhong
   Wang, Xi
   Gao, Hongchao
   Liu, Dongqin
BE Zeng, B
   Huang, Q
   ElSaddik, A
   Li, H
   Jiang, S
   Fan, X
TI Multi-lingual Scene Text Detection Based on Fully Convolutional Networks
SO ADVANCES IN MULTIMEDIA INFORMATION PROCESSING - PCM 2017, PT I
SE Lecture Notes in Computer Science
CT 18th Pacific-Rim Conference on Multimedia (PCM)
CY SEP 28-29, 2017
CL Harbin, PEOPLES R CHINA
DE Sence text detection; Multi-language; Transfer learning; Fully
   convolution networks
ID LOCALIZATION; IMAGES
AB In the paper, we propose a method based on transfer learning to detect multi-lingual text in natural scenes. First, a semantic segmentation map of the input image is obtained through a fully convolution network (FCN). In this map, each pixel is classified to text or none-text. And then, the candidate boxes of text regions are computed based on the map. In this procedure, VGG network is trained to obtain a basic character classifier of single language. Based on this VGG model, FCN has the ability to classify each pixel to text or none-text for multi-lingual with doing transfer learning. Finally, the bounding boxes of text are carry out by filtering the unsatisfied candidates with some rules. The experimental results show that our method achieves good performance on the task of multi-lingual text detection. And compared with other advanced method, the time cost of our method is shortest.
CR Cho H, 2016, PROC CVPR IEEE, P3566, DOI 10.1109/CVPR.2016.388
   Do C., 2005, P ADV NEUR INF PROC, P299
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157
   Kang L, 2014, PROC CVPR IEEE, P4034, DOI 10.1109/CVPR.2014.514
   Liao M., 2016, ARXIV161106779
   Liu CM, 2005, EIGHTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS 1 AND 2, PROCEEDINGS, P610
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yi CC, 2012, IEEE T IMAGE PROCESS, V21, P4256, DOI 10.1109/TIP.2012.2199327
   Yin XC, 2015, IEEE T PATTERN ANAL, V37, P1930, DOI 10.1109/TPAMI.2014.2388210
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Zhang Z, 2016, PROC CVPR IEEE, P4159, DOI 10.1109/CVPR.2016.451
NR 21
TC 0
Z9 0
SN 0302-9743
EI 1611-3349
BN 978-3-319-77380-3; 978-3-319-77379-7
PY 2018
VL 10735
BP 423
EP 432
DI 10.1007/978-3-319-77380-3_40
PN I
ER

PT S
AU Oliveira, H
   dos Santos, JA
AF Oliveira, Hugo
   dos Santos, Jefersson A.
GP IEEE
TI Deep Transfer Learning for Segmentation of Anatomical Structures in
   Chest Radiographs
SO PROCEEDINGS 2018 31ST SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND
   IMAGES (SIBGRAPI)
SE SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing
CT 31st Conference on Graphics, Patterns and Images (SIBGRAPI)
CY OCT 29-NOV 01, 2018
CL Foz do Iguacu, BRAZIL
AB Segmentation of anatomical structures in Chest Posterior-Anterior Radiographs is a classical task on biomedical image analysis. Deep Learning has been widely used for detection and diagnosis of illnesses in several medical image modalities over the last years, but the portability of deep methods is still limited, hampering the reusability of pre-trained models in new data. We address this problem by proposing a novel method for Cross-Dataset Transfer Learning in Chest X-Ray images based on Unsupervised Image Translation architectures. Our Transfer Learning approach achieved Jaccard values of 88.20% on lung field segmentation in the Montgomery Set by using a pre-trained model on the JSRT dataset and no labeled data from the target dataset. Several experiments in unsupervised and semi-supervised transfer were performed and our method consistently outperformed simple fine-tuning when a limited amount of labels is used. Qualitative analysis on the tasks of clavicle and heart segmentation are also performed on Montgomery samples and pre-trained models from JSRT dataset. Our secondary contributions encompass several experiments in anatomical structure segmentation on JSRT, achieving state-of-the-art results in lung field (96.02%), heart (89.64%) and clavicle segmentation (87.30%).
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Buades A, 2005, PROC CVPR IEEE, P60
   Candemir S, 2014, IEEE T MED IMAGING, V33, P577, DOI 10.1109/TMI.2013.2290491
   Chen HC, 2015, MED TEACH, V37, P1090, DOI 10.3109/0142159X.2015.1009431
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Dai W., 2018, SCAN STRUCTURE CORRE
   Efros AA, 2001, COMP GRAPH, P341
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR, P2672, DOI [DOI 10.1017/CBO9781139058452, DOI 10.1001/JAMAINTERNMED.2016.8245]
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hogeweg L, 2012, MED IMAGE ANAL, V16, P1490, DOI 10.1016/j.media.2012.06.009
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang X, 2018, ARXIV180404732
   Isola Phillip, 2016, ARXIV161107004
   Jaeger S, 2014, QUANT IMAGING MED SU, V4, P475, DOI 10.3978/j.issn.2223-4292.2014.11.20
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Liu  M., 2017, ADV NEURAL INFORM PR, P700
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Novikov A, 2018, IEEE T MED IMAGING
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Ronneberger O, 2015, INT C MED IM COMP CO, P234, DOI DOI 10.1007/978-3-319-24574-4_28
   Shiraishi J, 2000, AM J ROENTGENOL, V174, P71, DOI 10.2214/ajr.174.1.1740071
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   van Ginneken B, 2006, MED IMAGE ANAL, V10, P19, DOI 10.1016/j.media.2005.02.002
   Wang XS, 2017, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2017.369
   Zhang J., 2017, TRANSFER LEARNING CR
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhu J.-Y., 2017, ARXIV170310593
NR 30
TC 0
Z9 0
SN 1530-1834
BN 978-1-5386-9264-6
PY 2018
BP 204
EP 211
DI 10.1109/SIBGRAPI.2018.00033
ER

PT S
AU Zanlorensi, LA
   Luz, E
   Laroca, R
   Britto, AS
   Oliveira, LS
   Menotti, D
AF Zanlorensi, Luiz A.
   Luz, Eduardo
   Laroca, Rayson
   Britto, Alceu S., Jr.
   Oliveira, Luiz S.
   Menotti, David
GP IEEE
TI The Impact of Preprocessing on Deep Representations for Iris Recognition
   on Unconstrained Environments
SO PROCEEDINGS 2018 31ST SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND
   IMAGES (SIBGRAPI)
SE SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing
CT 31st Conference on Graphics, Patterns and Images (SIBGRAPI)
CY OCT 29-NOV 01, 2018
CL Foz do Iguacu, BRAZIL
AB The use of iris as a biometric trait is widely used because of its high level of distinction and uniqueness. Nowadays, one of the major research challenges relies on the recognition of iris images obtained in visible spectrum under unconstrained environments. In this scenario, the acquired iris are affected by capture distance, rotation, blur, motion blur, low contrast and specular reflection, creating noises that disturb the iris recognition systems. Besides delineating the iris region, usually preprocessing techniques such as normalization and segmentation of noisy iris images are employed to minimize these problems. But these techniques inevitably run into some errors. In this context, we propose the use of deep representations, more specifically, architectures based on VGG and ResNet-50 networks, for dealing with the images using (and not) iris segmentation and normalization. We use transfer learning from the face domain and also propose a specific data augmentation technique for iris images. Our results show that the approach using non-normalized and only circle-delimited iris images reaches a new state of the art in the official protocol of the NICE.II competition, a subset of the UBIRIS database, one of the most challenging databases on unconstrained environments, reporting an average Equal Error Rate (EER) of 13.98% which represents an absolute reduction of about 5%.
CR Ahmed NU, 2017, PATTERN RECOGN LETT, V91, P11, DOI 10.1016/j.patrec.2017.03.003
   Ahmed NU, 2016, INT C PATT RECOG, P176, DOI 10.1109/ICPR.2016.7899629
   Al-Waisy A. S., 2017, PATTERN ANAL APPL
   Andersen-Hoppe E., 2017, INT WORKSH BIOM FOR, P1
   Bowyer KW, 2008, COMPUT VIS IMAGE UND, V110, P281, DOI 10.1016/j.cviu.2007.08.005
   Bowyer KW, 2012, PATTERN RECOGN LETT, V33, P965, DOI 10.1016/j.patrec.2011.11.024
   Cao Q., 2017, CORR
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Daugman J, 2003, PATTERN RECOGN, V36, P279
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   De Marsico M, 2017, PATTERN RECOGN LETT, V91, P3, DOI 10.1016/j.patrec.2016.12.013
   De Marsico M, 2016, PATTERN RECOGN LETT, V82, P106, DOI 10.1016/j.patrec.2016.02.001
   De Marsico M, 2012, PATTERN RECOGN LETT, V33, P1006, DOI 10.1016/j.patrec.2011.09.010
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Du Y., 2016, P IEEE S VLSI CIRC J, P1
   Enright A. J., 2012, CORR
   Gangwar A, 2016, IEEE IMAGE PROC, P2301, DOI 10.1109/ICIP.2016.7532769
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Lei Z, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4563043
   Li PH, 2012, PATTERN RECOGN LETT, V33, P1012, DOI 10.1016/j.patrec.2011.06.017
   Li PH, 2012, PATTERN RECOGN LETT, V33, P1000, DOI 10.1016/j.patrec.2011.06.018
   Liu NAF, 2016, PATTERN RECOGN LETT, V82, P154, DOI 10.1016/j.patrec.2015.09.016
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lucio D. R., 2018, CORR
   Luz E., 2017, PATTERN RECOGNITION
   Marra F., 2017, PATTERN RECOGNITION
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Nguyen K, 2018, IEEE ACCESS, V6, P18848, DOI 10.1109/ACCESS.2017.2784352
   Parkhi Omkar M., 2015, P BRIT MACHINE VISIO, P1, DOI DOI 10.5244/C.29.41
   Phillips P. J., 2008, BIOM THEOR APPL SYST, P1
   Proenca H, 2018, IEEE T INF FOREN SEC, V13, P888, DOI 10.1109/TIFS.2017.2771230
   Proenca H, 2017, PROC CVPR IEEE, P6747, DOI 10.1109/CVPR.2017.714
   Proenca H, 2012, IEEE T INF FOREN SEC, V7, P798, DOI 10.1109/TIFS.2011.2177659
   Proenca H, 2010, IEEE T PATTERN ANAL, V32, P1529, DOI 10.1109/TPAMI.2009.66
   Santos G, 2012, PATTERN RECOGN LETT, V33, P984, DOI 10.1016/j.patrec.2011.08.017
   Severo E., 2018, CORR
   Sher A, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON FUNCTIONAL-STRUCTURAL PLANT GROWTH MODELING, SIMULATION, VISUALIZATION AND APPLICATIONS (FSPMA), P189, DOI 10.1109/FSPMA.2016.7818306
   Shin KY, 2012, PATTERN RECOGN LETT, V33, P991, DOI 10.1016/j.patrec.2011.08.016
   Simard PY, 2003, SEVENTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS I AND II, PROCEEDINGS, P958
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Szewczyk R, 2012, PATTERN RECOGN LETT, V33, P1019, DOI 10.1016/j.patrec.2011.08.018
   Tan TN, 2012, PATTERN RECOGN LETT, V33, P970, DOI 10.1016/j.patrec.2011.08.009
   Tan TN, 2010, IMAGE VISION COMPUT, V28, P223, DOI 10.1016/j.imavis.2009.05.008
   Tapia J, 2017, ADV COMPUT VIS PATT, P219, DOI 10.1007/978-3-319-61657-5_9
   Wang Q, 2012, PATTERN RECOGN LETT, V33, P978, DOI 10.1016/j.patrec.2011.08.014
   Wildes RP, 1997, P IEEE, V85, P1348, DOI 10.1109/5.628669
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 48
TC 0
Z9 0
SN 1530-1834
BN 978-1-5386-9264-6
PY 2018
BP 289
EP 296
DI 10.1109/SIBGRAPI.2018.00044
ER

PT S
AU Nazare, TS
   da Costa, GBP
   de Mello, RF
   Ponti, MA
AF Nazare, Tiago S.
   Paranhos da Costa, Gabriel B.
   de Mello, Rodrigo F.
   Ponti, Moacir A.
GP IEEE
TI Color quantization in transfer learning and noisy scenarios: an
   empirical analysis using convolutional networks
SO PROCEEDINGS 2018 31ST SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND
   IMAGES (SIBGRAPI)
SE SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing
CT 31st Conference on Graphics, Patterns and Images (SIBGRAPI)
CY OCT 29-NOV 01, 2018
CL Foz do Iguacu, BRAZIL
AB Transfer learning is seen as one of the most promising areas of machine learning. Lately, features from pre-trained models have been used to achieve state-of-the-art results in several machine vision problems. Those models are usually employed when the problem of interest does not have enough supervised examples to support the network training from scratch. Most applications use networks pre-trained on noise-free RGB image datasets, what is observed even when the target domain counts on grayscale images or when data is degraded by noise. In this paper, we evaluate the use of Convolutional Neural Networks (CNNs) on such transfer learning scenarios and the impact of using RGB trained networks on grayscale image tasks. Our results confirm that the use of networks trained using colored images on grayscale tasks hinders the overall performance when compared to a similar network trained on a quantized version of the original dataset. Results also show that higher quantization levels (resulting in less colors) increase the robustness of CNN features in the presence of noise.
CR Ahn J., 2018, PLOS ONE, V13
   Barbosa IB, 2018, COMPUT VIS IMAGE UND, V167, P50, DOI 10.1016/j.cviu.2017.12.002
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   da Costa G. B. Paranhos, 2016, 12 WORKSH VIS COMP W
   DALAL N, 2005, PROC CVPR IEEE, P886, DOI DOI 10.1109/CVPR.2005.177
   Deng J., 2009, CVPR09
   Dodge Samuel F., 2016, 8 INT C QUAL MULT EX, V2016, P1, DOI DOI 10.1109/QOMEX.2016.7498955
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Kanan C, 2012, PLOS ONE, V7, P133, DOI 10.1371/journal.pone.0029740
   Krizhevsky A., 2009, THESIS
   Kylberg G, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-17
   Nazare Tiago S., 2018, Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. 22nd Iberoamerican Congress, CIARP 2017. Proceedings: LNCS 10657, P416, DOI 10.1007/978-3-319-75193-1_50
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Ponti M, 2016, NEUROCOMPUTING, V173, P385, DOI 10.1016/j.neucom.2015.04.114
   Ponti MA, 2017, SIBGRAPI, P17, DOI 10.1109/SIBGRAPI-T.2017.12
   Ravanbakhsh M, 2018, IEEE WINT CONF APPL, P1689, DOI 10.1109/WACV.2018.00188
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Springenberg J. T., 2014, 14126806 ARXIV
   Szegedy  C., 2015, COMPUTER VISION PATT
   Xiao H., 2017, FASHION MNIST NOVEL
NR 21
TC 0
Z9 0
SN 1530-1834
BN 978-1-5386-9264-6
PY 2018
BP 377
EP 383
DI 10.1109/SIBGRAPI.2018.00055
ER

PT S
AU Peixinho, AZ
   Benato, BC
   Nonato, LG
   Falcao, AX
AF Peixinho, Alan Z.
   Benato, Barbara C.
   Nonato, Luis G.
   Falcao, Alexandre X.
GP IEEE
TI Delaunay Triangulation Data Augmentation guided by Visual Analytics for
   Deep Learning
SO PROCEEDINGS 2018 31ST SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND
   IMAGES (SIBGRAPI)
SE SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing
CT 31st Conference on Graphics, Patterns and Images (SIBGRAPI)
CY OCT 29-NOV 01, 2018
CL Foz do Iguacu, BRAZIL
AB It is well known that image classification problems can be effectively solved by Convolutional Neural Networks (CNNs). However, the number of supervised training examples from all categories must be high enough to avoid model over-fitting. In this case, two key alternatives are usually presented (a) the generation of artificial examples, known as data augmentation, and (b) reusing a CNN previously trained over a large supervised training set from another image classification problem - a strategy known as transfer learning. Deep learning approaches have rarely exploited the superior ability of humans for cognitive tasks during the machine learning loop. We advocate that the expert intervention through visual analytics can improve machine learning. In this work, we demonstrate this claim by proposing a data augmentation framework based on Encoder-Decoder Neural Networks (EDNNs) and visual analytics for the design of more effective CNN-based image classifiers. An EDNN is initially trained such that its encoder extracts a feature vector from each training image. These samples are projected from the encoder feature space on to a 2D coordinate space. The expert includes points to the projection space and the feature vectors of the new samples are obtained on the original feature space by interpolation. The decoder generates artificial images from the feature vectors of the new samples and the augmented training set is used to improve the CNN-based classifier. We evaluate methods for the proposed framework and demonstrate its advantages using data from a real problem as case study - the diagnosis of helminth eggs in humans. We also show that transfer learning and data augmentation by affine transformations can further improve the results.
CR Bargal SA, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P433, DOI 10.1145/2993148.2997627
   Caruana R., 1994, ADV NEURAL INFORM SY, V7, P657
   Cashman  D., 2017, VADL2017 WORKSH VIS
   Ciresan  D.C., 2012, ABS12022745 CORR
   Ciresan  D.C., 2011, ABS11020183 CORR
   Cox D, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385
   Glorot X., 2010, JLMR P TRACK, V9, P249, DOI DOI 10.1.1/207.2059
   Goodfellow  I., 2014, ADV NEURAL INFORM PR, P2672
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
   He K., 2015, DEEP RESIDUAL LEARNI
   Hoferlin B, 2012, IEEE CONF VIS ANAL, P23, DOI 10.1109/VAST.2012.6400492
   Huang G, 2016, ARXIV160806993
   Jaegul Choo, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P27, DOI 10.1109/VAST.2010.5652443
   Jeong DH, 2009, COMPUT GRAPH FORUM, V28, P767, DOI 10.1111/j.1467-8659.2009.01475.x
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Joia P, 2011, IEEE T VIS COMPUT GR, V17, P2563, DOI 10.1109/TVCG.2011.220
   Kingma  D., 2013, AUTOENCODING VARIATI
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Liu MC, 2017, IEEE T VIS COMPUT GR, V23, P91, DOI 10.1109/TVCG.2016.2598831
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Peixinho  A.Z., 2017, THESIS
   Pezzotti N, 2018, IEEE T VIS COMPUT GR, V24, P98, DOI 10.1109/TVCG.2017.2744358
   Rauber P., 2016, EUROVIS 16, P73
   Rauber P., 2017, INFORM VISUALIZATION
   Rauber  P.E., 2017, IEEE T VIS COMP GRAP, V23
   Salakhutdinov R., 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Simard P. Y., 2003, BEST PRACTICES CONVO
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Suzuki CTN, 2013, I S BIOMED IMAGING, P460
   Suzuki CTN, 2013, IEEE T BIO-MED ENG, V60, P803, DOI 10.1109/TBME.2012.2187204
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taylor  L., 2017, ABS170806020 CORR
   Torrey  L., 2009, TRANSFER LEARNING
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wang D., 2016, ARXIV160605718
   Wang YS, 2016, NEUROCOMPUTING, V184, P232, DOI 10.1016/j.neucom.2015.08.104
   Wong  S., 2016, ABS160908764 CORR
   Wright  G.B., 2003, RADIAL BASIS FUNCTIO
   Xia  Y., 2015, FINE TUNING IMAGE ST
   Zhang  X., 2015, ABS150303163 CORR
NR 46
TC 0
Z9 0
SN 1530-1834
BN 978-1-5386-9264-6
PY 2018
BP 384
EP 391
DI 10.1109/SIBGRAPI.2018.00056
ER

PT S
AU Jader, G
   Fontinele, J
   Ruiz, M
   Abdalla, K
   Pithon, M
   Oliveira, L
AF Jader, Gil
   Fontinele, Jefferson
   Ruiz, Marco
   Abdalla, Kalyf
   Pithon, Matheus
   Oliveira, Luciano
GP IEEE
TI Deep instance segmentation of teeth in panoramic X-ray images
SO PROCEEDINGS 2018 31ST SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND
   IMAGES (SIBGRAPI)
SE SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing
CT 31st Conference on Graphics, Patterns and Images (SIBGRAPI)
CY OCT 29-NOV 01, 2018
CL Foz do Iguacu, BRAZIL
ID DENTAL PERIAPICAL RADIOGRAPHS; BONE-MINERAL DENSITY; ALGORITHM;
   FRAMEWORK; FILTER; SET
AB In dentistry, radiological examinations help specialists by showing structure of the tooth bones with the goal of screening embedded teeth, bone abnormalities, cysts, tumors, infections, fractures, problems in the temporomandibular regions, just to cite a few. Sometimes, relying solely in the specialist's opinion can bring differences in the diagnoses, which can ultimately hinder the treatment. Although tools for complete automatic diagnosis are no yet expected, image pattern recognition has evolved towards decision support, mainly starting with the detection of teeth and their components in X-ray images. Tooth detection has been object of research during at least the last two decades, mainly relying in threshold and region-based methods. Following a different direction, this paper proposes to explore a deep learning method for instance segmentation of the teeth. To the best of our knowledge, it is the first system that detects and segment each tooth in panoramic X-ray images. It is noteworthy that this image type is the most challenging one to isolate teeth, since it shows other parts of patient's body (e.g., chin, spine and jaws). We propose a segmentation system based on mask regionbased convolutional neural network to accomplish an instance segmentation. Performance was thoroughly assessed from a 1500 challenging image data set, with high variation and containing 10 categories of different types of buccal image. By training the proposed system with only 193 images of mouth containing 32 teeth in average, using transfer learning strategies, we achieved 98% of accuracy, 88% of Fl-score, 94% of precision, 84% of recall and 99% of specificity over 1224 unseen images, results very superior than other 10 unsupervised methods.
CR Ait Skourt B, 2018, PROCEDIA COMPUT SCI, V127, P109, DOI 10.1016/j.procs.2018.01.104
   Ajaz A, 2013, 2013 INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND SIGNAL PROCESSING (ICCSP), P717, DOI 10.1109/iccsp.2013.6577149
   Ali R. B., 2015, INT C INT SYST DES A, V1, P505
   Alsmadi M. K., 2015, AIN SHAMS ENG J
   Amer YY, 2015, PROCEDIA COMPUT SCI, V65, P718, DOI 10.1016/j.procs.2015.09.016
   Bruellmann D, 2016, COMPUT BIOL MED, V72, P212, DOI 10.1016/j.compbiomed.2016.03.019
   Cameriere R, 2015, J FORENSIC RADIOL IM, V3, P61, DOI 10.1016/j.jofri.2014.10.001
   Dai W., 2017, SCAN STRUCTURE CORRE
   Dighe S., 2012, INT J SCI APPL INF T, V1, P52
   Economopotilos T, 2008, DENTOMAXILLOFAC RAD, V37, P185, DOI 10.1259/dmfr/26553364
   Geraets WGM, 2007, BONE, V40, P1217, DOI 10.1016/j.bone.2007.01.009
   Grafova L, 2013, DENTOMAXILLOFAC RAD, V42, DOI 10.1259/dmfr.20120391
   Hasan M. M., 2016, WAC, V1, P1
   He K., 2017, CORR
   Huang CH, 2008, ORAL SURG ORAL MED O, V105, P649, DOI 10.1016/j.tripleo.2007.08.019
   Indraswari R, 2015, INT CONF INFORM COMM, P49, DOI 10.1109/ICTS.2015.7379870
   Jain AK, 2004, PATTERN RECOGN, V37, P1519, DOI 10.1016/j.patcog.2003.12.016
   Kaur J., 2016, INT J ADV RES COMPUT, V6, P158
   Keshtkar F., 2007, CAN C EL COMP ENG, P328, DOI DOI 10.1109/CCECE.2006.277656
   Kingma D. P., 2014, P 3 INT C LEARN REPR
   Son LH, 2016, EXPERT SYST APPL, V46, P380, DOI 10.1016/j.eswa.2015.11.001
   Li H, 2012, INT CONF SIGN PROCES, P877, DOI 10.1109/ICoSP.2012.6491720
   Li S, 2006, COMPUT MED IMAG GRAP, V30, P65, DOI 10.1016/j.compmedimag.2005.10.007
   Li S, 2007, PATTERN RECOGN, V40, P2861, DOI 10.1016/j.patcog.2007.01.012
   Lin PL, 2015, COMPUT METH PROG BIO, V121, P117, DOI 10.1016/j.cmpb.2015.05.004
   Lin PL, 2013, INT CONF SYST SCI EN, P407, DOI 10.1109/ICSSE.2013.6614700
   Lin PL, 2014, COMPUT METH PROG BIO, V113, P433, DOI 10.1016/j.cmpb.2013.10.015
   Lin PL, 2010, PATTERN RECOGN, V43, P1380, DOI 10.1016/j.patcog.2009.10.005
   Lin PL, 2012, PATTERN RECOGN, V45, P934, DOI 10.1016/j.patcog.2011.08.027
   Lin T., 2014, CORR
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lurie A, 2012, OR SURG OR MED OR PA, V113, P549, DOI 10.1016/j.oooo.2011.10.002
   Modi CK, 2011, CAN CON EL COMP EN, P504, DOI 10.1109/CCECE.2011.6030501
   Niroshika AA, 2013, PROCEEDINGS OF THE 2013 8TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE & EDUCATION (ICCSE 2013), P396
   Nomir O, 2008, IEEE T INF FOREN SEC, V3, P223, DOI 10.1109/TIFS.2008.919343
   Nomir O, 2008, PATTERN RECOGN, V41, P130, DOI 10.1016/j.patcog.2007.05.015
   Phen-Lan Lin, 2012, 2012 International Conference on Machine Learning and Cybernetics (ICMLC 2012). Proceedings, P1821, DOI 10.1109/ICMLC.2012.6359652
   Rad Abdolvahab Ehsani, 2013, TELKOMNIKA, V11, P3109, DOI DOI 10.11591/ITELKOMNIKA.V11I6.2655
   Razali M. R. M., 2015, INT C COMP ASS SYST, P62
   Razali MRM, 2014, 2014 INTERNATIONAL CONFERENCE ON COMPUTER, COMMUNICATIONS, AND CONTROL TECHNOLOGY (I4CT), P353, DOI 10.1109/I4CT.2014.6914204
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O., 2015, U NET CONVOLUTIONAL, P1, DOI DOI 10.1007/978-3-319-24574-428
   Said EH, 2006, IEEE T INF FOREN SEC, V1, P178, DOI 10.1109/TIFS.2006.873606
   Senthilkumaran N., 2012, INT J ADV RES COMPUT, V1, P5236
   Senthilkumaran N., 2012, INT J COMPUTER SCI I, V3, P5236
   Silva G, 2018, EXPERT SYST APPL, V107, P15, DOI 10.1016/j.eswa.2018.04.001
   Sirinukunwattana K, 2016, IEEE T MED IMAGING, V35, P1196, DOI 10.1109/TMI.2016.2525803
   Subramanyam RB, 2014, INT J ENG RES APPL, V4, P173
   Tikhe SV, 2016, INT CONF ADV COMPU, P225, DOI 10.1109/IACC.2016.50
   Trivedi D. N., 2015, INT J ADV COMPUTER R, V4, P985
   Wang CW, 2016, MED IMAGE ANAL, V31, P63, DOI 10.1016/j.media.2016.02.004
NR 51
TC 0
Z9 0
SN 1530-1834
BN 978-1-5386-9264-6
PY 2018
BP 400
EP 407
DI 10.1109/SIBGRAPI.2018.00058
ER

PT S
AU Masi, I
   Wu, Y
   Hassner, T
   Natarajan, P
AF Masi, Iacopo
   Wu, Yue
   Hassner, Tal
   Natarajan, Prem
GP IEEE
TI Deep Face Recognition: a Survey
SO PROCEEDINGS 2018 31ST SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND
   IMAGES (SIBGRAPI)
SE SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing
CT 31st Conference on Graphics, Patterns and Images (SIBGRAPI)
CY OCT 29-NOV 01, 2018
CL Foz do Iguacu, BRAZIL
AB Face recognition made tremendous leaps in the last five years with a myriad of systems proposing novel techniques substantially backed by deep convolutional neural networks (DCNN). Although face recognition performance sky-rocketed using deep-learning in classic datasets like LFW, leading to the belief that this technique reached human performance, it still remains an open problem in unconstrained environments as demonstrated by the newly released IJB datasets.
   This survey aims to summarize the main advances in deep face recognition and, more in general, in learning face representations for verification and identification. The survey provides a clear, structured presentation of the principal, state-of-the-art (SOTA) face recognition techniques appearing within the past five years in top computer vision venues.
   The survey is broken down into multiple parts that follow a standard face recognition pipeline: (a) how SOTA systems are trained and which public data sets have they used; (b) face preprocessing part (detection, alignment, etc.); (c) architecture and loss functions used for transfer learning (d) face recognition for verification and identification. The survey concludes with an overview of the SOTA results at a glance along with some open issues currently overlooked by the community.
CR Bansal A., 2017, IJCB
   Bansal A., 2017, ICCV WORKSH
   Bledsoe W., 1966, MAN MACHINE FACIAL R
   Bledsoe W. W., 1966, PANORAMIC RES INC PA, V15, P2
   Borghi G., 2017, CVPR
   Bulat A., 2017, ICCV
   Cao C. L. X. T. Kaidi, 2018, CVPR
   Cao Q., 2018, AFGR
   Chang F., 2017, ICCV WORKSH
   Chatfield K., 2014, BMVC
   Chen J. -C., 2016, WACV
   Chen J. -C., 2015, CVPR WORKSH
   Crispell D. E., 2016, AIPR
   Crosswhite N., 2017, AFGR
   de Bittencourt Zavan F. H., 2017, SIBGRAPI
   Ferrari C., 2015, 3DV
   Ferrari C., 2018, TIP
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   Fukushima K., 1982, COMPETITION COOPERAT, P267, DOI DOI 10.1007/978-3-642-46466-9_18
   Goodfellow I., 2014, NIPS
   Guo Y., 2016, ECCV
   Hadsell R., 2006, CVPR
   Hassner T., 2016, CVPR WORKSH JUN
   Hassner T., 2015, CVPR
   He K., 2016, CVPR
   Huang G. B., 2007, 0749 UMASS
   Jaderberg M., 2015, NIPS
   Kemelmacher-Shlizerman I., 2016, CVPR
   Kim K., 2018, WACV
   Klare B. F., 2015, CVPR
   Klontz J., 2013, BTAS
   Krizhevsky  A., 2012, NIPS
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lin J., 2017, ACM ICMR
   Liu W., 2016, ICML
   Liu W., 2017, CVPR, V1
   Liu Y., 2018, CVPR
   Masi I., 2014, ICPR
   Masi I., 2017, AFGR
   Masi I., 2016, ECCV
   Masi I., 2018, TPAMI
   Masi I., 2016, CVPR
   Maze B., 2018, IJCB
   Mnih A., 2009, NIPS
   Nech A., 2017, CVPR
   Olga R., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, 10.1007/s11263-015-0816-y]
   Parkhi O. M., 2015, BMVC
   Peng X., 2017, ICCV
   Phillips P. J., 2011, HDB FACE RECOGNITION, P551
   Ranjan R., 2017, AFGR
   Ranjan R., 2017, ARXIV170309507
   Sankaranarayanan S., 2016, BTAS
   Sankaranarayanan S, 2016, ARXIV160203418
   Schroff F., 2015, CVPR
   Song H. Oh, 2016, CVPR
   Springenberg J. T., 2016, ICLR
   Sun Y., 2014, CVPR
   Sun Y., 2015, ARXIV150200873
   Taigman Y., 2014, CVPR
   Tran A. T., 2017, CVPR
   Tran L., 2017, CVPR
   Wang DY, 2017, IEEE T PATTERN ANAL, V39, P1122, DOI 10.1109/TPAMI.2016.2582166
   Wang F., 2018, ECCV
   Wen Y., 2016, ECCV
   Whitelam C., 2017, CVPR WORKSH JUL
   Wolf L., 2011, CVPR
   Wolf L., 2009, CVPR
   Wu C. -Y., 2017, ICCV
   Wu Y., 2018, TPAMI
   Wu Y., 2017, IEEE T CONTR SYST T, VPP, P1
   Yang J., 2017, CVPR
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   Yang S, 2018, IEEE T PATTERN ANAL, V40, P1845, DOI 10.1109/TPAMI.2017.2738644
   Yi D., 2014, LEARNING FACE REPRES, V1411, P7923
   Yucel M. K., 2018, ARXIV180507566
   Zafeiriou S, 2015, COMPUT VIS IMAGE UND, V138, P1, DOI 10.1016/j.cviu.2015.03.015
   Zhao J., 2018, TPAMI
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zheng Y., 2018, CVPR
   Zhou E., 2015, ARXIV150104690
   Zhu X., 2016, CVPR
NR 81
TC 0
Z9 0
SN 1530-1834
BN 978-1-5386-9264-6
PY 2018
BP 471
EP 478
DI 10.1109/SIBGRAPI.2018.00067
ER

PT S
AU Liu, LJ
   Lu, JW
   Zhou, J
AF Liu, Lijie
   Lu, Jiwen
   Zhou, Jie
BA Kosecka, J
BF Kosecka, J
BE Maciejewski, AA
   Okamura, A
   Bicchi, A
   Stachniss, C
   Song, DZ
   Lee, DH
   Chaumette, F
   Ding, H
   Li, JS
   Wen, J
   Roberts, J
   Masamune, K
   Chong, NY
   Amato, N
   Tsagwarakis, N
   Rocco, P
   Asfour, T
   Chung, WK
   Yasuyoshi, Y
   Sun, Y
   Maciekeski, T
   Althoefer, K
   AndradeCetto, J
   Chung, WK
   Demircan, E
   Dias, J
   Fraisse, P
   Gross, R
   Harada, H
   Hasegawa, Y
   Hayashibe, M
   Kiguchi, K
   Kim, K
   Kroeger, T
   Li, Y
   Ma, S
   Mochiyama, H
   Monje, CA
   Rekleitis, I
   Roberts, R
   Stulp, F
   Tsai, CHD
   Zollo, L
TI Adversarial Transfer Networks for Visual Tracking
SO 2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
CT 25th IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY OCT 01-05, 2018
CL Madrid, SPAIN
AB Visual tracking plays an important role in unmanned systems. In many cases, the system needs to keep track of targets it has never seen before, and the only training sample available is the specified object in the initial frame. In this paper, we propose a deep architecture called adversarial transfer networks (ATNet), which aims to make well use of offline video training data and solve the problem of lacking training samples in visual tracking. Different from most existing trackers which neglect significant differences between videos and gulp the training data all together, our method utilizes the special nature of tracking problem and concentrates on transferring domain-specific information across similar tracking tasks. We first propose an efficient way to select a training video that is most similar to online tracking task and regard it as source domain. With the labeled data in the selected source domain, we apply adversarial transfer learning to make the feature distribution of source-domain samples and target-domain samples as similar as possible. Therefore, the transferred sourcedomain samples can provide various possible appearance of tracked target for training and boost the tracking performance. Experimental results on three OTB tracking benchmarks show that our method outperforms the state-of-the-art trackers in both accuracy and robustness.
CR Bolme D. S., 2010, CVPR
   Bousmalis  K., 2017, CVPR
   Chatfield K., 2014, BMVC
   Danelljan  M., 2015, ICCVW
   Danelljan  M., 2017, CVPR
   Danelljan  M., 2016, ECCV
   Fu Y, 2008, IEEE T INF FOREN SEC, V3, P91, DOI 10.1109/TIFS.2007.916280
   Ganin Y, 2016, J MACH LEARN RES, V17
   Goodfellow I., 2014, NIPS
   Hu  J., 2015, CVPR
   Hu JL, 2016, IEEE T CIRC SYST VID, V26, P2056, DOI 10.1109/TCSVT.2015.2477936
   Kim T., 2017, ARXIV170305192
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Li  J., 2017, CVPR
   Ma  C., 2015, ICCV
   Ma  L., 2015, ICCV
   Nam  H., 2016, CVPR
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Qi  Y., 2016, CVPR
   Ren  L., 2018, ECCV
   Song  Y., 2017, ICCV
   Tao  R., 2016, CVPR
   Tzeng  E., 2017, CVPR
   Tzeng E., 2015, ICCV
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang  N., 2013, NIPS
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wulfmeier  M., 2018, ICRA
   Yun  S., 2017, CVPR
NR 29
TC 0
Z9 0
SN 2153-0858
BN 978-1-5386-8094-0
PY 2018
BP 75
EP 81
ER

PT S
AU Iuzzolino, ML
   Walker, ME
   Szafir, D
AF Iuzzolino, Michael L.
   Walker, Michael E.
   Szafir, Daniel
BA Kosecka, J
BF Kosecka, J
BE Maciejewski, AA
   Okamura, A
   Bicchi, A
   Stachniss, C
   Song, DZ
   Lee, DH
   Chaumette, F
   Ding, H
   Li, JS
   Wen, J
   Roberts, J
   Masamune, K
   Chong, NY
   Amato, N
   Tsagwarakis, N
   Rocco, P
   Asfour, T
   Chung, WK
   Yasuyoshi, Y
   Sun, Y
   Maciekeski, T
   Althoefer, K
   AndradeCetto, J
   Chung, WK
   Demircan, E
   Dias, J
   Fraisse, P
   Gross, R
   Harada, H
   Hasegawa, Y
   Hayashibe, M
   Kiguchi, K
   Kim, K
   Kroeger, T
   Li, Y
   Ma, S
   Mochiyama, H
   Monje, CA
   Rekleitis, I
   Roberts, R
   Stulp, F
   Tsai, CHD
   Zollo, L
TI Virtual-to-Real-World Transfer Learning for Robots on Wilderness Trails
SO 2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
CT 25th IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY OCT 01-05, 2018
CL Madrid, SPAIN
ID AERIAL
AB Robots hold promise in many scenarios involving outdoor use, such as search-and-rescue, wildlife management, and collecting data to improve environment, climate, and weather forecasting. However, autonomous navigation of outdoor trails remains a challenging problem. Recent work has sought to address this issue using deep learning. Although this approach has achieved state-of-the-art results, the deep learning paradigm may be limited due to a reliance on large amounts of annotated training data. Collecting and curating training datasets may not be feasible or practical in many situations, especially as trail conditions may change due to seasonal weather variations, storms, and natural erosion. In this paper, we explore an approach to address this issue through virtual-to-real-world transfer learning using a variety of deep learning models trained to classify the direction of a trail in an image. Our approach utilizes synthetic data gathered from virtual environments for model training, bypassing the need to collect a large amount of real images of the outdoors. We validate our approach in three main ways. First, we demonstrate that our models achieve classification accuracies upwards of 95% on our synthetic data set. Next, we utilize our classification models in the control system of a simulated robot to demonstrate feasibility. Finally, we evaluate our models on real-world trail data and demonstrate the potential of virtual-to-real-world transfer learning.
CR Chung J., 2014, CORR
   Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hsieh MA, 2007, J FIELD ROBOT, V24, P991, DOI 10.1002/rob.20222
   Jozefowicz R, 2015, P 32 INT C MACH LEAR, P2342, DOI DOI 10.1109/CVPR.2015.72987
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Peschel JM, 2013, IEEE T HUM-MACH SYST, V43, P53, DOI 10.1109/TSMCC.2012.2220133
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Santana P, 2013, J FIELD ROBOT, V30, P64, DOI 10.1002/rob.21423
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szafir D, 2017, INT J ROBOT RES, V36, P514, DOI 10.1177/0278364916688256
   Tai L, 2017, IEEE INT C INT ROBOT, P31, DOI 10.1109/IROS.2017.8202134
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 13
TC 0
Z9 0
SN 2153-0858
BN 978-1-5386-8094-0
PY 2018
BP 576
EP 582
ER

PT S
AU Shen, MC
   Habibi, G
   How, JP
AF Shen, Macheng
   Habibi, Golnaz
   How, Jonathan P.
BA Kosecka, J
BF Kosecka, J
BE Maciejewski, AA
   Okamura, A
   Bicchi, A
   Stachniss, C
   Song, DZ
   Lee, DH
   Chaumette, F
   Ding, H
   Li, JS
   Wen, J
   Roberts, J
   Masamune, K
   Chong, NY
   Amato, N
   Tsagwarakis, N
   Rocco, P
   Asfour, T
   Chung, WK
   Yasuyoshi, Y
   Sun, Y
   Maciekeski, T
   Althoefer, K
   AndradeCetto, J
   Chung, WK
   Demircan, E
   Dias, J
   Fraisse, P
   Gross, R
   Harada, H
   Hasegawa, Y
   Hayashibe, M
   Kiguchi, K
   Kim, K
   Kroeger, T
   Li, Y
   Ma, S
   Mochiyama, H
   Monje, CA
   Rekleitis, I
   Roberts, R
   Stulp, F
   Tsai, CHD
   Zollo, L
TI Transferable Pedestrian Motion Prediction Models at Intersections
SO 2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
CT 25th IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY OCT 01-05, 2018
CL Madrid, SPAIN
AB One desirable capability of autonomous cars is to accurately predict the pedestrian motion near intersections for safe and efficient trajectory planning. We are interested in developing transfer learning algorithms that can be trained on the pedestrian trajectories collected at one intersection and yet still provide accurate predictions of the trajectories at another, previously unseen intersection. We first discussed the feature selection for transferable pedestrian motion models in general. Following this discussion, we developed one transferable pedestrian motion prediction algorithm based on Inverse Reinforcement Learning (IRL) that infers pedestrian intentions and predicts future trajectories based on observed trajectory. We evaluated our algorithm at three intersections. We used the accuracy of augmented semi- nonnegative sparse coding (ASNSC), trained and tested at the same intersection as a baseline. The result shows that the proposed algorithm improves the baseline accuracy by a statistically significant percentage in both non- transfer task and transfer task.
CR Abbeel Pieter, 2011, ENCY MACHINE LEARNIN, P554
   Ballan L, 2016, LECT NOTES COMPUT SC, V9905, P697, DOI 10.1007/978-3-319-46448-0_42
   Bruce A., 2004, P INT C ROB AUT ICRA
   Chen YF, 2016, IEEE INT CONF ROBOT, P2527, DOI 10.1109/ICRA.2016.7487407
   Deisenroth MP, 2009, NEUROCOMPUTING, V72, P1508, DOI 10.1016/j.neucom.2008.12.019
   DUBUISSON MP, 1994, INT C PATT RECOG, P566, DOI 10.1109/ICPR.1994.576361
   Fouhey D. F., 2014, P IEEE C COMP VIS PA, P2019
   Joseph J, 2011, AUTON ROBOT, V31, P383, DOI 10.1007/s10514-011-9248-x
   Karasev V, 2016, IEEE INT CONF ROBOT, P2543, DOI 10.1109/ICRA.2016.7487409
   Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15
   Makris D., 2002, BMVC, P1
   Ramachandran D., 2007, URBANA, V51, P61801
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Vasquez D, 2009, IEEE T INTELL TRANSP, V10, P403, DOI 10.1109/TITS.2009.2020208
   Ziebart BD, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3931, DOI 10.1109/IROS.2009.5354147
NR 15
TC 0
Z9 0
SN 2153-0858
BN 978-1-5386-8094-0
PY 2018
BP 4547
EP 4553
ER

PT S
AU Mueller, C
   Venicx, J
   Hayes, B
AF Mueller, Carl
   Venicx, Jeff
   Hayes, Bradley
BA Kosecka, J
BF Kosecka, J
BE Maciejewski, AA
   Okamura, A
   Bicchi, A
   Stachniss, C
   Song, DZ
   Lee, DH
   Chaumette, F
   Ding, H
   Li, JS
   Wen, J
   Roberts, J
   Masamune, K
   Chong, NY
   Amato, N
   Tsagwarakis, N
   Rocco, P
   Asfour, T
   Chung, WK
   Yasuyoshi, Y
   Sun, Y
   Maciekeski, T
   Althoefer, K
   AndradeCetto, J
   Chung, WK
   Demircan, E
   Dias, J
   Fraisse, P
   Gross, R
   Harada, H
   Hasegawa, Y
   Hayashibe, M
   Kiguchi, K
   Kim, K
   Kroeger, T
   Li, Y
   Ma, S
   Mochiyama, H
   Monje, CA
   Rekleitis, I
   Roberts, R
   Stulp, F
   Tsai, CHD
   Zollo, L
TI Robust Robot Learning from Demonstration and Skill Repair Using
   Conceptual Constraints
SO 2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
CT 25th IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY OCT 01-05, 2018
CL Madrid, SPAIN
AB Learning from demonstration (LfD) has enabled robots to rapidly gain new skills and capabilities by lever-aging examples provided by novice human operators. While effective, this training mechanism presents the potential for sub-optimal demonstrations to negatively impact performance due to unintentional operator error. In this work we introduce Concept Constrained Learning from Demonstration (CC-LfD), a novel algorithm for robust skill learning and skill repair that incorporates annotations of conceptually-grounded constraints (in the form of planning predicates) during live demonstrations into the LfD process. Through our evaluation, we show that CC-LfD can be used to quickly repair skills with as little as a single annotated demonstration without the need to identify and remove low-quality demonstrations. We also provide evidence for potential applications to transfer learning, whereby constraints can be used to adapt demonstrations from a related task to achieve proficiency with few new demonstrations required.
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Akgun B, 2016, AUTON ROBOT, V40, P211, DOI 10.1007/s10514-015-9448-x
   Akgun B, 2012, INT J SOC ROBOT, V4, P343, DOI 10.1007/s12369-012-0160-0
   Akgun B, 2012, ACMIEEE INT CONF HUM, P391
   Alexandrova  S., 2014, ROBOTICS SCI SYSTEMS
   Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024
   Bakker P., 1996, WORKSH LEARN ROB AN, P3
   Basu  C., 2018, 13 ACM IEEE INT C HU
   Billard A, 2008, SPRINGER HDB ROBOTIC, P1371, DOI DOI 10.1007/978-3-540-30301-5_60
   Cakmak M, 2012, ACMIEEE INT CONF HUM, P17
   Calinon S., 2007, 2007 2nd Annual Conference on Human-Robot Interaction (HRI), P255
   Chao C., 2011, IEEE INT C DEV LEARN, V2, P1
   Chernova Sonia, 2014, SYNTHESIS LECT ARTIF, V8, P1
   Chitta S, 2012, IEEE ROBOT AUTOM MAG, V19, P18, DOI 10.1109/MRA.2011.2181749
   Coates A., 2008, P 25 INT C MACH LEAR, P144, DOI DOI 10.1145/1390156.1390175
   Duda Richard O., 1973, PATTERN CLASSIFICATI, V2
   Ekvall S, 2008, INT J ADV ROBOT SYST, V5, P223, DOI 10.5772/5611
   Grollman Daniel H., 2011, 2011 IEEE International Conference on Robotics and Automation, P3804
   Hayes Bradley, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P6586, DOI 10.1109/ICRA.2017.7989778
   Hayes  B., AUTONOMOUSLY CONSTRU
   Hayes B, 2014, IEEE INT C INT ROBOT, P4442, DOI 10.1109/IROS.2014.6943191
   Hayes G. M., 1994, ROBOT CONTROLLER USI
   Jain  A., 2013, P ADV NEUR INF PROC, P575
   Keogh E, 2005, KNOWL INF SYST, V7, P358, DOI 10.1007/s10115-004-0154-9
   Kurenkov A, 2015, IEEE INT C INT ROBOT, P3608, DOI 10.1109/IROS.2015.7353881
   Nehaniv CL, 2007, IMITATION AND SOCIAL LEARNING IN ROBOTS, HUMANS AND ANIMALS: BEHAVIOURAL, SOCIAL AND COMMUNICATIVE DIMENSIONS, P1, DOI 10.1017/CBO9780511489808
   Pignat E, 2017, ROBOT AUTON SYST, V93, P61, DOI 10.1016/j.robot.2017.03.017
   Quigley M, 2009, ICRA WORKSH OP SOURC, V3, P2
   Russell S., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Russell SJ, 2003, ARTIFICIAL INTELLIGE, V2
   Stenmark Maj, 2017, 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI), P463, DOI 10.1145/2909824.3020227
   Vakanski A, 2012, IEEE T SYST MAN CY B, V42, P1039, DOI 10.1109/TSMCB.2012.2185694
   Vukovic N, 2015, ENG APPL ARTIF INTEL, V45, P388, DOI 10.1016/j.engappai.2015.07.002
NR 33
TC 0
Z9 0
SN 2153-0858
BN 978-1-5386-8094-0
PY 2018
BP 6029
EP 6036
ER

PT S
AU Yan, Z
   Sun, L
   Ducketi, T
   Bellotto, N
AF Yan, Zhi
   Sun, Li
   Ducketi, Tom
   Bellotto, Nicola
BA Kosecka, J
BF Kosecka, J
BE Maciejewski, AA
   Okamura, A
   Bicchi, A
   Stachniss, C
   Song, DZ
   Lee, DH
   Chaumette, F
   Ding, H
   Li, JS
   Wen, J
   Roberts, J
   Masamune, K
   Chong, NY
   Amato, N
   Tsagwarakis, N
   Rocco, P
   Asfour, T
   Chung, WK
   Yasuyoshi, Y
   Sun, Y
   Maciekeski, T
   Althoefer, K
   AndradeCetto, J
   Chung, WK
   Demircan, E
   Dias, J
   Fraisse, P
   Gross, R
   Harada, H
   Hasegawa, Y
   Hayashibe, M
   Kiguchi, K
   Kim, K
   Kroeger, T
   Li, Y
   Ma, S
   Mochiyama, H
   Monje, CA
   Rekleitis, I
   Roberts, R
   Stulp, F
   Tsai, CHD
   Zollo, L
TI Multisensor Online Transfer Learning for 3D LiDAR-based Human Detection
   with a Mobile Robot
SO 2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
CT 25th IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY OCT 01-05, 2018
CL Madrid, SPAIN
ID PEOPLE TRACKING; RGB
AB Human detection and tracking is an essential task for service robots, where the combined use of multiple sensors has potential advantages that are yet to be fully exploited. In this paper, we introduce a framework allowing a robot to learn a new 3D LiDAR-based human classifier from other sensors over time, taking advantage of a multisensor tracking system. The main innovation is the use of different detectors for existing sensors (i.e. RGB-D camera, 2D LiDAR) to train, online, a new 3D LiDAR-based human classifier based on a new "trajectory probability". Our framework uses this probability to check whether new detection belongs to a human trajectory, estimated by different sensors and/or detectors, and to learn a human classifier in a semi-supervised fashion. The framework has been implemented and tested on a real-world dataset collected by a mobile robot. We present experiments illustrating that our system is able to effectively learn from different sensors and from the environment, and that the performance of the 3D LiDAR-based human classification improves with the number of sensors/detectors used.
CR Arras KO, 2007, IEEE INT CONF ROBOT, P3402, DOI 10.1109/ROBOT.2007.363998
   Bar-Shalom Y., 1995, MULTITARGET MULTISEN
   Bellotto N, 2010, AUTON ROBOT, V28, P425, DOI 10.1007/s10514-009-9167-2
   Bellotto N, 2009, IEEE T SYST MAN CY B, V39, P167, DOI 10.1109/TSMCB.2008.2004050
   Burgard W., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P476, DOI 10.1109/ROBOT.2000.844100
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Dondrup C., 2015, ICRA WORKSH MACH LEA
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Goldberg Andrew B., 2009, INTRO SEMISUPERVISED
   Gonzalez A, 2015, IEEE INT VEH SYM, P356, DOI 10.1109/IVS.2015.7225711
   Held D, 2013, IEEE INT CONF ROBOT, P1138, DOI 10.1109/ICRA.2013.6630715
   Jafari OH, 2014, IEEE INT CONF ROBOT, P5636, DOI 10.1109/ICRA.2014.6907688
   Keerthi SS, 2003, NEURAL COMPUT, V15, P1667, DOI 10.1162/089976603321891855
   Kobilarov M, 2006, IEEE INT CONF ROBOT, P557, DOI 10.1109/ROBOT.2006.1641769
   Koide K, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4187, DOI 10.1109/IROS.2016.7759616
   Linder T, 2016, IEEE INT CONF ROBOT, P5512, DOI 10.1109/ICRA.2016.7487766
   Misu K., 2012, P IAS, P705
   MORAVEC HP, 1988, AI MAG, V9, P61
   Munaro M, 2014, AUTON ROBOT, V37, P227, DOI 10.1007/s10514-014-9385-0
   Premebida C, 2014, IEEE INT C INT ROBOT, P4112, DOI 10.1109/IROS.2014.6943141
   Quigley M., 2009, ICRA WORKSH OP SOURC
   Read Jesse, 2012, Advances in Intelligent Data Analysis XI. Proceedings 11th International Symposium, IDA 2012, P313, DOI 10.1007/978-3-642-34156-4_29
   Schulz D, 2003, INT J ROBOT RES, V22, P99, DOI 10.1177/0278364903022002002
   Spinello L, 2010, INT J ROBOT RES, V29, P1498, DOI 10.1177/0278364910377533
   Sun L., 2018, P ICRA BRISB AUSTR M
   Sun L., 2018, IEEE ROBOTICS AUTOMA
   Teichman A, 2012, INT J ROBOT RES, V31, P804, DOI 10.1177/0278364912442751
   Yan Z, 2017, IEEE INT C INT ROBOT, P864, DOI 10.1109/IROS.2017.8202247
   Yan Z, 2013, INT J ADV ROBOT SYST, V10, DOI 10.5772/57313
NR 30
TC 0
Z9 0
SN 2153-0858
BN 978-1-5386-8094-0
PY 2018
BP 7635
EP 7640
ER

PT B
AU Thazhackal, SS
   Devi, VS
AF Thazhackal, Sharun S.
   Devi, V. Susheela
BE Sundaram, S
TI A Hybrid Deep Learning Model to Predict Business Closure from Reviews
   and User Attributes Using Sentiment Aligned Topic Model
SO 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI)
CT 8th IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 18-21, 2018
CL Bengaluru, INDIA
DE business closure prediction; sentiment aligned topic model; NLP; lexicon
   generation; apsect-wise ratings; yelp; review analysis; deep learning;
   hybrid neural network; CNN
AB Business closure is a very good indicator for success or failure of a business. This will help investors and banks as to whether to invest or lend to a particular business for future growth and benefits. Traditional machine learning techniques require extensive manual feature engineering and still do not perform satisfactorily due to significant class imbalance problem and little difference in the attributes for open and closed businesses. We have used historical data besides taking care of the class imbalance problem. Transfer learning also has been used to tackle the issue of having small categorical dalasets. A hybrid deep learning model has been proposed to predict whether a business would be shut down within a specific period of time. Sentiment Aligned Topic Model (SATM) is used to extract aspect-wise sentiment scores from user reviews. Our results show a marked improvement over traditional machine learning techniques. It also shows how the aspect-wise sentiment scores corresponding to each business, computed using SATM, help to give better results.
CR Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blunsom P., 2014, P 52 ANN M ASS COMP
   Demiroz Gulsen, DAT MIN WORKSH ICDMW
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Dumais ST, 2004, ANNU REV INFORM SCI, V38, P189
   Kingma Diederik P., 2015, 3 INT C LEARN REPR S
   Lu J, 2015, KNOWL-BASED SYST, V80, P14, DOI 10.1016/j.knosys.2015.01.010
   Martineau J., 2009, ICWSM
   Mejia J., 2015, ACAD MANAGEMENT P
   Nair V., 2010, P 27 INT C MACH LEAR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Turney PD, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P417
   Wang Hao, 2014, C EMP METH NAT LANG, P1192
   Zhao Kui, 2017, ARXIV170807946CSLG
NR 14
TC 0
Z9 0
BN 978-1-5386-9276-9
PY 2018
BP 397
EP 404
ER

PT B
AU Xie, YQ
   Chen, KX
   Murphey, YL
AF Xie, Yongquan
   Chen, Kexun
   Murphey, Yi Lu
BE Sundaram, S
TI Real-time and Robust Driver Yawning Detection with Deep Neural Networks
SO 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI)
CT 8th IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 18-21, 2018
CL Bengaluru, INDIA
AB Yawning is an important indicator of drivers' drowsiness or fatigue. Techniques for automatic detection of driver's yawning have been developed for use as a component of driver fatigue monitoring system. However, detecting driver's yawning event accurately in real-time is still a challenging task, in particular in applications such as driver fatigue detection, illumination conditions vary in a broad range, driver facial features vary in size, shape, texture and degrees of distortion. In this paper, we present a deep neural network model built using transfer learning and sequential learning from yawning video clips as well as augmented images for yawning detection. As a result, unlike many other methods that follow a sequence of processes such as face ROI detection, eye/nose/mouth positioning and mouth open/dose determination, the proposed yawning detection system detect yawning events directly from video images without requiring any facial part positions. The system is robust to variations in object scales, positions and subject view angles. The system has been evaluated on publicly available yawning data sets, YawDD and NTHU-DDD, as well as a data set containing challenging yawning videos. The experimental results show that the proposed yawning detection system has the capability of detecting yawning events in high precision even when face turns away from camera up to 70 degrees, while exhibiting capability of being scale- and spatial-invariant. In addition, the model demonstrates the capability of discriminating yawning events very well from the actions involving mouth opening-closing motions such as talking and laughing.
CR Abtahi S., 2014, P 5 ACM MULT SYST C, P24
   Abtahi S., 2011, 2011 IEEE INT INSTR, P1, DOI DOI 10.1109/IMTC.2011.5944101
   Ali SI, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON INFORMATION SYSTEMS AND COMPUTER NETWORKS (ISCON), P15, DOI 10.1109/ICISCON.2014.6965210
   Andreu-Cabedo Y, 2015, IEEE INT CON MULTI
   Dehkordi M. T., 2018, INT RES J ENG TECHNO, V5, P646
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ioffe S., 2015, ARXIV150203167
   Jabbar Rateb, 2018, Procedia Computer Science, V130, P400, DOI 10.1016/j.procs.2018.04.060
   Ji YY, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.051205
   Kang HB, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P616, DOI 10.1109/ICCVW.2013.85
   Kaplan S, 2015, IEEE T INTELL TRANSP, V16, DOI 10.1109/TITS.2015.2462084
   Klauer S., 2006, IMPACT DRIVER INATTE
   Kumar N, 2014, IJCSNS, V5, P7821
   Lin M., 2013, ARXIV13124400
   Lyu J., 2018, ARXIV180102325
   Motorist S., SMART MOTORIST
   Neubeck A, 2006, INT C PATT RECOG, P850
   Omidyeganeh M, 2016, IEEE T INSTRUM MEAS, V65, P570, DOI 10.1109/TIM.2015.2507378
   Reddy B, 2017, IEEE COMPUT SOC CONF, P438, DOI 10.1109/CVPRW.2017.59
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Szegedy C., 2015, CVPR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Weng CH, 2017, LECT NOTES COMPUT SC, V10118, P117, DOI 10.1007/978-3-319-54526-4_9
   Yen IL, 2017, 2017 11TH IEEE SYMPOSIUM ON SERVICE-ORIENTED SYSTEM ENGINEERING (SOSE), P1, DOI 10.1109/SOSE.2017.26
   Yuanyuan Liu, 2009, Proceedings of the 2009 International Conference on Computational Intelligence and Security (CIS 2009), P515, DOI 10.1109/CIS.2009.70
   Zhang W, 2015, EVID-BASED COMPL ALT, V2015, P1
NR 27
TC 0
Z9 0
BN 978-1-5386-9276-9
PY 2018
BP 532
EP 538
ER

PT B
AU Carvalho, M
   Pratama, M
AF Carvalho, Marcus
   Pratama, Mahardhika
BE Sundaram, S
TI Improving shallow neural network by compressing deep neural network
SO 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI)
CT 8th IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 18-21, 2018
CL Bengaluru, INDIA
DE Dark Knowledge; Deep Learning; Model Compression; Neural Network;
   Transfer Learning
AB This paper focuses on a simple technique to extract the dark knowledge of a Deep Multi-Column Deep Learning Network, and its compression into a shallow neural network, causing not only the improvement of the train and test performance of the latter but a cheap way to approximate the former results but with fewer parameters. First, we built a Multi-Column Deep Learning Network, i.e., a Committee Machine, using simple techniques to improve its training accuracy. Finally, we transfer its knowledge to a shallow neural network, compressing its learned information and demonstrating that dark knowledge techniques still have a huge impact on Deep Multi-Layer Perceptron's studies. This paper validates the performance of the proposed model in the MNIST database, comparing it with popular neural nets used before, where we were able to achieve better scores.
CR Bengio, 2010, P 13 INT C ART INT S
   Caruana, 2006, MODEL COMPRESSION
   Ciresan, 2012, NEURAL NETWORKS
   Couprie, 2013, INDOOR SEMANTIC SEGM
   Dahl G. E., 2010, ADV NEURAL INFORM PR, V24, P469
   Deng, 2012, IEE T AUDIO SPEECH L
   Farabet, 2013, CAUSAL GRAPH BASED V
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   Hinton, 2012, IEEE SIGNAL PROCESSI
   Hinton Geoffrey, 2014, BAYLEARN 2, V2
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Seide, 2011, INTERSPEECH
   Sermanet, 2014, INT C LEARN REPR ICL
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
NR 14
TC 0
Z9 0
BN 978-1-5386-9276-9
PY 2018
BP 1382
EP 1387
ER

PT B
AU Ravi, A
   Venugopal, H
   Paul, S
   Tizhoosh, HR
AF Ravi, Aravind
   Venugopal, Harshwin
   Paul, Sruthy
   Tizhoosh, Hamid R.
BE Sundaram, S
TI A Dataset and Preliminary Results for Umpire Pose Detection Using SVM
   Classification of Deep Features
SO 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI)
CT 8th IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 18-21, 2018
CL Bengaluru, INDIA
DE video summarization; transfer learning; cricket; deep convolutional
   networks; image classification; inceptionv3; vgg19
AB In recent years, there has been increased interest in video summarization and automatic sports highlights generation. In this work, we introduce a new dataset, called SNOW, for umpire pose detection in the game of cricket. The proposed dataset is evaluated as a preliminary aid for developing systems to automatically generate cricket highlights. In cricket, the umpire has the authority to make important decisions about events on the field. The umpire signals important events using unique hand signals and gestures. We identify four such events for classification namely SIX, NO BALL, OFT and WIDE based on detecting the pose of the umpire from the frames of a cricket video. Pre-trained convolutional neural networks such as Inception V3 and VGG19 networks arc selected as primary candidates for feature extraction. The results are obtained using a linear SVM classifier. The highest classification performance was achieved for the SVM trained on features extracted from the VGG19 network. The preliminary results suggest that the proposed system is an effective solution for the application of cricket highlights generation.
CR Chambers GS, 2004, LECT NOTES COMPUT SC, V3138, P859
   Cheng CC, 2006, IEEE T MULTIMEDIA, V8, P585, DOI 10.1109/TMM.2006.870726
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Hari R., 2014, IEEE INT C COMP INT, P1
   Harikrishna N., 2011, COMM NCC 2011 NAT C, P1
   Huang YP, 2009, EXPERT SYST APPL, V36, P9907, DOI 10.1016/j.eswa.2009.01.078
   Kieffer B., 2017, ARXIV171005726
   Kolekar MH, 2011, MULTIMED TOOLS APPL, V54, P27, DOI 10.1007/s11042-010-0544-9
   Kolekar MH, 2010, MULTIMED TOOLS APPL, V47, P545, DOI 10.1007/s11042-009-0337-1
   Kumar M. D., 2017, ARXIV171001249
   Lu SY, 2014, IEEE T MULTIMEDIA, V16, P1497, DOI 10.1109/TMM.2014.2319778
   Naphade M. R., 2004, P 12 ANN ACM INT C M, P660
   Narasimhan H, 2010, GECCO-2010 COMPANION PUBLICATION: PROCEEDINGS OF THE 12TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P2051
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Razavian A. S., 2014, COMP VIS PATT REC WO
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Simonyan K., 2014, ARXIV14091556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang A., 2012, P SIGCHI C HUM FACT, P1569
   Tizhoosh HR, 2018, IEEE T BIO-MED ENG, V65, P2267, DOI 10.1109/TBME.2018.2791567
   van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37
   Zaharia M., 2010, HOTCLOUD, V10, P95
   Zhou W., 2000, P ACM MULT 2000 WORK, P213
NR 27
TC 0
Z9 0
BN 978-1-5386-9276-9
PY 2018
BP 1396
EP 1402
ER

PT B
AU Zhou, ZJ
   Xu, H
AF Zhou, Zejian
   Xu, Hao
BE Sundaram, S
TI Switching Deep Reinforcement Learning based Intelligent Online Decision
   Making for Autonomous Systems under Uncertain Environment
SO 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI)
CT 8th IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 18-21, 2018
CL Bengaluru, INDIA
DE switching reinforcement learning; decision making; autonomous system;
   uncertain environment
AB In this paper, finite horizon intelligent decision making problem has been investigated for autonomous systems especially under uncertain environment. According to latest studies, the uncertainty of environment will seriously affect the effectiveness of decision making especially for autonomous systems. To handle this issues, transfer learning and deep reinforcement learning has been presented recently. However, those existing Learning algorithms commonly needs a large set of state space which cause the algorithm to be time consuming and not suitable for real-lime application. Therefore, in this paper, a library of polices trained using Deep Q-Learning under similar environments are built firstly. Then, a neural network is designed to estimate the environment. Using the learned environment, a novel of switching policy will be developed and integrated with the designed deep reinforcement learning which can efficiently stop learning according to the practical error tolerance. Meanwhile, through the novel policy evaluation method based on the environment estimator, the autonomous agent will select the best policy to follow in an online manner. Eventually, simulation results are provided to demonstrate the effectiveness of the designed algorithm.
CR Csaji BC, 2008, J MACH LEARN RES, V9, P1679
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Doya K, 2002, NEURAL COMPUT, V14, P1347, DOI 10.1162/089976602753712972
   Fernandez F, 2010, ROBOT AUTON SYST, V58, P866, DOI 10.1016/j.robot.2010.03.007
   Fernndez Fernando, 2006, P 5 INT JOINT C AUT
   Hadoux Emmanuel, 2014, INT C SCAL UNC MAN
   Hernandez-Leal Pablo, 2016, IDENTIFYING TRACKING
   Mnih V., 2013, ARXIV13125602
   Nagabandi A., 2017, ARXIV170802596
   Rosman B, 2016, MACH LEARN, V104, P99, DOI 10.1007/s10994-016-5547-y
   Shorten R, 2007, SIAM REV, V49, P545, DOI 10.1137/05063516X
   Silva Da, 2006, P 23 INT C MACH LEAR
   Sutton R. S., 1998, INTRO REINFORCEMENT, V135
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Szita Istvn, 2002, J MACHINE LEARNING R, V3, P145
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Zuazua E, 2011, J EUR MATH SOC, V13, P85, DOI 10.4171/JEMS/245
NR 17
TC 0
Z9 0
BN 978-1-5386-9276-9
PY 2018
BP 1453
EP 1460
ER

PT B
AU Prasad, M
   Rajora, S
   Gupta, D
   Daraghmi, YA
   Daraghmi, E
   Yadav, P
   Tiwari, P
   Saxena, A
AF Prasad, Mukesh
   Rajora, Shantanu
   Gupta, Deepak
   Daraghmi, Yousef-Awwad
   Daraghmi, Eman
   Yadav, Pranay
   Tiwari, Prayag
   Saxena, Amit
BE Sundaram, S
TI Fusion based En-FEC Transfer Learning Approach for Automobile Parts
   Recognition System
SO 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI)
CT 8th IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 18-21, 2018
CL Bengaluru, INDIA
DE Deep learning; convolution neural networks; transfer learning; image
   classification
ID FEATURES
AB The artificially supervised classification of real world entities have gained a phenomenal significance in recent year of computational advancements. An intelligent classification model focuses on rendering accurate outcomes vide the implicated paradigms with respect to the subjected data employed to train the classifier. This paper proposes a novel deep learning approach to classify the various parts of any operational engine such as crank shafts, rock-arms, distributer, air duct, assecorybelt etc. deployed in automobiles. The proposed architecture distinctively utilizes convolution neural networks for this typical classification problem and altogether constructs a robust transfer learning paradigm to render the correct class label against the validation and test images as the conclusive result of the classification. The proposed methodology poses in such a way that it can qualitatively classify and henceforth give the corresponding class label of the machinery/engine part under consideration. This computationally intelligent architecture requires the user to feed the image of the engine part to the model in order to achieve the requisite responses of classification. The main contribution of the proposed method is the development of a robust algorithm that can exhibit pronounced results without training the entire ConvNet architecture from scratch, thereby enabling the proposed paradigm to be deployable in application instances wherein limited labeled training data is available.
CR Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Donahue  J., 2014, P 31 INT C MACH LEAR, P647
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   Huang Z., 2017, REMOTE SENSING, V9
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nasr-Esfahani E, 2016, IEEE ENG MED BIO, P1373, DOI 10.1109/EMBC.2016.7590963
   Razavian A. S., 2014, COMP VIS PATT REC WO
   Simonyan K., 2015, 3 INT C LEARN REPR
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 15
TC 0
Z9 0
BN 978-1-5386-9276-9
PY 2018
BP 2193
EP 2199
ER

PT B
AU Oliveira, FRD
   Farias, FC
AF da Silva Oliveira, Flavio Rosendo
   Farias, Felipe Costa
GP IEEE
TI Comparing transfer learning approaches applied to distracted driver
   detection
SO 2018 IEEE LATIN AMERICAN CONFERENCE ON COMPUTATIONAL INTELLIGENCE
   (LA-CCI)
CT 5th IEEE Latin American Conference on Computational Intelligence
   (LA-CCI)
CY NOV 06-09, 2018
CL Guadalajara, MEXICO
DE computer vision; deep learning; transfer learning; distracted driver
   detection
AB Studies show that the volume of traffic deaths per year is high and that a large part of these accidents are caused by distractions whose risk is aggravated by the use of cell phones while driving. This work presents results of a comparative study of three transfer learning approaches applied to classification of driver images in moments of concentration or distraction. For this study, four architectures of deep convolutional neural networks were evaluated: VGG19, Inception v3, Resnet152 and Densenet161. Results suggested that for the studied database, end-to-end transfer learning outperformed fine tuning only fully connected layers and also outperformed shallow classifiers trained with features extracted by the same deep convolutional networks.
CR Backes A., 2016, INTRO VISAO COMPUTAC
   Berg A, 2010, LARGE SCALE VISUAL R
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Hssayeni M., 2017, ELECT IMAGING, P20
   Huang Gao, DENSELY CONNECTED CO
   Koesdwiady A., IMAGE ANAL RECOGNITI, P11
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Masood S., PATTERN RECOGNITION
   McEvoy SP, 2005, BRIT MED J, V331, P428, DOI 10.1136/bmj.38537.397512.55
   National Highway Traffic Safety Administration, 2006, IMP DRIV IN NEAR CRA
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Simonyan  K., 2015, ICLR
   Szegedy C., RETHINKING INCEPTION
   World Health Organization, ROAD SAF BRAZ
NR 16
TC 0
Z9 0
BN 978-1-5386-4626-7
PY 2018
ER

PT B
AU Masita, KL
   Hasan, AN
   Paul, S
AF Masita, Katleho L.
   Hasan, Ali N.
   Paul, Satyakama
GP IEEE
TI Pedestrian Detection Using R-CNN Object Detector
SO 2018 IEEE LATIN AMERICAN CONFERENCE ON COMPUTATIONAL INTELLIGENCE
   (LA-CCI)
CT 5th IEEE Latin American Conference on Computational Intelligence
   (LA-CCI)
CY NOV 06-09, 2018
CL Guadalajara, MEXICO
DE Pedestrian detection; Deep learning; Convolutional neural networks;
   Region-based neural networks (RCNN)
ID HOUGH TRANSFORM
AB Pedestrian detection continues to hold a significant role in the concept, analysis and function of computer vision. Deep learning techniques in pedestrian detection have demonstrated powerful results in recent experiments and research. In this paper a powerful deep learning technique of R-CNN is evaluated for Pedestrian detection on two different pedestrian detection datasets. The experiment involves the use of a deep learning feature extraction model along with the R-CNN detector. The deep learning feature extraction used is the Alexnet. Transfer learning is performed on the feature extraction model to adjust the weights of the convolutional neural networks to favour classification on the selected datasets. The R-CNN detector is then trained on the deep learning feature extraction model for pedestrian detection. The results of the experiments as evidently demonstrated, indicate some important truths about the performance of R-CNN detector on varying datasets.
CR BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1
   BIGUN J, 1991, IEEE T PATTERN ANAL, V13, P775, DOI 10.1109/34.85668
   Cyganek B, 2008, INT J NEURAL SYST, V18, P339, DOI 10.1142/S0129065708001646
   DALAL N, 2005, PROC CVPR IEEE, P886, DOI DOI 10.1109/CVPR.2005.177
   Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Farayola A. M., 2018, INT J INNOVATIVE COM, V14
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow I., 2013, DEEP LEARNING, P654
   Hasan A. N., 2014, INT JOINT C NEUR NET
   Hasan AN, 2016, INT J INNOV COMPUT I, V12, P1777
   Hazan T, 2005, IEEE I CONF COMP VIS, P50
   Hough P. V. C., 1959, P INT C HIGH EN ACC
   Hu D., 2015, COMPUTER SCI ENG TEC, P87
   Jahne B., 2005, DIGITAL IMAGE PROCES
   Kazemi V., 2013, P BMVC 2013
   Ke Y, 2004, PROC CVPR IEEE, P506
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Li HL, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P796, DOI 10.1109/CISP-BMEI.2016.7852818
   Lopes N., 2015, STUDIES IN BIG DATA, V7
   McLaughlin RA, 1998, IEEE T PATTERN ANAL, V20, P396, DOI 10.1109/34.677267
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Nam W, 2014, ADV NEURAL INFORM PR, P424
   Ouyang W., 2016, IEEE T PATTERN ANAL
   Papageorgiou CP, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P555, DOI 10.1109/ICCV.1998.710772
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Simonyan  K., 2014, VERY DEEP CONVOLUTIO
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wang LM, 2007, LECT NOTES COMPUT SC, V4843, P189
   Wu CH, 2017, IEEE WINT CONF APPL, P540, DOI 10.1109/WACV.2017.66
   Wu HY, 1999, IEEE T PATTERN ANAL, V21, P557, DOI 10.1109/34.771326
   Yan JJ, 2014, PROC CVPR IEEE, P2497, DOI 10.1109/CVPR.2014.320
   Zhang SS, 2015, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2015.7298784
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 36
TC 0
Z9 0
BN 978-1-5386-4626-7
PY 2018
ER

PT B
AU Oldridge, E
AF Oldridge, Even
GP ACM
TI Adapting Session Based Recommendation for Features Through Transfer
   Learning
SO 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS)
CT 12th ACM Conference on Recommender Systems (RecSys)
CY OCT 02-07, 2018
CL Vancouver, CANADA
DE Recommender Systems; Deep Learning; Transfer Learning
AB This industry talk covers the deep learning architecture developed at Realtor.com to recommend real estate listings to our userbase. The recommendation of homes is a different problem than most other domains both in the sense that listings are unique and that there are additional geographic and time constraints that increase the sparsity of interactions and make recommendation of individual listings more challenging. In particular time on market in a hot area can be limited to weeks or even days, and listing cold-start is critical to providing up to date market information. Thankfully the structured feature data for listings is incredibly rich and provides a framework from which to map listings into a meaningful vector space. User first impressions are also incredibly important in this highly competitive field, and offline recommendation or models that don't adapt during the users session are less desirable.
   In order to solve this recommendation problem we have developed a model based off of session based recommendation [1]. The architecture utilizes state of the art techniques from Natural Language Processing, including the AWD-LSTM language model developed by Salesforce [2]. To solve for coldstart of listings a structured data based denoising autoencoder was adapted from the methodology described in the winning entry of the Puerto Segurno Safe Driver Kaggle Competition [3]. This model is not used in the common way of generating fixed feature vectors, but rather the entire head of the autoencoder model, from the feature inputs to the middle layer commonly used as the vector output, is first trained to encode listing features, and then becomes the input to the AWD-LSTM architecture. This style of transfer learning is common in Computer Vision, and has recently been utilized in NLP to achieve state of the art results for text classification [4]. By including the head we are able to further optimize the listing encoder network and embeddings to take user interactions into account. As in traditional session based recommendation users are represented as the sequence of listings that they view, however those listings are fed into the model as the sequence of features.
   The final system consists of several components. The first attempts to calculate and maintain the users' feature vector and model hidden weights in near realtime, providing a representation for the user within the system. This representation is used by several downstream components, most notably the search rerank and recommendation modules which calculate users' interest in listings both in the context of the output of more traditional elasticsearch queries via cosine similarity of user/listing vectors and through approximate nearest neighbor vector space searches for relevant listings which form the input set for a pointwise scoring model trained on time on listing as done by YouTube [5].
CR Covington, DEEP NEURAL NETWORKS
   Hidasi Karatzoglou, 2016, ICLR
   Merity Keskar, REGULARIZING OPTIMIZ
NR 3
TC 0
Z9 0
BN 978-1-4503-5901-6
PY 2018
BP 481
EP 481
DI 10.1145/3240323.3241728
ER

PT B
AU Sahebi, SS
   Zheng, Y
   Pan, WK
   Fernandez, I
AF Sahebi, Shaghayegh (Sherry)
   Zheng, Yong
   Pan, Weike
   Fernandez, Ignacio
GP ACM
TI The 2ndWorkshop on Intelligent Recommender Systems by Knowledge Transfer
   & Learning (RecSysKTL)
SO 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS)
CT 12th ACM Conference on Recommender Systems (RecSys)
CY OCT 02-07, 2018
CL Vancouver, CANADA
DE cross-domain; knowledge transfer; recommender system
AB Having data from multiple sources, cross-domain and context-aware recommender systems, with the help of transfer learning approaches, aim to integrate such data to improve recommendation quality and alleviate issues such as cold-start problem. With the advantages of these techniques, we host the second international workshop on intelligent recommender systems by knowledge transfer and learning (RecSysKTL) to provide such a forum for both academia and industry researchers as well as application developers from around the world to present their work and discuss exciting research ideas or outcomes. The workshop is held in conjunction with the ACM Conference on Recommender Systems 2018 on October 6th in Vancouver, Canada.
NR 0
TC 0
Z9 0
BN 978-1-4503-5901-6
PY 2018
BP 523
EP 524
DI 10.1145/3240323.3240339
ER

PT B
AU Kataoka, D
   Tajima, K
AF Kataoka, Daisuke
   Tajima, Keishi
GP IEEE
TI SNS Retrieval Based on User Profile Estimation Using Transfer Learning
   from Web Search
SO 2018 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE (WI 2018)
CT IEEE/WIC/ACM International Conference on Web Intelligence (WI)
CY DEC 03-06, 2018
CL Santiago, CHILE
DE microblog; profile estimation; transfer learning
AB In this paper, we propose a method of retrieving posts on social networking services (SNSs) by specifying a pair of queries: a topic query and an entity query. A topic query specifies the topic of the posts to retrieve (e.g., "iPhone") and an entity query specifies the type of users who posted them (e.g., "students"). In the existing search systems for SNS posts, we can specify topics of posts by keywords, but we cannot specify types of users. Even if we include keywords specifying types of users in a query, such keywords are not usually included in tweets or user profile data. In our method, we estimate types of users by learning vocabulary whose appearance is correlated with specific types of users. We learn it from the datasets obtained through Web search. We retrieve Web documents through the search with a keyword specifying the type of users (e.g., "student"), and we also retrieve Web documents by using a keyword specifying its opposite (e.g., "adult"). We regard the documents retrieved by these queries as positive and negative examples of documents describing the target type, and we train a model for recognizing users of the given type. We recognize users of the target type by inputting their posts and their profile data into the model. We use Web documents instead of SNS posts for training the model because the Web has more documents describing types of people.
CR Albakour  M., 2013, P 10 C OP RES AR INF, P173
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 2017, CLASSIFICATION REGRE
   Choi  J., 2012, P 21 ACM INT C INF K, P2491
   Efron M, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P787
   Halpin  H., 2011, IJCAI, P2250
   Herzig D.M., 2012, P WWW NEW YORK, P141, DOI DOI 10.1145/2187836.2187856
   Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Kataoka D, 2017, 2017 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE (WI 2017), P823, DOI 10.1145/3106426.3106527
   Lau C. H., 2011, P 20 TEXT RETRIEVAL
   Liang  F., 2012, P 12 ACM IEEE CS JOI, P267, DOI DOI 10.1145/2232817.2232867.3
   Lu JL, 2016, IEICE T INF SYST, VE99D, P2295, DOI 10.1587/transinf.2016EDP7015
   Luo  Z., 2012, P AAAI
   Metzler D., 2012, P 2012 C N AM CHAPT, P646
   Miyanishi T., 2013, P 22 ACM INT C INF K, P439
   Nagmoti Rinkesh, 2010, Proceedings 2010 IEEE/ACM International Conference on Web Intelligence-Intelligent Agent Technology (WI-IAT), P153, DOI 10.1109/WI-IAT.2010.170
   Peddinti V. M. K., 2011, ANAL MICROTEXT, V11
   Pochampally  R., 2011, WORKSH ENR INF RETR, P1
   Rieman  D., 2017, P 8 INT JOINT C NAT, V1, P764
   Rocchio J. J., 1971, SMART RETRIEVAL SYST
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Whiting Stewart, 2012, Advances in Information Retrieval. Proceedings of the 34th European Conference on IR Research (ECIR 2012), P522, DOI 10.1007/978-3-642-28997-2_55
   Whiting  S., 2011, P 34 INT ACM SIGIR C, P1245
NR 23
TC 0
Z9 0
BN 978-1-5386-7325-6
PY 2018
BP 278
EP 285
DI 10.1109/WI.2018.00-79
ER

PT B
AU Ali, K
   Wang, CY
   Chen, YS
AF Ali, Khurshed
   Wang, Chih-Yu
   Chen, Yi-Shin
GP IEEE
TI Boosting Reinforcement Learning in Competitive Influence Maximization
   with Transfer Learning
SO 2018 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE (WI 2018)
CT IEEE/WIC/ACM International Conference on Web Intelligence (WI)
CY DEC 03-06, 2018
CL Santiago, CHILE
AB Companies aim to promote their products under competitions and try to gain more profit than other companies. This problem is formulated as a Competitive Influence Maximization (CIM). Recently, a reinforcement learning has been used to solve the CIM problem, that is, to find an optimal strategy against competitor in order to maximize the commutative reward under the competition from other agents. However, reinforcement learning agents require huge training time to find an optimal strategy whenever the settings of the agents or the networks change. To tackle this issue, we propose a transfer learning method in reinforcement learning to reduce the training time and utilize the knowledge gained on source network to target network. Our method relies on two ideas, the first one is the state representation of the source and target networks in order to efficiently utilize the knowledge gained on source network to target network. The second idea is to transfer the final Q-solution of source network while learning on the target network. We validate our transfer learning method in similar or different settings of source and target networks while competing against the competitor's known strategies. Experimental results show that our proposed transfer learning method achieves similar or better performance as a baseline model while significantly reducing training time in all settings.
CR Borodin A, 2010, LECT NOTES COMPUT SC, V6484, P539, DOI 10.1007/978-3-642-17572-5_48
   Budak C., 2011, P 20 INT C WORLD WID, P665, DOI DOI 10.1145/1963405.1963499
   Chang CW, 2015, PROCEEDINGS OF THE 2015 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2015), P1592, DOI 10.1145/2808797.2809349
   Cheng WN, 2013, APPL LINGUIST, V34, P173, DOI 10.1093/applin/ams038
   Even-Dar E, 2003, J MACH LEARN RES, V5, P1
   Harathi S, 2007, LECT NOTES COMPUT SC, V4858, P306
   He X., 2012, SDM, P463, DOI [DOI 10.1137/1.9781611972825.40, DOI 10.1137/1.9781611972825.40.]
   Lazaric A., 2008, P 25 INT C MACH LEAR, P544
   Leskovec J, 2014, SNAP DATASETS STANFO
   Lin S.-C., 2015, P 21 ACM SIGKDD INT, P695, DOI DOI 10.1145/2783258.2783392
   Liu B, 2012, IEEE DATA MINING, P439, DOI 10.1109/ICDM.2012.158
   Ohsaka N., 2016, P EUR C MACH LEARN K, V9851, P132
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Tanaka F, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, VOLS I-III, PROCEEDINGS, P1108
   Taylor M, 2006, CONTEMP POLIT TH-SER, P1, DOI 10.2277/ 0521687047
   Taylor M. E., 1999, P NAT C ART INT, V20, P880
   Torrey L., 2009, HDB RES MACHINE LEAR, V1, P242, DOI DOI 10.1016/J.JBI.2011.04.009
   Wilson A., 2007, P 24 INT C MACH LEAR, P1015
NR 18
TC 0
Z9 0
BN 978-1-5386-7325-6
PY 2018
BP 395
EP 400
DI 10.1109/WI.2018.00-62
ER

PT B
AU Motshoane, K
   Tu, CL
   Owolawi, PA
AF Motshoane, Kefentse
   Tu, Chunling
   Owolawi, Pius Adewale
GP IEEE
TI Prohibition Signage Classification for the Visually Impaired Using
   AlexNet Transfer Learning Approach
SO 2018 INTERNATIONAL CONFERENCE ON INTELLIGENT AND INNOVATIVE COMPUTING
   APPLICATIONS (ICONIC)
CT International Conference on Intelligent & Innovative Computing
   Applications (ICONIC)
CY DEC 06-07, 2018
CL Plaine Magnien, MAURITIUS
DE CNN; AlexNet; Transfer Learning; Computer Vision; MSER; OCR
AB Prohibition signs are commonly used for safety purposes in order to prevent and protect individuals from dangerous situations. These signs are placed in or around areas whereby they are clearly visible to the public. However, the visually impaired cannot visualize such signs. To help them, this paper proposes a system that combines Convolutional Neural Network (CNN) model and Computer Vision (CV) algorithms to detect and recognize prohibition signs in real scenes. The system uses pre-trained AlexNet model, fine-tuned using Prohibition Signage Boards (PSB) dataset and combined with Maximally Stable Extremal Regions (MSER) and Optical Character Recognition (OCR) techniques for text extraction and classification, to enhance the system performance. The experiments indicate that high recognition accuracies are achieved from a variety of prohibition images and prohibition texts.
CR Acilo J. P. N., 2018, P 2018 IEEE 14 INT C
   Chen X, 2017, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON RELIABILITY SYSTEMS ENGINEERING (ICRSE 2017)
   Cordova-Cruzatty A., 2017, P 2017 IEEE 2 EC TEC
   Elmahdy M. S., 2017, P 2017 IEEE EMBS INT
   Gu S., 2017, P 2017 IEEE 10 INT W
   He K., 2015, P 2015 COMP VIS PATT
   Islam R., 2016, P 2016 5 INT C INF E
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Motshoane K, 2017, PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P2802, DOI 10.1109/CompComm.2017.8323043
   Salahat E., 2017, P 2017 IEEE INT C IN
   Stolar M. N., 2017, P 2017 11 INT C SIGN
   Swathika R., 2016, P 2016 INT C INV COM
   Szegedy C., 2015, P 2015 COMP VIS PATT
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Zhu Z., 2017, 2017 INT C DIG IM CO
NR 15
TC 0
Z9 0
BN 978-1-5386-6477-3
PY 2018
BP 298
EP 302
ER

PT S
AU Nadeem, S
   Tahir, MA
   Naqvi, SSA
   Zaid, M
AF Nadeem, Shees
   Tahir, Muhammad Atif
   Naqvi, Syed Sadiq Ali
   Zaid, Muhammad
BE Nguyen, NT
   Pimenidis, E
   Khan, Z
   Trawinski, B
TI Ensemble of Texture and Deep Learning Features for Finding Abnormalities
   in the Gastro-Intestinal Tract
SO COMPUTATIONAL COLLECTIVE INTELLIGENCE, ICCCI 2018, PT II
SE Lecture Notes in Artificial Intelligence
CT 10th International Conference on Computational Collective Intelligence
   (ICCCI)
CY SEP 05-07, 2018
CL Bristol, ENGLAND
DE Gastro intestinal tract; Deep learning; Texture; Ensemble; Transfer
   learning
ID POLYP DETECTION; SYSTEM
AB An endoscopy is a strategy in which a specialist utilizes specific instruments to see and work on the inward vessels and organs of the body. This paper expects to predict the abnormalities and diseases in the Gastro-Intestinal Tract, utilizing multimedia data acquired from endoscopy. Deep Analysis of GI tract pictures can foresee diseases and abnormalities, in its early stages and accordingly spare human lives. In this paper, a novel ensemble method is presented, where texture and deep learning features are integrated to improve the prediction of the abnormalities in the GI tract e.g. Peptic ulcer disease. Multimedia content analysis (to extricate data from the visual information) and machine learning (for classification) have been explored. Deep learning has additionally been joined by means of Transfer learning. Medieval Benchmarking Initiative for Multimedia Evaluation provided the dataset, which includes 8000 pictures. The data is gathered from conventional colonoscopy process. Using logistic regression and ensemble of different extracted features, 83% accuracy and a F1 score of 0.821 is achieved on testing sample. The proposed approach is compared with several state-of-the-art methods and results have indicated significant performance gains when compared with other approaches.
CR Alexandre LA, 2007, LECT NOTES ARTIF INT, V4702, P358
   Bae SH, 2015, IEEE T MED IMAGING, V34, P2379, DOI 10.1109/TMI.2015.2434398
   Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002
   Bishop C. M., 2006, INFORM SCI STAT
   Cai D, 2011, VLDB J, V20, P21, DOI 10.1007/s00778-010-0189-3
   Cong Y, 2015, PATTERN RECOGN, V48, P907, DOI 10.1016/j.patcog.2014.09.010
   Ervik M., 2016, CANC TODAY
   Figueiredo Pedro N, 2011, Diagn Ther Endosc, V2011, P182435, DOI 10.1155/2011/182435
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hwang S., 2007, P ICIP
   Hwang S., 2011, P 7 INT C ADV VIS CO, P320
   Iakovidis DK, 2006, COMPUT BIOL MED, V36, P1084, DOI 10.1016/j.compbiomed.2005.09.008
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li B., 2009, ANN INT C IEEE ENG M
   Magoulas GD, 2004, APPL SOFT COMPUT, V4, P369, DOI 10.1016/j.asoc.2004.01.005
   Mamonov AV, 2014, IEEE T MED IMAGING, V33, P1488, DOI 10.1109/TMI.2014.2314959
   Munzer B., 2013, P ICME
   Munzer B., 2013, P CBMS
   Naqvi S. A., 2017, P MEDIAEVAL WORKSH B
   Nawarathna R, 2014, NEUROCOMPUTING, V144, P70, DOI 10.1016/j.neucom.2014.02.064
   Nawarathna RD, 2010, LECT NOTES COMPUT SC, V6165, P153, DOI 10.1007/978-3-642-13923-9_16
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Park  S., 2015, POLYP DETECTION COLO
   Pogorelov K., 2017, P MEDIAEVAL 2017 WOR
   Pogorelov Konstantin, 2017, P MMSYS, P164, DOI DOI 10.1145/3083187.3083212
   Ribeiro E., 2016, COMPUT MATH METHOD M, V2016, P16
   Riegler M, 2017, MEDIAEVAL 2017
   Tahir MA, 2013, IEEE T MULTIMEDIA, V15, P1653, DOI 10.1109/TMM.2013.2264927
   Yoshida H, 2001, IEEE T MED IMAGING, V20, P1261, DOI 10.1109/42.974921
   Zhao S, 2011, I C WIREL COMM NETW
   Zhu R., 2015, 2015 8 INT C IM SIGN
NR 31
TC 0
Z9 0
SN 0302-9743
EI 1611-3349
BN 978-3-319-98446-9; 978-3-319-98445-2
PY 2018
VL 11056
BP 469
EP 478
DI 10.1007/978-3-319-98446-9_44
ER

PT B
AU Sengur, A
   Akhtar, Z
   Akbulut, Y
   Ekici, S
   Budak, U
AF Sengur, Abdulkadir
   Akhtar, Zahid
   Akbulut, Yaman
   Ekici, Sami
   Budak, Umit
GP IEEE
TI Deep Feature Extraction for Face Liveness Detection
SO 2018 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND DATA
   PROCESSING (IDAP)
CT International Conference on Artificial Intelligence and Data Processing
   (IDAP)
CY SEP 28-30, 2018
CL Inonu Univ, Malatya, TURKEY
HO Inonu Univ
DE Face recognition; Face spoof detection; Deep learning; CNN; Feature
   extraction
ID CHALLENGES; IMAGE
AB Face recognition is now widely being used to verify the identity of the person in various applications ranging from border crossing to mobile authentication. However, most face recognition systems are vulnerable to spoofing or presentation attacks, where a photo, a video, or a 3D mask of a genuine user's face may be utilized to fool the biometric system. Although many face spoof detection techniques have been proposed, the issue is still unsolved. Recently deep learning based models have achieved impressive results in various challenging image and video classification tasks. Consequently, very few works have applied convolutional neural networks (CNNs) for face liveness detection. Nonetheless, it is still unclear how different CNN features and methods compare with each other for face spoof detection, since prior CNN based face liveness detection approaches employ different fine-tuning procedures and/or datasets for training. Thus, in this paper, an approach based on transfer learning using some well-known and well-adopted pre-trained CNNs architectures is presented. This study explores different deep features and compares them on a common ground for face liveness detection in videos. Experimental analysis on two publicly available databases, NUAA and CASIA-FASD, shows that the proposed method is able to attain satisfactory and comparable results to the state-of-the-art methods.
CR Agarwal A, 2017, IEEE COMPUT SOC CONF, P275, DOI 10.1109/CVPRW.2017.40
   Akbulut Y., 2017, INT ART INT DAT PROC, P1
   Akhtar Z, 2017, COMPUTER, V50, P80, DOI 10.1109/MC.2017.119
   Akhtar Z, 2015, IEEE SECUR PRIV, V13, P63, DOI 10.1109/MSP.2015.116
   Anjos A., 2011, BIOM IJCB 2011 INT J, P1, DOI DOI 10.1109/IJCB.2011.6117503
   Bao W, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND SIGNAL PROCESSING, P233
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Li JW, 2004, P SOC PHOTO-OPT INS, V5404, P296, DOI 10.1117/12.541955
   Li LF, 2016, CRYSTALS, V6, DOI 10.3390/cryst6040045
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Zhang Z, 2012, IEEE INT C BIO BIO W
NR 18
TC 0
Z9 0
BN 978-1-5386-6878-8
PY 2018
ER

PT B
AU Wang, JD
   Zheng, VW
   Chen, YQ
   Huang, MY
AF Wang, Jindong
   Zheng, Vincent W.
   Chen, Yiqiang
   Huang, Meiyu
GP ACM
TI Deep Transfer Learning for Cross-domain Activity Recognition
SO PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON CROWD SCIENCE AND
   ENGINEERING (ICCSE 2018)
CT 3rd International Conference on Crowd Science and Engineering (ICCSE)
CY JUL 28-31, 2018
CL Nanyang Technol Univ, Singapore, SINGAPORE
HO Nanyang Technol Univ
DE Transfer Learning; Activity Recognition; Deep Learning; Domain
   Adaptation
ID KERNEL
AB Human activity recognition plays an important role in people's daily life. However, it is often expensive and time-consuming to acquire sufficient labeled activity data. To solve this problem, transfer learning leverages the labeled samples from the source domain to annotate the target domain which has few or none labels. Unfortunately, when there are several source domains available, it is difficult to select the right source domains for transfer. The right source domain means that it has the most similar properties with the target domain, thus their similarity is higher, which can facilitate transfer learning. Choosing the right source domain helps the algorithm perform well and prevents the negative transfer. In this paper, we propose an effective Unsupervised Source Selection algorithm for Activity Recognition (USSAR). USSAR is able to select the most similar K source domains from a list of available domains. After this, we propose an effective Transfer Neural Network to perform knowledge transfer for Activity Recognition (TNNAR). TNNAR could capture both the time and spatial relationship between activities while transferring knowledge. Experiments on three public activity recognition datasets demonstrate that: 1) The USSAR algorithm is effective in selecting the best source domains. 2) The TNNAR method can reach high accuracy when performing activity knowledge transfer.
CR Barshan B, 2014, COMPUT J, V57, P1649, DOI 10.1093/comjnl/bxt075
   Ben Tan, 2017, 31 AAAI C ART INT
   Ben-David Shai, 2007, ADV NEURAL INFORM PR, P137, DOI DOI 10.1007/S10994-009-5152-4
   Bhatt Himanshu S, 2016, IJCAI 2016, P3691
   Blitzer J., 2006, P 2006 C EMP METH NA, P120
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Bulling A, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2499621
   Chattopadhyay R, 2012, ACM T KNOWL DISCOV D, V6, DOI 10.1145/2382577.2382582
   Chavarriaga R, 2013, PATTERN RECOGN LETT, V34, P2033, DOI 10.1016/j.patrec.2012.12.014
   Chen Yiqiang, 2016, ACM INT JOINT C ACM, P33
   Collier Edward, 2018, ARXIV180407846
   Cook D, 2013, KNOWL INF SYST, V36, P537, DOI 10.1007/s10115-013-0665-3
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Fodor I.K., 2002, US DOE OFFICE SCI TE, V9, P1, DOI DOI 10.2172/15002155
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hamm J., 2008, P 25 INT C MACH LEAR, P376
   Hammerla N. Y., 2015, P 29 AAAI C ART INT, P1742
   Hu LS, 2016, 2016 INT IEEE CONFERENCES ON UBIQUITOUS INTELLIGENCE & COMPUTING, ADVANCED & TRUSTED COMPUTING, SCALABLE COMPUTING AND COMMUNICATIONS, CLOUD AND BIG DATA COMPUTING, INTERNET OF PEOPLE, AND SMART WORLD CONGRESS (UIC/ATC/SCALCOM/CBDCOM/IOP/SMARTWORLD), P327, DOI [10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0066, 10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.90]
   Lara OD, 2013, IEEE COMMUN SURV TUT, V15, P1192, DOI 10.1109/SURV.2012.110112.00192
   Lasecki Walter S., 2013, P 2013 C COMP SUPP C, P1203, DOI DOI 10.1145/2441776.2441912
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liu YP, 2018, IEEE INT CONF COMM
   Long M., 2017, P 34 INT C MACH LEAR, P2208
   Long MS, 2015, IEEE T KNOWL DATA EN, V27, P1519, DOI 10.1109/TKDE.2014.2373376
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Lu Z., 2014, P NAT C ART INT, P122
   Morales F. J. O., 2016, P 2016 ACM INT S WEA, P92
   Nguyen LT, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P1053, DOI 10.1145/2750858.2804256
   Pan S., 2008, AAAI, P677
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Plotz T., 2011, IJCAI P INT JOINT C, V2, P1729, DOI DOI 10.5591/978-1-57735-516-8/LICA111-290
   Reiss A, 2012, IEEE INT SYM WRBL CO, P108, DOI 10.1109/ISWC.2012.13
   Roy Nirmalya, 2016, P IEEE INT C PERV CO, P1
   Sung Flood, 2017, ARXIV171106025
   Tzeng E., 2014, ARXIV14123474
   Wang JD, 2017, IEEE DATA MINING, P1129, DOI 10.1109/ICDM.2017.150
   Wang Jindong, 2018, PATTERN RECOGNITION
   Wen Jiahui, 2016, 2016 IEEE INT C PERV, P1
   Xiang EW, 2011, IJCAI P INT JOINT C, V22, P2355
   Xu H, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P208, DOI 10.1145/2971648.2971668
   Yao Y, 2010, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2010.5539857
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
   Zhao Ming, 2017, ICML
   Zhao Z., 2011, IJCAI, P2545
   Zheng V. W., 2011, IJCAI P INT JOINT C, V22, P2085
NR 48
TC 1
Z9 1
BN 978-1-4503-6587-1
PY 2018
DI 10.1145/3265689.3265705
ER

PT S
AU Baumann, U
   Huang, YY
   Glaser, C
   Herman, M
   Banzhaf, H
   Zollner, JM
AF Baumann, Ulrich
   Huang, Yuan-Yao
   Glaeser, Claudius
   Herman, Michael
   Banzhaf, Holger
   Zoellner, J. Marius
GP IEEE
TI Classifying Road Intersections using Transfer-Learning on a Deep Neural
   Network
SO 2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)
SE IEEE International Conference on Intelligent Transportation Systems-ITSC
CT 21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC)
CY NOV 04-07, 2018
CL Maui, HI
AB With the steady rise of advanced driver assistance systems (ADAS), more and more aspects of the driving task are transferred from the human driver to the vehicle's control system. In order to handle many of these responsibilities, vehicles need to understand their environment and adjust their behavior according to it. An important aspect of the vehicle environment is the layout of the road segment right ahead of the vehicle, such as the presence and type of an intersection, as it defines the scenario, provides context information and constrains the future motion of traffic participants. The knowledge of upcoming intersections can help to improve various aspects in the context of driver assistance systems and automated driving, such as the prediction of traffic participants or the adjustment of a system with respect to the current scenario. The contribution of this paper is threefold: First, it introduces a model for intersection identification and classification ahead of a vehicle solely from on-board sensor data via deep learning. Second, it proposes a transfer-learning technique allowing to train with fewer samples and showing that intermediate features from path prediction are also beneficial for intersection classification tasks. Third, it allows to reduce necessary computational power since feature extraction is partially shared between the path prediction and the intersection classification model.
CR Barsi A., 2003, INT ARCH PHOTOGRAMME, V34, P113
   Baumann U., 2018, INT C ROB AUT ICRA
   Bittel S, 2017, IEEE SYS MAN CYBERN, P52, DOI 10.1109/SMC.2017.8122577
   Caltagirone L, 2017, IEEE INT C INTELL TR
   Chen Tongtong, 2011, Proceedings of the Sixth International Conference on Image and Graphics (ICIG 2011), P754, DOI 10.1109/ICIG.2011.69
   Fathi A, 2010, LECT NOTES COMPUT SC, V6292, P56, DOI 10.1007/978-3-642-15300-6_5
   Glaser C, 2013, IEEE INT C INTELL TR, P1503, DOI 10.1109/ITSC.2013.6728443
   Glaser C, 2014, IEEE INT VEH SYM, P1270, DOI 10.1109/IVS.2014.6856388
   Hata A, 2013, COMM COM INF SC, V383, P112
   Ioffe S, 2015, INT C MACH LEARN
   Kingma D. P., 2014, 3 INT C LEARN REPR
   Kodagoda KRS, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P19, DOI 10.1109/IRDS.2002.1041355
   Nair V, 2010, P 27 INT C MACH LEAR
   Rasmussen C, 2003, IEEE IV2003: INTELLIGENT VEHICLES SYMPOSIUM, PROCEEDINGS, P422, DOI 10.1109/IVS.2003.1212948
   Rebai K, 2013, IEEE CONF OPEN SYST, P100, DOI 10.1109/ICOS.2013.6735056
   Seeger C., 2016, IEEE COMP SOC C COMP, P2722
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Teichmann M., 2016, CORR
   Wang L, 2017, IEEE INT VEH SYM, P1440, DOI 10.1109/IVS.2017.7995912
   Zhu QW, 2012, IEEE INT C INTELL TR, P1191, DOI 10.1109/ITSC.2012.6338795
   Zhu QW, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P456, DOI 10.1109/IVS.2012.6232219
NR 21
TC 0
Z9 0
SN 2153-0009
BN 978-1-7281-0323-5
PY 2018
BP 683
EP 690
ER

PT S
AU Krishnakumari, P
   Perotti, A
   Pinto, V
   Cats, O
   van Lint, H
AF Krishnakumari, Panchamy
   Perotti, Alan
   Pinto, Viviana
   Cats, Oded
   van Lint, Hans
GP IEEE
TI Understanding Network Traffic States using Transfer Learning
SO 2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)
SE IEEE International Conference on Intelligent Transportation Systems-ITSC
CT 21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC)
CY NOV 04-07, 2018
CL Maui, HI
DE Network traffic; convolutional neural networks; deep learning; transfer
   learning
ID TRAVEL-TIME PREDICTION; FLOW PREDICTION
AB Large-scale network traffic analysis is crucial for many transport applications, ranging from estimation and prediction to control and planning. One of the key issues is how to integrate spatial and temporal analyses efficiently. Deep Learning is gaining momentum as a go-to approach for artificial vision, and transfer learning approaches allow to exploit pretrained models and apply them to new domains. In this paper, we encode traffic states as images and use a pretrained deep convolutional neural network as a feature extractor. Experimental results show how the extracted feature vectors cluster naturally into meaningful network traffic states and illustrate how these network states can be used for traffic state prediction.
CR Clark S, 2003, J TRANSP ENG, V129, P161, DOI 10.1061/(ASCE)0733-947X(2003)129:2(161)
   Everitt B., 2002, CAMBRIDGE DICT STAT, V106
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Geroliminis N, 2008, TRANSPORT RES B-METH, V42, P759, DOI 10.1016/j.trb.2008.02.002
   Geron Aurelien, 2017, HANDS ON MACHINE LEA
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Ji YX, 2012, TRANSPORT RES B-METH, V46, P1639, DOI 10.1016/j.trb.2012.08.005
   Koesdwiady A, 2016, IEEE T VEH TECHNOL, V65, P9508, DOI 10.1109/TVT.2016.2585575
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lopez C, 2017, TRANSPORT RES REC, P98, DOI 10.3141/2623-11
   Lopez C, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-14237-8
   Lv YS, 2015, IEEE T INTELL TRANSP, V16, P865, DOI 10.1109/TITS.2014.2345663
   Ma XL, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17040818
   Oh S, 2015, TRANSPORT REV, V35, P4, DOI 10.1080/01441647.2014.992496
   Polson NG, 2017, TRANSPORT RES C-EMER, V79, P1, DOI 10.1016/j.trc.2017.02.024
   Rice J, 2001, 2001 IEEE INTELLIGENT TRANSPORTATION SYSTEMS - PROCEEDINGS, P227, DOI 10.1109/ITSC.2001.948660
   Rokach L, 2005, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, P321, DOI 10.1007/0-387-25465-X_15
   Saeedmanesh M, 2016, TRANSPORT RES B-METH, V91, P250, DOI 10.1016/j.trb.2016.05.008
   Struyf A., 1997, J STAT SOFTW, V1, P1, DOI DOI 10.18637/JSS.V001.I04
   Szegedy C., 2017, AAAI, V4, P12
   van Hinsbergen CPIJ, 2012, IEEE T INTELL TRANSP, V13, P385, DOI 10.1109/TITS.2011.2175728
   van Lint JWC, 2008, IEEE T INTELL TRANSP, V9, P38, DOI [10.1109/TITS.2008.915649, 10.1109/TITS.2007.915649]
   Van Lint J. W. C., 2012, ARTIF INTELL, V22, P22
   Vlahogianni EI, 2004, TRANSPORT REV, V24, P533, DOI 10.1080/0144164042000196000
   Vlahogianni EI, 2014, TRANSPORT RES C-EMER, V43, P3, DOI 10.1016/j.trc.2014.01.005
   Wang JQ, 2016, ENG APPL ARTIF INTEL, V52, P145, DOI 10.1016/j.engappai.2016.02.012
   Wang R, 2016, TRANSPORT RES C-EMER, V71, P521, DOI 10.1016/j.trc.2016.08.003
   Wang Y., 2006, IEEE T CONTROL SYSTE, V14
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wu C.-H., 2003, P 2003 IEEE C INT TR
   Xu YY, 2016, J ADV TRANSPORT, V50, P489, DOI 10.1002/atr.1356
   Yang H., 2016, IEEE T NEURAL NETWOR
   YU HY, 2017, SENSORS BASEL, V17, DOI DOI 10.3390/S17071501
   Yuan YF, 2012, IEEE T INTELL TRANSP, V13, P59, DOI 10.1109/TITS.2011.2178837
   Zhang YL, 2013, COMPUT-AIDED CIV INF, V28, P594, DOI 10.1111/mice.12014
NR 35
TC 0
Z9 0
SN 2153-0009
BN 978-1-7281-0323-5
PY 2018
BP 1396
EP 1401
ER

PT S
AU Kreidieh, AR
   Wu, C
   Bayen, AM
AF Kreidieh, Abdul Rahman
   Wu, Cathy
   Bayen, Alexandre M.
GP IEEE
TI Dissipating stop-and-go waves in closed and open networks via deep
   reinforcement learning
SO 2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)
SE IEEE International Conference on Intelligent Transportation Systems-ITSC
CT 21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC)
CY NOV 04-07, 2018
CL Maui, HI
ID CONVECTIVE INSTABILITY
AB This article demonstrates the ability for modelfree reinforcement learning (RL) techniques to generate traffic control strategies for connected and automated vehicles (CAVs) in various network geometries. This method is demonstrated to achieve near complete wave dissipation in a straight open road network with only 10% CAV penetration, while penetration rates as low as 2.5% are revealed to contribute greatly to reductions in the frequency and magnitude of formed waves. Moreover, a study of controllers generated in closed network scenarios exhibiting otherwise similar densities and perturbing behaviors confirms that closed network policies generalize to open network tasks, and presents the potential role of transfer learning in fine-tuning the parameters of these policies. Videos of the results are available at: https://sites.google.com/view/itsc-dissipating-waves.
CR Abdulhai B, 2003, J TRANSP ENG, V129, P278, DOI 10.1061/(ASCE)0733-947X(2003)129:3(278)
   Belletti F, 2018, IEEE T INTELL TRANSP, V19, P1198, DOI 10.1109/TITS.2017.2725912
   BELLMAN R, 1957, J MATH MECH, V6, P679
   Chung J., 2014, CORR
   Duan Y., 2016, INT C MACH LEARN, V48, P1329
   Haykin S., COMPREHENSIVE FDN
   HERMAN R, 1959, OPER RES, V7, P86, DOI 10.1287/opre.7.1.86
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Krajzewicz D., 2012, RECENT DEV APPL SUMO
   Levine S., 2015, ICML, P1889
   Li L, 2016, IEEE-CAA J AUTOMATIC, V3, P247, DOI 10.1109/JAS.2016.7508798
   Li ZB, 2017, IEEE T INTELL TRANSP, V18, P3204, DOI 10.1109/TITS.2017.2687620
   Mitarai N, 2000, J PHYS SOC JPN, V69, P3752, DOI 10.1143/JPSJ.69.3752
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Orosz G, 2006, P R SOC A, V462, P2643, DOI 10.1098/rspa.2006.1660
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Schmidt-Dumont T., 2015, IEEE T INTELL TRANSP, V14, P1
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Stern R. E., 2017, ARXIV170501693
   Sugiyama Y, 2008, NEW J PHYS, V10, DOI 10.1088/1367-2630/10/3/033001
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   Treiber M, 2000, PHYS REV E, V62, P1805, DOI 10.1103/PhysRevE.62.1805
   Treiber M., 2013, TRAFFIC FLOW DYNAMIC, DOI [10.1007/978-3-642-32460-4, DOI 10.1007/978-3-642-32460-4]
   Treiber M., 2017, TRANSPORTATION RES B
   Treiber M, 2011, TRANSPORT RES B-METH, V45, P1362, DOI 10.1016/j.trb.2011.05.011
   Ward J. A., 2011, P ROYAL SOC LOND MAT
   Wiering Marco, 2000, P 17 INT C MACH LEAR, P1151
   Wu C., 2017, CORR
   Wu C., 2017, C ROB LEARN, P398
NR 30
TC 0
Z9 0
SN 2153-0009
BN 978-1-7281-0323-5
PY 2018
BP 1475
EP 1480
ER

PT S
AU Yang, NM
   Kanji, T
   Fang, YC
   Fei, XX
   Kazunori, I
   Yuuki, I
AF Yang Naiming
   Kanji, Tanaka
   Fang Yichu
   Fei Xiaoxiao
   Kazunori, Inagami
   Yuuki, Ishikawa
GP IEEE
TI Long-Term Vehicle Localization Using Compressed Visual Experiences
SO 2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)
SE IEEE International Conference on Intelligent Transportation Systems-ITSC
CT 21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC)
CY NOV 04-07, 2018
CL Maui, HI
AB Vision-based global localization without a prior location estimate is a fundamental task for safe and efficient vehicle navigation in GPS-denied environments. Cross-season localization, in which query and database images involve different seasons is one of the most challenging task scenarios, owing to appearance variations among seasons. Because of recent advances in deep convolutional neural networks (DCNs) and transfer learning techniques, the task can be solved accurately by training and fine-tuning a DCN-based visual place classifier. However, the direct implementation of this would require collecting and storing a large amount of visual experiences (i.e., training data) for every new season, which is impractical. The goal of our study is to suppress the space cost for long-term memory and to develop a constant cost framework for long-term global localization. Moreover, we formulate and consider the task of experience compression as a scheduling problem of how to choose the part of the previous season's experience that is to be replaced with the current season's experience, to achieve an optimal tradeoff between localization accuracy and training efficiency. Experimental results using the publicly available North Campus Long-Term autonomy dataset validate the efficacy of our proposed approach.
CR Anati R. C., 2016, SEMANTIC LOCALIZATIO
   Arroyo R., 2017, AUTON ROBOT, P1
   Arroyo R, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4656, DOI 10.1109/IROS.2016.7759685
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Cao J, 2017, COMPUT ELECTR ENG, V63, P79, DOI 10.1016/j.compeleceng.2017.03.015
   Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638
   Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961
   Fei XX, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P109, DOI 10.23919/MVA.2017.7986802
   Gavves E, 2015, IEEE I CONF COMP VIS, P2731, DOI 10.1109/ICCV.2015.313
   Geiger A., 2011, INT VEH S 4
   Islam M, 2003, IEEE T NEURAL NETWOR, V14, P820, DOI 10.1109/TNN.2003.813832
   Kanji T, 2015, IEEE INT C INT ROBOT, P729, DOI 10.1109/IROS.2015.7353453
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kumar D, 2017, 2017 14TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2017), P32, DOI 10.1109/CRV.2017.26
   Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823
   Mancini M., 2018, IEEE ROBOTICS AUTOMA
   Massiceti Daniela, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5118, DOI 10.1109/ICRA.2017.7989598
   Morley D., 2017, P IEEE C COMP VIS PA
   Se S, 2002, INT J ROBOT RES, V21, P735, DOI 10.1177/027836402761412467
   Shojaedini E., 2017, CORR
   Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377
   Takahashi Y., 2017, IEEE INT C INT TRANS, P842
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 23
TC 0
Z9 0
SN 2153-0009
BN 978-1-7281-0323-5
PY 2018
BP 2203
EP 2208
ER

PT S
AU Nezafat, RV
   Salahshour, B
   Cetin, M
AF Nezafat, Reza Vatani
   Salahshour, Behrouz
   Cetin, Mecit
GP IEEE
TI Classification of truck body types using a deep transfer learning
   approach
SO 2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)
SE IEEE International Conference on Intelligent Transportation Systems-ITSC
CT 21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC)
CY NOV 04-07, 2018
CL Maui, HI
DE Vehicle classification; Convolutional neural network; Resnet152; Support
   vector machines; Transfer learning
ID CONVOLUTIONAL NEURAL-NETWORKS
AB Classification of vehicles is one of the most important tasks in intelligent transportation systems (ITS). While there are various types of sensors for measuring vehicle characteristics, this paper is focused on an image-based vehicle classification system. Most traditional approaches for image-based vehicle classification are computationally extensive and typically require a large amount of data for model training. This paper investigates whether it is possible to transfer the learning of a highly accurate pre-trained model for classifying truck images based on body type. Results show that using a pre-trained model to extract lowlevel features of images increases the accuracy of the model significantly, even with a relatively small size of training data. Furthermore, a convolutional neural network (CNN) is shown to outperform other types of models to classify trucks based on the extracted features.
CR Chen XY, 2014, IEEE GEOSCI REMOTE S, V11, P1797, DOI 10.1109/LGRS.2014.2309695
   Chen ZZ, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND INTELLIGENT SYSTEMS, PROCEEDINGS, VOL 4, P214, DOI 10.1109/ICICISYS.2009.5357707
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Deo RC, 2018, RENEW ENERG, V116, P309, DOI 10.1016/j.renene.2017.09.078
   Donahue  J., 2014, P 31 INT C MACH LEAR, P647
   Girshick R, 2015, ARXIV150408083
   Gopalakrishnan K, 2017, CONSTR BUILD MATER, V157, P322, DOI [10.1016/j.conbuildmat2017.09.110, 10.1016/j.conbuildmat.2017.09.110]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hernandez SV, 2016, TRANSPORT RES C-EMER, V68, P1, DOI 10.1016/j.trc.2016.03.003
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Kafai M, 2012, IEEE T IND INFORM, V8, P100, DOI 10.1109/TII.2011.2173203
   Kim HT, 2013, 2013 13TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS 2013), P1342, DOI 10.1109/ICCAS.2013.6704164
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Mita Y., 1995, RANGE MEASUREMENT TY
   Ng LT, 2014, LECT NOTES ELECTR EN, V291, P221, DOI 10.1007/978-981-4585-42-2_26
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Peiyu L., 2006, EMBEDDED FLEXIBLE AS
   Peterson L.E., 2009, SCHOLARPEDIA, V4, P1883, DOI DOI 10.4249/SCHOLARPEDIA.1883
   Qassim H, 2018, 2018 IEEE 8TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P169, DOI 10.1109/CCWC.2018.8301729
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Richards J. A, 2013, REMOTE SENSING DIGIT, P247
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, ARXIV14091556
   Sivaraman S, 2010, IEEE T INTELL TRANSP, V11, P267, DOI 10.1109/TITS.2010.2040177
   Sohrabi S., 2017, 96 ANN M TRANSP RES
   Szegedy C, 2015, GOING DEEPER CONVOLU
   Wangerin Kristen A., 2014, 2014 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC), P1, DOI [10.1109/URSIGASS.2014.6929321, 10.1109/NSSMIC.2014.7430811]
   Wei-Lin Ku, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457829
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhou YR, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P276, DOI 10.1109/ICDSP.2016.7868561
   Zhuo L, 2017, MACH VISION APPL, V28, P793, DOI 10.1007/s00138-017-0846-2
NR 33
TC 0
Z9 0
SN 2153-0009
BN 978-1-7281-0323-5
PY 2018
BP 3144
EP 3149
ER

PT S
AU Zhan, WJ
   Chen, JX
   Fan, L
   Ou, XQ
   Chen, L
AF Zhan, Wujing
   Chen, Jiaxing
   Fan, Lei
   Ou, Xinqi
   Chen, Long
GP IEEE
TI A New Feature Pyramid Network For Road Scene Segmentation
SO 2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
   (ITSC)
SE IEEE International Conference on Intelligent Transportation Systems-ITSC
CT 21st IEEE International Conference on Intelligent Transportation Systems
   (ITSC)
CY NOV 04-07, 2018
CL Maui, HI
AB Road scene segmentation is of great significance in intelligent transportation system for different applications such as autonomous driving and semantic map building. Despite great progress in this field with the deep learning methods, there are still many difficulties such as robust segmentation of small objects and same type of objects with different sizes in different scenes. In this paper, we propose a new pyramid architecture for scene segmentation, which is a top-down architecture with lateral connections for multi-scale semantic feature maps building, and sufficiently incorporate the momentous global scenery prior. Besides, we also propose a novel training method, which combines the re-sampling, pixel-wise cost learning and transfer learning together, to deal with the imbalance problem. Experimental results on KITTI and Cityscapes dataset demonstrate effectiveness of the proposed method.
CR Chen L, 2017, IEEE T INTELL TRANSP, V18, P3093, DOI 10.1109/TITS.2017.2680538
   Cordts M., 2016, P IEEE C COMP VIS PA
   Driggs-Campbell K, 2015, IEEE INT C INTELL TR, P739, DOI 10.1109/ITSC.2015.125
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Li QQ, 2014, IEEE T VEH TECHNOL, V63, P540, DOI 10.1109/TVT.2013.2281199
   Lin T. Y., 2017, P IEEE C COMP VIS PA, V1, P4
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Menze M., 2015, C COMP VIS PATT REC
   Murali V., 2018, ARXIV180100858
   Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203
   Paszke A., 2016, ARXIV COMPUTER VISIO
   ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Siam M., 2017, ARXIV170702432
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164
   Zhang W., 2017, ARXIV171208745
   Zhao H., 2017, IEEE C COMP VIS PATT, P2881
NR 17
TC 0
Z9 0
SN 2153-0009
BN 978-1-7281-0323-5
PY 2018
BP 3487
EP 3492
ER

PT S
AU Ren, ZZ
   Lee, YJ
AF Ren, Zhongzheng
   Lee, Yong Jae
GP IEEE
TI Cross-Domain Self-supervised Multi-task Feature Learning using Synthetic
   Imagery
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB In human learning, it is common to use multiple sources of information jointly. However, most existing feature learning approaches learn from only a single task. In this paper, we propose a novel multi-task deep network to learn generalizable high-level visual representations. Since multitask learning requires annotations for multiple properties of the same training instance, we look to synthetic images to train our network. To overcome the domain difference between real and synthetic data, we employ an unsupervised feature space domain adaptation method based on adversarial learning. Given an input synthetic RGB image, our network simultaneously predicts its surface normal, depth, and instance contour, while also minimizing the feature space domain differences between real and synthetic data. Through extensive experiments, we demonstrate that our network learns more transferable representations compared to single-task baselines. Our learned representation produces state-of-the-art transfer learning results on PASCAL VOC 2007 classification and 2012 detection.
CR Agrawal P., 2015, ICCV
   Arandjelovic R., 2017, ICCV
   Aubry  Mathieu, 2014, CVPR
   Bansal A., 2017, ARXIV170206506
   Bansal  Aayush, 2016, CVPR
   Bengio Y., 2013, PAMI
   Bojanowski P., 2017, ICML
   Bousmalis  K., 2017, CVPR
   Caruana  R., 1997, MACHINE LEARNING
   Chang A. X, 2015, ARXIV151203012
   Deng J., 2009, CVPR
   Doersch C., 2017, ICCV
   Doersch Carl, 2015, ICCV
   Donahue J., 2017, ICLR
   Eigen D., 2015, ICCV
   Eigen  D., 2014, NIPS
   Everingham M., 2010, IJCV
   Fei- Fei L., 2004, CVPR WORKSH
   Gaidon A., 2016, CVPR
   Ganin Y., 2015, ICML
   Ganin  Yaroslav, 2016, JMLR
   Girshick Ross, 2015, ICCV
   Gkioxari G., 2014, R CNNS POSE ESTIMATI
   Goodfellow I., 2014, NIPS
   Grauman K, 2016, CVPR
   Hinton G. E., 2006, SCIENCE
   Huang Q., 2015, SIGGRAPH
   Ioffe  S., 2015, ICML
   Isola P., 2017, CVPR
   Jayaraman Dinesh, 2015, ICCV
   Kim T., 2017, ICML
   Kokkinos I., 2017, CVPR
   Krahenbuhl P., 2016, ICLR
   Krizhevsky  A., 2012, NIPS
   Ladicky L., 2014, ECCV
   Larsson G., 2017, CVPR
   LeCun Y., 1998, P IEEE
   Lerer A., 2016, ICML
   Lin T., 2014, CORR
   Liu M.-Y., 2016, NIPS
   Long  J., 2015, CVPR
   Mayer N., 2016, CVPR
   McCormac J., 2017, ICCV
   Misra I., 2016, CVPR
   Misra Ishan, 2016, ECCV
   Mottaghi Roozbeh, 2016, CVPR
   Noroozi M., 2016, ECCV
   Noroozi M., 2017, ICCV
   Owens A., 2016, ECCV
   Pathak D., 2017, CVPR
   Pathak D., 2016, CVPR
   Peng X., 2015, ICCV
   Pinto L., 2016, ECCV
   Pinto  L., 2017, ICRA
   Richter S. R., 2016, ECCV
   Ros  G., 2016, CVPR
   Saenko K., 2010, ECCV
   Shafaei A., 2016, BMVC
   Shilane P., 2004, SHAPE MODELING INT
   Shrivastava Ashish, 2017, CVPR
   Silberman N., 2012, ECCV
   Song S., 2017, CVPR
   Su H., 2014, SIGGRAPH
   Tzeng  E., 2017, CVPR
   Tzeng E., 2015, ICCV
   Vincent P., 2010, JMLR
   Wang X., 2015, ICCV
   Wang X., 2017, ICCV
   Wu  Jiajun, 2015, NIPS, P2
   Xie  S., 2015, ICCV
   Yu F., 2015, ICLR
   Zhang R., 2017, CVPR
   Zhang R., 2016, ECCV
   Zhang  Y., 2016, UNREALSTEREO SYNTHET
   Zhang Y., 2017, CVPR
   Zhang Z., 2014, ECCV
   Zhou B., 2016, PLACES IMAGE DATABAS
   Zhu J.-Y., 2017, ICCV
   Zhu Y., 2017, ICRA
NR 79
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 762
EP 771
DI 10.1109/CVPR.2018.00086
ER

PT S
AU Uijlings, JRR
   Popov, S
   Ferrari, V
AF Uijlings, Jasper R. R.
   Popov, S.
   Ferrari, V.
GP IEEE
TI Revisiting knowledge transfer for training object class detectors
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB We propose to revisit knowledge transfer for training object detectors on target classes from weakly supervised training images, helped by a set of source classes with bounding-box annotations. We present a unified knowledge transfer framework based on training a single neural network multi-class object detector over all source classes, organized in a semantic hierarchy. This generates proposals with scores at multiple levels in the hierarchy, which we use to explore knowledge transfer over a broad range of generality, ranging from class-specific (bycicle to motorbike) to class-generic (objectness to any class). Experiments on the 200 object classes in the ILSVRC 2013 detection dataset show that our technique (1) leads to much better performance on the target classes (70.3% CorLoc, 36.9% mAP) than a weakly supervised baseline which uses manually engineered objectness [11] (50.5% CorLoc, 25.4% mAP). (2) delivers target object detectors reaching 80% of the mAP of their fully supervised counterparts. (3) outperforms the best reported transfer learning results on this dataset (+41% CorLoc and + 3% mAP over [18, 46], +16.2% mAP over [32]). Moreover, we also carry out several acrossdataset knowledge transfer experiments [27, 24, 35] and find that (4) our technique outperforms the weakly supervised baseline in all dataset pairs by 1.5 x -1.9x, establishing its general applicability.
CR Alexe B., 2010, CVPR
   Bilen H., 2016, CVPR
   Bilen H., 2015, CVPR
   Bilen H., 2014, BMVC
   Cesa-Bianchi N., 2006, ICML
   Cinbis R., 2016, IEEE T PAMI
   Deselaers T., 2010, ECCV
   Deselaers T., 2012, IJCV
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Dollar P., 2014, ECCV
   Everingham M., 2010, IJCV
   Girshick  R., 2014, CVPR
   Girshick Ross, 2015, ICCV
   Gokberk Cinbis R., 2014, CVPR
   Guillaumin M., 2012, CVPR
   He K., 2014, ECCV
   He K., 2016, CVPR
   Hoffman J., 2016, JMLR
   Huang J., 2017, CVPR
   Jia  Y., 2013, CAFFE OPEN SOURCE CO
   Kantorov V., 2010, ECCV
   Kim G., 2009, NIPS, P2
   Koller D., 1997, ICML
   Krasin I., 2017, OPEN IMAGES PUBLIC D
   Krizhevsky  A., 2012, NIPS
   Lin D., 1998, ICML
   Lin T.-Y., 2014, ECCV
   Liu  W., 2016, ECCV
   Nguyen M., 2009, ICCV
   Pandey M., 2011, ICCV
   Prest A., 2012, CVPR
   Redmon J, 2017, CVPR
   Ren  Shaoqing, 2015, NIPS
   Rochan M., 2015, CVPR
   Russakovsky O., 2012, ECCV
   Russakovsky Olga, 2015, IJCV
   Shapovalova N., 2012, ECCV
   Shi Z., 2012, BMVC
   Silla C., 2011, DATA MINING KNOWLEDG
   Siva P., 2011, ICCV
   Song  H., 2014, ICML
   Song H. O., 2014, NIPS
   Szegedy  C., 2016, CVPR
   Szegedy C., 2017, AAAI
   Tang K., 2014, CVPR
   Tang Y., 2016, CVPR
   Uijlings J., 2013, IJCV
   Wang C, 2015, IEEE T IMAGE PROCESS, V24, P1371, DOI 10.1109/TIP.2015.2396361
   Wang L., 2014, ECCV
   Zeng X., 2016, ECCV
NR 50
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 1101
EP 1110
DI 10.1109/CVPR.2018.00121
ER

PT S
AU Shen, T
   Lin, GS
   Shen, CH
   Reid, I
AF Shen, Tong
   Lin, Guosheng
   Shen, Chunhua
   Reid, Ian
GP IEEE
TI Bootstrapping the Performance of Webly Supervised Semantic Segmentation
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB Fully supervised methods for semantic segmentation require pixel-level class masks to train, the creation of which is expensive in terms of manual labour and time. In this work, we focus on weak supervision, developing a method for training a high-quality pixel-level classifier for semantic segmentation, using only image-level class labels as the provided ground-truth. Our method is formulated as a two-stage approach in which we first aim to create accurate pixel-level masks for the training images via a bootstrapping process, and then use these now-accurately segmented images as a proxy ground-truth in a more standard supervised setting. The key driver for our work is that in the target dataset we typically have reliable ground-truth image-level labels, while data crawled from the web may have unreliable labels, but can be filtered to comprise only easy images to segment, therefore having reliable boundaries. These two forms of information are complementary and we use this observation to build a novel bi-directional transfer learning framework. This framework transfers knowledge between two domains, target domain and web domain, bootstrapping the performance of weakly supervised semantic segmentation. Conducting experiments on the popular benchmark dataset PASCAL VOC 2012 based on both a VGG16 network and on ResNet50, we reach state-of-the-art performance with scores of 60.2% IoU and 63.9% IoU respectively(1).
CR Bearman A., 2016, ECCV
   Chen L.-C., 2015, INT C LEARN REPR, V1, P6
   Chen T., 2015, NIPS
   Chen X, 2013, APPL MECH MATER, V311, P3, DOI 10.4028/www.scientific.net/AMM.311.3
   Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168
   Dai J., 2015, ICCV
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Hariharan B., 2011, ICCV
   He K., 2016, CVPR
   Hong S., 2017, CVPR
   Jin B., 2017, CVPR
   Khoreva A., 2017, CVPR
   Kolesnikov A., 2016, ECCV
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
   Krizsan A, 2012, GENDER POLIT, P1
   Lin D., 2016, CVPR
   Lin G., 2017, P IEEE C COMP VIS PA, V1, P3
   Lin G., 2016, CVPR
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Noh H., 2015, ICCV, V1
   Oh S. Joon, 2017, CVPR
   Papandreou G., 2015, ICCV
   Pathak  Deepak, 2015, ICCV
   Pinheiro P. O., 2015, CVPR
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shen T., 2017, IJCAI
   Shen T., 2017, BMVC
   Simonyan  K., 2015, ICLR
   Wei Y., 2017, TPAMI
   Wei Y., 2017, CVPR
   Xiao T., 2015, CVPR
   Zagoruyko S., 2016, BMVC
   Zhao Haiyu, 2017, CVPR
   Zhou B, 2016, CVPR
NR 34
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 1363
EP 1371
DI 10.1109/CVPR.2018.00148
ER

PT S
AU Pal, A
   Balasubramanian, VN
AF Pal, Arghya
   Balasubramanian, Vineeth N.
GP IEEE
TI Adversarial Data Programming: Using GANs to Relax the Bottleneck of
   Curated Labeled Data
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
ID REPRESENTATIONS; RECOGNITION
AB Paucity of large curated hand-labeled training data forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label, given a set of weak labeling functions. We validated our method on the MNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many state-of-the-art models. We conducted extensive experiments to study its usefulness, as well as showed how the proposed ADP framework can be used for transfer learning as well as multi-task learning, where data from two domains are generated simultaneously using the framework along with the label information. Our future work will involve understanding the theoretical implications of this new framework from a game-theoretic perspective, as well as explore the performance of the method on more complex datasets.
CR Alfonseca E., 2012, ACL, P54
   Arjovsky M., 2017, ARXIV170107875
   Barnes C, 2011, COMMUN ACM, V54, P103, DOI 10.1145/2018396.2018421
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Breuleux O., UNLEARNING BETTER MI
   Bunescu R., 2007, ACL
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen  X., 2016, ADV NEURAL INFORM PR, P2172
   Chen X., 2014, ADV NEURAL INFORM PR, V27, P1709
   Ciresan D., 2010, DEEP BIG SIMPLE NEUR
   Danihelka I, 2017, ARXIV170505263
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   DeVries T., 2017, ARXIV170205538
   Doersch C., 2016, ARXIV160605908
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Gan Z., 2017, ARXIV170906548
   Gao H, 2011, IEEE INTELL SYST, V26, P10, DOI 10.1109/MIS.2011.52
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR, P2672, DOI [DOI 10.1017/CBO9781139058452, DOI 10.1001/JAMAINTERNMED.2016.8245]
   Graham B., 2014, ARXIV14126071
   Hauberg S., 2016, ARTIF INTELL, P342
   Hu Z., 2017, ARXIV170300955
   Kim T., 2017, ARXIV170305192
   Krizhevsky  A., 2009, LEARNING MULTIPLE LA
   Kumar VV, 2010, SADHANA-ACAD P ENG S, V35, P419, DOI 10.1007/s12046-010-0031-z
   Laude E., 2017, ARXIV170505020
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Chongxuan, 2017, ARXIV170302291
   Li Y., 2015, P 32 INT C MACH LEAR, P1718
   Liu M., 2016, COUPLED GENERATIVE A
   Mandal S., 2011, INT C IM INF PROC IC, P1
   Miao Zhenjiang, 1994, ISSIPNN '94. 1994 International Symposium on Speech, Image Processing and Neural Networks Proceedings (Cat. No.94TH0638-7), P460, DOI 10.1109/SIPNN.1994.344871
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Moudni H., 2013, INT J ADV COMPUTER S, P41
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2011, P5
   Odena A., 2016, ARXIV161009585
   Radford A., 2015, ARXIV151106434
   Radford A., 2015, ARXIV151106434
   Ratner A. J., 2017, ARXIV170901643
   Ratner Alexander, 2016, Adv Neural Inf Process Syst, V29, P3567
   Riedel S, 2010, LECT NOTES ARTIF INT, V6323, P148, DOI 10.1007/978-3-642-15939-8_10
   Rothacker L, 2012, INT CONF FRONT HAND, P149, DOI 10.1109/ICFHR.2012.185
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Sun C, 2017, ARXIV170702968
   Theis L., 2015, ARXIV151101844
   Varma P., 2017, ARXIV170902477
   Vondrick C., 2013, ICCV
   Xiao H., 2017, ARXIV170807747
   Yi Z., 2017, ARXIV170402510
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zhang H., 2016, THESIS
   Zhu J.-Y., 2017, ARXIV170310593
NR 52
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 1556
EP 1565
DI 10.1109/CVPR.2018.00168
ER

PT S
AU Cao, ZJ
   Long, MS
   Wang, JM
   Jordan, MI
AF Cao, Zhangjie
   Long, Mingsheng
   Wang, Jianmin
   Jordan, Michael I.
GP IEEE
TI Partial Transfer Learning with Selective Adversarial Networks
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB Adversarial learning has been successfully embedded into deep networks to learn transferable features, which reduce distribution discrepancy between the source and target domains. Existing domain adversarial networks assume fully shared label space across domains. In the presence of big data, there is strong motivation of transferring both classification and representation models from existing large-scale domains to unknown small-scale domains. This paper introduces partial transfer learning, which relaxes the shared label space assumption to that the target label space is only a subspace of the source label space. Previous methods typically match the whole source domain to the target domain, which are prone to negative transfer for the partial transfer problem. We present Selective Adversarial Network (SAN), which simultaneously circumvents negative transfer by selecting out the outlier source classes and promotes positive transfer by maximally matching the data distributions in the shared label space. Experiments demonstrate that our models exceed state-of-the-art results for partial transfer learning tasks on several benchmark datasets.
CR Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Busto P. P., 2017, P IEEE INT C COMP VI, V1, P3
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Donahue J, 2014, ICML
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Ganin Y, 2016, J MACH LEARN RES, V17
   Glorot X., 2011, ICML
   Gong B., 2012, CVPR
   Grandvalet Y., 2004, NIPS, P529
   Gretton A., 2012, NIPS
   Griffin G., 2007, TECHNICAL REPORT
   He K., 2016, IEEE C COMP VIS PATT
   Hoffman J., 2014, NIPS
   Krizhevsky  A., 2012, NIPS
   Long M., 2013, ICCV
   Long M., 2016, ADV NEURAL INFORM PR, P136
   Long M., 2015, ICML
   Mansour Y., 2009, COLT
   Ngiam  Jiquan, 2011, ICML
   Oquab M., 2013, CVPR
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Russakovsky O., 2014, IMAGENET LARGE SCALE
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saenko K., 2010, ECCV
   Simonyan  K., 2015, ICLR
   Sun B., 2016, AAAI
   Tzeng E., 2014, DEEP DOMAIN CONFUSIO
   Tzeng  E., 2017, CVPR
   Tzeng E., 2015, ICCV
   Wang X., 2014, NIPS
   Yosinski J., 2014, NIPS
   Zhang K., 2013, ICML
NR 34
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 2724
EP 2732
DI 10.1109/CVPR.2018.00288
ER

PT S
AU Kozerawski, J
   Turk, M
AF Kozerawski, Jedrzej
   Turk, Matthew
GP IEEE
TI CLEAR: Cumulative LEARning for One-Shot One-Class Image Recognition
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB This work addresses the novel problem of one-shot one class classification. The goal is to estimate a classification decision boundary for a novel class based on a single image example. Our method exploits transfer learning to model the transformation from a representation of the input, extracted by a Convolutional Neural Network, to a classification decision boundary. We use a deep neural network to learn this transformation from a large labelled dataset of images and their associated class decision boundaries generated from ImageNet, and then apply the learned decision boundary to classify subsequent query images. We tested our approach on several benchmark datasets and significantly outperformed the baseline methods.
CR Al- Sahaf H., 2013, LECT NOTES COMPUTER, P110
   Athiwaratkun B, 2015, ARXIV150702313
   Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504
   Burgess J., 2017, ARXIV170705562
   Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946
   Fu Y., 2017, ARXIV171004837
   Griffin G., 2007, CALTECH 256 OBJECT C
   Guyon I, 2014, MACH VISION APPL, V25, P1929, DOI 10.1007/s00138-014-0596-3
   Hussein N., 2017, ARXIV170502148
   Jia Y., 2014, ARXIV14085093
   Jiang B, 2017, PROC CVPR IEEE, P550, DOI 10.1109/CVPR.2017.66
   Karessli N., 2016, ARXIV161109309
   Khan SS, 2014, KNOWL ENG REV, V29, P345, DOI 10.1017/S026988891300043X
   Koch G., SIAMESE NEURAL NETWO
   Kodirov E., 2017, ARXIV170408345
   Krishna R., 2016, ARXIV160207332
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lake  Brenden, 2011, P COGN SCI SOC, V33
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li Y., 2017, ARXIV170305002
   Lin  T.-Y., 2014, EUR C COMP VIS, P740, DOI DOI 10.1007/978-3-319-10602-1_48
   Long Y., 2017, ARXIV170501782
   Manevitz L. M., 2000, SIGIR Forum, V34, P304
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Romera-Paredes  Bernardino, 2015, P 32 INT C MACH LEAR, P2152
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scholkopf B, 2000, ADV NEUR IN, V12, P582
   Socher Richard, 2013, ADV NEURAL INFORM PR, P935
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tax D. M. J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P251
   Tax D. M. J., 2001, ONE CLASS CLASSIFICA
   Tax DMJ, 1999, PATTERN RECOGN LETT, V20, P1191, DOI 10.1016/S0167-8655(99)00087-2
   Tax DMJ, 2002, J MACH LEARN RES, V2, P155, DOI 10.1162/15324430260185583
   Thrun S., 2012, LEARNING LEARN
   Vinyals O., 2016, ADV NEURAL INFORM PR, P3630
   Wah C., 2011, CNSTR2011001 CALTECH
   Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37
   Wu D., 2012, IEEE COMP SOC C COMP, P7, DOI [10. 1109/CVPRW. 2012. 6239179, DOI 10.1109/CVPRW.2012.6239179]
NR 40
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 3446
EP 3455
DI 10.1109/CVPR.2018.00363
ER

PT S
AU Zamir, AR
   Sax, A
   Shen, W
   Guibas, L
   Malik, J
   Savarese, S
AF Zamir, Amir R.
   Sax, Alexander
   Shen, William
   Guibas, Leonidas
   Malik, Jitendra
   Savarese, Silvio
GP IEEE
TI Taskonomy: Disentangling Task Transfer Learning
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
ID NEURAL-NETWORKS
AB Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity.
   We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. We provide a set of tools for computing and probing this taxonomical structure including a solver users can employ to find supervision policies for their use cases.
CR Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13
   Andrychowicz M., 2016, ADV NEURAL INFORM PR, P3981
   Armeni I., 2017, ARXIV170201105
   Arora S., 2014, P 31 INT C MACH LEAR, P584
   Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Berkhin P., 2006, GROUP MULTIDIMENS DA, V25, P71
   Bienenstock E, 1997, ADV NEUR IN, V9, P838
   Bilen H., 2016, ADV NEURAL INFORM PR, P235
   Bingel J., 2017, ARXIV170208303
   Boiman O., 2007, ADV NEURAL INFORM PR, P177
   Chang A., 2017, ARXIV170906158
   Chen Z., 2016, LIFELONG MACHINE LEA
   CPLEX I. I., 2009, INT BUSINESS MACHINE, V46, P157
   Doersch C., 2017, ARXIV170807860
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Donahue  J., 2014, P 31 INT C MACH LEAR, P647
   Donahue Jeff, 2016, ARXIV160509782
   Duan Y., 2016, ARXIV161102779
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Faktor A, 2012, LECT NOTES COMPUT SC, V7578, P474, DOI 10.1007/978-3-642-33786-4_35
   Fei-Fei L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1134, DOI 10.1109/ICCV.2003.1238476
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Finn C., 2016, ABS161200429 CORR
   Finn C., 2017, ABS170904905 CORR
   Finn  C., 2017, ARXIV170303400
   Finn C., 2016, ABS160300448 CORR
   Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173
   Fodor Imola K, 2002, TECHNICAL REPORT
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Ge R., 2013, THESIS
   Geman S, 2002, Q APPL MATH, V60, P707, DOI 10.1090/qam/1939008
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Gopnik A, 2004, PSYCHOL REV, V111, P3, DOI 10.1037/0033-295X.111.1.3
   Gopnik A., 1999, SCI CRIB MINDS BRAIN
   Graves A., 2014, ARXIV14105401
   Henry K., 2008, THESIS
   Hinton  G., 2015, ARXIV150302531
   Hoffman J, 2014, PROC CVPR IEEE, P867, DOI 10.1109/CVPR.2014.116
   Hoshen Y., 2015, ABS150602264 CORR
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   I. Gurobi Optimization, 2016, GUROBI OPTIMIZER REF
   Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Kingma D.P., 2013, ARXIV13126114
   Kokkinos I., 2016, ARXIV160902132
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li Y., 2016, ARXIV161107709
   Lin  T.-Y., 2014, EUR C COMP VIS, P740, DOI DOI 10.1007/978-3-319-10602-1_48
   Liu F., 2015, ABS150308263 CORR
   Luo Z., LABEL EFFICIENT LEAR
   Malik J, 2016, PATTERN RECOGN LETT, V72, P4, DOI 10.1016/j.patrec.2016.01.019
   Masuda N, 2017, PHYS REP, V716, P1, DOI 10.1016/j.physrep.2017.07.007
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI DOI 10.1016/S0079-7421(08)60536-8
   Mihalkova L., 2007, P NAT C ART INT, V1, P608
   Mikolov  Tomas, 2013, ABS13094168 CORR
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Niculescu-Mizil A., 2007, J MACH LEARN RES WOR, P339
   Noroozi M., 2017, ARXIV170806734
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Norouzi  M., 2013, ARXIV13125650
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Pentina Anastasia, 2017, STAT, V1050, P1
   Piaget J., 1952, ORIGINS INTELLIGENCE, V8
   Pratt L. Y., 1993, ADV NEURAL INFORM PR, P204
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Richter S. R., 2017, INT C COMP VIS ICCV
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   SAATY RW, 1987, MATH MODELLING, V9, P161, DOI 10.1016/0270-0255(87)90473-8
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Salakhutdinov R., 2012, P ICML WORKSH UNS TR, P195
   Schulman J., 2015, ABS150205477 CORR
   Silver D. L., 2013, AAAI SPRING SERIES
   Silver DL, 2008, MACH LEARN, V73, P215, DOI 10.1007/s10994-008-5087-1
   Socher Richard, 2013, ADV NEURAL INFORM PR, P935
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C., 2013, ABS13126199 CORR
   Tenenbaum J. B., 2001, BEHAV BRAIN SCI, V24
   Tenenbaum JB, 2006, TRENDS COGN SCI, V10, P309, DOI 10.1016/j.tics.2006.05.009
   Tenenbaum JB, 2011, SCIENCE, V331, P1279, DOI 10.1126/science.1192788
   Tervo DGR, 2016, CURR OPIN NEUROBIOL, V37, P99, DOI 10.1016/j.conb.2016.01.014
   Tessler C., 2017, AAAI, P1553
   Thrun S., 2012, LEARNING LEARN
   Turing A., 1950, MIND, V59, P433, DOI DOI 10.1093/MIND/LIX.236.433
   Wang X., 2017, ARXIV170802901
   Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
   Winograd T., 1991, THINKING MACHINES CA, V200
   Yang J., 2007, 7 IEEE INT C DAT MIN, P69
   Zamir A. R., 2018, 2018 IEEE C COMP VIS
   Zamir AR, 2016, LECT NOTES COMPUT SC, V9907, P535, DOI 10.1007/978-3-319-46487-9_33
   Zhang  C., 2016, ABS161103530 CORR
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhou  B., 2014, ADV NEURAL INFORM PR, P487, DOI DOI 10.1162/153244303322533223
NR 100
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 3712
EP 3722
DI 10.1109/CVPR.2018.00391
ER

PT S
AU Cui, Y
   Song, Y
   Sun, C
   Howard, A
   Belongie, S
AF Cui, Yin
   Song, Yang
   Sun, Chen
   Howard, Andrew
   Belongie, Serge
GP IEEE
TI Large Scale Fine-Grained Categorization and Domain-Specific Transfer
   Learning
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via fine-tuning offers an effective solution for domain-specific fine-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car make & model). In such scenarios, data annotation often calls for specialized domain knowledge and thus is difficult to scale. In this work, we first tackle a problem in large scale FGVC. Our method won first place in iNaturalist 2017 large scale species classification challenge. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via fine-tuning from large scale datasets to small scale, domain specific FGVC datasets. We propose a measure to estimate domain similarity via Earth Mover's Distance and demonstrate that transfer learning benefits from pre-training on a source domain that is similar to the target domain by this measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on multiple commonly used FGVC datasets.
CR Abadi M, 2016, OSDI
   Azizpour H., 2016, PAMI
   Bao J., 2017, ICCV
   Bossard L., 2014, ECCV
   Branson S., 2014, BMVC
   Branson S., 2010, ECCV
   Cai S., 2017, ICCV
   Cui Y., 2016, CVPR
   Cui Y., 2017, CVPR
   Deng J., 2009, CVPR
   Deng J., 2016, PAMI
   Donahue J, 2014, ICML
   Everingham M., 2010, IJCV
   Fu J., 2017, CVPR
   Gao Y., 2016, CVPR
   Gebru T., 2017, ICCV
   Girshick  R., 2014, CVPR
   He  K., 2016, ECCV
   He K., 2016, CVPR
   He X., 2017, CVPR
   Hendricks L. Anne, 2016, CVPR
   Hu J., 2017, ARXIV170901507
   Huh M., 2016, NIPS WORKSH
   Ioffe  S., 2015, ICML
   Jaderberg M., 2015, NIPS
   Johns E., 2015, CVPR
   Khosla A., 2011, CVPR WORKSH
   Kong S., 2017, CVPR
   Krause J., 2013, ICCV WORKSH
   Krause J., 2016, ECCV
   Krause J., 2015, CVPR
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lin T.-Y., 2014, ECCV
   Lin T.-Y., 2017, BMVC
   Lin  Tsung-Yu, 2015, ICCV
   Mac Aodha O., 2018, CVPR
   Maji S, 2013, ARXIV13065151
   Nilsback M.-E., 2008, ICVGIP
   Oquab M., 2014, CVPR
   Rachev S. T., 1985, THEORY PROBABILITY I
   Reed S., 2016, CVPR
   Rubner Y., 2000, IJCV
   Russakovsky Olga, 2015, IJCV
   Schroff F., 2015, CVPR
   Sharif Razavian  A., 2014, CVPR WORKSH
   Simon M., 2017, ICCV
   Simonyan K., 2014, ARXIV14091556
   Sun C., 2017, ICCV
   Szegedy  C., 2016, CVPR
   Szegedy C., 2017, AAAI
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Torralba A., 2011, CVPR
   Van Horn G., 2017, ARXIV17091450
   Van Horn G., 2018, CVPR
   Van Horn G., 2015, CVPR
   Vedaldi A., 2014, CVPR
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wegner J. D., 2016, CVPR
   Xiao T., 2015, CVPR
   Xie  S., 2017, CVPR
   Xu Z., 2016, PAMI
   Yang L., 2015, CVPR
   Yosinski J., 2014, NIPS
   Yu F., 2018, CVPR
   Zhang N., 2016, ICLR WORKSH
   Zhang N., 2014, ECCV
   Zhang X., 2016, CVPR
   Zheng H., 2017, ICCV
   Zhou B., 2017, PAMI
   Zhu X., 2014, CVPR
   Zoph B., 2018, CVPR
NR 71
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 4109
EP 4118
DI 10.1109/CVPR.2018.00432
ER

PT S
AU Chen, SX
   Zhang, CJ
   Dong, M
AF Chen, Shixing
   Zhang, Caojin
   Dong, Ming
GP IEEE
TI Coupled End-to-end Transfer Learning with Generalized Fisher Information
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB In transfer learning, one seeks to transfer related information from source tasks with sufficient data to help with the learning of target task with only limited data. In this paper, we propose a novel Coupled End-to-end Transfer Learning (CETL) framework, which mainly consists of two convolutional neural networks (source and target) that connect to a shared decoder. A novel loss function, the coupled loss, is used for CETL training. From a theoretical perspective, we demonstrate the rationale of the coupled loss by establishing a learning bound for CETL. Moreover, we introduce the generalized Fisher information to improve multi-task optimization in CETL. From a practical aspect, CETL provides a unified and highly flexible solution for various learning tasks such as domain adaption and knowledge distillation. Empirical result shows the superior performance of CETL on cross-domain and cross-task image classification.
CR Al-Rfou R., 2016, ARXIV160502688
   Azizpour H, 2016, IEEE T PATTERN ANAL, V38, P1790, DOI 10.1109/TPAMI.2015.2500224
   Ben-David Shai, 2007, ADV NEURAL INFORM PR, P137, DOI DOI 10.1007/S10994-009-5152-4
   Bousmalis K., 2017, IEEE C COMP VIS PATT
   Chopra S., 2013, ICML WORKSH CHALL RE, V2
   Coates Adam, 2011, J MACHINE LEARNING R, P215
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Ganin  Y., 2015, INT C MACH LEARN, P1180
   Ge W., 2017, IEEE C COMP VIS PATT
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong B., 2013, P 30 INT C MACH LEAR, P222
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Haeusser P., 2017, IEEE C COMP VIS PATT
   Haeusser P, 2017, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2017.301
   He K., 2016, IEEE C COMP VIS PATT
   Hinton  G., 2015, ARXIV150302531
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Jia Y., 2014, ARXIV14085093
   Kirkpatrick J., 2017, P NATL ACAD SCI
   Krizhevsky A., 2009, THESIS
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kuzborskij I., 2013, IEEE C COMP VIS PATT
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Littwin E., 2016, IEEE C COMP VIS PATT
   Long M., 2015, INT C MACH LEARN, P97
   Mao J., 2017, IEEE C COMP VIS PATT
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2011, P5
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Romero A., 2014, ARXIV14126550
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Szegedy  C., 2015, IEEE C COMP VIS PATT
   Tommasi T, 2010, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2010.5540064
   Tzeng E., 2017, IEEE C COMP VIS PATT
   Xu D., 2017, IEEE C COMP VIS PATT
   Yan H, 2017, IEEE GLOB COMM CONF
   Yim J., 2017, IEEE C COMP VIS PATT
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 39
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 4329
EP 4338
DI 10.1109/CVPR.2018.00455
ER

PT S
AU Maqueda, AI
   Loquercio, A
   Gallego, G
   Garcia, N
   Scaramuzza, D
AF Maqueda, Ana I.
   Loquercio, Antonio
   Gallego, Guillermo
   Garcia, Narciso
   Scaramuzza, Davide
GP IEEE
TI Event-based Vision meets Deep Learning on Steering Prediction for
   Self-driving Cars
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle's steering angle. To make the best out of this sensor-algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset (approximate to 1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.
CR Perez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Binas Jonathan, 2017, ICML WORKSH MACH LEA
   Bojarski Mariusz, 2016, ARXIV E PRINTS
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Buehler M, 2009, SPRINGER TRAC ADV RO, V56, P1, DOI 10.1007/978-3-642-03991-1
   Gallego Guillermo, 2017, IEEE T PATTERN ANAL
   Gallego Guillermo, 2018, IEEE INT C COMP VIS
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21
   Kim J, 2017, IEEE I CONF COMP VIS, P2961, DOI 10.1109/ICCV.2017.320
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Lungu IA, 2017, IEEE INT SYMP CIRC S, P624
   Moeys Diederik Paul, 2016, INT C EV BAS COMM SI
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Orchard G, 2013, BIOMED CIRC SYST C, P298, DOI 10.1109/BioCAS.2013.6679698
   Pomerleau D.A., 1989, ADV NEURAL INFORMATI, P305
   Razavian Ali Sharif, 2014, IEEE INT C COMP VIS
   Rebecq H, 2018, INT J COMPUT VISION, V126, P1394, DOI 10.1007/s11263-017-1050-6
   Rebecq H, 2017, IEEE ROBOT AUTOM LET, V2, P593, DOI 10.1109/LRA.2016.2645143
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Walch F, 2017, IEEE I CONF COMP VIS, P627, DOI 10.1109/ICCV.2017.75
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Xu HZ, 2017, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2017.376
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 25
TC 3
Z9 3
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 5419
EP 5427
DI 10.1109/CVPR.2018.00568
ER

PT S
AU Hu, HX
   Chao, WL
   Sha, F
AF Hu, Hexiang
   Chao, Wei-Lun
   Sha, Fei
GP IEEE
TI Learning Answer Embeddings for Visual Question Answering
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB We propose a novel probabilistic model for visual question answering (Visual QA). The key idea is to infer two sets of embeddings: one for the image and the question jointly and the other for the answers. The learning objective is to learn the best parameterization of those embeddings such that the correct answer has higher likelihood among all possible answers. In contrast to several existing approaches of treating Visual QA as multi-way classification, the proposed approach takes the semantic relationships (as characterized by the embeddings) among answers into consideration, instead of viewing them as independent ordinal numbers. Thus, the learned embedded function can be used to embed unseen answers (in the training dataset). These properties make the approach particularly appealing for transfer learning for open-ended Visual QA, where the source dataset on which the model is learned has limited overlapping with the target dataset in the space of answers. We have also developed large-scale optimization techniques for applying the model to datasets with a large number of answers, where the challenge is to properly normalize the proposed probabilistic models. We validate our approach on several Visual QA datasets and investigate its utility for transferring models across datasets. The empirical results have shown that the approach performs well not only on in-domain learning but also on transfer learning.
CR Agrawal A., 2016, IJCV
   Anderson P., 2018, CVPR
   Antol S., 2015, ICCV
   Ben-Younes H., 2017, ICCV
   Chao W.-L., 2018, NAACL
   Frome A., 2013, NIPS
   Fukui A., 2016, EMNLP
   Gao H., 2015, ADV NEURAL INFORM PR, P2296, DOI DOI 10.1145/2733373.2807418
   Goyal Y., 2017, CVPR
   Gupta A. K., 2017, ARXIV170503865
   He K., 2016, CVPR
   Ilievski I., 2017, CVPR WORKSH
   Jabri A., 2016, ECCV
   Kafle K, 2017, COMPUT VIS IMAGE UND, V163, P3, DOI 10.1016/j.cviu.2017.06.005
   Kazemi V, 2017, ARXIV170403162
   Krishna R., 2017, IJCV
   Lin T.-Y., 2014, ECCV
   Lu J., 2016, ADV NEURAL INFORM PR, P289
   Malinowski M., 2014, NIPS
   Norouzi M., 2014, ICLR
   Pennington  Jeffrey, 2014, EMNLP
   Ren M., 2015, NIPS
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shih K. J., 2016, CVPR
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Wu Z., 1994, ACL
   Xu H., 2016, ECCV
   Yang Z., 2016, CVPR
   Yu Z., 2017, ICCV
   Zhou Y., 2017, ARXIV170803619
   Zhou Y., 2017, ICCV
   Zhu Y., 2016, CVPR
NR 32
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 5428
EP 5436
DI 10.1109/CVPR.2018.00569
ER

PT S
AU Lee, KH
   He, XD
   Zhang, L
   Yang, LJ
AF Lee, Kuang-Huei
   He, Xiaodong
   Zhang, Lei
   Yang, Linjun
GP IEEE
TI CleanNet: Transfer Learning for Scalable Image Classifier Training with
   Label Noise
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
ID MACHINE; SUPPORT
AB In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task. Source code and dataset will be available at kuanghuei.github.io/CleanNetProject.
CR Azadi S., 2016, ICLR
   Bossard L., 2014, ECCV
   Chen X., 2015, ICCV
   Dean T., 2013, CVPR
   Deng J., 2009, CVPR
   Fergus R, 2010, P IEEE, V98, P1453, DOI 10.1109/JPROC.2010.2048990
   Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Frome A., 2013, NIPS
   He K., 2016, CVPR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Krause J., 2016, ECCV
   Krizhevsky  A., 2012, NIPS
   Li  Wen, 2017, ARXIV170802862
   Li Y., 2017, ICCV
   Lin T.-Y., 2014, ECCV
   Liu M.-Y., 2016, NIPS
   Liu W., 2014, CVPR
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z
   Patrini G., 2017, CVPR
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Rolnick D, 2017, ARXIV170510694
   Salvador A., 2017, CVPR
   Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133
   Socher R., 2013, NIPS
   Sukhbaatar S., 2014, ARXIV14062080
   Szegedy C., 2017, AAAI
   Thongkam J, 2008, LECT NOTES COMPUT SC, V4977, P99, DOI 10.1007/978-3-540-89376-9_10
   Tsai Y.-H. H., 2017, ICCV
   Tzeng  E., 2017, CVPR
   Veit A., 2017, CVPR
   Vinyals O., 2016, NIPS
   Xia Y., 2015, ICCV
   Xiao T., 2015, CVPR
   Yang Z., 2016, NAACL HLT
   Yu F., 2015, ARXIV150603365
   Zhou B, 2017, IEEE T PATTERN ANAL
   Zhou D., 2004, NIPS
   Zhu X., 2002, CMUCALD02107
   Zhuang B., 2017, CVPR
NR 41
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 5447
EP 5456
DI 10.1109/CVPR.2018.00571
ER

PT S
AU Fajtl, J
   Argyriou, V
   Monekosso, D
   Remagnino, P
AF Fajtl, Jiri
   Argyriou, Vasileios
   Monekosso, Dorothy
   Remagnino, Paolo
GP IEEE
TI AMNet: Memorability Estimation with Attention
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB In this paper we present the design and evaluation of an end-to-end trainable, deep neural network with a visual attention mechanism for memorability estimation in still images. We analyze the suitability of transfer learning of deep models from image classification to the memorability task. Further on we study the impact of the attention mechanism on the memorability estimation and evaluate our network on the SUN Memorability and the LaMem datasets. Our network outperforms the existing state of the art models on both datasets in terms of the Spearman's rank correlation as well as the mean squared error, closely matching human consistency.
CR Bahdanau D., 2014, ARXIV14090473
   Baveye Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P491, DOI 10.1145/2964284.2967269
   Bylinskii Z, 2015, VISION RES, V116, P165, DOI 10.1016/j.visres.2015.03.005
   Celikkale B, 2013, IEEE COMPUT SOC CONF, P976, DOI 10.1109/CVPRW.2013.142
   DALAL N, 2005, PROC CVPR IEEE, P886, DOI DOI 10.1109/CVPR.2005.177
   Drucker H, 1997, ADV NEUR IN, V9, P155
   Dubey R, 2015, IEEE I CONF COMP VIS, P1089, DOI 10.1109/ICCV.2015.130
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Greff K., 2017, IEEE T NEURAL NETWOR
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Isola P, 2014, IEEE T PATTERN ANAL, V36, P1469, DOI 10.1109/TPAMI.2013.200
   Isola P, 2011, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2011.5995721
   Isola Phillip, 2011, ADV NEURAL INFORM PR, P2429
   Jing PG, 2017, IEEE T MULTIMEDIA, V19, P1050, DOI 10.1109/TMM.2016.2644866
   Joachims T., 2006, P 12 ACM SIGKDD INT, P217, DOI DOI 10.1145/1150402.1150429
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Khosla A., 2012, ADV NEURAL INFORM PR, P296
   Khosla A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P867, DOI 10.1145/2566486.2567996
   Khosla A, 2015, IEEE I CONF COMP VIS, P2390, DOI 10.1109/ICCV.2015.275
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Kriznar A, 2012, ACTA ARTIS ACADEMICA 2012: KNOWLEDGE AND EXPERIENCE IN THE FINE ART, P25
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Machaj J, 2010, PROCEEDINGS OF THE 20TH INTERNATIONAL CONFERENCE, RADIOELETRONIKA 2010, P83, DOI 10.1145/1873951.1873965
   Mancas M, 2013, IEEE IMAGE PROC, P196, DOI 10.1109/ICIP.2013.6738041
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Peng HW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1147, DOI 10.1145/2733373.2806303
   Pirie W., 1988, ENCY STAT SCI
   Ramanathan S, 2010, LECT NOTES COMPUT SC, V6314, P30, DOI 10.1007/978-3-642-15561-1_3
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saleh B, 2013, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2013.107
   Shechtman E., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383198
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xu K., 2015, INT C MACH LEARN, P2048
   Zarezadeh S, 2017, IRAN CONF ELECTR ENG, P2176, DOI 10.1109/IranianCEE.2017.7985423
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhou  B., 2014, ADV NEURAL INFORM PR, P487, DOI DOI 10.1162/153244303322533223
NR 39
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 6363
EP 6372
DI 10.1109/CVPR.2018.00666
ER

PT S
AU Lv, JM
   Chen, WH
   Li, Q
   Yang, C
AF Lv, Jianming
   Chen, Weihang
   Li, Qing
   Yang, Can
GP IEEE
TI Unsupervised Cross-dataset Person Re-identification by Transfer Learning
   of Spatial-Temporal Patterns
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB Most of the proposed person re-identification algorithms conduct supervised training and testing on single labeled datasets with small size, so directly deploying these trained models to a large-scale real-world camera network may lead to poor performance due to underfitting. It is challenging to incrementally optimize the models by using the abundant unlabeled data collected from the target domain. To address this challenge, we propose an unsupervised incremental learning algorithm, TFusion, which is aided by the transfer learning of the pedestrians' spatio-temporal patterns in the target domain. Specifically, the algorithm firstly transfers the visual classifier trained from small labeled source dataset to the unlabeled target dataset so as to learn the pedestrians' spatial-temporal patterns. Secondly, a Bayesian fusion model is proposed to combine the learned spatio-temporal patterns with visual features to achieve a significantly improved classifier. Finally, we propose a learning-to-rank based mutual promotion procedure to incrementally optimize the classifiers based on the unlabeled data in the target domain. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm gains significant improvement compared with the state-of-art cross-dataset unsupervised person re identification algorithms.
CR Ahmed E., 2015, CVPR
   Change C., 2009, COMP VIS IEEE INT C
   Chen SZ, 2016, IEEE T IMAGE PROCESS, V25, P2353, DOI 10.1109/TIP.2016.2545929
   Dapeng C., 2016, ECCV
   Evgeniya U., 2016, NIPS
   Gray D., 2008, ECCV
   Gray D., 2007, EVALUATING APPEARANC
   He K, 2015, ARXIV151203385
   Huang W., 2016, MMM
   Javed O, 2008, COMPUT VIS IMAGE UND, V109, P146, DOI 10.1016/j.cviu.2007.01.003
   Kostinger M., 2012, CVPR
   Layne R., 2013, ARTEMIS ACM MULTIMED
   Liang C., 2015, MM
   Liao S., 2015, CVPR
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Ma AJ, 2015, IEEE T IMAGE PROCESS, V24, P1599, DOI 10.1109/TIP.2015.2395715
   Ma LY, 2014, IEEE T IMAGE PROCESS, V23, P3656, DOI 10.1109/TIP.2014.2331755
   Ma XL, 2017, PATTERN RECOGN, V65, P197, DOI 10.1016/j.patcog.2016.11.018
   Martinel N, 2017, IEEE T CYBERNETICS, V47, P3530, DOI 10.1109/TCYB.2016.2568264
   Paisitkriangkrai S., 2015, CVPR
   Peixi P., 2016, CVPR
   Rahul V., 2016, ECCV
   Song B., 2017, CVPR
   Tao DP, 2016, IEEE T IMAGE PROCESS, V25, P2726, DOI 10.1109/TIP.2016.2553446
   Tetsu M., 2016, CVPR
   Wang H., 2014, BMVC
   Wang H., 2016, ICIP
   Wei L., 2013, COMP VIS IEEE INT C
   Wei L., 2017, CVPR
   Wu L, 2017, PATTERN RECOGN, V65, P238, DOI 10.1016/j.patcog.2016.12.022
   Yang Y., 2014, ECCV
   Yifan S., 2017, ICCV
   Yingcong C., 2017, CVPR
   Zhao R., 2014, CVPR
   Zhao  R., 2013, CVPR
   Zheng L., 2015, COMP VIS IEEE INT C
   Zheng Z., 2017, TOMM
NR 37
TC 2
Z9 2
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 7948
EP 7956
DI 10.1109/CVPR.2018.00829
ER

PT S
AU Rebuffi, SA
   Bilen, H
   Vedaldi, A
AF Rebuffi, Sylvestre-Alvise
   Bilen, Hakan
   Vedaldi, Andrea
GP IEEE
TI Efficient parametrization of multi-domain deep neural networks
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB A practical limitation of deep neural networks is their high degree of specialization to a single task and visual domain. Recently, inspired by the successes of transfer learning, several authors have proposed to learn instead universal feature extractors that, used as the first stage of any deep network, work well for several tasks and domains simultaneously. Nevertheless, such universal features are still somewhat inferior to specialized networks.
   To overcome this limitation, in this paper we propose to consider instead universal parametric families of neural networks, which still contain specialized problem-specific models, but differing only by a small number of parameters. We study different designs for such parametrization, including series and parallel residual adapters, joint adapter compression, and parameter allocations, and empirically identify the ones that yield the highest compression. We show that, in order to maximize performance, it is necessary to adapt both shallow and deep layers of a deep network, but the required changes are very small. We also show that these universal parametrization are very effective for transfer learning, where they outperform traditional fine-tuning techniques.
CR Bertinetto  L., 2016, ADV NEURAL INFORM PR, P523
   Bilen H., 2016, P NIPS
   Bilen H., 2017, ARXIV170107275
   Bousmalis K., 2016, ADV NEURAL INFORM PR, P343
   Caruana R., 1997, MACHINE LEARNING, V28
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [DOI 10.1145/1390156.1390177, 10.1145/1390156.1390177]
   Dahl GE, 2014, ARXIV14061231
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Ganin Y., 2015, P ICML
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He  Kaiming, 2017, ARXIV170306870
   Huang JT, 2013, INT CONF ACOUST SPEE, P7304, DOI 10.1109/ICASSP.2013.6639081
   Kirkpatrick J., 2017, OVERCOMING CATASTROP
   Kokkinos I., 2017, P CVPR
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Li D., 2017, ARXIV171003463
   LI ZZ, 2016, P ECCV, V9908, P614, DOI DOI 10.1007/978-3-319-46493-0_37
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Meyerson E., 2017, ARXIV171100108
   Mitchell T., 2010, NEVER ENDING LEARNIN
   Rannen A., 2017, CVPR, P1320
   Rebuffi S., 2017, P NIPS
   Rebuffi S. A., 2017, P CVPR
   Rosenfeld A., 2017, ARXIV170504228
   Rusu A.A., 2016, ARXIV160604671
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Terekhov AV, 2015, LECT NOTES ARTIF INT, V9222, P268, DOI 10.1007/978-3-319-22979-9_27
   Thrun S, 1998, LEARNING TO LEARN, P181
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Yang Y., 2016, ICLR
   Zagoruyko S., 2016, ARXIV160507146
   Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z
   Zhang Z., 2014, P ECCV
NR 35
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 8119
EP 8127
DI 10.1109/CVPR.2018.00847
ER

PT S
AU Liu, B
   Wang, XD
   Dixit, M
   Kwitt, R
   Vasconcelos, N
AF Liu, Bo
   Wang, Xudong
   Dixit, Mandar
   Kwitt, Roland
   Vasconcelos, Nuno
GP IEEE
TI Feature Space Transfer for Data Augmentation
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
ID DIMENSIONALITY REDUCTION
AB The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder/ decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category-and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one/few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.
CR Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Deng J., 2009, CVPR
   Dixit M., 2017, CVPR
   Fei-Fei L., 2015, CVPR
   Finn C., 2017, CORR
   Gatys L. A., 2016, CVPR
   Girshick Ross, 2015, ICCV
   Goodfellow I., 2014, NIPS
   Hariharan B., 2016, CORR
   He K., 2016, CVPR
   Isola P., 2017, CVPR
   Kalogerakis E., 2016, CORR
   Knopp J., 2010, ECCV
   Krizhevsky  A., 2012, NIPS
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   LeCun Y, 2004, CVPR
   Nene S. A., 1996, CUCS00696
   Pathak D., 2016, CVPR
   Peng X., 2015, ICCV
   Qi C. R., 2016, CORR
   Ravi S., 2017, ICLR
   Ren  Shaoqing, 2015, NIPS
   Romera-Paredes  Bernardino, 2015, ICML
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Santoro A, 2016, ICML
   Simonyan K, 2015, P INT C LEARN REPR, P1, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Socher R., 2013, NIPS
   Song S., 2015, CVPR
   Su H., 2015, ICCV
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang K., 2010, CVPR
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinjays O., 2015, CVPR
   Vinyals O., 2016, NIPS
   Wu Z., 2015, CVPR
NR 35
TC 2
Z9 2
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 9090
EP 9098
DI 10.1109/CVPR.2018.00947
ER

PT S
AU Caicedo, JC
   McQuin, C
   Goodman, A
   Singh, S
   Carpenter, AE
AF Caicedo, Juan C.
   McQuin, Claire
   Goodman, Allen
   Singh, Shantanu
   Carpenter, Anne E.
GP IEEE
TI Weakly Supervised Learning of Single-Cell Feature Embeddings
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
ID CANCER
AB We study the problem of learning representations for single cells in microscopy images to discover biological relationships between their experimental conditions. Many new applications in drug discovery and functional genomics require capturing the morphology of individual cells as comprehensively as possible. Deep convolutional neural networks (CNNs) can learn powerful visual representations, but require ground truth for training; this is rarely available in biomedical profiling experiments. While we do not know which experimental treatments produce cells that look alike, we do know that cells exposed to the same experimental treatment should generally look similar. Thus, we explore training CNNs using a weakly supervised approach that uses this information for feature learning. In addition, the training stage is regularized to control for unwanted variations using mixup or RNNs. We conduct experiments on two different datasets; the proposed approach yields single-cell embeddings that are more accurate than the widely adopted classical features, and are competitive with previously proposed transfer learning approaches.
CR Bray MA, 2016, NAT PROTOC, V11, P1757, DOI 10.1038/nprot.2016.105
   Caicedo JC, 2017, NAT METHODS, V14, P849, DOI [10.1038/NMETH.4397, 10.1038/nmeth.4397]
   Caicedo JC, 2016, CURR OPIN BIOTECH, V39, P134, DOI 10.1016/j.copbio.2016.04.003
   Caie PD, 2010, MOL CANCER THER, V9, P1913, DOI 10.1158/1535-7163.MCT-09-1148
   Carpenter AE, 2006, GENOME BIOL, V7, DOI 10.1186/gb-2006-7-10-r100
   Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168
   Chung J., 2014, EMPIRICAL EVALUATION
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Godinez WJ, 2017, BIOINFORMATICS, V33, P2010, DOI 10.1093/bioinformatics/btx069
   Goldsborough P., 2017, BIORXIV
   Goodfellow  I., 2014, GENERATIVE ADVERSARI
   Gough A, 2017, SLAS DISCOV, V22, P213, DOI 10.1177/2472555216682725
   Gross S., 2017, ARXIV170406363
   He K., 2015, DEEP RESIDUAL LEARNI
   He  Kaiming, 2017, ARXIV170306870
   Huang Y., 2017, ARXIV170502596
   Joulin A, 2016, LECT NOTES COMPUT SC, V9911, P67, DOI 10.1007/978-3-319-46478-7_5
   Kraus OZ, 2017, MOL SYST BIOL, V13, DOI 10.15252/msb.20177551
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kudlur M, 2015, ARXIV151106391
   Lin  T.-Y., 2014, EUR C COMP VIS, P740, DOI DOI 10.1007/978-3-319-10602-1_48
   Ljosa V, 2013, J BIOMOL SCREEN, V18, P1321, DOI 10.1177/1087057113503553
   McLean C., 2017, IMPROVING PHENOTYPIC
   Pawlowski N., 2016, AUTOMATING MORPHOLOG
   Pawlowski N., 2016, THESIS
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rohban MH, 2017, ELIFE, V6, DOI 10.7554/eLife.24060
   Ronneberger  O., 2015, U NET CONVOLUTIONAL
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan  K., 2014, VERY DEEP CONVOLUTIO
   Singh S, 2014, J MICROSC-OXFORD, V256, P231, DOI 10.1111/jmi.12178
   Styles E. B., 2016, TRENDS CELL BIOL
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang X., 2017, ARXIV170502315
   Zhang C, 2016, ARXIV161103530
   Zhang H., 2017, MIXUP EMPIRICAL RISK
   Zhuang B., 2016, ATTEND GROUPS WEAKLY
NR 37
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 9309
EP 9318
DI 10.1109/CVPR.2018.00970
ER

PT S
AU Mundhenk, TN
   Ho, D
   Chen, BY
AF Mundhenk, T. Nathan
   Ho, Daniel
   Chen, Barry Y.
GP IEEE
TI Improvements to context based self-supervised learning
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
AB We develop a set of methods to improve on the results of self-supervised learning using context. We start with a baseline of patch based arrangement context learning and go from there. Our methods address some overt problems such as chromatic aberration as well as other potential problems such as spatial skew and mid-level feature neglect. We prevent problems with testing generalization on common self-supervised benchmark tests by using different datasets during our development. The results of our methods combined yield top scores on all standard self-supervised benchmarks, including classification and detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and "linear tests" on the ImageNet and CSAIL Places datasets. We obtain an improvement over our baseline method of between 4.0 to 7.1 percentage points on transfer learning classification tests. We also show results on different standard network architectures to demonstrate generalization as well as portability. All data, models and programs are available at: https://gdo-datasci.llnl.gov/selfsupervised/.
CR Agrawal P., 2015, ICCV
   [Anonymous], 2009, CORNELL LAB ORNITHOL, V23
   Bojanowski P., 2017, ICML
   Campr P., IMAGENET 21K INCEPTI
   Ciresan D., 2012, CORR
   Ciresan D. C., 2011, CORR
   Deng J., 2009, CVPR
   Dodge A., IAIN MTEFFECT WIKIME
   Doersch C., 2017, ICCV
   Doersch Carl, 2015, ICCV
   Donahue J., 2017, ICLR
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Girshick Ross, 2015, ICCV
   Gomez L., 2017, CVPR
   He K, 2015, ARXIV151203385
   Huang G., 2017, CVPR
   Ioffe  S., 2015, ICML
   Jayaraman Dinesh, 2015, ICCV
   Jia Y., 2014, ARXIV14085093
   Kim D., 2018, WACV
   Krahenbuhl P., 2016, ICLR
   Krizhevsky A., 2013, NIPS
   Larsson G., 2017, CVPR
   Lee H.-Y., 2017, ICCV
   Li D., 2016, ECCV
   Livingstone M., 2002, 1 STAGES PROCESSING, P46
   Long  J., 2015, CVPR
   Misra Ishan, 2016, ECCV
   Mundhenk T. N., 2016, ECCV
   Noroozi M., 2016, CORR
   Noroozi M., 2016, ECCV
   Noroozi M., 2017, ICCV
   Owens A., 2016, ECCV
   Pathak D., 2017, CVPR
   Pathak D., 2016, CVPR
   Simard P., 2003, P 7 INT C DOC AN REC
   Simonyan K., 2014, CORR
   Szegedy C, 2016, ARXIV160207261
   Szegedy C., 2015, CVPR
   THOMPSON P, 1980, PERCEPTION, V9, P483, DOI 10.1068/p090483
   Umbert, KOLORA ABERACIO WIKI
   Wang X., 2015, ICCV
   Wang X., 2017, ICCV
   Welinder P., 2010, CNSTR2010001 CAL I T
   Yang L., 2015, CVPR
   Zhang R., 2017, CVPR
   Zhang R., 2016, ECCV
   Zhou B., 2014, NIPS
NR 49
TC 0
Z9 0
SN 1063-6919
BN 978-1-5386-6420-9
PY 2018
BP 9339
EP 9348
DI 10.1109/CVPR.2018.00973
ER

PT B
AU Yao, YB
   Cai, YG
   Wei, W
   Shuai, H
AF Yao Yeboah
   Cai Yanguang
   Wei Wu
   Shuai He
GP ACM
TI Autonomous Indoor Robot Navigation via Siamese Deep Convolutional Neural
   Network
SO 2018 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND PATTERN
   RECOGNITION (AIPR 2018)
CT International Conference on Artificial Intelligence and Pattern
   Recognition (AIPR)
CY AUG 18-20, 2018
CL Beijing, PEOPLES R CHINA
DE Deep Convolutional Neural Networks (DCNN); Indoor navigation; Semantic
   segmentation; Siamese network
ID LOCALIZATION
AB The vast majority of indoor navigation algorithms either rely on manual scene augmentation and labelling or exploit multi-sensor fusion techniques in achieving simultaneous localization and mapping (SLAM), leading to high computational costs, hardware complexities and robustness deficiencies. This paper proposes an efficient and robust deep learning-based indoor navigation framework for robots. Firstly, we put forward an end-to-end trainable siamese deep convolutional neural network (DCNN) which decomposes navigation into orientation and localization in one branch, while achieving semantic scene mapping in another. In mitigating the computational costs associated with DCNNs, the proposed model design shares a significant amount of convolutional operations between the two branches, streamlining the model and optimizing for efficiency in terms of memory and inference latency. Secondly, a transfer learning regime is explored in demonstrating how such siamese DCNNs can be efficiently trained for high convergence rates without extensive manual dataset labelling. The resulting siamese framework combines semantic scene understanding with orientation estimation towards predicting collision-free and optimal navigation paths. Experimental results demonstrate that the proposed framework achieves accurate and efficient navigation and outperforms existing "navigation-by-classification" variants.
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bostelman RV, 2005, 2005 12th International Conference on Advanced Robotics, P460
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Chang CK, 2013, IEEE INT C INT ROBOT, P2079, DOI 10.1109/IROS.2013.6696642
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Correa D. S. O., 2012, 2012 Second Brazilian Conference on Critical Embedded Systems (CBSEC 2012), P36, DOI 10.1109/CBSEC.2012.18
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hou J, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON BIG DATA ANALYSIS (ICBDA), P725, DOI 10.1109/ICBDA.2017.8078731
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Liu W., 2015, CORR
   Rao D. J., 2017, CORR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, CORR
   Solak S, 2015, 2015 9TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONICS ENGINEERING (ELECO), P685, DOI 10.1109/ELECO.2015.7394442
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Talib O., 2014, P INF TECHN BAS HIGH, P1
   Wei H, 2018, IEEE T IMAGE PROCESS, V27, P3164, DOI 10.1109/TIP.2018.2818931
   Xi WN, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (IEEE ICIA 2017), P1012, DOI 10.1109/ICInfA.2017.8079050
   Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446
   Yuan W, 2016, IEEE ICARM 2016 - 2016 INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS AND MECHATRONICS (ICARM), P82, DOI 10.1109/ICARM.2016.7606899
   Zheng WX, 2017, C IND ELECT APPL, P924, DOI 10.1109/ICIEA.2017.8282971
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou YM, 2014, IEEE ROMAN, P147, DOI 10.1109/ROMAN.2014.6926245
NR 27
TC 0
Z9 0
BN 978-1-4503-6524-6
PY 2018
BP 113
EP 119
DI 10.1145/3268866.3268886
ER

PT B
AU Diasse, A
   Li, ZY
AF Diasse, Abdoullahi
   Li, Zhiyong
GP ACM
TI Big Cities transfer learning: An unsupervised multi-view cross-domain
   classification with misses
SO PROCEEDINGS OF 2018 10TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING
   AND COMPUTING (ICMLC 2018)
CT 10th International Conference on Machine Learning and Computing (ICMLC)
CY FEB 26-28, 2018
CL Univ Macau, Zhuhai, PEOPLES R CHINA
HO Univ Macau
DE Big data; multi-view; transfer learning; data Insufficiency; subspace
   learning; Low rank
AB Big data has brought many new challenges for machine learning research. In many learning tasks, we have to deal with diverse data from different domains, different representations, different distributions, scale, and density in order to achieve a good performance. With the recent advances in data storage and internet technology, data become more prominent, noisier and more complex which bring new opportunities and challen ges into Transfer learning. In Urban computing when inferring knowledge for new or less developed cities we often need to deal with large-scale, multi-view, noisy and incomplete data. This calls for advanced techniques that can make practical use of massive, sparse and noisy data to efficiently transfer knowledge of multiple and diverse datasets (views) from a source domain to a target domain. Such a problem becomes much more challenging in an unsupervised learning setting where we do not dispose any label in the target domain which is not uncommon in my real-world scenarios. To tackle this challenge, in this paper we propose novel unsupervised multi-view transfer with missing data by learning a shared subspace across views from different domains through a latent low-rank transfer. Before performing knowledge transfer our approach learns an enriched representation of the source domain via a novel joint multi-view dictionary learning based on low-rank tensor. We also propose a multi-view co-classifier to predict the label in the target domain. Tailored for big data applications with EM-ADMM based optimization algorithm our method can efficiently perform knowledge transfer from a multi-view source domain to an unlabeled multi-view target domain with a high rate of missing values and noise.
CR Andersson F, 2014, IEEE T SIGNAL PROCES, V62, P5761, DOI 10.1109/TSP.2014.2358961
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Ding Z., 2014, P 28 AAAI C ART INT
   Ding ZM, 2015, IEEE T IMAGE PROCESS, V24, P4322, DOI 10.1109/TIP.2015.2462023
   Ding Zhengming, 2016, IEEE T NEURAL NETWOR
   Duda R. O., 2012, PATTERN CLASSIFICATI
   Fang Z., 2013, P 22 ACM INT C INF K, P1321
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   He J., 2011, P 28 INT C MACH LEAR, P25
   Jin X., 2014, P 23 ACM INT C INF K, P441
   Lin F, 2013, IEEE T AUTOMAT CONTR, V58, P2426, DOI 10.1109/TAC.2013.2257618
   Liu G., LATENT LOW RANK REPR
   Liu G., 2012, J MACH LEARN RES P T, P703
   liu G., 2010, CORR
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900
   Shao M, 2014, INT J COMPUT VISION, V109, P74, DOI 10.1007/s11263-014-0696-6
   Shekhar S, 2013, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.2013.53
   Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126
   Tan B., 2013, SDM
   Wei Y., 2016, KDD 16
   Yang P., 2015, KDD, P1375
   Yang P., 2013, P 23 INT JOINT C ART, P1848
   Yang P., 2012, ACL, P270
   Yu Zheng, 2015, IEEE T BIG DATA  MAY
   Zhang Changqing, 2015, ICCV
   Zhang D., 2011, KDD, P1208
   Zhang J., 2012, P 18 ACM SIGKDD INT, P543
   Zheng Yu, 2015, KDD 15
   Zhou D., 2003, NIPS
NR 30
TC 0
Z9 0
BN 978-1-4503-6353-2
PY 2018
BP 312
EP 321
DI 10.1145/3195106.3195121
ER

PT B
AU Secerovic, L
   Papic, V
AF Secerovic, Luka
   Papic, Veljko
GP IEEE
TI Detecting missing products in commercial refrigerators using
   convolutional neural networks
SO 2018 14TH SYMPOSIUM ON NEURAL NETWORKS AND APPLICATIONS (NEUREL)
CT 14th Symposium on Neural Networks and Applications (NEUREL)
CY NOV 20-21, 2018
CL Belgrade, SERBIA
DE computer vision; convolutional neural network; object detection; out of
   stock; tensorflow
AB Out of stock (OOS) is a problem all stores are facing and it reduces their profit. Standard procedures for solving OOS are mostly manual and not scalable. This paper analyzes and proposes an automated and scalable solution for solving OOS problem inside commercial refrigerators. Small, low resolution cameras are placed inside refrigerators. Images taken with those cameras are analyzed with Faster R-CNN and Single Shot Multibox (SSD) models for object detection. Models were trained using transfer learning and their performances were analyzed and compared. After object detection, K-mean clustering algorithm is used to group objects on same shelves. Distance between objects on the same shelf determines if and where the OOS problem is present.
CR Girshick Ross B., 2015, 2015 IEEE INT C COMP
   Gonzalez R. C., 2008, DIGITAL IMAGE PROCES
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Springenberg J. T., 2014, 14126806 ARXIV
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Zeiler M. D, 2013, ARXIV13112901
NR 7
TC 0
Z9 0
BN 978-1-5386-6974-7
PY 2018
ER

PT S
AU Zelinka, M
AF Zelinka, Mikulas
GP IEEE
TI Baselines for Reinforcement Learning in Text Games
SO 2018 IEEE 30TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL
   INTELLIGENCE (ICTAI)
SE Proceedings-International Conference on Tools With Artificial
   Intelligence
CT 30th IEEE International Conference on Tools with Artificial Intelligence
   (ICTAI)
CY NOV 05-07, 2018
CL Volos, GREECE
DE Text games; reinforcement learning; neural networks
AB The ability to learn optimal control policies in systems where action space is defined by sentences in natural language would allow many interesting real-world applications such as automatic optimisation of dialogue systems. Text-based games with multiple endings and rewards are a promising platform for this task, since their feedback allows us to employ reinforcement learning techniques to jointly learn text representations and control policies. We argue that the key property of AI agents, especially in the text-games context, is their ability to generalise to previously unseen games. We present a minimalistic text-game playing agent, testing its generalisation and transfer learning performance and showing its ability to play multiple games at once. We also present pyfiction, an open-source library for universal access to different text games that could, together with our agent that implements its interface, serve as a baseline for future research.
CR Bellman Richard, 2013, DYNAMIC PROGRAMMING
   Branavan S. R. K., 2014, ABS14015390 CORR
   Brockman G., 2016, ABS160601540 CORR
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Chentanez N., 2004, 18 ANN C NEUR INF PR, P1281, DOI DOI 10.1109/TAMD.2010.2051031
   Dauphin Y. N., 2015, ABS150204390 CORR
   He J., 2015, ABS151104636 CORR
   Hochreiter S., 1991, DIPLOMA TU MUNCHEN, V91
   Huang A, 2008, P 6 NZ COMP SCI RES, P49
   Mikolov T., 2013, ADV NEURAL INFORM PR, P1, DOI 10.1162/jmlr.2003.3.4-5.951
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Montfort N., 2005, TWISTY LITTLE PASSAG
   Narasimhan K., 2015, ABS150608941 CORR
   Schaul T., 2015, PRIORITIZED EXPERIEN
   Silver  D., 2017, ABS171201815 CORR
   Sutton R. S., 1998, NON TRADITIONAL REF
   TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P2
   Zelinka M., 2018, ABS180101999 CORR
NR 19
TC 0
Z9 0
SN 1082-3409
BN 978-1-5386-7449-9
PY 2018
BP 320
EP 327
DI 10.1109/ICTAI.2018.00058
ER

PT S
AU Du, YT
   Chen, Q
   Lu, HY
   Wang, CJ
AF Du, Yun-tao
   Chen, Qian
   Lu, Heng-yang
   Wang, Chong-jun
GP IEEE
TI Online Single Homogeneous Source Transfer Learning Based on AdaBoost
SO 2018 IEEE 30TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL
   INTELLIGENCE (ICTAI)
SE Proceedings-International Conference on Tools With Artificial
   Intelligence
CT 30th IEEE International Conference on Tools with Artificial Intelligence
   (ICTAI)
CY NOV 05-07, 2018
CL Volos, GREECE
DE transfer learning; online learning; online transfer learning;
   homogeneous transfer
AB Transfer learning has made great achievements in many fields and many excellent algorithms have been proposed. In recent years, many scholars have focused on a new research area called online transfer learning, which is different from general transfer learning. Online transfer learning concentrates on how to build a good classifier on the target domain when the training data arrive in an online/sequential manner. This paper focuses on online transfer learning problem based on a single source domain under homogeneous space. The existing algorithms HomOTL-I and HomOTL-II simply ensemble the classifiers on the source and target domains directly. When the distribution difference between the source domain and the target domain is large, it will not result in a good transfer effect. We are inspired by the idea of the boosting algorithm, that is we could form a strong classification model by a combination of multiple weak classifications. We train multiple classifiers on the source domain in an offfine manner using AdaBoost algorithm, combine these classifiers on source domain with the classifier trained in an online manner on the target domain to form multiple weak combination in an ensemble manner. Based on the above ideas, we propose two algorithms AB-HomOTL-I and AB-HomOTLII, which have different ways to adjust the weights. We tested our algorithms on sentiment analysis dataset and 20newsgroup dataset. The results show that our algorithms are superior to other baseline algorithms.
CR Crammer K, 2006, J MACH LEARN RES, V7, P551
   Dai  W., 2007, P 24 INT C MACH LEAR, P193, DOI DOI 10.1145/1273496.1273521
   Dalessandro B., 2014, P 20 ACM SIGKDD INT, P1573
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Ge Liang, 2013, P 22 ACM INT C INF K, P2423
   Huang J., 2006, ADV NEURAL INFORM PR, V19, P601
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Wu QC, 2018, IEEE T SYST MAN CY-S, V48, P1005, DOI 10.1109/TSMC.2017.2771227
   Xia H., 2013, IEEE T PATTERN ANAL, V36, P1
   Yan YG, 2016, LECT NOTES COMPUT SC, V9915, P467, DOI 10.1007/978-3-319-49409-8_38
   Yuan Lei, 2012, KDD, P1149
   Zhao PL, 2014, ARTIF INTELL, V216, P76, DOI 10.1016/j.artint.2014.06.003
NR 14
TC 0
Z9 0
SN 1082-3409
BN 978-1-5386-7449-9
PY 2018
BP 344
EP 349
DI 10.1109/ICTAI.2018.00061
ER

PT S
AU Chen, Q
   Du, YT
   Xu, M
   Wang, CJ
AF Chen, Qian
   Du, Yun-tao
   Xu, Ming
   Wang, Chong-jun
GP IEEE
TI HetEOTL: An Algorithm for Heterogeneous Online Transfer Learning
SO 2018 IEEE 30TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL
   INTELLIGENCE (ICTAI)
SE Proceedings-International Conference on Tools With Artificial
   Intelligence
CT 30th IEEE International Conference on Tools with Artificial Intelligence
   (ICTAI)
CY NOV 05-07, 2018
CL Volos, GREECE
DE online transfer learning; heterogeneous transfer learning; ensemble
   learning
AB Transfer learning is an important topic in machine learning and has been broadly studied for many years. However, most existing transfer learning methods assume the training sets are prepared in advance, which is often not the case in practice. Fortunately, online transfer learning (OTL), which addresses the transfer learning tasks in an online fashion, has been proposed to solve the problem. This paper mainly focuses on the heterogeneous OTL, which is in general very challenging because the feature space of target domain is different from that of the source domain. In order to enhance the learning performance, we design the algorithm called Heterogeneous Ensembled Online Transfer Learning (HetEOTL) using ensemble learning strategy. Finally, we evaluate our algorithm on some benchmark datasets, and the experimental results show that HetEOTL has better performance than some other existing online learning and transfer learning algorithms, which proves the effectiveness of HetEOTL.
CR Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Freund Yoav, 1995, COMPUT LEARN THEORY, P119
   Ge Liang, 2013, P 22 ACM INT C INF K, P2423
   Hoi Steven C. H, 2014, LIBOL LIB ONLINE LEA
   Optiz D., 1999, J ARTIFICIAL INTELLI, V11, P169, DOI DOI 10.1613/JAIR.614
   Pan WK, 2013, ARTIF INTELL, V197, P39, DOI 10.1016/j.artint.2013.01.003
   Rosenblatt F, 1988, NEUROCOMPUTING FDN R, P386
   Thrun S., 1995, P NIPS, P640
   Wang Jialei, 2013, P 7 ACM C REC SYST, P237
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wu QC, 2018, IEEE T SYST MAN CY-S, V48, P1005, DOI 10.1109/TSMC.2017.2771227
   Wu QY, 2014, IEEE INTELL SYST, V29, P26, DOI 10.1109/MIS.2013.32
   Xia H, 2014, IEEE T PATTERN ANAL, V36, P536, DOI 10.1109/TPAMI.2013.149
   Xiang EW, 2011, IJCAI P INT JOINT C, V22, P2355
   Zhao P., 2010, P INT C MACH LEARN, P1231, DOI DOI 10.1145/2505515.2505603
   Zhao PL, 2014, ARTIF INTELL, V216, P76, DOI 10.1016/j.artint.2014.06.003
NR 17
TC 0
Z9 0
SN 1082-3409
BN 978-1-5386-7449-9
PY 2018
BP 350
EP 357
DI 10.1109/ICTAI.2018.00062
ER

PT S
AU Kohli, N
   Yadav, D
   Noore, A
AF Kohli, Naman
   Yadav, Daksha
   Noore, Afzel
GP IEEE
TI Face Verification with Disguise Variations via Deep Disguise Recognizer
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
AB The performance of current automatic face recognition algorithms is hindered by different covariates such as facial aging, disguises, and pose variations. Specifically, disguises are employed for intentional or unintentional modifications in the facial appearance for hiding one's own identity or impersonating someone else's identity. In this paper, we utilize deep learning based transfer learning approach for face verification with disguise variations. We employ Residual Inception network framework with center loss for learning inherent face representations. The training for the Inception-ResNet model is performed using a large-scale face database which is followed by inductive transfer learning to mitigate the impact of facial disguises. To evaluate the performance of the proposed Deep Disguise Recognizer (DDR) framework, Disguised Faces in the Wild and IIIT-Delhi Disguise Version 1 face databases are used. Experimental evaluation reveals that for the two databases, the proposed DDR framework yields 90.36% and 66.9% face verification accuracy at the false accept rate of 10%.
CR Cao  Q., 2018, IEEE C AUT FAC GEST
   Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49
   Dhamecha TI, 2013, INT CONF BIOMETR
   Dhamecha TI, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0099212
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He K., 2015, ABS151203385 CORR
   Kim J, 2005, LECT NOTES ARTIF INT, V3533, P65
   Kohl N, 2015, IEEE ACCESS, V3, P2572, DOI 10.1109/ACCESS.2015.2505243
   Kushwaha V., 2018, DISGUISED FACES WILD
   Martinez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Nguyen D.-L., 2018, IEEE INT C BIOM
   Olivas E. S., 2009, HDB RES MACHINE LEAR
   Parkhi O.M., 2015, P BR MACH VIS, V1, P6
   Ramanathan N, 2004, IEEE IMAGE PROC, P1999
   Rui Min, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P442, DOI 10.1109/FG.2011.5771439
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Singh A., 2017, ARXIV170809317
   Singh R, 2010, IEEE T INF FOREN SEC, V5, P441, DOI 10.1109/TIFS.2010.2054083
   Singh R, 2009, IMAGE VISION COMPUT, V27, P245, DOI 10.1016/j.imavis.2007.06.010
   Sun Y., 2015, ARXIV150200873
   Szegedy C., 2017, AAAI, V4, P12
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   Zhang CL, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P584, DOI 10.1109/ASRU.2017.8268989
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
NR 28
TC 0
Z9 0
SN 2160-7508
BN 978-1-5386-6100-0
PY 2018
BP 17
EP 24
DI 10.1109/CVPRW.2018.00010
ER

PT S
AU Iglovikov, V
   Seferbekov, S
   Buslaev, A
   Shvets, A
AF Iglovikov, Vladimir
   Seferbekov, Selim
   Buslaev, Alexander
   Shvets, Alexey
GP IEEE
TI TernausNetV2: Fully convolutional network for Instance Segmentation
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
AB The most common approaches to instance segmentation are complex and use two-stage networks with object proposals, conditional random-fields, template matching or recurrent neural networks. In this work we present Ternaus-NetV2 - a simple fully convolutional network that allows extracting objects from a high-resolution satellite imagery on an instance level. The network has popular encoder-decoder type of architecture with skip connections but has a few essential modifications that allows using for semantic as well as for instance segmentation tasks. This approach is universal and allows to extend any network that has been successfully applied for semantic segmentation to perform instance segmentation task. In addition, we generalize network encoder that was pre-trained for RGB images to use additional input channels. It makes possible to use transfer learning from visual to a wider spectral range. For DeepGlobe-CVPR 2018 building detection sub-challenge, based on public leaderboard score, our approach shows superior performance in comparison to other methods.
CR Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305
   BulO S. R., 2017, ARXIV171202616
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Demir I., 2018, ARXIV180506561
   Glorot X, 2011, INT C ARTIF INTELLIG, P315, DOI DOI 10.1177/1753193410395357
   Goldberg HR, 2018, PROC SPIE, V10645, DOI 10.1117/12.2304682
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Iglovikov V., 2018, ARXIV180105746
   Iglovikov V., 2017, ARXIV170606169
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Neuhold G., 2017, P INT C COMP VIS ICC, P22
   Ronneberger O, 2015, INT C MED IM COMP CO, P234, DOI DOI 10.1007/978-3-319-24574-4_28
   San D. K., 2010, INT ARCH PHOTOGRAM 8, V38
   Zhang A., 2017, ARXIV170708952
NR 14
TC 0
Z9 0
SN 2160-7508
BN 978-1-5386-6100-0
PY 2018
BP 228
EP 232
DI 10.1109/CVPRW.2018.00042
ER

PT S
AU Li, YJ
   Yang, FE
   Liu, YC
   Yeh, YY
   Du, XF
   Wang, YCF
AF Li, Yu-Jhe
   Yang, Fu-En
   Liu, Yen-Cheng
   Yeh, Yu-Ying
   Du, Xiaofei
   Wang, Yu-Chiang Frank
GP IEEE
TI Adaptation and Re-Identification Network: An Unsupervised Deep Transfer
   Learning Approach to Person Re-Identification
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
AB Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training.
CR Bousmalis K., 2016, ADV NEURAL INFORM PR, P343
   Cheng D., 2016, P IEEE C COMP VIS PA
   Deng W., 2018, P IEEE C COMP VIS PA
   Fan  H., 2017, UNSUPERVISED PERSON
   Geng  M., 2016, ARXIV161105244
   Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hermans  A., 2017, DEFENSE TRIPLET LOSS
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin Y., 2017, IMPROVING PERSON RE
   Liu M.-Y., 2016, ADV NEURAL INFORM PR
   Liu  Ming-Yu, 2017, ADV NEURAL INFORM PR
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Si J., 2018, DUAL ATTENTION MATCH
   Sun Y., 2017, SVDNET PEDESTRIAN RE
   Sun YH, 2017, IEEE ICC
   Tzeng E., 2014, DEEP DOMAIN CONFUSIO
   Wang LL, 2016, PROCEEDINGS OF THE ASME 35TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING , 2016, VOL 9
   Yu H.-X., 2017, P IEEE INT C COMP VI
   Zhang L, 2016, IEEE IC COMP COM NET
   Zheng L., 2016, PERSON REIDENTIFICAT
   Zheng L., 2015, PROCEEDINGS OF THE I
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z., 2017, CAMERA STYLE ADAPTAT
NR 24
TC 0
Z9 0
SN 2160-7508
BN 978-1-5386-6100-0
PY 2018
BP 285
EP 291
DI 10.1109/CVPRW.2018.00054
ER

PT S
AU Huang, XY
   Cheng, XJ
   Geng, QC
   Cao, BB
   Zhou, DF
   Wang, P
   Lin, YQ
   Yang, RG
AF Huang, Xinyu
   Cheng, Xinjing
   Geng, Qichuan
   Cao, Binbin
   Zhou, Dingfu
   Wang, Peng
   Lin, Yuanqing
   Yang, Ruigang
GP IEEE
TI The ApolloScape Dataset for Autonomous Driving
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
ID OBJECT CLASSES
AB Scene parsing aims to assign a class (semantic) label for each pixel in an image. It is a comprehensive analysis of an image. Given the rise of autonomous driving, pixel-accurate environmental perception is expected to be a key enabling technical piece. However, providing a large scale dataset for the design and evaluation of scene parsing algorithms, in particular for outdoor scenes, has been difficult. The per-pixel labelling process is prohibitively expensive, limiting the scale of existing ones. In this paper, we present a large-scale open dataset, ApolloScape, that consists of RGB videos and corresponding dense 3D point clouds. Comparing with existing datasets, our dataset has the following unique properties. The first is its scale, our initial release contains over 140K images - each with its per-pixel semantic mask, up to 1M is scheduled. The second is its complexity. Captured in various traffic conditions, the number of moving objects averages from tens to over one hundred (Figure 1). And the third is the 3D attribute, each image is tagged with high-accuracy pose information at cm accuracy and the static background point cloud has mm relative accuracy. We are able to label these many images by an interactive and efficient labelling pipeline that utilizes the high-quality 3D point cloud. Moreover, our dataset also contains different lane markings based on the lane colors and styles. We expect our new dataset can deeply benefit various autonomous driving related applications that include but not limited to 2D/3D scene understanding, localization, transfer learning, and driving simulation.
CR [Anonymous], 2018, RIEGL VMX 1HA
   [Anonymous], 2018, HDL 64E
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Cordts M., 2016, P IEEE C COMP VIS PA
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Geiger  A., 2013, INT J ROBOTICS RES I
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Neuhold G., 2017, P INT C COMP VIS ICC, P22
   Qi Charles Ruizhongtai, 2017, ADV NEURAL INFORM PR, P5105
   Richter S. R., 2017, INT C COMP VIS ICCV
   ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352
   Wang PC, 2017, IEEE INT CONF COMP V, P1005, DOI 10.1109/ICCVW.2017.123
   Wu Z., 2016, CORR
   Xie J, 2016, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2016.401
NR 15
TC 0
Z9 0
SN 2160-7508
BN 978-1-5386-6100-0
PY 2018
BP 1067
EP 1073
DI 10.1109/CVPRW.2018.00141
ER

PT S
AU Levy, D
   Belfer, Y
   Osherov, E
   Bigal, E
   Scheinin, AP
   Nativ, H
   Tchernov, D
   Treibitz, T
AF Levy, Deborah
   Belfer, Yuval
   Osherov, Elad
   Bigal, Eyal
   Scheinin, Aviad P.
   Nativ, Hagai
   Tchernov, Dan
   Treibitz, Tali
GP IEEE
TI Automated Analysis of Marine Video With Limited Data
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
AB Monitoring of the marine environment requires large amounts of data, simply due to its vast size. Therefore, underwater autonomous vehicles and drones are increasingly deployed to acquire numerous photographs. However, ecological conclusions from them are lagging as the data requires expert annotation and thus realistically cannot be manually processed. This calls for developing automatic classification algorithms dedicated for this type of data. Current out-of-the-box solutions struggle to provide optimal results in these scenarios as the marine data is very different from everyday data. Images taken under water display low contrast levels and reduced visibility range thus making objects harder to localize and classify. Scale varies dramatically because of the complex 3 dimensionality of the scenes. In addition, the scarcity of labeled marine data prevents training these dedicated networks from scratch. In this work, we demonstrate how transfer learning can be utilized to achieve high quality results for both detection and classification in the marine environment. We also demonstrate tracking in videos that enables counting and measuring the organisms. We demonstrate the suggested method on two very different marine datasets, an aerial dataset and an underwater one.
CR Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   [Anonymous], 2017, AUT SHARK DET UAV
   Balk H., 2000, AQUATIC LIVING RESOU, V13
   Berman D., 2017, P BRIT MACH VIS C BM
   Bertrand A., 2000, ICES J MARINE SCI, V57
   Bewley A., 2016, P IEEE ICIP
   Casella E, 2017, CORAL REEFS, V36, P269, DOI 10.1007/s00338-016-1522-0
   Cutter G., 2015, P IEEE APPL COMP VIS
   Deng J., 2009, P IEEE CVPR
   Durban J. W., 2016, MARINE MAMMAL SCI, V32
   Fisheries N. D., 2017, DRONES DETECT SHARKS
   Fukunaga K, 2013, INTRO STAT PATTERN R
   Girshick R., 2015, P IEEE ICCV
   Girshick R., 2014, P IEEE CVPR
   Goebel ME, 2015, POLAR BIOL, V38, P619, DOI 10.1007/s00300-014-1625-4
   Goodfellow I. J., 2013, P IEEE ICML
   Harvey E., 2002, FISHERIES RES, V57
   He K, 2016, IEEE INT CONF MULTI
   Hodgson A, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079556
   Holmes J. A., 2006, ICES J MARINE SCI, V63
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   KALMAN R.E, 1960, J BASIC ENG
   Kim C, 2015, INT CONF ASIC
   Kingma Diederik, 2014, ARXIV14126980
   Kinzey Douglas, 2003, Journal of Cetacean Research and Management, V5, P159
   Kiszka J. J., 2016, MARINE ECOLOGY PROGR, V560
   Krizhevsky A., 2012, P IEEE NIPS
   Kuhn H. W., 1955, NAVAL RES LOGISTICS, V2
   Le Cun B. B., 1990, P IEEE NIPS
   Letessier TB, 2017, BIOL REV, V92, P627, DOI 10.1111/brv.12246
   Li X., 2015, P MTS IEEE OCEANS
   Lin T. Y., 2014, P ECCV
   Lin T.-Y., 2017, P IEEE CVPR
   Lin  TY, 2017, ARXIV170802002
   Liu W., 2016, P ECCV
   Marburg A., 2016, P MTS IEEE OCEANS
   Morais E. F., 2005, P IEEE BRAZ S COMP G
   Osherov E., 2017, P IEEE ICCV
   Pan S. J., 2010, IEEE T KNOWLEDGE DAT, V2
   Pavuluri VK, 2015, IEEE VEHICLE POWER
   Pepik B., 2015, GERM C PATT REC
   Redmon J., 2016, P IEEE CVPR
   Redmon J., 2017, P IEEE CVPR
   Reid D. B., 1979, IEEE T AUTOMATIC CON
   Ren S., 2015, P IEEE NIPS
   Robbins WD, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083456
   Shrivakshan G., 2012, IJCSI INT J COMPUTER, V9
   Spampinato C., 2012, VISAPP
   Spampinato C., 2008, VISAPP
   Srivastava N., 2014, IJML, V15
   Sung M, 2017, OCEANS-IEEE
   Tripathi S., 2016, ARXIV160704648
   Ventura D., 2016, ESTUARINE COASTAL SH, V171
   Wan L., 2013, P IEEE ICML
   Xie Y., 1997, PACIFIC SALMON COMMI, P11
   Zeiler M. D., 2014, P ECCV
   Zhou J., 2006, P IEEE CAN C COMP RO
   Zivkovic Z., 2004, IEEE P ICPR
NR 58
TC 0
Z9 0
SN 2160-7508
BN 978-1-5386-6100-0
PY 2018
BP 1466
EP 1474
DI 10.1109/CVPRW.2018.00187
ER

PT S
AU Singh, A
   Nigam, A
AF Singh, Avantika
   Nigam, Aditya
GP IEEE
TI Encapsulating the impact of transfer learning, domain knowledge and
   training strategies in deep-learning based architecture: A biometric
   based case study
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
AB In this paper, efforts have been made to analyze the impact of training strategies, transfer learning and domain knowledge on two biometric-based problems namely: three class oculus classification and fingerprint sensor classification. For analyzing these problems we have considered deep-learning based architecture and evaluated our results on benchmark contact-lens datasets like IIIT-D, ND, IIT-K ( our model is publicly available) and on fingerprint datasets like FVC-2002, FVC-2004, FVC-2006, IIITD-MOLF, IIT-K. In-depth feature analysis of various proposed deep-learning models has been done in order to infer that indeed training in different ways along with transfer learning and domain knowledge plays a vital role in deciding the learning ability of any network.
CR Agarwal A, 2016, INT C PATT RECOG, P3001, DOI 10.1109/ICPR.2016.7900094
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Raghavendra R, 2017, IEEE WINT CONF APPL, P1160, DOI 10.1109/WACV.2017.134
   Yadav D, 2014, IEEE T INF FOREN SEC, V9, P851, DOI 10.1109/TIFS.2014.2313025
NR 4
TC 0
Z9 0
SN 2160-7508
BN 978-1-5386-6100-0
PY 2018
BP 1947
EP 1949
DI 10.1109/CVPRW.2018.00242
ER

PT S
AU Khademi, M
   Schulte, O
AF Khademi, Mahmoud
   Schulte, Oliver
GP IEEE
TI Image Caption Generation with Hierarchical Contextual Visual Spatial
   Attention
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
AB We present a novel context-aware attention-based deep architecture for image caption generation. Our architecture employs a Bidirectional Grid LSTM, which takes visual features of an image as input and learns complex spatial patterns based on two-dimensional context, by selecting or ignoring its input. The Grid LSTM has not been applied to image caption generation task before. Another novel aspect is that we leverage a set of local region-grounded texts obtained by transfer learning. The region-grounded texts often describe the properties of the objects and their relationships in an image. To generate a global caption for the image, we integrate the spatial features from the Grid LSTM with the local region-grounded texts, using a two-layer Bidirectional LSTM. The first layer models the global scene context such as object presence. The second layer utilizes a novel dynamic spatial attention mechanism, based on another Grid LSTM, to generate the global caption word-by-word, while considering the caption context around a word in both directions. Unlike recent models that use a soft attention mechanism, our dynamic spatial attention mechanism considers the spatial context of the image regions. Experimental results on MS-COCO dataset show that our architecture outperforms the state-of-the-art.
CR Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen X., 2014, ARXIV14115654
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Denkowski M., 2014, P 9 WORKSH STAT MACH
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He K, 2015, ARXIV151203385
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Johnson  Justin, 2015, ARXIV151107571
   Kalchbrenner N., 2015, ARXIV150701526
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R., 2014, P 31 INT C MACH LEAR, P595
   Krishna R., 2016, ARXIV160207332
   Kumar  A., 2016, P INT C MACH LEARN, P1378
   Lebret R., 2015, ARXIV150203671
   Lu  J., 2017, P IEEE C COMP VIS PA, V6
   Mao J., 2015, ABS150406692 CORR
   Mao J., 2014, ARXIV14126632
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   POSNER MI, 1980, Q J EXP PSYCHOL, V32, P3, DOI 10.1080/00335558008248231
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Vinyals  Oriol, 2014, ABS14114555 CORR
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xiong C, 2016, ARXIV160301417
   Xu Kelvin, 2015, ARXIV150203044, V2, P5
   Yang  Zhilin, 2016, ADV NEURAL INFORM PR, P2361
   Yao T., 2016, ARXIV161101646
   You Q., 2016, ABS160303925
NR 29
TC 0
Z9 0
SN 2160-7508
BN 978-1-5386-6100-0
PY 2018
BP 2024
EP 2032
DI 10.1109/CVPRW.2018.00260
ER

PT S
AU Ranjan, R
   De Mello, S
   Kautz, J
AF Ranjan, Rajeev
   De Mello, Shalini
   Kautz, Jan
GP IEEE
TI Light-weight Head Pose Invariant Gaze Tracking
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
AB Unconstrained remote gaze tracking using off-the-shelf cameras is a challenging problem. Recently, promising algorithms for appearance-based gaze estimation using convolutional neural networks (CNN) have been proposed. Improving their robustness to various confounding factors including variable head pose, subject identity, illumination and image quality remain open problems. In this work, we study the effect of variable head pose on machine learning regressors trained to estimate gaze direction. We propose a novel branched CNN architecture that improves the robustness of gaze classifiers to variable head pose, without increasing computational cost. We also present various procedures to effectively train our gaze network including transfer learning from the more closely related task of object viewpoint estimation and from a large high-fidelity synthetic gaze dataset, which enable our ten times faster gaze network to achieve competitive accuracy to its current state-of-the-art direct competitor.
CR Chuang MC, 2014, IEEE COMPUT SOC CONF, P165, DOI 10.1109/CVPRW.2014.30
   Deng H, 2017, IEEE I CONF COMP VIS, P3162, DOI 10.1109/ICCV.2017.341
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton  G., 2015, ARXIV150302531
   Huang Q., 2015, ARXIV150801244
   Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LECUN Y, 1995, NEURAL NETW STAT MEC, V261, P276
   Lu F, 2014, IEEE T PATTERN ANAL, V36, P2033, DOI 10.1109/TPAMI.2014.2313123
   Mora K. A. F, 2012, P IEEE COMP SOC C CO, P25
   Mora KAF, 2014, PROC CVPR IEEE, P1773, DOI 10.1109/CVPR.2014.229
   Patney Anjul, 2016, ACM SIGGRAPH 2016 EM, P17
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shrivastava Ashish, 2017, P IEEE C COMP VIS PA, V3, P6
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Smith B. A., 2013, P 26 ANN ACM S US IN, P271, DOI DOI 10.1145/2501988.2501994
   Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308
   Sugano Y, 2014, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2014.235
   Wang K, 2017, IEEE I CONF COMP VIS, P1003, DOI 10.1109/ICCV.2017.114
   Wood E, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P131, DOI 10.1145/2857491.2857492
   Wood E, 2016, LECT NOTES COMPUT SC, V9905, P297, DOI 10.1007/978-3-319-46448-0_18
   Wood E, 2015, IEEE I CONF COMP VIS, P3756, DOI 10.1109/ICCV.2015.428
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zeng HQ, 2017, PROC INT CONF RECON
   Zhang X., 2017, IEEE T PATTERN ANAL
   Zhang XC, 2015, PROC CVPR IEEE, P4511, DOI 10.1109/CVPR.2015.7299081
NR 27
TC 0
Z9 0
SN 2160-7508
BN 978-1-5386-6100-0
PY 2018
BP 2237
EP 2245
DI 10.1109/CVPRW.2018.00290
ER

PT S
AU Mormont, R
   Geurts, P
   Maree, R
AF Mormont, Romain
   Geurts, Pierre
   Maree, Raphael
GP IEEE
TI Comparison of deep transfer learning strategies for digital pathology
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
ID CONVOLUTIONAL NEURAL-NETWORKS; CLASSIFICATION; CANCER
AB In this paper, we study deep transfer learning as a way of overcoming object recognition challenges encountered in the field of digital pathology. Through several experiments, we investigate various uses of pre-trained neural network architectures and different combination schemes with random forests for feature selection. Our experiments on eight classification datasets show that densely connected and residual networks consistently yield best performances across strategies. It also appears that network fine-tuning and using inner layers features are the best performing strategies, with the former yielding slightly superior results.
CR Antony J, 2016, INT C PATT RECOG, P1195, DOI 10.1109/ICPR.2016.7899799
   Bar Y, 2015, I S BIOMED IMAGING, P294, DOI 10.1109/ISBI.2015.7163871
   Bayramoglu N, 2016, LECT NOTES COMPUT SC, V9915, P532, DOI 10.1007/978-3-319-49409-8_46
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Chollet F., 2015, KERAS
   Ciompi F, 2015, MED IMAGE ANAL, V26, P195, DOI 10.1016/j.media.2015.08.001
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue  J., 2014, P 31 INT C MACH LEAR, P647
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Greenspan H, 2016, IEEE T MED IMAGING, V35, P1153, DOI 10.1109/TMI.2016.2553401
   Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Han ZY, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-04075-z
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Howard A. G., 2017, ARXIV170404861
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kieffer B., 2017, ARXIV171005726
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Kraus OZ, 2017, MOL SYST BIOL, V13, DOI 10.15252/msb.20177551
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Maree R, 2016, I S BIOMED IMAGING, P1033, DOI 10.1109/ISBI.2016.7493442
   Maree R., 2017, J PATHOLOGY INFORN, V8
   Maree R, 2016, BIOINFORMATICS, V32, P1395, DOI 10.1093/bioinformatics/btw013
   Maree R, 2016, PATTERN RECOGN LETT, V74, P17, DOI 10.1016/j.patrec.2016.01.006
   McCann M., 2014, IEEE SIGNAL PROCESSI
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Ravishankar H, 2016, LECT NOTES COMPUT SC, V10008, P188, DOI 10.1007/978-3-319-46976-8_20
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176.ARXIV:1312.6229
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Szegedy C., 2017, AAAI, V4, P12
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   van Ginneken B, 2015, I S BIOMED IMAGING, P286, DOI 10.1109/ISBI.2015.7163869
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
NR 41
TC 0
Z9 0
SN 2160-7508
BN 978-1-5386-6100-0
PY 2018
BP 2343
EP 2352
DI 10.1109/CVPRW.2018.00303
ER

PT B
AU Bonini, RC
   Da Silva, FL
   Glatt, R
   Spina, E
   Costa, AHR
AF Bonini, Rodrigo Cesar
   Da Silva, Felipe Leno
   Glatt, Ruben
   Spina, Edison
   Reali Costa, Anna Helena
GP IEEE
TI A Framework to Discover and Reuse Object-Oriented Options in
   Reinforcement Learning
SO 2018 7TH BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS)
CT 7th Brazilian Conference on Intelligent Systems (BRACIS)
CY OCT 22-25, 2018
CL IBM Res, Sao Paulo, BRAZIL
HO IBM Res
DE reinforcement learning; transfer learning; object-oriented options
AB Reinforcement Learning is a successful yet slow technique to train autonomous agents. Option-based solutions can be used to accelerate learning and to transfer learned behaviors across tasks by encapsulating a partial policy. However, commonly these options are specific for a single task, do not take in account similar features between tasks and may not correspond exactly to an optimal behavior when transferred to another task. Therefore, unprincipled transfer might provide bad options to the agent, hampering the learning process. We here propose a way to discover and reuse learned object-oriented options in a probabilistic way in order to enable better actuation choices to the agent in multiple different tasks. Our experimental evaluation show that our proposal is able to learn and successfully reuse options across different tasks.
CR Bacon Pierre- Luc, 2017, P AAAI, P1726
   Barto A., 2009, ADV NEURAL INFORM PR, V22, P1015
   Bernstein D. S., 1999, TECH REP
   Bonini R. C., 2017, AAAI WORKSH HUM MACH, P1
   Brunskill E., 2014, P 31 INT C MACH LEAR, P316
   Butz M. V., 2004, URBANA, V51, P61801
   Diuk C., 2008, P 25 INT C MACH LEAR, P240
   Fernandez F., 2006, P 5 INT JOINT C AUT, P720
   Glatt R., 2017, WORKSH SCAL UP REINF, P1
   Glatt R, 2016, PROCEEDINGS OF 2016 5TH BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS 2016), P91, DOI [10.1109/BRACIS.2016.17, 10.1109/BRACIS.2016.027]
   Koga ML, 2015, IEEE T CYBERNETICS, V45, P77, DOI 10.1109/TCYB.2014.2319733
   Macglashan J., 2013, THESIS
   Madden MG, 2004, ARTIF INTELL REV, V21, P375, DOI 10.1023/B:AIRE.0000036264.95672.64
   McGovern A., 1997, GRACE HOPPER CELEBRA, V1317
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ng AY, 2006, SPRINGER TRAC ADV RO, V21, P363
   Pickett M., 2002, P 19 INT C MACH LEAR, V2, P506
   Roijers D. M., 2014, SURVEY MULTIOBJECTIV
   Silva F. L., 2018, IJCAI
   Silva F. L., 2017, AAMAS WORKSH TRANSF
   Subramanian K., 2011, IJCAI
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   TESAURO G, 1994, NEURAL COMPUT, V6, P215, DOI 10.1162/neco.1994.6.2.215
   Topin N, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3856
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
NR 27
TC 0
Z9 0
BN 978-1-5386-8023-0
PY 2018
BP 109
EP 114
DI 10.1109/BRACIS.2018.00027
ER

PT B
AU Bispo, A
   Prudencio, R
   Veras, D
AF Bispo, Alysson
   Prudencio, Ricardo
   Veras, Douglas
GP IEEE
TI Instance Selection and Class Balancing Techniques for Cross Project
   Defect Prediction
SO 2018 7TH BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS)
CT 7th Brazilian Conference on Intelligent Systems (BRACIS)
CY OCT 22-25, 2018
CL IBM Res, Sao Paulo, BRAZIL
HO IBM Res
DE Software defect prediction; Cross-project; Transfer learning
ID METRICS
AB Various software metrics and statistical models have been developed to help companies to predict software defects. Traditional software defect prediction approaches use historical data about previous bugs on a project in order to build predictive machine learning models. However, in many cases the historical testing data available in a project is scarce, i.e., very few or even no labeled training instances are available, which will result on a low quality defect prediction model. In order to overcome this limitation, Cross-Project Defect Prediction (CPDP) can be adopted to learn a defect prediction model for a project of interest (i.e., a target project) by reusing (transferring) data collected from several previous projects (i.e., source projects). In this paper, we focused on neighborhood-based instance selection techniques for CPDP which select labeled instances in the source projects that are similar to the unlabeled instances available in the target project. Despite its simplicity, these techniques have limitations which were addressed in our work. First, although they can select representative source instances, the quality of the selected instances is usually not addressed. Additionally, bug prediction datasets are normally unbalanced (i.e., there are more nondefect instances than defect ones), which can harm learning performance. In this paper, we proposed a new transfer learning approach for CPDP, in which instances selected by a neighborhood-based technique are filtered by the FuzzyRough Instance Selection (FRIS) technique in order to remove noisy instances in the training set. Following, in order to solve class balancing problems, the Synthetic Minority Oversampling Technique (SMOTE) technique is adopted to oversample the minority (defect-prone) class, thus increasing the chance of finding bugs correctly. Experiments were performed on a benchmark set of Java projects, achieving promising results.
CR Aggarwal CC, 2001, LECT NOTES COMPUT SC, V1973, P420
   Canfora G, 2013, 2013 IEEE SIXTH INTERNATIONAL CONFERENCE ON SOFTWARE TESTING, VERIFICATION AND VALIDATION (ICST 2013), P252, DOI 10.1109/ICST.2013.38
   Catal C, 2011, EXPERT SYST APPL, V38, P4626, DOI 10.1016/j.eswa.2010.10.024
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   CHIDAMBER SR, 1994, IEEE T SOFTWARE ENG, V20, P476, DOI 10.1109/32.295895
   Hall T, 2012, IEEE T SOFTWARE ENG, V38, P1276, DOI 10.1109/TSE.2011.103
   Jensen R., 2010, P IEEE INT C FUZZY S, P1
   Jureczko M., 6 INT C PRED MOD SOF
   Jureczko M., 2010, MODELS METHODS SYSTE, P69
   Kabacoff R. I., 2010, R IN ACTION
   Kim S, 2011, 2011 33RD INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE), P481, DOI 10.1145/1985793.1985859
   Liu Y, 2010, IEEE T SOFTWARE ENG, V36, P852, DOI 10.1109/TSE.2010.51
   Ma Y, 2012, INFORM SOFTWARE TECH, V54, P248, DOI 10.1016/j.infsof.2011.09.007
   Menzies T, 2010, AUTOMAT SOFTW ENG, V17, P375, DOI 10.1007/s10515-010-0069-5
   Nam J., 2014, TECH REP
   Nam J, 2013, PROCEEDINGS OF THE 35TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE 2013), P382, DOI 10.1109/ICSE.2013.6606584
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   PAWLAK Z, 1982, INT J COMPUT INF SCI, V11, P341, DOI 10.1007/BF01001956
   Peters Fayola, 2013, 2013 10th IEEE Working Conference on Mining Software Repositories (MSR 2013), P409, DOI 10.1109/MSR.2013.6624057
   Shepperd M, 2013, IEEE T SOFTWARE ENG, V39, P1208, DOI 10.1109/TSE.2013.11
   Turhan B, 2009, EMPIR SOFTW ENG, V14, P540, DOI 10.1007/s10664-008-9103-7
   Yu X., 2017, 29 INT C SOFTW ENG K
   Zhang  F., 2014, P 11 WORK C MIN SOFT, P182
   Zhang F, 2016, PROC INT CONF SOFTW, P309, DOI 10.1145/2884781.2884839
   Zimmerman T, 2009, 7TH JOINT MEETING OF THE EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND THE ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING, P91, DOI 10.1145/1595696.1595713
NR 25
TC 0
Z9 0
BN 978-1-5386-8023-0
PY 2018
BP 552
EP 557
DI 10.1109/BRACIS.2018.00101
ER

PT B
AU He, M
   Zhang, JL
   Yang, P
   Yao, KS
AF He, Ming
   Zhang, Jiuling
   Yang, Peng
   Yao, Kaisheng
GP ACM
TI Robust Transfer Learning for Cross-domain Collaborative Filtering Using
   Multiple Rating Patterns Approximation
SO WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB
   SEARCH AND DATA MINING
CT 11th ACM International Conference on Web Search and Data Mining
CY FEB 05-09, 2018
CL Marina Del Rey, CA
ID RECOMMENDATION; ALGORITHM
AB Collaborative filtering techniques are a common approach for building recommendations, and have been widely applied in real recommender systems. However, collaborative filtering usually suffers from limited performance due to the sparsity of user-item interaction. To address this issue, auxiliary information is usually used to improve the performance. Transfer learning provides the key idea of using knowledge from auxiliary domains. An assumption of transfer learning in collaborative filtering is that the source domain is a full rating matrix, which may not hold in many real-world applications. In this paper, we investigate how to leverage rating patterns from multiple incomplete source domains to improve the quality of recommender systems. First, by exploiting the transferred learning, we compress the knowledge from the source domain into a cluster-level rating matrix. The rating patterns in the low-level matrix can be transferred to the target domain. Specifically, we design a knowledge extraction method to enrich rating patterns by relaxing the full rating restriction on the source domain. Finally, we propose a robust multiple-rating-pattern transfer learning model for cross-domain collaborative filtering, which is called MINDTL, to accurately predict missing values in the target domain. Extensive experiments on real-world datasets demonstrate that our proposed approach is effective and outperforms several alternative methods.
CR Abdollahi B, 2016, P INT C COMP WORLD W, P5, DOI DOI 10.1145/2872518.2889405
   Alfeld M, 2017, MICROCHEM J, V132, P179, DOI 10.1016/j.microc.2017.02.001
   Alqadah F, 2015, KNOWL INF SYST, V44, P475, DOI 10.1007/s10115-014-0771-x
   Bokde D, 2015, PROCEDIA COMPUT SCI, V49, P136, DOI 10.1016/j.procs.2015.04.237
   Chinnu P., 2016, INT J COMPUTER APPL, V133
   Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277
   Ding Chris, 2006, P 12 ACM SIGKDD INT, V2006, P126, DOI DOI 10.1145/1150402.1150420
   Fang Z, 2015, 2015 IEEE International Conference on Data Mining Workshop (ICDMW), P1235, DOI 10.1109/ICDMW.2015.133
   Fu JJ, 2016, SYST CONTROL LETT, V93, P1, DOI 10.1016/j.sysconle.2016.03.006
   Gao S., 2011, P 20 ACM INT C INF K, P1169
   Grolman E, 2016, KNOWL-BASED SYST, V107, P70, DOI 10.1016/j.knosys.2016.05.057
   Ji K, 2016, NEUROCOMPUTING, V173, P912, DOI 10.1016/j.neucom.2015.08.046
   Ji K, 2015, NEUROCOMPUTING, V165, P228, DOI 10.1016/j.neucom.2015.03.013
   Jiang M, 2015, IEEE T KNOWL DATA EN, V27, P3084, DOI 10.1109/TKDE.2015.2432811
   Kiers HAL, 2002, COMPUT STAT DATA AN, V41, P157, DOI 10.1016/S0167-9473(02)00142-1
   Kumar V, 2017, INFORM SCIENCES, V380, P1, DOI 10.1016/j.ins.2016.11.003
   Langseth H., 2015, SCALABLE LEARNING PR
   Li B, 2009, P 26 ANN INT C MACH, P617, DOI DOI 10.1145/1553374.1553454
   Li B, 2015, IEEE T CYBERNETICS, V45, P1054, DOI 10.1109/TCYB.2014.2343982
   Li B, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P2052
   Li JQ, 2017, IEEE ACCESS, V5, P35, DOI 10.1109/ACCESS.2016.2600258
   Li X, 2017, NEUROCOMPUTING, V230, P197, DOI 10.1016/j.neucom.2016.12.024
   Li ZC, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2624140
   Liu JX, 2016, IEEE T SERV COMPUT, V9, P686, DOI 10.1109/TSC.2015.2433251
   Liu XF, 2017, CONT TRENDS ISS SCI, V45, P1
   Moreno O., 2012, P 21 ACM INT C INF K, P425
   Santos A, 2017, IEEE T INSTRUM MEAS, V66, P661, DOI 10.1109/TIM.2017.2663478
   Vander Aa T, 2016, IEEE INT C CL COMP, P346, DOI 10.1109/CLUSTER.2016.13
   Wang Yu Xiang, 2012, COMPUTER SCI NUMERIC, V49, P136
   Wu X., 1939, IEEE T SERV COMPUT, V10. 3, P352
   Yu YH, 2017, APPL INTELL, V46, P521, DOI 10.1007/s10489-016-0841-8
   Zhao LL, 2017, ARTIF INTELL, V245, P38, DOI 10.1016/j.artint.2016.12.004
   Zong LL, 2017, NEURAL NETWORKS, V88, P74, DOI 10.1016/j.neunet.2017.02.003
NR 33
TC 0
Z9 0
BN 978-1-4503-5581-0
PY 2018
BP 225
EP 233
DI 10.1145/3159652.3159675
ER

PT B
AU Yu, JF
   Qiu, MH
   Jiang, J
   Huang, J
   Song, SY
   Chu, W
   Chen, HQ
AF Yu, Jianfei
   Qiu, Minghui
   Jiang, Jing
   Huang, Jun
   Song, Shuangyong
   Chu, Wei
   Chen, Haiqing
GP ACM
TI Modelling Domain Relationships for Transfer Learning on Retrieval-based
   Question Answering Systems in E-commerce
SO WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB
   SEARCH AND DATA MINING
CT 11th ACM International Conference on Web Search and Data Mining
CY FEB 05-09, 2018
CL Marina Del Rey, CA
AB Nowadays, it is a heated topic for many industries to build automatic question-answering (QA) systems. A key solution to these QA systems is to retrieve from a QA knowledge base the most similar question of a given question, which can be reformulated as a paraphrase identification (PI) or a natural language inference (NLI) problem. However, most existing models for PI and NLI have at least two problems: They rely on a large amount of labeled data, which is not always available in real scenarios, and they may not be efficient for industrial applications.
   In this paper, we study transfer learning for the PI and NLI problems, aiming to propose a general framework, which can effectively and efficiently adapt the shared knowledge learned from a resource-rich source domain to a resource-poor target domain. Specifically, since most existing transfer learning methods only focus on learning a shared feature space across domains while ignoring the relationship between the source and target domains, we propose to simultaneously learn shared representations and domain relationships in a unified framework. Furthermore, we propose an efficient and effective hybrid model by combining a sentence encoding-based method and a sentence interaction-based method as our base model. Extensive experiments on both paraphrase identification and natural language inference demonstrate that our base model is efficient and has promising performance compared to the competing models, and our transfer learning method can help to significantly boost the performance. Further analysis shows that the inter-domain and intra-domain relationship captured by our model are insightful. Last but not least, we deploy our transfer learning model for PI into our online chatbot system, which can bring in significant improvements over our existing system.
CR Argyriou Andreas, 2007, NIPS
   arikh Ankur P, 2016, EMNLP
   Blitzer John, 2006, EMNLP
   Bowman S. R., 2015, EMNLP
   Bowman Samuel R., 2016, ACL
   Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2
   Chen Qian, 2017, ACL
   Cui L, 2017, IEEE C ELEC DEVICES
   Dai Wenyuan, 2007, ICML
   Daume Hal, 2007, ACL
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gillick Laurence, 1989, ICASSP
   Hu Baotian, 2014, NIPS
   Jeon Jiwoon, 2005, CIKM
   Jiang J., 2007, ACL
   Kusner M. J., 2015, ICML
   Lee Su-In, 2007, ICML
   Liu Pengfei, 2017, ACL
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Mou Lili, 2016, EMNLP
   Mou Lili, 2016, ACL
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pang Liang, 2016, AAAI
   Pennington  Jeffrey, 2014, EMNLP
   Qiu Minghui, 2017, ICDM
   Qiu Minghui, 2017, ACL
   Rocktaschel Tim, 2015, ICLR
   Socher Richard, 2011, NIPS
   Taigman Y., 2017, ICLR
   Vinyals O., 2015, ARXIV150605869
   Wang Chang, 2008, ICML
   Wang Di, 2015, ACL IJCNLP
   Wang Dong, 2015, SIGN INF PROC ASS AN
   Wang  Shuohang, 2017, ICLR
   Wieting John, 2016, ICLR
   Williams Adina, 2017, ARXIV170405426
   Wu HC, 2008, ACM T INFORM SYST, V26, DOI 10.1145/1361684.1361686
   Yan Rui, 2016, SIGIR
   Yang Zhilin, 2017, ICLR
   Yin W., 2016, T ASS COMPUT LINGUIS, V4, P259
   Yin Wenpeng, 2015, NAACL HLT
   Yosinski J., 2014, NIPS
   Yu Jianfei, 2016, EMNLP
   Zhang Yu, 2010, UAI
NR 45
TC 0
Z9 0
BN 978-1-4503-5581-0
PY 2018
BP 682
EP 690
DI 10.1145/3159652.3159685
ER

PT B
AU Xu, JJ
   Tong, HH
   Lu, TC
   He, JR
   Bliss, N
AF Xu, Jiejun
   Tong, Hanghang
   Lu, Tsai-Ching
   He, Jingrui
   Bliss, Nadya
GP ACM
TI GTA(3) 2018: Workshop on Graph Techniques for Adversarial Activity
   Analytics
SO WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB
   SEARCH AND DATA MINING
CT 11th ACM International Conference on Web Search and Data Mining
CY FEB 05-09, 2018
CL Marina Del Rey, CA
AB Networks are natural analytic tools in modeling adversarial activities (e.g., human trafficking, illicit drug production, terrorist financial transaction) using different intelligence data sources. However, such activities are often covert and embedded across multiple domains and contexts. They are generally not detectable and recognizable from the perspective of an isolated network, and only become apparent when multiple networks are analyzed in a joint manner. Thus, one of the main research topics in modeling adversarial activities is to develop effective techniques to align and fuse information from different networks into a unified representation for global analysis. Based on the combined network representation, an equally important research topic is on detecting and matching indicating patterns to recognize the underlining adversarial activities in the integrated network. Two key challenge problems involved in the modeling process include:
   Network alignment and merging: develop accurate and scalable methods for mapping of nodes across heterogeneous networks based on different associational and causal dependencies.
   Subgraph detection and matching: develop robust and efficient algorithms for richly attributed networks to support recognition of complex query patterns for networks.
   The focus of this workshop is to gather together the researchers from all relevant fields to share their experience and opinions on graph mining techniques in the era of big data, with emphasis on two fundamental problems - "Connecting the dots" and "finding a needle in a haystack", in the context of graph-based adversarial activity analytics. A best paper will be selected and announced in our workshop based on the collective feedback from our reviewers.
   This workshop (co-located with the 11th ACM Conference on Web Search and Data Mining) aims to bring together a cross-disciplinary audience of researchers from both academia and industry to share experience with techniques, resources and best practices, and to exchange perspectives and future directions. We expect the workshop to develop a community of interested researchers and facilitate their future collaborations.
   Topics of Interest
   Data integration and alignment from multiple heterogeneous networks
   Novel algorithms for subgraph detection and matching in large networks
   Graph construction and modeling for different domains (e.g., financial fraud, human trafficking,DDoS attack)
   Complex anomaly (e.g., group anomaly) detection and interpretation
   Atypical behavior and rare event detection
   Limits of detectability and identifiability
   Evolution analysis and forecasting
   Game theoretic approach on anticipating opponent intents and actions
   Identification of novel datasets and/or evaluation metrics
   Multilayer and multiplex network analytics
   Clustering and ranking methods for composite networks
   Large-scaled link prediction and recommendation algorithms
   Community detection in big networks
   Information diffusion and influence maximization
   Interactive visualization for big graphs
   New methods and frontiers in spectral graph theory
   Analysis of network topologies (e.g., centrality and network motif analysis)
   Semi-supervised learning, Transductive inference, Active learning, and Transfer learning
NR 0
TC 0
Z9 0
BN 978-1-4503-5581-0
PY 2018
BP 803
EP 803
DI 10.1145/3159652.3160595
ER

PT B
AU Serra, E
   Sharma, A
   Joaristi, M
   Korzh, O
AF Serra, Edoardo
   Sharma, Ashish
   Joaristi, Mikel
   Korzh, Oxana
BE Brandes, U
   Reddy, C
   Tagarelli, A
TI Unknown Landscape Identification with CNN Transfer Learning
SO 2018 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS
   ANALYSIS AND MINING (ASONAM)
CT IEEE/ACM International Conference on Advances in Social Networks
   Analysis and Mining (ASONAM)
CY AUG 28-31, 2018
CL Barcelona, SPAIN
ID CLASSIFICATION
AB Unknown landscape identification is the problem of identifying an unknown landscape from a set of already provided landscape images that are considered to be known. The aim of this work is to extract the intrinsic semantic of landscape images in order to automatically generalize concepts like a stadium, roads, a parking lot etc., and use this concept to identify unknown landscapes. This problem can be easily extended to many security applications. We propose two effective semi-supervised novelty detection approaches for the unknown landscape identification problem using Convolutional Neural Network (CNN) Transfer Learning. This is based on the use of pre-trained CNNs (i.e. already trained on large datasets) already containing general image knowledge that we transfer to our domain. Our best values of AUROC and Average Precision scores for the identification problem are 0.96 and 0.94, respectively. In addition, we statistically prove that our semi-supervised methods outperform the baseline.
CR Agarwal D, 2007, KNOWL INF SYST, V11, P29, DOI 10.1007/s10115-006-0036-4
   Dai DX, 2011, IEEE GEOSCI REMOTE S, V8, P173, DOI 10.1109/LGRS.2010.2055033
   Deng J., 2009, CVPR 09
   Han J., 2005, DATA MINING CONCEPTS
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17
   Marsland S., 2003, NEURAL COMP SURVEYS
   Nogueira K, 2017, PATTERN RECOGN, V61, P539, DOI 10.1016/j.patcog.2016.07.001
   Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026
   Reyes A.K., 2015, FINE TUNING DEEP CON
   Sabokrou M., 2016, ARXIV160900866
   Shamir L., 2013, THEORY APPL MATH COM, V3, P13
   Shin H., 2016, IEEE T MED IMAGING
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270, DOI DOI 10.1145/1869790.1869829
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 19
TC 0
Z9 0
BN 978-1-5386-6051-5
PY 2018
BP 813
EP 820
ER

PT B
AU Dalal, R
   Moh, TS
AF Dalal, Rahul
   Moh, Teng-Sheng
BE Brandes, U
   Reddy, C
   Tagarelli, A
TI Fine-Grained Object Detection Using Transfer Learning and Data
   Augmentation
SO 2018 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS
   ANALYSIS AND MINING (ASONAM)
CT IEEE/ACM International Conference on Advances in Social Networks
   Analysis and Mining (ASONAM)
CY AUG 28-31, 2018
CL Barcelona, SPAIN
DE Object Detection; Computer Vision; Deep Learning; Convolutional Neural
   Networks; Region based Convolutional Neural Network; Inception; You Only
   Look Once; Single Shot Detection
AB Object detection plays a vital role in many real-world computer vision applications such as self-driving cars, human-less stores and general purpose robotic systems. Convolutional Neural Network(CNN) based Deep Learning has evolved to become the backbone of most computer vision algorithms, including object detection. Most of the research has focused on detecting objects that differ significantly e.g. a car, a person, and a bird. Achieving fine-grained object detection to detect different types within one class of objects can be crucial in tasks like automated retail checkout. This research has developed deep learning models to detect 200 types of similar birds. The models were trained and tested on CUB-200-2011 dataset. To the best of our knowledge, by attaining a mean Average Precision (mAP) of 71.5% we achieved an improvement of 5 percentage points over the previous best mAP of 66.2%.
CR Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Girshick R., 2015, IEEE INT C COMP VIS
   Girshick R., 2015, REGION BASED CONVOLU
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin M., 2013, ABS13124400 CORR
   Lin T.-Y., 2014, ECCV
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Redmon J, 2016, ARXIV161208242
   Redmon Joseph, 2015, ARXIV150602640
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sermanet P, 2014, INT C LEARN REPR ICL
   Szegedy C., 2014, ABS14094842 CORR
   Turner J. T., 2015, ABS160300502 CORR
   Wah C., CALTECH UCSD BIRDS 2
NR 14
TC 0
Z9 0
BN 978-1-5386-6051-5
PY 2018
BP 893
EP 896
ER

PT B
AU Skryzalin, J
   Link, H
   Wendt, J
   Field, R
   Richter, SN
AF Skryzalin, Jacek
   Link, Hamilton
   Wendt, Jeremy
   Field, Richard
   Richter, Samuel N.
BE Brandes, U
   Reddy, C
   Tagarelli, A
TI Efficient transfer learning for neural network language models
SO 2018 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS
   ANALYSIS AND MINING (ASONAM)
CT IEEE/ACM International Conference on Advances in Social Networks
   Analysis and Mining (ASONAM)
CY AUG 28-31, 2018
CL Barcelona, SPAIN
AB We apply transfer learning techniques to create topically and/or stylistically biased natural language models from small data samples, given generic long short-term memory (LSTM) language models trained on larger data sets. Although LSTM language models are powerful tools with wide-ranging applications, they require enormous amounts of data and time to train. Thus, we build general purpose language models that take advantage of large standing corpora and computational resources proactively, allowing us to build more specialized analytical tools from smaller data sets on demand. We show that it is possible to construct a language model from a small, focused corpus by first training an LSTM language model on a large corpus (e.g., the text from English Wikipedia) and then retraining only the internal transition model parameters on the smaller corpus. We also show that a single general language model can be reused through transfer learning to create many distinct special purpose language models quickly with modest amounts of data.
CR Abadi Martin, 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Blitzer J., 2006, P 2006 C EMP METH NA, P120
   Bowman S. R., 2016, P 54 ANN M ASS COMP, V1, P1466
   Chelba  Ciprian, 2013, ARXIV13123005
   Dyer Chris, 2016, NAACL HLT, P199
   Filippova K, 2015, P 2015 C EMP METH NA, P360
   Gers FA, 2000, IEEE IJCNN, P189, DOI 10.1109/IJCNN.2000.861302
   Ghosh S, 2016, ARXIV160206291
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Howard J, 2018, ARXIV180106146
   Hu Z., 2017, P ICML 17, P1587
   Jean S., 2015, ACL ICJNLP
   Ji S., 2016, ICLR
   Ji Y., 2015, ARXIV151103962
   Jozefowicz R., 2016, ARXIV160202410
   Kingma D., 2015, ICLR
   Li Jiwei, 2016, P 54 ANN M ASS COMP, P994, DOI DOI 10.18653/V1/P16-1094
   Lipton Z. C., 2015, CAPTURING MEANING PR
   Luong T., 2015, P 2015 C EMP METH NA, P1412, DOI DOI 10.18653/V1/D15-1166
   Manning C.D., 2014, P 52 ANN M ASS COMP, P55, DOI DOI 10.3115/V1/P14-5010
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Mueller J., 2017, INT C MACH LEARN, P2536
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pang B., 2004, P ACL
   Rush A. M., 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044
   Sak H., 2014, 15 ANN C INT SPEECH
   Shen T., 2017, P ADV NEUR INF PROC, P6833
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vaswani A., 2017, ADV NEURAL INFORM PR, P6000
   Xiong C., 2016, P INT C MACH LEARN, P2397
   Yang Z., 2016, HLT NAACL, P1480
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
   Zaremba W., 2015, ICLR
NR 33
TC 0
Z9 0
BN 978-1-5386-6051-5
PY 2018
BP 897
EP 902
ER

PT B
AU Miscikis, J
   Brijacak, I
   Yahyanejad, S
   Glette, K
   Elle, OJ
   Torresen, J
AF Miscikis, Justinas
   Brijacak, Inka
   Yahyanejad, Saeed
   Glette, Kyrre
   Elle, Ole Jakob
   Torresen, Jim
GP IEEE
TI Transfer Learning for Unseen Robot Detection and Joint Estimation on a
   Multi-Objective Convolutional Neural Network
SO 2018 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SAFETY FOR
   ROBOTICS (ISR)
CT IEEE International Conference on Intelligence and Safety for Robotics
   (ISR)
CY AUG 24-27, 2018
CL Shenyang, PEOPLES R CHINA
AB A significant problem of using deep learning techniques is the limited amount of data available for training. There are some datasets available for the popular problems like item recognition and classification or self-driving cars, however, it is very limited for the industrial robotics field. In previous work, we have trained a multi-objective Convolutional Neural Network (CNN) to identify the robot body in the image and estimate 3D positions of the joints by using just a 2D image, but it was limited to a range of robots produced by Universal Robots (UR). In this work, we extend our method to work with a new robot arm - Kuka LBR iiwa, which has a significantly different appearance and an additional joint. However, instead of collecting large datasets once again, we collect a number of smaller datasets containing a few hundred frames each and use transfer learning techniques on the CNN trained on UR robots to adapt it to a new robot having different shapes and visual features. We have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects.
CR Fankhauser P., 2015, IEEE INT C ADV UNPUB
   Greenspan H, 2016, IEEE T MED IMAGING, V35, P1153, DOI 10.1109/TMI.2016.2553401
   Heikkila T, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P2292, DOI 10.1109/IROS.2000.895310
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lee Jay, 2015, Manufacturing Letters, V3, P18, DOI 10.1016/j.mfglet.2014.12.001
   Mainprice J, 2013, IEEE INT C INT ROBOT, P299, DOI 10.1109/IROS.2013.6696368
   Meeussen W., 2012, URDF UNIFIED ROBOT D
   Miseikis J., 2016, COMP INT SSCI 2016 I, P1
   Miseikis J., 2018, ARXIV180403005
   Miseikis J, 2016, IEEE/SICE I S SYS IN, P735, DOI 10.1109/SII.2016.7844087
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Sucan I. A., 2013, MOVEIT
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Xie M., 2015, ARXIV151000098
NR 16
TC 0
Z9 0
BN 978-1-5386-5547-4
PY 2018
BP 337
EP 342
ER

PT B
AU Bonazza, P
   Miteran, J
   Ginhac, D
   Dubois, J
AF Bonazza, Pierre
   Miteran, Johel
   Ginhac, Dominique
   Dubois, Julien
GP ACM
TI PhD Forum : Machine Learning VS Transfer Learning Smart Camera
   Implementation for Face Authentication
SO PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART
   CAMERAS (ICDSC'18)
CT 12th International Conference on Distributed Smart Cameras (ICDSC)
CY SEP 03-04, 2018
CL Eindhoven, NETHERLANDS
DE Face authentication; Machine Learning; Transfer Learning
AB The aim of this paper is to highlight differences between classical machine learning and transfer learning applied to low cost real-time face authentication. Furthermore, in an access control context, the size of biometric data should be minimized so it can be stored on a remote personal media. These constraints have led us to compare only lightest versions of these algorithms. Transfer learning applied on Mobilenet vl raises to 85% of accuracy, for a 457Ko model, with 3680s and 1.43s for training and prediction tasks. In comparison, the fastest integrated method (Random Forest) shows accuracy up to 90% for a 7,9Ko model, with a fifth of a second to be trained and a hundred of microseconds for the prediction, enabling embedded real-time face authentication at 10 fps.
CR Bonazza P., 2017, IM PROC C GRETS 2017
   Howard A. G., 2017, MOBILENETS EFFICIENT
   Learned-miller E., 2016, LABELED FACES WILD S
   Sandler M., 2018, MOBILENETV2 INVERTED
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
NR 5
TC 0
Z9 0
BN 978-1-4503-6511-6
PY 2018
DI 10.1145/3243394.3243710
ER

PT B
AU Bircanoglu, C
   Atay, M
   Beser, F
   Genc, O
   Kizrak, MA
AF Bircanoglu, Cenk
   Atay, Meltem
   Beser, Fuat
   Genc, Ozgun
   Kizrak, Merve Ayyuce
BE Yildirim, T
   Manolopoulos, Y
   Angelov, P
   Iliadis, L
TI RecycleNet: Intelligent Waste Sorting Using Deep Neural Networks
SO 2018 INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS (INISTA)
CT IEEE (SMC) International Conference on Innovations in Intelligent
   Systems and Applications (INISTA)
CY JUL 03-05, 2018
CL Thessaloniki, GREECE
AB Waste management and recycling is the fundamental part a sustainable economy. For more efficient and safe recycling, it is necessary to use intelligent systems instead of employing humans as workers in the dump-yards. This is one of the early works demonstrating the efficiency of latest intelligent approaches. In order to provide the most efficient approach, we experimented on well-known deep convolutional neural network architectures. For training without any pre-trained weights, Inception-Resnet, Inception-v4 outperformed all others with 90% test accuracy. For transfer learning and fine-tuning of weight parameters using ImageNet, DenseNet121 gave the best result with 95% test accuracy. One disadvantage of these networks, however, is that they are slightly slower in prediction time. To enhance the prediction performance of the models we altered the connection patterns of the skip connections inside dense blocks. Our model RecycleNet is carefully optimized deep convolutional neural network architecture for classification of selected recyclable object classes. This novel model reduced the number of parameters in a 121 layered network from 7 million to about 3 million.
CR A. Google, 2017, GOOGL COL
   Alchon Suzanne Austin, 2003, PEST LAND NEW WORLD
   Alvarez-Chavez CR, 2012, J CLEAN PROD, V23, P47, DOI 10.1016/j.jclepro.2011.10.003
   Amick DS, 2014, LITHIC TECHNOL, V39, P64, DOI 10.1179/0197726113Z.00000000025
   Awe O., 2017, SMART TRASH NET WAST
   Barles S, 2014, BASIC ENV HIST, P199
   Chollet F., 2016, XCEPTION DEEP LEARNI
   Chollet F., 2015, KERAS
   CUMMINGS LD, 1977, J VOLUNT ACTION RES, V6, P153
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Howard A. G., 2017, ARXIV170404861
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lipsett C. H., 1974, 100 YEARS RECYCLING
   Liu C, 2010, PROC CVPR IEEE, P239, DOI [10.1109/CVPR.2010.5540207, 10.1109/ICCET.2010.5485248]
   PALMER JA, 1995, ENV ED RES, V0001
   Szegedy C., 2017, AAAI, V4, P12
   Williams PT, 2005, WASTE TREATMENT AND DISPOSAL, 2ND EDITION, P1, DOI 10.1002/0470012668
   Witkowski TH, 2003, J ADVERTISING, V32, P69, DOI 10.1080/00913367.2003.10639053
   Yang M., 2016, CLASSIFICATION TRASH
   Zeiler M. D., 2012, ARXIV12125701
NR 23
TC 0
Z9 0
BN 978-1-5386-5150-6
PY 2018
ER

PT B
AU Markowska-Kaczmar, U
   Skiba, M
AF Markowska-Kaczmar, Urszula
   Skiba, Michal
BE Yildirim, T
   Manolopoulos, Y
   Angelov, P
   Iliadis, L
TI Is Convolutional Network Competitive for Vision Method in the Furniture
   Dowel Quality Control?
SO 2018 INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS (INISTA)
CT IEEE (SMC) International Conference on Innovations in Intelligent
   Systems and Applications (INISTA)
CY JUL 03-05, 2018
CL Thessaloniki, GREECE
DE deep learning; convolutional network; quality control system; production
   spoilage detection
ID NEURAL-NETWORKS
AB The paper presents a method of the furniture dowel quality control based on the convolutional neural network. We applied AlexNet, testing the ability of transfer learning. The paper presents details of the method and experiments performed to justify our solution on images datasets collected from real dowels production. The outcomes are compared with the results of baseline vision method. The experiments show that both methods have their strengths and weaknesses, but they complement each other.
CR Abdel-Hakim A. E., 2006, P IEEE COMP SOC C CO, V2, P1978, DOI DOI 10.1109/CVPR.2006.95
   CHIN RT, 1982, IEEE T PATTERN ANAL, V4, P557, DOI 10.1109/TPAMI.1982.4767309
   Clevert D. A, 2016, FAST ACCURATE DEEP N
   He K, 2015, DELVING DEEP RECTIFI
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vasilic S, 2006, 2006 IEEE International Symposium on Industrial Electronics, Vols 1-7, P469, DOI 10.1109/ISIE.2006.295640
   Wang T, 2018, INT J ADV MANUF TECH, V94, P3465, DOI 10.1007/s00170-017-0882-0
   Weimer D, 2016, CIRP ANN-MANUF TECHN, V65, P417, DOI 10.1016/j.cirp.2016.04.072
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 11
TC 0
Z9 0
BN 978-1-5386-5150-6
PY 2018
ER

PT B
AU Goh, GB
   Siegel, C
   Vishnu, A
   Hodas, N
AF Goh, Garrett B.
   Siegel, Charles
   Vishnu, Abhinav
   Hodas, Nathan
GP ACM
TI Using Rule-Based Labels for Weak Supervised Learning A ChemNet for
   Transferable Chemical Property Prediction
SO KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY & DATA MINING
CT 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
CY AUG 19-23, 2018
CL London, ENGLAND
DE Bioinformatics; Cheminformatics; Computer Vision; Natural Language
   Processing; Transfer Learning; Weak Supervised Learning
ID QSAR
AB With access to large datasets, deep neural networks (DNN) have achieved human-level accuracy in image and speech recognition tasks. However, in chemistry data is inherently small and fragmented. In this work, we develop an approach of using rule-based knowledge for training ChemNet, a transferable and generalizable deep neural network for chemical property prediction that learns in a weak-supervised manner from large unlabeled chemical databases. When coupled with transfer learning approaches to predict other smaller datasets for chemical properties that it was not originally trained on, we show that ChemNet's accuracy outperforms contemporary DNN models that were trained using conventional supervised learning. Furthermore, we demonstrate that the ChemNet pre-training approach is equally effective on both CNN (Chemception) and RNN (SMILES2vec) models, indicating that this approach is network architecture agnostic and is effective across multiple data modalities. Our results indicate a pre-trained ChemNet that incorporates chemistry domain knowledge and enables the development of generalizable neural networks for more accurate prediction of novel chemical properties.
CR Abadi Martin, 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   BALABAN AT, 1982, CHEM PHYS LETT, V89, P399, DOI 10.1016/0009-2614(82)80009-2
   Bjerrum EJ, 2017, ARXIV170307076
   Buchwald P, 2002, DRUG FUTURE, V27, P577, DOI 10.1358/dof.2002.027.06.856934
   Cherkasov A, 2014, J MED CHEM, V57, P4977, DOI 10.1021/jm4004285
   Chodera JD, 2011, CURR OPIN STRUC BIOL, V21, P150, DOI 10.1016/j.sbi.2011.01.011
   Chollet F., 2015, KERAS
   Dahl GE, 2014, ARXIV14061231
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR, P2224
   Gaulton A, 2012, NUCLEIC ACIDS RES, V40, pD1100, DOI 10.1093/nar/gkr777
   Gawehn E, 2016, MOL INFORM, V35, P3, DOI 10.1002/minf.201501008
   Goh G. B., 2017, J COMPUTATIONAL CHEM
   Goh G. B, 2017, ARXIV170606689
   Goh Garrett B, 2017, ARXIV171202034
   Goh GB, 2017, ARXIV171002238
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hinton G, 2012, NEURAL NETWORKS MACH, V6e
   Hughes TB, 2016, ACS CENTRAL SCI, V2, P529, DOI 10.1021/acscentsci.6b00162
   Kearnes S, 2016, J COMPUT AID MOL DES, V30, P595, DOI 10.1007/s10822-016-9938-8
   Kim S, 2016, NUCLEIC ACIDS RES, V44, pD1202, DOI 10.1093/nar/gkv951
   Kruhlak NL, 2007, ADV DRUG DELIVER REV, V59, P43, DOI 10.1016/j.addr.2006.10.008
   Landrum G., 2016, RDKIT OPEN SOURCE CH
   Mayr A, 2016, FRONT ENV SCI-SWITZ, V3, DOI 10.3389/fenvs.2015.00080
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   PLATT JR, 1947, J CHEM PHYS, V15, P419, DOI 10.1063/1.1746554
   Ramsundar B, 2015, ARXIV150202072
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shie CK, 2015, IEEE ENG MED BIO, P711, DOI 10.1109/EMBC.2015.7318461
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Todeschini R, 2008, HDB MOL DESCRIPTORS, V11
   Wallach I., 2015, ARXIV151002855
   WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005
   Wu Z., 2017, ARXIV170300564
NR 33
TC 0
Z9 0
BN 978-1-4503-5552-0
PY 2018
BP 302
EP 310
DI 10.1145/3219819.3219838
ER

PT B
AU Hu, A
   Flaxman, S
AF Hu, Anthony
   Flaxman, Seth
GP ACM
TI Multimodal Sentiment Analysis To Explore the Structure of Emotions
SO KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY & DATA MINING
CT 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
CY AUG 19-23, 2018
CL London, ENGLAND
DE Deep learning; sentiment analysis; visual analysis; transfer learning;
   natural language processing
AB We propose a novel approach to multimodal sentiment analysis using deep neural networks combining visual analysis and natural language processing. Our goal is different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment; instead, we aim to infer the latent emotional state of the user. Thus, we focus on predicting the emotion word tags attached by users to their Tumblr posts, treating these as "self-reported emotions." We demonstrate that our multi-modal model combining both text and image features outperforms separate models based solely on either images or text. Our model's results are interpretable, automatically yielding sensible word lists associated with emotions. We explore the structure of emotions implied by our model and compare it to what has been posited in the psychology literature, and validate our model on a set of images that have been used in psychology studies. Finally, our work also provides a useful tool for the growing academic study of images-both photographs and memes-on social networks.
CR [Anonymous], 2017, DISGUSTED POST
   [Anonymous], 2017, OPTIMISTIC POST
   [Anonymous], 2017, SURPRISED POST
   [Anonymous], 2017, AMBIGUOUS SURPRISED
   [Anonymous], 2017, SAD POST
   [Anonymous], 2017, ANGRY POST
   [Anonymous], 2017, HAPPY POST
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bengio Y, 2007, LARGE SCALE KERNEL M
   Bollen J, 2011, J COMPUT SCI-NETH, V2, P1, DOI 10.1016/j.jocs.2010.12.007
   Borth Damian, 2013, ACM INT C MULT ACM M
   Chen Tao, 2015, AAAI
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Flaxman Seth, 2015, KDD
   Gilbert D, 2006, STUMBLING HAPPINESS
   Golder SA, 2011, SCIENCE, V333, P1878, DOI 10.1126/science.1202775
   Gong Yunchao, 2014, INT J COMPUTER VISIO, V2014
   Goodman JT, 2001, COMPUT SPEECH LANG, V15, P403, DOI 10.1006/csla.2001.0174
   Guillaumin Matthieu, 2010, CVPR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Katsurai M., 2016, ICASSP
   Kurdi B, 2017, BEHAV RES METHODS, V49, P457, DOI 10.3758/s13428-016-0715-3
   Lang P. J., 2005, TECHNICAL REPORT
   Lindquist Kristen A., 2013, 100 YEAR EMOTION WAR, P2013
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Mikolov Tomas, 2011, INT C AC SPEECH SIGN
   Miller Daniel, 2017, VISUALISING FACEBOOK
   Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060
   Pak Alexander, 2010, LREC
   Pennebaker James W., 2007, OPERATORS MANUAL
   Pennington J, 2014, EMPIRICAL METHODS NA
   Plutchik R, 2001, AM SCI, V89, P344, DOI 10.1511/2001.4.344
   Posner J, 2005, DEV PSYCHOPATHOL, V17, P715, DOI 10.1017/S0954579405050340
   Radford  Alec, 2016, ICLR
   Rosenthal S, 2017, P 11 INT WORKSH SEM, P502
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shifman L, 2014, MIT PRESS ESSENT, P1
   Sutskever  Ilya, 2014, NIPS
   Szegedy C., 2015, CVPR
   Tausczik YR, 2010, J LANG SOC PSYCHOL, V29, P24, DOI 10.1177/0261927X09351676
   Tellegen A, 1999, PSYCHOL SCI, V10, P297, DOI 10.1111/1467-9280.00157
   Wang Y., 2015, IJCAI
   Watson D., 1999, PANAS X MANUAL POSIT
   Wojnicki Andrea C., 2008, WORD OF MOUTH SELF E
   You Q., 2015, AAAI
NR 46
TC 0
Z9 0
BN 978-1-4503-5552-0
PY 2018
BP 350
EP 358
DI 10.1145/3219819.3219853
ER

PT B
AU Liu, ZY
   Shen, YY
   Zhu, YM
AF Liu, Zhaoyang
   Shen, Yanyan
   Zhu, Yanmin
GP ACM
TI Where Will Dockless Shared Bikes be Stacked? - Parking Hotspots
   Detection in a New City
SO KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY & DATA MINING
CT 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
CY AUG 19-23, 2018
CL London, ENGLAND
DE Dockless shared bikes; hotspots detection; urban computing; transfer
   learning
AB Dockless shared bikes, which aim at providing a more flexible and convenient solution to the first-and-last mile connection, come into China and expand to other countries at a very impressing speed. The expansion of shared bike business in new cities brings many challenges among which, the most critical one is the parking chaos caused by too many bikes yet insufficient demands. To allow possible actions to be taken in advance, this paper studies the problem of detecting parking hotspots in a new city where no dockless shared bike has been deployed. We propose to measure road hotness by bike density with the help of the Kernal Density Estimation. We extract useful features from multi-source urban data and introduce a novel domain adaption network for transferring hotspots knowledge learned from one city with shared bikes to a new city. The extensive experimental results demonstrate the effectiveness of our proposed approach compared with various baselines.
CR Bao Jie, 2017, TRAJECTORIES
   Burges C., 2005, P 22 INT C MACH LEAR, P89, DOI DOI 10.1145/1102351.1102363
   Burges C. J., 2007, P ADV NEUR INF PROC, P193
   Burges Christopher JC, 2010, LEARNING, V11, P81
   Cao Zhangjie, 2017, PARTIAL TRANSFER LEA
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dehnad Khosrow, 2012, TECHNOMETRICS, V29, P495
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   Ganin  Y., 2015, INT C MACH LEARN, P1180
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gast N, 2015, P 24 ACM INT C INF K, P703, DOI DOI 10.1145/2806416.2806569
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR, P2672, DOI [DOI 10.1017/CBO9781139058452, DOI 10.1001/JAMAINTERNMED.2016.8245]
   Hoang MX, 2016, 24TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2016), DOI 10.1145/2996913.2996934
   Hoffman  J., 2014, ADV NEURAL INFORM PR, P3536
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jean N, 2016, SCIENCE, V353, P790, DOI 10.1126/science.aaf7894
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Lin M, 2013, COMPUTER SCI
   Liu J, 2016, P 22 ACM SIGKDD INT, P1005
   Liu JM, 2015, IEEE DATA MINING, P883, DOI 10.1109/ICDM.2015.99
   Long M., 2016, ADV NEURAL INFORM PR, P136
   Long M., 2015, INT C MACH LEARN, P97
   Manning CD, 2008, INTRO INFORM RETRIEV, V1
   Martinez LM, 2012, PROCD SOC BEHV, V54, P513, DOI 10.1016/j.sbspro.2012.09.769
   Pan S., 2008, AAAI, P677
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Sculley D, 2010, P 16 ACM SIGKDD INT, P979, DOI DOI 10.1145/1835804.1835928
   SHEATHER SJ, 1991, J ROY STAT SOC B MET, V53, P683
   Singla A., 2015, P 29 AAAI C ART INT, P723
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Toh Michelle, 2017, CHINAS BIKE SHARING
   Tzeng E., 2014, DEEP DOMAIN CONFUSIO
   Van Brummelen Glen, 2012, HEAVENLY MATH FORGOT, P1
   VANDERMAATEN L, 2008, VIGILIAE CHRISTIAN, V9, P2579
   Yang Z., 2016, P 14 ANN INT C MOB S, P165, DOI DOI 10.1145/2906388
   Zeng Ming, 2016, IMPROVING DEMAND PRE
NR 38
TC 0
Z9 0
BN 978-1-4503-5552-0
PY 2018
BP 566
EP 575
DI 10.1145/3219819.3219920
ER

PT B
AU Sato, I
   Nomura, Y
   Hanaoka, S
   Miki, S
   Hayashi, N
   Abe, O
   Masutani, Y
AF Sato, Issei
   Nomura, Yukihiro
   Hanaoka, Shouhei
   Miki, Soichiro
   Hayashi, Naoto
   Abe, Osamu
   Masutani, Yoshitaka
GP ACM
TI Managing Computer-Assisted Detection System Based on Transfer Learning
   with Negative Transfer Inhibition
SO KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY & DATA MINING
CT 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
CY AUG 19-23, 2018
CL London, ENGLAND
DE Medical image analysis; Computer-assisted detection; Machine learning;
   Transfer learning; Negative transfer
ID CONVOLUTIONAL NEURAL-NETWORKS; AIDED DIAGNOSIS; DEEP; SEGMENTATION
AB The reading workload for radiologists is increasing because the numbers of examinations and images per examination are increasing due to the technical progress on imaging modalities such as computed tomography and magnetic resonance imaging. A computer-assisted detection (CAD) system based on machine learning is expected to assist radiologists. The preliminary results of a multi-institutional study indicate that the performance of the CAD system for each institution improved using training data of other institutions. This indicates that transfer learning may be useful for developing the CAD systems among multiple institutions. In this paper, we focus on transfer learning without sharing training data due to the need to protect personal information in each institution. Moreover, we raise a problem of negative transfer in CAD system and propose an algorithm for inhibiting negative transfer. Our algorithm provides a theoretical guarantee for managing CAD software in terms of transfer learning and exhibits experimentally better performance compared to that of the current algorithm in cerebral aneurysm detection.
CR Belharbi S, 2017, COMPUT BIOL MED, V87, P95, DOI 10.1016/j.compbiomed.2017.05.018
   Cesa-Bianchi N, 2006, PREDICTION LEARNING
   Conjeti S, 2016, MED IMAGE ANAL, V32, P1, DOI 10.1016/j.media.2016.02.005
   Giger ML, 2008, MED PHYS, V35, P5799, DOI 10.1118/1.3013555
   Greenspan H, 2016, IEEE T MED IMAGING, V35, P1153, DOI 10.1109/TMI.2016.2553401
   Gruszauskas NP, 2009, RADIOLOGY, V253, P661, DOI 10.1148/radiol.2533090280
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li H, 2008, ACAD RADIOL, V15, P1437, DOI 10.1016/j.acra.2008.05.004
   Littlestone N., 1994, INFORM COMPUT, V108, P2
   Mozer Michael C., 2001, ADV NEURAL INFORM PR, V14, P1409
   Nomura Y., 2014, J BIOMEDICAL GRAPHIC, V4, P12
   Nomura Y., 2010, T MASS DATA ANAL IMA, V2, P112
   Nomura Y, 2013, 2013 FIRST INTERNATIONAL SYMPOSIUM ON COMPUTING AND NETWORKING (CANDAR), P320, DOI 10.1109/CANDAR.2013.57
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Samala RK, 2016, MED PHYS, V43, P6654, DOI 10.1118/1.4967345
   Schapire RE, 1998, ANN STAT, V26, P1651
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Sonoyama S, 2015, IEEE ENG MED BIO, P785, DOI 10.1109/EMBC.2015.7318479
   Summers RM, 2008, AM J ROENTGENOL, V191, P168, DOI 10.2214/AJR.07.3354
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   van Engelen A, 2015, IEEE T MED IMAGING, V34, P1294, DOI 10.1109/TMI.2014.2384733
   van Ginneken B, 2011, RADIOLOGY, V261, P719, DOI 10.1148/radiol.11091710
   van Opbroek A, 2015, IEEE T MED IMAGING, V34, P1018, DOI 10.1109/TMI.2014.2366792
   Willinek WA, 2003, RADIOLOGY, V229, P913, DOI 10.1148/radiol.2293020782
   Zhao P., 2010, P INT C MACH LEARN, P1231, DOI DOI 10.1145/2505515.2505603
NR 27
TC 0
Z9 0
BN 978-1-4503-5552-0
PY 2018
BP 695
EP 704
DI 10.1145/3219819.3219868
ER

PT B
AU Shashikumar, SP
   Shah, AJ
   Clifford, GD
   Nemati, S
AF Shashikumar, Supreeth P.
   Shah, Amit J.
   Clifford, Gari D.
   Nemati, Shamim
GP ACM
TI Detection of Paroxysmal Atrial Fibrillation using Attention-based
   Bidirectional Recurrent Neural Networks
SO KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY & DATA MINING
CT 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
CY AUG 19-23, 2018
CL London, ENGLAND
DE atrial fibrillation; convolutional neural network; recurrent neural
   network; deep learning; transfer learning
ID PHYSIOLOGICAL TIME-SERIES; QRST CANCELLATION; WAVELET TRANSFORM;
   ENTROPY; BURDEN
AB Detection of atrial fibrillation (AF), a type of cardiac arrhythmia, is difficult since many cases of AF are usually clinically silent and undiagnosed. In particular paroxysmal AF is a form of AF that occurs occasionally, and has a higher probability of being undetected. In this work, we present an attention based deep learning framework for detection of paroxysmal AF episodes from a sequence of windows. Time-frequency representation of 30 seconds recording windows, over a 10 minute data segment, are fed sequentially into a deep convolutional neural network for image-based feature extraction, which are then presented to a bidirectional recurrent neural network with an attention layer for AF detection. To demonstrate the effectiveness of the proposed framework for transient AF detection, we use a database of 24 hour Holter Electrocardiogram (ECG) recordings acquired from 2850 patients at the University of Virginia heart station. The algorithm achieves an AUC of 0.94 on the testing set, which exceeds the performance of baseline models. We also demonstrate the cross-domain generalizablity of the approach by adapting the learned model parameters from one recording modality (ECG) to another (photoplethysmogram) with improved AF detection performance. The proposed high accuracy, low false alarm algorithm for detecting paroxysmal AF has potential applications in long-term monitoring using wearable sensors.
CR Abadi Martin, 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Andreotti Fernando, 2017, COMPUTING CARDIOLOGY, V44
   Bahdanau D., 2014, ARXIV14090473
   Ball J, 2013, INT J CARDIOL, V167, P1807, DOI 10.1016/j.ijcard.2012.12.093
   Behar J, 2013, IEEE T BIO-MED ENG, V60, P1660, DOI 10.1109/TBME.2013.2240452
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Camm AJ, 2012, EUR HEART J, V33, P2719, DOI 10.1093/eurheartj/ehs253
   Carlucci Fabio Maria, 2017, INT C COMP VIS
   Carrara M, 2015, PHYSIOL MEAS, V36, P1873, DOI 10.1088/0967-3334/36/9/1873
   Chorowski J.K., 2015, ADV NEURAL INFORM PR, P577
   Clifford G., 2017, COMPUTING CARDIOLOGY, V44
   Clifford Gari, 2017, PHYSIONET CHALLENGE
   Colloca R, 2013, COMPUT CARDIOL, V40, P1047
   Cook NR, 2009, ANN INTERN MED, V150, P795, DOI 10.7326/0003-4819-150-11-200906020-00007
   Dash S, 2009, ANN BIOMED ENG, V37, P1701, DOI 10.1007/s10439-009-9740-z
   DAUBECHIES I, 1990, IEEE T INFORM THEORY, V36, P961, DOI 10.1109/18.57199
   Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312
   Drew BJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0110274
   Fuster V, 2006, EUROPACE, V8, P651, DOI 10.1093/europace/eul097
   Ghassemi M, 2014, COMPUT CARDIOL, V41, P993
   GILES CL, 1994, IEEE T NEURAL NETWOR, V5, P153
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Grinsted A, 2004, NONLINEAR PROC GEOPH, V11, P561, DOI 10.5194/npg-11-561-2004
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hsu Yen-Chang, 2017, ARXIV171110125
   Johnson AEW, 2015, PHYSIOL MEAS, V36, P1665, DOI 10.1088/0967-3334/36/8/1665
   Johnson AEW, 2014, COMPUT CARDIOL, V41, P281
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lake DE, 2011, AM J PHYSIOL-HEART C, V300, pH319, DOI 10.1152/ajpheart.00561.2010
   Larochelle H, 2010, P ADV NEUR INF PROC, P1243
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Yann, 1995, HDB BRAIN THEORY NEU, V3361, P10
   Li Q, 2008, PHYSIOL MEAS, V29, P15, DOI 10.1088/0967-3334/29/1/002
   Linker David Thor, 2009, US Patent, Patent No. [7,630,756, 7630756]
   Lip GYH, 2001, QJM-INT J MED, V94, P665, DOI 10.1093/qjmed/94.12.665
   Long M., 2016, ARXIV160506636
   Martinez JP, 2004, IEEE T BIO-MED ENG, V51, P570, DOI 10.1109/TBME.2003.821031
   MATLAB, 2016, VERS 9 1 R2016B
   Mnih V., 2014, ADV NEURAL INFORM PR, P2204
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Moody G. B., 1983, Computers in Cardiology 10th Annual Meeting (IEEE Cat. No. 83CH1927-3), P227
   Nemati S, 2016, IEEE ENG MED BIO, P3394, DOI 10.1109/EMBC.2016.7591456
   Peng Zhou, 2016, P 54 ANN M ASS COMP, P207, DOI DOI 10.18653/V1/P16-2034
   Petrenas A, 2012, IEEE T BIO-MED ENG, V59, P2950, DOI 10.1109/TBME.2012.2212895
   Rajpurkar P., 2017, ARXIV170701836
   Richman JS, 2000, AM J PHYSIOL-HEART C, V278, pH2039
   Rush A. M., 2015, ARXIV150900685
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Settles B., 2012, SYNTHESIS LECT ARTIF, V6, P1, DOI [DOI 10.2200/S00429ED1V01Y201207AIM018, 10.2200/S00429ED1V01Y201207AIM018]
   Shashikumar SP, 2017, 2017 IEEE EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL & HEALTH INFORMATICS (BHI), P141, DOI 10.1109/BHI.2017.7897225
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Stewart S, 2002, AM J MED, V113, P359, DOI 10.1016/S0002-9343(02)01236-6
   Stridh M, 2001, IEEE T BIO-MED ENG, V48, P105, DOI 10.1109/10.900266
   Sun B., 2016, AAAI, V6, P8
   Tateno K, 2001, MED BIOL ENG COMPUT, V39, P664, DOI 10.1007/BF02345439
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P2
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wong CX, 2012, ARCH INTERN MED, V172, P739, DOI 10.1001/archinternmed.2012.878
   Wu Y., 2016, ARXIV160908144
   Xu K., 2015, INT C MACH LEARN, P2048
   Yang Z., 2016, HLT NAACL, P1480
   Zhu TT, 2015, ANN BIOMED ENG, V43, P2892, DOI 10.1007/s10439-015-1344-1
   Zhu TT, 2014, ANN BIOMED ENG, V42, P871, DOI 10.1007/s10439-013-0964-6
NR 63
TC 1
Z9 1
BN 978-1-4503-5552-0
PY 2018
BP 715
EP 723
DI 10.1145/3219819.3219912
ER

PT B
AU Di, SM
   Peng, JS
   Shen, YY
   Chen, L
AF Di, Shimin
   Peng, Jingshu
   Shen, Yanyan
   Chen, Lei
GP ACM
TI Transfer Learning via Feature Isomorphism Discovery
SO KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY & DATA MINING
CT 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
CY AUG 19-23, 2018
CL London, ENGLAND
DE Transfer learning; cross-lingual; subgraph isomorphism
AB Transfer learning has gained increasing attention due to the inferior performance of machine learning algorithms with insufficient training data. Most of the previous homogeneous or heterogeneous transfer learning works aim to learn a mapping function between feature spaces based on the inherent correspondence across the source and target domains or labeled instances. However, in many real world applications, existing methods may not be robust when the correspondence across domains is noisy or labeled instances are not representative. In this paper, we develop a novel transfer learning framework called Transfer Learning via Feature Isomorphism Discovery (abbreviated to TLFid), which owns high tolerance for noisy correspondence between domains as well as scarce or non-existing labeled instances. More specifically, we propose a feature isomorphism approach to discovering common substructures across feature spaces and learning a feature mapping function from the target domain to the source domain. We evaluate the performance of TLFid on the cross-lingual sentiment classification tasks. The results show that our method achieves significant improvement in terms of accuracy compared with the state-of-the-art methods.
CR Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Conte Donatello, 2007, Journal of Graph Algorithms and Applications, V11, P99
   Cook S. A., 1971, Proceedings of the 3rd annual ACM symposium on theory of computing, P151
   Cordella LP, 2004, IEEE T PATTERN ANAL, V26, P1367, DOI 10.1109/TPAMI.2004.75
   Dai  Wenyuan, 2009, P NEUR INF PROC SYST, P353
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100
   Duan L., 2012, ARXIV12064660
   Egghe L, 2009, J AM SOC INF SCI TEC, V60, P1027, DOI 10.1002/asi.21009
   Evans JD, 1996, STRAIGHTFORWARD STAT
   Fernandez AM, 2016, J ARTIF INTELL RES, V55, P131
   Grover Aditya, 2016, KDD, V2016, P855
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hoffman Judy, 2013, ARXIV13013224
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolov T., 2013, ADV NEURAL INFORM PR, P1, DOI 10.1162/jmlr.2003.3.4-5.951
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Prettenhofer P, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P1118
   Quan CQ, 2010, COMPUT SPEECH LANG, V24, P726, DOI 10.1016/j.csl.2010.02.002
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shi XX, 2013, IEEE T KNOWL DATA EN, V25, P906, DOI 10.1109/TKDE.2011.252
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang C., 2011, IJCAI, P1541
   Xiao M, 2014, P 28 AAAI C ART INT, P1607
   Zhou J. T., 2014, AAAI, P2213
NR 28
TC 0
Z9 0
BN 978-1-4503-5552-0
PY 2018
BP 1301
EP 1309
DI 10.1145/3219819.3220029
ER

PT B
AU Kumagai, A
   Iwata, T
AF Kumagai, Atsutoshi
   Iwata, Tomoharu
GP ACM
TI Learning Dynamics of Decision Boundaries without Additional Labeled Data
SO KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY & DATA MINING
CT 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
CY AUG 19-23, 2018
CL London, ENGLAND
DE Transfer learning; Semi-supervised learning; Concept drift
AB We propose a method for learning the dynamics of the decision boundary to maintain classification performance without additional labeled data. In various applications, such as spam-mail classification, the decision boundary dynamically changes over time. Accordingly, the performance of classifiers deteriorates quickly unless the classifiers are retrained using additional labeled data. However, continuously preparing such data is quite expensive or impossible. The proposed method alleviates this deterioration in performance by using newly obtained unlabeled data, which are easy to prepare, as well as labeled data collected beforehand. With the proposed method, the dynamics of the decision boundary is modeled by Gaussian processes. To exploit information on the decision boundaries from unlabeled data, the low-density separation criterion, i.e., the decision boundary should not cross high-density regions, but instead lie in low-density regions, is assumed with the proposed method. We incorporate this criterion into our framework in a principled manner by introducing the entropy posterior regularization to the posterior of the classifier parameters on the basis of the generic regularized Bayesian framework. We developed an efficient inference algorithm for the model based on variational Bayesian inference. The effectiveness of the proposed method was demonstrated through experiments using two synthetic and four real-world data sets.
CR Abdallah ZS, 2012, PROC INT C TOOLS ART, P1163, DOI 10.1109/ICTAI.2012.169
   Babcock B., 2002, P 21 ACM SIGMOD SIGA, P1, DOI DOI 10.1145/543613.543615
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Billsus D, 2000, USER MODEL USER-ADAP, V10, P147, DOI 10.1023/A:1026501525781
   Bishop CM., 2006, PATTERN RECOGNITION
   Bitarafan A, 2016, IEEE T KNOWL DATA EN, V28, P2128, DOI 10.1109/TKDE.2016.2551241
   Crammer Koby, 2009, NIPS
   Dyer KB, 2014, IEEE T NEUR NET LEAR, V25, P12, DOI 10.1109/TNNLS.2013.2277712
   Freund Yoav, 1996, ICML
   Gama J, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2523813
   Gao Jin, 2014, ECCV
   Goldberg Andrew B, 2008, ECML PKDD
   Gomes Heitor M, 2017, MACH LEARN, P1
   Goodfellow  I., 2016, DEEP LEARNING, V1
   Grandvalet Y., 2004, NIPS
   Hoffman J., 2014, CVPR
   Kalman R. E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kanamori Takafumi, 2009, NIPS
   Kingma Diederik P, 2014, NIPS, P4
   Kingma Diederik P., 2014, ICLR
   Klinkenberg R., 2004, Intelligent Data Analysis, V8, P281
   Kolter JZ, 2007, J MACH LEARN RES, V8, P2755
   Koychev I, 2000, P ECAI 2000 WORKSH C
   Kumagai Atsutoshi, 2017, IJCAI
   Kumagai Atsutoshi, 2016, AAAI
   Kumagai Atsutoshi, 2017, AAAI
   Li Peipei, 2010, ACML
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Long M., 2016, NIPS
   Long M., 2017, ICML
   Ma Justin, 2009, ICML
   Miyato  T., 2016, ICLR
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Rasmussen C. E, 2006, GAUSSIAN PROCESS MAC
   Ruvolo Paul, 2013, ICML
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Souza Vinicius MA, 2015, SDM
   Sykacek Peter, 2003, NIPS
   Tang K., 2012, NIPS
   Thang D., 2018, ICLR
   Umer Muhammad, 2017, IJCAN
   Wang Boyu, 2015, AAAI
   Wang Haixun, 2003, SIGKDD
   Wang J, 2012, ICML
   Yoon JW, 2009, NEURAL NETWORKS, V22, P1286, DOI 10.1016/j.neunet.2009.06.005
   Zenke Friedemann, 2017, ICML
   Zhang P, 2009, IEEE DATA MINING, P627, DOI 10.1109/ICDM.2009.76
   Zhao Peilin, 2010, ICML
   Zhu Jun, 2014, JMLR, V15, P17
   Zliobaite I, 2014, IEEE T NEUR NET LEAR, V25, P27, DOI 10.1109/TNNLS.2012.2236570
NR 51
TC 0
Z9 0
BN 978-1-4503-5552-0
PY 2018
BP 1627
EP 1636
DI 10.1145/3219819.3219967
ER

PT B
AU Satoh, S
   Takahashi, Y
   Yamakawa, H
AF Satoh, Seiya
   Takahashi, Yoshinobu
   Yamakawa, Hiroshi
GP ACM
TI Accelerated Equivalence Structure Extraction via Pairwise Incremental
   Search
SO KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON
   KNOWLEDGE DISCOVERY & DATA MINING
CT 24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
CY AUG 19-23, 2018
CL London, ENGLAND
DE equivalence structure; transfer learning; imitation learning
AB Equivalence structure (ES) extraction can allow for finding correspondence relations between different sequential datasets. A K-dimensional ES is a set of K-tuples to specify K-dimensional sequences that are considered equivalent. Whether or not two K-dimensional sequences are equivalent is decided based on comparisons of all of their subsequences. ES extraction can be used for preprocessing for transfer learning or imitation learning, as well as an analysis of multidimensional sequences. A recently proposed method called incremental search (IS) was much faster than brute-force search. However, IS can still take a long time to obtain ESs, because ESs obtained by IS can be subsets of other ESs and such subsets must be removed in the process. In this paper, we propose a new fast method called pairwise incremental search (PIS). In the process of PIS, the aforementioned problem about subsets of ESs does not exist, because the elements of ESs are searched pairwise. As shown by results of two experiments we conducted, PIS was 48 times faster than IS in an experiment using synthetic datasets and 171 times faster in an experiment using motion capture datasets.
CR Du Simon S., 2017, ADV NEURAL INFORM PR, P574
   Duan  Yan, 2017, ADV NEURAL INF PROCE, V30, P1087
   Hussein A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054912
   Luo Zelun, 2017, ADV NEURAL INFORM PR, P164
   Yeh CCM, 2016, IEEE DATA MINING, P1317, DOI [10.1109/ICDM.2016.89, 10.1109/ICDM.2016.0179]
   Minnen D, 2007, IEEE DATA MINING, P601, DOI 10.1109/ICDM.2007.52
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Satoh S, 2017, FRONT ROBOT AI, V4, DOI 10.3389/frobt.2017.00063
   Satoh S, 2017, IEEE IJCNN, P1518, DOI 10.1109/IJCNN.2017.7966032
   Sipser Michael, 2006, INTRO THEORY COMPUTA, V2
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Yeh Chin-Chia Michael, 2017, 2017 IEEE 17 INT C D
   Zeestraten MJA, 2017, IEEE ROBOT AUTOM LET, V2, P1240, DOI 10.1109/LRA.2017.2657001
NR 13
TC 0
Z9 0
BN 978-1-4503-5552-0
PY 2018
BP 2160
EP 2169
DI 10.1145/3219819.3220011
ER

PT S
AU Yin, HW
   Li, FZ
   Zhang, L
AF Yin, Hongwei
   Li, Fanzhang
   Zhang, Li
GP IEEE
TI Multi-Source Clustering based on spectral recovery
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
DE multi-source clustering; Laplace operator; spectral learning; multi-view
   spectral embedding
AB The research and analysis on multi-source data is one of important tasks in information science. Compared with traditional single-source data learning algorithms, multi-source data learning ones can describe objects more real and complete. Meanwhile, the learning process of multi-source data is more in line with the cognitive mechanism of human brain. So far, the research on multi-source data learning algorithms includes three classes, multi-source data transfer learning, multi-source data collaborative learning and multi-source multi-view learning. The traditional multi-source multi-view learning algorithms lack the ability of handling with the data missing issue, which means that these algorithms require the multi-source data to be complete. This paper proposes a multi-source clustering algorithm. Based on the spectral properties of Laplace operator, we first obtain the complete representation of multi-source data. Then, we utilize the multi-view spectral embedding (MVSE) to construct the fusion model. Experimental results show that our proposed method can improve the ability of clustering efficiently in the case of data missing.
CR Dhillon  P., 2011, ADV NEURAL INFORM PR, P199
   FAN K, 1949, P NATL ACAD SCI USA, V35, P652, DOI 10.1073/pnas.35.11.652
   Fang M, 2015, PATTERN RECOGN LETT, V51, P101, DOI 10.1016/j.patrec.2014.08.011
   Feng L., 2016, S CHINA GEOL MAG, P1
   Fromont E, 2004, LECT NOTES ARTIF INT, V3202, P503
   Guven B, 2012, LINEAR ALGEBRA APPL, V436, P3337, DOI 10.1016/j.laa.2011.11.028
   Haeffele B. D., 2014, P 31 INT C MACH LEAR, V32, P2007, DOI 10.1109/ICGPR.2014.6970495
   Kumar A, 2011, ADV NEURAL INFORM PR, V24, P1413
   Kumar Abhishek, 2011, P 28 INT C MACH LEAR, V94, P393
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li S.-Y., 2014, AAAI, P1968
   Li Y., 2015, P 29 AAAI C ART INT, V29, P2750
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nie  F., 2016, P 30 AAAI C ART INT, P1969
   Ouyang WL, 2014, PROC CVPR IEEE, P2337, DOI 10.1109/CVPR.2014.299
   Rosasco L, 2010, J MACH LEARN RES, V11, P905
   Shao WX, 2016, IEEE IJCNN, P2714, DOI 10.1109/IJCNN.2016.7727540
   Sun SL, 2013, NEURAL COMPUT APPL, V23, P2031, DOI 10.1007/s00521-013-1362-6
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Xia  R., 2014, AAAI, P2149
   Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566
   Xiao Cai, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1977, DOI 10.1109/CVPR.2011.5995740
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Xu XX, 2016, IEEE T PATTERN ANAL, V38, P1113, DOI 10.1109/TPAMI.2015.2476813
   Xu ZJ, 2012, LECT NOTES COMPUT SC, V7665, P332, DOI 10.1007/978-3-642-34487-9_41
   Yager RR, 2004, INFORM SCIENCES, V163, P175, DOI 10.1016/j.ins.2003.03.018
   Yin Hongwei, 2015, Journal of Frontiers of Computer Science and Technology, V9, P1409, DOI 10.3778/j.issn.1673-9418.1505049
   Yu Zheng, 2015, IEEE Transactions on Big Data, V1, P16, DOI 10.1109/TBDATA.2015.2465959
   Zhou D., 2007, P 24 INT C MACH LEAR, P1159, DOI DOI 10.1145/1273496.1273642
NR 29
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 231
EP 236
ER

PT S
AU Lu, W
   Chung, FL
AF Lu, Wei
   Chung, Fu-lai
GP IEEE
TI A Deep Graphical Model for Layered Knowledge Transfer
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
ID DOMAIN ADAPTATION
AB Deep architectures can now be well trained on massive labeled data. However, there exist many application scenarios where labeled data are sparse or absent. Domain adaptation and multi-task transfer learning provide attractive options when related labeled data or tasks are abundant from different domains. In this paper, a new graphical modeling approach to multi-layer factorization based domain adaptation is explored to address the scenarios that insufficient labeled data are available for supervised learning. A deep convolutional factorization based transfer learning (DCFTL) algorithm is proposed to facilitate layer-wise transfer learning between domains. Completely based on graphical model representation, the proposed framework can seamlessly merge inference and learning, and has clear interpretability of conditional independence. The empirical performances on image classification tasks in both supervised and semi-supervised adaptation settings illustrate the effectiveness and generalization of the proposed deep layered knowledge transfer framework.
CR Chen B., 2013, IEEE T PATTERN ANAL, V35
   Chen M., 2014, P MACHINE LEARNING R, V32, P1476
   Chopra S, 2005, PROC CVPR IEEE, P539
   Daume H, 2006, J ARTIF INTELL RES, V26, P101, DOI 10.1613/jair.1872
   Duan LX, 2012, IEEE T NEUR NET LEAR, V23, P504, DOI 10.1109/TNNLS.2011.2178556
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Ganin  Y., 2015, INT C MACH LEARN, P1180
   Ganin Y, 2016, J MACH LEARN RES, V17
   Glorot X., 2011, P 28 INT C MACH LEAR, P513
   Hinton G. E., 2009, SCHOLARPEDIA, V4, P5947, DOI DOI 10.4249/SCH0LARPEDIA.5947
   Kandemir M., 2015, P 32 INT C MACH LEAR, P730
   Lee H., 2009, P ANN INT C MACH LEA, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]
   Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Saenko K., 2010, ADAPTING VISUAL CATE, P213
   Saul L, 1998, NATO ADV SCI I D-BEH, V89, P541
   Shao M, 2014, INT J COMPUT VISION, V109, P74, DOI 10.1007/s11263-014-0696-6
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xiao  T., 2016, ARXIV160407528
   Zhou  B., 2014, ADV NEURAL INFORM PR, P487, DOI DOI 10.1162/153244303322533223
NR 22
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 260
EP 265
ER

PT S
AU Manessi, F
   Rozza, A
   Bianco, S
   Napoletano, P
   Schettini, R
AF Manessi, Franco
   Rozza, Alessandro
   Bianco, Simone
   Napoletano, Paolo
   Schettini, Raimondo
GP IEEE
TI Automated Pruning for Deep Neural Network Compression
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
AB In this work we present a method to improve the pruning step of the current state-of-the-art methodology to compress neural networks. The novelty of the proposed pruning technique is in its differentiability, which allows pruning to be performed during the backpropagation phase of the network training. This enables an end-to-end learning and strongly reduces the training time. The technique is based on a family of differentiable pruning functions and a new regularizer specifically designed to enforce pruning. The experimental results show that the joint optimization of both the thresholds and the network weights permits to reach a higher compression rate, reducing the number of weights of the pruned network by a further 14% to 33% compared to the current state-of-the-art. Furthermore, we believe that this is the first study where the generalization capabilities in transfer learning tasks of the features extracted by a pruned network are analyzed. To achieve this goal, we show that the representations learned using the proposed pruning methodology maintain the same effectiveness and generality of those learned by the corresponding non-compressed network on a set of different recognition tasks.
CR Abadi Martin, 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Arandjelovic R, 2011, IEEE I CONF COMP VIS, P375, DOI 10.1109/ICCV.2011.6126265
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Chen W., 2015, P INT C MACH LEARN, P2285
   Chollet F., 2015, KERAS
   Coates Adam, 2011, J MACHINE LEARNING R, P215
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123
   Denil M., 2013, ADV NEURAL INFORM PR, V26, P2148
   Denton E. L., 2014, ADV NEURAL INFORM PR, V27, P1269
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Glorot X., 2010, JLMR P TRACK, V9, P249, DOI DOI 10.1.1/207.2059
   Gong Y., 2014, ARXIV14126115
   Gregor K., 2010, ARXIV10060448
   Han S, 2015, ADV NEURAL INFORM PR, P1135
   Han S., 2016, ICLR
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   HANSON SJ, 1989, ADV NEURAL INFORMATI, V1, P177
   HASSIBI B, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P293, DOI 10.1109/ICNN.1993.298572
   Hubara I., 2016, ADV NEURAL INFORM PR, P4107
   Hwang K., 2014, SIGN PROC SYST SIPS, P1, DOI [10.1109/SiPS.2014.6986082, DOI 10.1109/SIPS.2014.6986082]
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Kingma D., 2015, ICLR
   Konda K., 2014, ARXIV14023337
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Le Cun Y., 1990, ADV NEURAL INFORMATI, V2, P598
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1998, MNIST DATABASE HANDW
   MICHALSKI RS, 1983, ARTIF INTELL, V20, P111, DOI 10.1016/0004-3702(83)90016-4
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Nister D, 2006, IEEE COMP SOC C COMP, V2, P2161, DOI DOI 10.1109/CVPR.2006.264
   Philbin J., 2007, COMPUT VIS PATTERN R, P1, DOI DOI 10.1109/CVPR.2007.383172
   Philbin J., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587635
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486
   Shwartz-Ziv R., 2017, ARXIV170300810
   Thrun S., 2012, LEARNING LEARN
   Vanhoucke V., 2011, P DEEP LEARN UNS FEA, V1, P4
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 44
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 657
EP 664
ER

PT S
AU Yan, YP
   Rangarajan, A
   Ranka, S
AF Yan, Yupeng
   Rangarajan, Anand
   Ranka, Sanjay
GP IEEE
TI An Efficient Deep Representation Based Framework for Large-Scale Terrain
   Classification
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
DE remote sensing; superpixel segmentation; convolutional neural network;
   transfer learning; feature selection; semi-supervised learning
AB In this paper, we present a novel terrain classification framework for large-scale remote sensing images. A well-performing multi-scale superpixel tessellation based segmentation approach is employed to generate homogeneous and irregularly shaped regions, and a transfer learning technique is sequentially deployed to derive representative deep features by utilizing successful pre-trained convolutional neural network (CNN) models. This design is aimed to overcome the big problem of lacking available ground-truth data and to increase the generalization power of the multi-pixel descriptor. In the subsequent classification step, we train a fast and robust support vector machine (SVM) to assign the pixel-level labels. Its maximum-margin property can be easily combined with a graph Laplacian propagation approach. Moreover, we analyze the advantages of applying a feature selection technique to the deep CNN features which are extracted by transfer learning. In the experiments, we evaluate the whole framework based on different geographical types. Compared with other region-based classification methods, the results show that our framework can obtain state-of-the-art performance w.r.t. both classification accuracy and computational efficiency.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110
   Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Athiwaratkun B, 2015, ARXIV150702313
   Audebert N, 2016, INT GEOSCI REMOTE SE, P5091, DOI 10.1109/IGARSS.2016.7730327
   Castelluccio M., 2015, ARXIV150800092
   Chen YS, 2016, IEEE T GEOSCI REMOTE, V54, P6232, DOI 10.1109/TGRS.2016.2584107
   Cheng G, 2015, IEEE T GEOSCI REMOTE, V53, P4238, DOI 10.1109/TGRS.2015.2393857
   Gu Q., 2012, ARXIV12023725
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Maggiori E, 2017, IEEE T GEOSCI REMOTE, V55, P645, DOI 10.1109/TGRS.2016.2612821
   Penatti Otavio A. B., 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P44, DOI 10.1109/CVPRW.2015.7301382
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sethi M., 2015, P 21 ACM SIGKDD INT, P2069
   Sethi M, 2014, INT CONF CONTEMP, P635, DOI 10.1109/IC3.2014.6897247
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Vatsavai Ranga Raju, 2013, 19 ACM SIGKDD INT C, P1419
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Yan Y., 2013, MULT EXP ICME 2013 I, P1
   Yan Y., 2017, INT J BIG DATA INTEL, V4, P108, DOI [10.1504/IJBDI.2017.083130, DOI 10.1504/IJBDI.2017.083130]
   Yu W., 2016, INT C MACH LEARN ICM
   Yupeng Yan, 2017, 2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech). Proceedings, P1127, DOI 10.1109/DASC-PICom-DataCom-CyberSciTec.2017.182
   Zhang GY, 2015, IEEE T GEOSCI REMOTE, V53, P5861, DOI 10.1109/TGRS.2015.2423688
NR 25
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 940
EP 945
ER

PT S
AU Yi, HY
   Xu, Z
   Wen, YM
   Fan, ZG
AF Yi, Haiyang
   Xu, Zhi
   Wen, Yimin
   Fan, Zhigang
GP IEEE
TI Multi-source Domain Adaptation for Face Recognition
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
DE domain adaptation; multi-source transfer learning; common subspace
   learning; face recognition
AB For transfer learning, many research works have demonstrated that effective use of information from multi-source domains will improve classification performance. In this paper, we propose a method of Targetize Multi-source Domain Bridged by Common Subspace (TMSD) for face recognition, which transfers rich supervision knowledge from more than one labeled source domains to the unlabeled target domain. Specifically, a common subspace is learnt for several domains by keeping the maximum total correlation. In this way, the discrepancy of each domain is reduced, and the structures of both the source and target domains are well preserved for classification. In the common subspace, each sample projected from the source domains is sparsely represented as a linear combination of several samples projected from the target domain, such that the samples projected from different domains can be well interlaced. Then, in the original image space, each source domain image can be represented as a linear combination of neighbors in the target domain. Finally, the discriminant subspace can be obtained by targetized multi-source domain images using supervised learning algorithm. The experimental results illustrate the superiority of TMSD over those competitive ones.
CR Banerjee S., 2014, IND C COMP VIS GRAPH
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Blitzer J., 2006, P 2006 C EMP METH NA, P120
   Chattopadhyay R., 2011, P 17 ACM SIGKDD INT, P717
   Deng J, 2017, IEEE SIGNAL PROC LET, V24, P500, DOI 10.1109/LSP.2017.2672753
   Dredze M, 2010, MACH LEARN, V79, P123, DOI 10.1007/s10994-009-5148-0
   Duan L, 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Efron B, 2004, ANN STAT, V32, P407
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Ho HT, 2014, INT J COMPUT VISION, V109, P110, DOI 10.1007/s11263-014-0720-x
   Kan MN, 2014, INT J COMPUT VISION, V109, P94, DOI 10.1007/s11263-013-0693-1
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Peng Y, 2017, NEUROCOMPUTING, V261, P242, DOI 10.1016/j.neucom.2016.05.113
   Qiu Q, 2015, IEEE T IMAGE PROCESS, V24, P5152, DOI 10.1109/TIP.2015.2479456
   Rupnik  J., 2010, P C DAT MIN DAT WAR, P1
   Shao M, 2012, IEEE DATA MINING, P1104, DOI 10.1109/ICDM.2012.102
   Shawe-Taylor J, 2011, NEUROCOMPUTING, V74, P3609, DOI 10.1016/j.neucom.2011.06.026
   Shi  Y., 2012, P 29 INT C MACH LEAR, P1275
   Sun SN, 2017, NEUROCOMPUTING, V257, P79, DOI 10.1016/j.neucom.2016.11.063
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Uribe D, 2010, 2010 Ninth International Conference on Machine Learning and Applications (ICMLA 2010), P857, DOI 10.1109/ICMLA.2010.133
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
NR 29
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 1349
EP 1354
ER

PT S
AU Elezi, I
   Torcinovich, A
   Vascon, S
   Pelillo, M
AF Elezi, Ismail
   Torcinovich, Alessandro
   Vascon, Sebastiano
   Pelillo, Marcello
GP IEEE
TI Transductive Label Augmentation for Improved Deep Network Learning
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
ID ALGORITHM
AB A major impediment to the application of deep learning to real-world problems is the scarcity of labeled data. Small training sets are in fact of no use to deep networks as, due to the large number of trainable parameters, they will very likely be subject to overfitting phenomena. On the other hand, the increment of the training set size through further manual or semi-automatic labellings can be costly, if not possible at times. Thus, the standard techniques to address this issue are transfer learning and data augmentation, which consists of applying some sort of "transformation" to existing labeled instances to let the training set grow in size. Although this approach works well in applications such as image classification, where it is relatively simple to design suitable transformation operators, it is not obvious how to apply it in more structured scenarios. Motivated by the observation that in virtually all application domains it is easy to obtain unlabeled data, in this paper we take a different perspective and propose a label augmentation approach. We start from a small, curated labeled dataset and let the labels propagate through a larger set of unlabeled data using graph transduction techniques. This allows us to naturally use (second-order) similarity information which resides in the data, a source of information which is typically neglected by standard augmentation techniques. In particular, we show that by using known game theoretic transductive processes we can create larger and accurate enough labeled datasets which use results in better trained neural networks. Preliminary experiments are reported which demonstrate a consistent improvement over standard image classification datasets.
CR CIRESAN D, 2012, PROC CVPR IEEE, P3642, DOI [DOI 10.1109/CVPR.2012.6248110, 10.1109/CVPR.2012.6248110]
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Erdem A, 2012, NEURAL COMPUT, V24, P700, DOI 10.1162/NECO_a_00233
   FUKUSHIMA K, 1982, PATTERN RECOGN, V15, P455, DOI 10.1016/0031-3203(82)90024-3
   Griffin G., 2007, TECH REP
   Haeusser P, 2017, PROC CVPR IEEE, P626, DOI 10.1109/CVPR.2017.74
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   HOWSON JT, 1972, MANAGE SCI, V18, P312, DOI 10.1287/mnsc.18.5.312
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kadar I., 2014, EUR C COMP VIS ECCV, P385
   Kingma D. P., 2014, ADV NEURAL INFORM PR, P3581
   Kingma D. P., 2014, INT C LEARN REPR ICL
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Krizhevsky G. H. Alex, 2009, LEARNING MULTIPLE LA
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee D. hyun, 2013, ICML, V2, P3
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maynard Smith J., 1982, EVOLUTION THEORY GAM
   MILLER DA, 1991, OPER RES LETT, V10, P285, DOI 10.1016/0167-6377(91)90015-H
   NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529
   Pelillo M, 1997, J MATH IMAGING VIS, V7, P309, DOI 10.1023/A:1008255111261
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tripodi R, 2016, INT C PATT RECOG, P1719, DOI 10.1109/ICPR.2016.7899884
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vascon S., 2018, PATTERN RECOGNITION
   WEIBULL J. W., 1997, EVOLUTIONARY GAME TH
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
   Zelnik-Manor L., 2005, ADV NEURAL INFORM PR, V17, P1601
   Zhu X., 2005, THESIS
NR 34
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 1432
EP 1437
ER

PT S
AU Combinido, JS
   Mendoza, JR
   Aborot, J
AF Samuel Combinido, Jay
   Robert Mendoza, John
   Aborot, Jeffrey
GP IEEE
TI A Convolutional Neural Network Approach for Estimating Tropical Cyclone
   Intensity Using Satellite-based Infrared Images
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
AB Existing techniques for satellite-based tropical cyclone (TC) intensity estimation involve an explicit feature extraction step to model TC intensity on a set of relevant TC features or patterns such as eye formation and cloud organization. However, crafting such a feature set is often time-consuming and requires expert knowledge. In this paper, a convolutional neural network (CNN) approach, which eliminates explicit feature extraction, for estimating the intensity of tropical cyclones is proposed. Utilizing a Visual Geometry Group 19-layer CNN (VGG19) model pre-trained on ImageNet, transfer learning experiments were performed using grayscale IR images of TCs obtained from various geostationary satellites in the Western North Pacific region (1996 - 2016) to estimate TC intensity. The model re-trained on TC images achieved a root-mean-square error (RMSE) of 13.23 knots - a performance comparable to existing feature-based approaches (RMSE ranging from 12 to 20 knots). Moreover, the model was able to learn generic TC features that were previously identified in feature-based approaches as important indicators of TC intensity.
CR Bankert RL, 2002, J APPL METEOROL, V41, P461, DOI 10.1175/1520-0450(2002)041<0461:AAMTET>2.0.CO;2
   Bessho K, 2016, J METEOROL SOC JPN, V94, P151, DOI 10.2151/jmsj.2016-009
   Castelluccio M., 2015, ARXIV150800092
   Chandra R., 2015, LECT NOTES COMPUTER, V9491
   Chollet F., 2016, CONVOLUTIONAL NEURAL
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dvorak V. F., 1984, 11 NAT OC ATM ADM, V11
   DVORAK VF, 1975, MON WEATHER REV, V103, P420, DOI 10.1175/1520-0493(1975)103<0420:TCIAAF>2.0.CO;2
   GENTRY RC, 1980, MON WEATHER REV, V108, P445, DOI 10.1175/1520-0493(1980)108<0445:PTCIUS>2.0.CO;2
   GMS (Geostationary Meteorological Satellite) Series of Japan, GMS GEOST MET SAT SE
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Huang ZL, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090907
   Jaiswal N, 2012, ATMOS RES, V118, P215, DOI 10.1016/j.atmosres.2012.07.006
   Japan Meteorological Agency (JMA), MET SAT JAP MET AG J
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kingma D.P., 2015, P 3 INT C LEARN REPR
   Koba H., 1991, Geophysical Magazine, V44, P15
   Koba H., 1989, J METEOR RES, V41, P157
   Kochi University, 2015, GMS GOES9 MTSAT DAT
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kulkarni A., 2010, ASPRS ANN C SAN DIEG
   Langkvist M, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8040329
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu Y., 2016, INT C ADV BIG DAT AN
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pineros MF, 2011, WEATHER FORECAST, V26, P690, DOI 10.1175/WAF-D-10-05062.1
   Pineros MF, 2008, IEEE T GEOSCI REMOTE, V46, P3574, DOI 10.1109/TGRS.2008.2000819
   Roy C., 2016, THESIS
   Sakuragi T., 2014, TECH REP, V16
   Simonyan Karen, 2015, P IEEE C COMP VIS PA
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Velden C, 2006, B AM METEOROL SOC, V87, P1195, DOI 10.1175/BAMS-87-9-1195
   Velden CS, 1998, WEATHER FORECAST, V13, P172, DOI 10.1175/1520-0434(1998)013<0172:DOAOST>2.0.CO;2
   Yao SX, 2016, ATMOSPHERE-BASEL, V7, DOI 10.3390/atmos7010005
NR 35
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 1474
EP 1480
ER

PT S
AU Das, A
   Roy, S
   Bhattacharya, U
   Parui, SK
AF Das, Arindam
   Roy, Saikat
   Bhattacharya, Ujjwal
   Parui, Swapan K.
GP IEEE
TI Document Image Classification with Intra-Domain Transfer Learning and
   Stacked Generalization of Deep Convolutional Neural Networks
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
DE document structure learning; deep convolutional neural network; document
   recognition; deep learning; transfer learning; intra-domain; neural
   network
AB In this article, a region-based Deep Convolutional Neural Network framework is presented for document structure learning. The contribution of this work involves efficient training of region based classifiers and effective ensembling for document image classification. A primary level of 'inter-domain' transfer learning is used by exporting weights from a pre-trained VGG16 architecture on the ImageNet dataset to train a document classifier on whole document images. Exploiting the nature of region based influence modelling, a secondary level of 'intra-domain' transfer learning is used for rapid training of deep learning models for image segments. Finally, a stacked generalization based ensembling is utilized for combining the predictions of the base deep neural network models. The proposed method achieves state-of-the-art accuracy of 92.21% on the popular RVL-CDIP document image dataset, exceeding the benchmarks set by the existing algorithms.
CR Afzal M. Z., 2009, ARXIV170403557
   Afzal M. Z., 2015, P 13 INT C DOC AN RE, P1111
   Appiani E., 2001, International Journal on Document Analysis and Recognition, V4, P69, DOI 10.1007/PL00010904
   Cesarini F., 2001, Proceedings of Sixth International Conference on Document Analysis and Recognition, P1131, DOI 10.1109/ICDAR.2001.953962
   Chen N, 2007, INT J DOC ANAL RECOG, V10, P1, DOI 10.1007/s10032-006-0020-2
   Csurka G., 2016, ARXIV160301076
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dengel A., 1993, Proceedings of the Second International Conference on Document Analysis and Recognition (Cat. No.93TH0578-5), P86, DOI 10.1109/ICDAR.1993.395776
   Dengel A., 1994, TECH REP
   Diligenti M, 2003, IEEE T PATTERN ANAL, V25, P519, DOI 10.1109/TPAMI.2003.1190578
   Gordo A, 2013, PATTERN RECOGN, V46, P1898, DOI 10.1016/j.patcog.2012.12.004
   Harley AW, 2015, 2015 13TH IAPR INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION (ICDAR), P991, DOI 10.1109/ICDAR.2015.7333910
   Heroux P, 1998, INT C PATT RECOG, P926, DOI 10.1109/ICPR.1998.711385
   Hoch R., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P31
   Hu J., 2000, Information Retrieval, V2, P227, DOI 10.1023/A:1009910911387
   Junker M, 1997, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS 1 AND 2, P1060, DOI 10.1109/ICDAR.1997.620671
   Kang L, 2014, INT C PATT RECOG, P3168, DOI 10.1109/ICPR.2014.546
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Kolsch A., 2017, ARXIV171105862
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kumar J, 2014, PATTERN RECOGN LETT, V43, P119, DOI 10.1016/j.patrec.2013.10.030
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   RABINER LR, 1989, READINGS SPEECH RECO, V77, P257
   Roy S, 2016, INT C PATT RECOG, P1273, DOI 10.1109/ICPR.2016.7899812
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shin C., 2001, International Journal on Document Analysis and Recognition, V3, P232, DOI 10.1007/PL00013566
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang Y., 1991, P INT C DOC AN REC, P17
   Tensmeyer C., 2017, ARXIV170803273
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
NR 35
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 3180
EP 3185
ER

PT S
AU Nguyen, D
   Nguyen, K
   Sridharan, S
   Abbasnejad, I
   Dean, D
   Fookes, C
AF Dung Nguyen
   Kien Nguyen
   Sridharan, Sridha
   Abbasnejad, Iman
   Dean, David
   Fookes, Clinton
GP IEEE
TI Meta Transfer Learning for Facial Emotion Recognition
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
AB The use of deep learning techniques for automatic facial expression recognition has recently attracted great interest but developed models are still unable to generalize well due to the lack of large emotion datasets for deep learning. To overcome this problem, in this paper, we propose utilizing a novel transfer learning approach relying on PathNet and investigate how knowledge can be accumulated within a given dataset and how the knowledge captured from one emotion dataset can be transferred into another in order to improve the overall performance. To evaluate the robustness of our system, we have conducted various sets of experiments on two emotion datasets: SAVEE and eNTERFACE. The experimental results demonstrate that our proposed system leads to improvement in performance of emotion recognition and performs significantly better than the recent state-of-the-art schemes adopting fine-tuning/pre-trained approaches.
CR Abbasnejad I., 2017, ICCV
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Fernando C., 2017, CORR
   Ghasemi A., 2017, 2017 ICIP
   Gideon J., 2017, CORR
   Hamester  D., 2015, 2015 INT JOINT C NEU, P1
   Haq S., 2009, P INT C AUD VIS SPEE
   Harvey Inman, 2011, ADV ARTIFICIAL LIFE, V5778, P126
   Hasani B, 2017, IEEE COMPUT SOC CONF, P2278, DOI 10.1109/CVPRW.2017.282
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Howard A. G., 2017, CORR
   Jung H., 2015, CORR
   Kaya H, 2017, IMAGE VISION COMPUT, V65, P66, DOI 10.1016/j.imavis.2017.01.012
   Krizhevsky A., 2009, THESIS
   Lee  Sang-Woo, 2017, NIPS, P4655
   Lopes AT, 2015, SIBGRAPI, P273, DOI 10.1109/SIBGRAPI.2015.14
   Mallya A., 2017, CORR
   Martin O., 2006, P 22 INT C DAT ENG W, P8
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Nguyen D, 2017, IEEE WINT CONF APPL, P1215, DOI 10.1109/WACV.2017.140
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Rusu A. A., 2016, CORR
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Szegedy C., 2017, AAAI
   Szegedy C., 2015, ARXIV151200567
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Victor-Emil N., 2013, RECENT ADV IMAGE AUD, P93
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Yunfan Liu, 2014, 2014 International Conference on Smart Computing (SMARTCOMP), P125, DOI 10.1109/SMARTCOMP.2014.7043849
   Zhang SQ, 2018, IEEE T CIRC SYST VID, V28, P3030, DOI 10.1109/TCSVT.2017.2719043
NR 33
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 3543
EP 3548
ER

PT S
AU Niu, XS
   Han, H
   Shan, SG
   Chen, XL
AF Niu, Xuesong
   Han, Hu
   Shan, Shiguang
   Chen, Xilin
GP IEEE
TI SynRhythm: Learning a Deep Heart Rate Estimator from General to Specific
SO 2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
CT 24th International Conference on Pattern Recognition (ICPR)
CY AUG 20-24, 2018
CL Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA
HO Chinese Acad Sci, Inst Automat
ID NONCONTACT
AB Remote photoplethysmography (rPPG) based non-contact heart rate (HR) measurement from a face video has drawn increasing attention recently because of its potential applications in many scenarios such as training aid, health monitoring, and nursing care. Although a number of methods have been proposed, most of them are designed under certain assumptions and could fail when such assumptions do not hold. At the same time, while deep learning based methods have been reported to achieve promising results in many computer vision tasks, their use in rPPG-based heart rate estimation has been limited due to the very limited data available in public domain. To overcome this limitation and leverage the strong modeling ability of deep neural networks, in this paper, we propose a novel spatial-temporal representation for the HR signal and design a general-to-specific transfer learning strategy to train a deep heart rate estimator from a large volume of synthetic rhythm signals and a limited number of available face video data. Experiment results on the public-domain databases show the effectiveness of the proposed approach.
CR Balakrishnan G, 2013, PROC CVPR IEEE, P3430, DOI 10.1109/CVPR.2013.440
   BLAND JM, 1986, LANCET, V1, P307
   de Haan G, 2013, IEEE T BIO-MED ENG, V60, P2878, DOI 10.1109/TBME.2013.2266196
   Gee-Sern Hsu M.-S. C., 2017, P IJCB
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kumar M, 2015, BIOMED OPT EXPRESS, V6, P1565, DOI 10.1364/BOE.6.001565
   Kwon S., 2015, P EMBS, P851
   Lam A, 2015, IEEE I CONF COMP VIS, P3640, DOI 10.1109/ICCV.2015.415
   Li XB, 2014, PROC CVPR IEEE, P4264, DOI 10.1109/CVPR.2014.543
   Mishkin D., 2015, ARXIV151106422
   Niu X., 2017, P IJCB
   Patel K, 2016, LECT NOTES COMPUT SC, V9967, P611, DOI 10.1007/978-3-319-46654-5_67
   Poh MZ, 2011, IEEE T BIO-MED ENG, V58, P7, DOI 10.1109/TBME.2010.2086456
   Poh MZ, 2010, OPT EXPRESS, V18, P10762, DOI 10.1364/OE.18.010762
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Tulyakov S., 2016, P IEEE CVPR
   Verkruysse W, 2008, OPT EXPRESS, V16, P21434, DOI 10.1364/OE.16.021434
   Wang WJ, 2015, IEEE T BIO-MED ENG, V62, P415, DOI 10.1109/TBME.2014.2356291
   White B, 2015, IEEE W CONTR MODEL
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   YungChien Hsu, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4433, DOI 10.1109/ICASSP.2014.6854440
NR 24
TC 0
Z9 0
SN 1051-4651
BN 978-1-5386-3788-3
PY 2018
BP 3580
EP 3585
ER

PT B
AU Yang, K
   Wan, WG
   Lu, J
AF Yang, Kai
   Wan, Wanggen
   Lu, Jie
GP IEEE
TI Domain Adaptation for Gaussian Process Classification
SO 2018 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING
   (ICALIP)
CT International Conference on Audio, Language and Image Processing
   (ICALIP)
CY JUL 16-17, 2018
CL Shanghai, PEOPLES R CHINA
DE Gaussian Process; Domain Adaptation; Transfer Learning; Homogeneous
AB Traditional machining learning method aims at using the labeled data or unlabeled data to train a mathematic model then it can be used to predict the unlabeled data for Data mining problem, but it requires that the data which be trained should have same distribution with the predicting data. For the real world datasets, it is hard to get enough training datasets which has the same distribution. Thus, how to train a good mathematic model by using different distribution data is crucial problem, and the researchers using the probability view to solve transfer classification problem is relative less. In this paper, we propose a transfer classification algorithm based on the Gaussian Process model, which can be used to solve the homogeneous transfer classification problem. We use the probability theory to propose a novel classification transfer learning model based on the Gaussian Process (GP) model. We experiment on the synthetic and real-world datasets and compare to other method, the result has verified the effectiveness of our approach.
CR Cao B, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P407
   Dai  W., 2007, P 24 INT C MACH LEAR, P193, DOI DOI 10.1145/1273496.1273521
   Lawrence N. D., 2004, P 21 INT C MACH LEAR
   Long MS, 2015, IEEE T KNOWL DATA EN, V27, P1519, DOI 10.1109/TKDE.2014.2373376
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63
   Williams C. K., 2006, GAUSSIAN PROCESSES M, V2, P33
   Wu P., 2004, P 21 INT C MACH LEAR, P110
NR 9
TC 0
Z9 0
BN 978-1-5386-5195-7
PY 2018
BP 226
EP 229
ER

PT S
AU Hu, H
   Hong, X
   Hou, DY
   Shi, ZZ
AF Hu, Hong
   Hong, Xin
   Hou, Dan Yang
   Shi, Zhongzhi
BE Shi, Z
   MercierLaurent, E
   Li, J
TI Forward Learning Convolutional Neural Network
SO INTELLIGENT INFORMATION PROCESSING IX
SE IFIP Advances in Information and Communication Technology
CT 10th IFIP TC 12 International Conference on Intelligent Information
   Processing (IIP)
CY OCT 19-22, 2018
CL Nanning, PEOPLES R CHINA
DE Forward learning; Convolutional neural network; Transfer learning;
   Extreme learning machine
AB A conventional convolutional neural network (CNN) is trained by back-propagation (BP) from output layer to input layer through the entire network. In this paper, we propose a novel training approach such that CNN can be trained in forward way unit by unit. For example, we separate a CNN network with three convolutional layers into three units. Each unit contains one convolutional layer and will be trained one by one in sequence. Experiments shows that training can be restricted in local unit and processed one by one from input to output. In most cases, our novel feed forward approach has equal or better performance compared to the traditional approach. In the worst case, our novel feed forward approach is inferior to the traditional approach less than 5% accuracy. Our training approach also obtains benefits from transfer learning by setting different targets for middle units. As the full network back propagation is unnecessary, BP learning becomes more efficiently and least square method can be applied to speed learning. Our novel approach gives out a new focus on training methods of convolutional neural network.
CR Aghdam H.H., 2017, CONVOLUTIONAL NEURAL
   Atlas L.E., 1987, NEURAL INFORM PROCES, P31
   CIRESAN D, 2012, PROC CVPR IEEE, P3642, DOI [DOI 10.1109/CVPR.2012.6248110, 10.1109/CVPR.2012.6248110]
   Clouse DS, 1997, IEEE T NEURAL NETWOR, V8, P1065, DOI 10.1109/72.623208
   Glauner P. O., 2015, IEEE-ACM T AUDIO SPE, V22, P1533
   Haykin S., 2009, THESIS
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hu H, 2014, EXPERT SYST APPL, V41, P2729, DOI 10.1016/j.eswa.2013.11.006
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Callet P, 2006, IEEE T NEURAL NETWOR, V17, P1316, DOI 10.1109/TNN.2006.879766
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Rumelhart D. E., 1986, PARALLEL DISTRIBUTED, V1, P45
   Rumelhart David E., 1988, LEARNING REPRESENTAT
   Szegedy C., 2015, CVPR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Xie S., 2016, ARXIV161105431
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 21
TC 0
Z9 0
SN 1868-4238
EI 1868-422X
BN 978-3-030-00828-4; 978-3-030-00827-7
PY 2018
VL 538
BP 51
EP 61
DI 10.1007/978-3-030-00828-4_6
ER

PT S
AU Tensmeyer, C
   Wigington, C
   Davis, B
   Stewart, S
   Martinez, T
   Barrett, W
AF Tensmeyer, Chris
   Wigington, Curtis
   Davis, Brian
   Stewart, Seth
   Martinez, Tony
   Barrett, William
GP IEEE
TI Language Model Supervision for Handwriting Recognition Model Adaptation
SO PROCEEDINGS 2018 16TH INTERNATIONAL CONFERENCE ON FRONTIERS IN
   HANDWRITING RECOGNITION (ICFHR)
SE International Conference on Handwriting Recognition
CT 16th International Conference on Frontiers in Handwriting Recognition
   (ICFHR)
CY AUG 05-08, 2018
CL Niagara Falls, NY
DE Handwriting Recognition; Language Model; Transfer Learning; Bootstrap;
   Historical Document Analysis
AB Not all languages and domains of handwriting have large labeled datasets available for training handwriting recognition (HWR) models. One way to address this problem is to leverage high resource languages to help train models for low resource languages. In this work, we adapt HWR models trained on a source language to a target language that uses the same writing script. We do so using only labeled data in the source language, unlabeled data in the target language, and a language model in the target language. The language model is used to produce target transcriptions to allow regular example based training. Using this approach we demonstrate improved transferability among French, English, and Spanish languages using both historical and modern handwriting datasets.
CR Sanchez JA, 2014, INT CONF FRONT HAND, P785, DOI 10.1109/ICFHR.2014.137
   Augustin E., 2006, WORKSH FRONT HANDWR
   Ball Gregory R, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P26, DOI 10.1109/ICDAR.2009.249
   Bluche Theodore, 2014, Statistical Language and Speech Processing. Second International Conference, SLSP 2014, P199, DOI 10.1007/978-3-319-11397-5_15
   Chen SF, 1999, COMPUT SPEECH LANG, V13, P359, DOI 10.1006/csla.1999.0128
   Elarian Y, 2015, PATTERN RECOGN, V48, P849, DOI 10.1016/j.patcog.2014.09.013
   Frinken V, 2011, PROC INT CONF DOC, P314, DOI 10.1109/ICDAR.2011.71
   Granell E, 2018, J IMAGING, V4, DOI 10.3390/jimaging4010015
   Graves A., 2006, P 23 INT C MACH LEAR, V2006, P369, DOI DOI 10.1145/1143844.1143891
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Koehn P., 2005, MT SUMMIT, P79, DOI DOI 10.3115/1626355.1626380
   Lee DH, 1998, INT J PATTERN RECOGN, V12, P45, DOI 10.1142/S0218001498000051
   Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071
   Mohri M., 2008, SPRINGER HDB SPEECH, P559, DOI DOI 10.1007/978-3-540-49127-9_28
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Povey  D., 2011, WORKSH AUT SPEECH RE
   Serrano N., 2010, INT C LANG RES EV, P19
   Simard PY, 2003, SEVENTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS I AND II, PROCEEDINGS, P958
   TUNG CH, 1994, PATTERN RECOGN, V27, P221, DOI 10.1016/0031-3203(94)90055-8
   Varga T, 2008, STUD COMPUT INTELL, V90, P333
   Voigtlaender P, 2016, INT CONF FRONT HAND, P228, DOI [10.1109/ICFHR.2016.0052, 10.1109/ICFHR.2016.48]
   Wigington Curtis, 2017, 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), P639, DOI 10.1109/ICDAR.2017.110
   Wolf C, 2002, INT C PATT RECOG, P1037, DOI 10.1109/ICPR.2002.1048482
NR 24
TC 0
Z9 0
SN 2167-6445
BN 978-1-5386-5875-8
PY 2018
BP 133
EP 138
DI 10.1109/ICFHR-2018.2018.00032
ER

PT S
AU Granet, A
   Morin, E
   Mouchere, H
   Quiniou, S
   Viard-Gaudin, C
AF Granet, Adeline
   Morin, Emmanuel
   Mouchere, Harold
   Quiniou, Solen
   Viard-Gaudin, Christian
GP IEEE
TI Separating Optical and Language Models through Encoder-Decoder Strategy
   for Transferable Handwriting Recognition
SO PROCEEDINGS 2018 16TH INTERNATIONAL CONFERENCE ON FRONTIERS IN
   HANDWRITING RECOGNITION (ICFHR)
SE International Conference on Handwriting Recognition
CT 16th International Conference on Frontiers in Handwriting Recognition
   (ICFHR)
CY AUG 05-08, 2018
CL Niagara Falls, NY
DE Handwriting recognition; knowledge transfer; Optical model; Language
   model
AB Lack of data can be an issue when beginning a new study on historical handwritten documents. To deal with this, we propose a deep-learning based recognizer which separates the optical and the language models in order to train them separately using different resources. In this work, we present the optical encoder part of a multilingual transductive transfer learning applied to historical handwriting recognition. The optical encoder transforms the input word image into a non-latent space that depends only on the letter-n-grams: it enables it to be independent of the language. This transformation avoids embedding a language model and operating the transfer learning across languages using the same alphabet. The language decoder creates from a vector of letter-n-grams a word as a sequence of characters. Experiments show that separating optical and language model can be a solution for multilingual transfer learning.
CR Bahdanau D., 2014, P ICLR
   Bengio S., 2014, P INT
   Bluche T., 2017, P ICDAR
   Bojanowski Piotr, 2017, TACL, V5, P135, DOI DOI 10.1162/tacl_a_00051
   Cho K., 2014, P 8 WORKSH SYNT SEM, P103, DOI DOI 10.3115/V1/W14-4012
   Cloppet F, 2016, INT CONF FRONT HAND, P590, DOI [10.1109/ICFHR.2016.106, 10.1109/ICFHR.2016.0113]
   Fischer A, 2012, PATTERN RECOGN LETT, V33, P934, DOI 10.1016/j.patrec.2011.09.009
   Frinken V., 2013, P 2 INT WORKSH HIST, P67
   Granell E, 2018, J IMAGING, V4, DOI 10.3390/jimaging4010015
   Granet A., 2018, P LREC
   Granet A., 2018, P COLING
   Graves A., 2006, P 23 INT C MACH LEAR, V2006, P369, DOI DOI 10.1145/1143844.1143891
   Grosicki E, 2011, PROC INT CONF DOC, P1459, DOI 10.1109/ICDAR.2011.290
   Llados J., 2012, INT J PRAI, V26
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nakayama H, 2017, MACH TRANSL, V31, P49, DOI 10.1007/s10590-017-9197-z
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Sanchez Joan Andreu, 2017, 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), P1383, DOI 10.1109/ICDAR.2017.226
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Voigtlaender P, 2016, INT CONF FRONT HAND, P228, DOI [10.1109/ICFHR.2016.0052, 10.1109/ICFHR.2016.48]
NR 20
TC 0
Z9 0
SN 2167-6445
BN 978-1-5386-5875-8
PY 2018
BP 309
EP 314
DI 10.1109/ICFHR-2018.2018.00061
ER

PT S
AU Roy, P
   Ghosh, S
   Pal, U
AF Roy, Prasun
   Ghosh, Subhankar
   Pal, Umapada
GP IEEE
TI A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing
SO PROCEEDINGS 2018 16TH INTERNATIONAL CONFERENCE ON FRONTIERS IN
   HANDWRITING RECOGNITION (ICFHR)
SE International Conference on Handwriting Recognition
CT 16th International Conference on Frontiers in Handwriting Recognition
   (ICFHR)
CY AUG 05-08, 2018
CL Niagara Falls, NY
DE Air-writing; human-computer interaction; gesture recognition;
   handwritten character recognition; convolutional neural networks
AB Air-writing refers to virtually writing linguistic characters through hand gestures in three dimensional space with six degrees of freedom. In this paper a generic video camera dependent convolutional neural network (CNN) based air-writing framework has been proposed. Gestures are performed using a marker of fixed color in front of a generic video camera followed by color based segmentation to identify the marker and track the trajectory of marker tip. A pre-trained CNN is then used to classify the gesture. The recognition accuracy is further improved using transfer learning with the newly acquired data. The performance of the system varies greatly on the illumination condition due to color based segmentation. In a less fluctuating illumination condition the system is able to recognize isolated unistroke numerals of multiple languages. The proposed framework achieved 97.7%, 95.4% and 93.7% recognition rate in person independent evaluation over English, Bengali and Devanagari numerals, respectively.
CR Chen MY, 2016, IEEE T HUM-MACH SYST, V46, P436, DOI 10.1109/THMS.2015.2492599
   Chen MY, 2016, IEEE T HUM-MACH SYST, V46, P403, DOI 10.1109/THMS.2015.2492598
   Dash Ayushman, 2017, 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), P908, DOI 10.1109/ICDAR.2017.153
   Kristensson P. O., 2012, P 2012 ACM INT C INT, P89, DOI DOI 10.1145/2166966.2166983
   LeCun Y., 1998, MNIST DATABASE HANDW
   Microsoft Corporation, 2010, NON TRADITIONAL REF
   Pal U, 2007, ICDAR 2007: NINTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS I AND II, PROCEEDINGS, P749
   Schick A, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P217
NR 8
TC 0
Z9 0
SN 2167-6445
BN 978-1-5386-5875-8
PY 2018
BP 404
EP 409
DI 10.1109/ICFHR-2018.2018.00077
ER

PT S
AU Aradillas, JC
   Murillo-Fuentes, JJ
   Olmos, PM
AF Carlos Aradillas, Jose
   Jose Murillo-Fuentes, Juan
   Olmos, Pablo M.
GP IEEE
TI Boosting Handwriting Text Recognition in Small Databases with Transfer
   Learning
SO PROCEEDINGS 2018 16TH INTERNATIONAL CONFERENCE ON FRONTIERS IN
   HANDWRITING RECOGNITION (ICFHR)
SE International Conference on Handwriting Recognition
CT 16th International Conference on Frontiers in Handwriting Recognition
   (ICFHR)
CY AUG 05-08, 2018
CL Niagara Falls, NY
AB In this paper we deal with the offline handwriting text recognition (HTR) problem with reduced training data sets. Recent HTR solutions based on artificial neural networks exhibit remarkable solutions in referenced databases. These deep learning neural networks are composed of both convolutional (CNN) and long short-term memory recurrent units (LSTM). In addition, connectionist temporal classification (CTC) is the key to avoid segmentation at character level, greatly facilitating the labeling task. One of the main drawbacks of the CNN-LSTM-CTC (CRNN) solutions is that they need a considerable part of the text to be transcribed for every type of calligraphy, typically in the order of a few thousands of lines. Furthermore, in some scenarios the text to transcribe is not that long, e.g. in the Washington database. The CRNN typically overfits for this reduced number of training samples. Our proposal is based on the transfer learning (TL) from the parameters learned with a bigger database. We first investigate, for a reduced and fixed number of training samples, 350 lines, how the learning from a large database, the IAM, can be transferred to the learning of the CRNN of a reduced database, Washington. We focus on which layers of the network could not be re-trained. We conclude that the best solution is to re-train the whole CRNN parameters initialized to the values obtained after the training of the CRNN from the larger database. We also investigate results when the training size is further reduced. For the sake of comparison, we study the character error rate (CER) with no dictionary or any language modeling technique. The differences in the CER are more remarkable when training with just 350 lines, a CER of 3.3% is achieved with TL while we have a CER of 18.2% when training from scratch. As a byproduct, the learning times are quite reduced. Similar good results are obtained from the Parzival database when trained with this reduced number of lines and this new approach.
CR Bluche T., 2016, SCAN ATTEND READ END
   Bluche T., 2016, P ADV NEUR INF PROC, V29, P838
   Bluche T, 2013, PROC INT CONF DOC, P285, DOI 10.1109/ICDAR.2013.64
   Fischer A, 2012, PATTERN RECOGN LETT, V33, P934, DOI 10.1016/j.patrec.2011.09.009
   Granet A., 2018, INT C PATT REC APPL
   Graves A., 2009, ADV NEURAL INFORM PR, V22, P545
   Graves A., 2006, P 23 INT C MACH LEAR, V2006, P369, DOI DOI 10.1145/1143844.1143891
   Grosicki Emmanuele, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1398, DOI 10.1109/ICDAR.2009.184
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma D. P., 2014, INT C LEARN REPR
   Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Puigcerver Joan, 2017, 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), P67, DOI 10.1109/ICDAR.2017.20
   Serrano N., 2010, LREC
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Voigtlaender P, 2016, INT CONF FRONT HAND, P228, DOI [10.1109/ICFHR.2016.0052, 10.1109/ICFHR.2016.48]
   Pham V, 2014, INT CONF FRONT HAND, P285, DOI 10.1109/ICFHR.2014.55
   Yaeger L., 1996, NIPS, P807
   Yousefi MR, 2015, 2015 13TH IAPR INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION (ICDAR), P1121, DOI 10.1109/ICDAR.2015.7333935
NR 20
TC 0
Z9 0
SN 2167-6445
BN 978-1-5386-5875-8
PY 2018
BP 429
EP 434
DI 10.1109/ICFHR-2018.2018.00081
ER

PT B
AU Azarbarzin, S
   Afsari, F
AF Azarbarzin, Samaneh
   Afsari, Fatemeh
GP IEEE
TI Robust two stage unsupervised metric learning for domain adaptation
SO 2018 8TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING
   (ICCKE)
CT 8th International Conference on Computer and Knowledge Engineering
   (ICCKE)
CY OCT 25-26, 2018
CL Ferdowsi Univ Mashhad, Mashhad, IRAN
HO Ferdowsi Univ Mashhad
DE transfer learning; metric learning; marginalized denoising; low-rank
   constraint
AB Most commonly used metric learning procedures suppose that the input feature space and domain of the training and test data are identical. In such cases these algorithms cannot improve target learning problems. This paper presents a robust distance metric for domain adaptation in two stages. At first stage both source and target features are transferred to a newly found latent feature space, which minimizes the difference between domains as well as the data properties are preserved. Then in the second stage, the desired metric is learned with a marginalized denoising strategy and the low-rank constraint. To show the superiority and power of the proposed method it is tested on distinct kinds of cross-domain image categorization datasets and the results prove that our approach remarkably exceeds other existing domain adaptation algorithms in the classification tasks.
CR Alcala-Fdez J, 2009, SOFT COMPUT, V13, P307, DOI 10.1007/s00500-008-0323-y
   Bellet A., 2013, ARXIV13066709
   Cao Q, 2013, IEEE I CONF COMP VIS, P2408, DOI 10.1109/ICCV.2013.299
   Chen M., 2012, ARXIV12064683
   Davis J. V., 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Ding Z., 2015, P 11 IEEE INT C WORK, V1, P1
   Ding ZM, 2017, IEEE T IMAGE PROCESS, V26, P660, DOI 10.1109/TIP.2016.2631887
   Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107
   Guoqiang Zhong, 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P1266, DOI 10.1109/ICDM.2011.95
   Hu J, 2015, PROC CVPR IEEE, P325, DOI 10.1109/CVPR.2015.7298629
   Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924
   Li Z., 2012, P ACM INT C MULT, P853
   Lin L, 2017, IEEE T PATTERN ANAL, V39, P1089, DOI 10.1109/TPAMI.2016.2567386
   Liu Wei, 2015, P 29 AAAI C ART INT, P2792
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517
   Pan S., 2008, AAAI, P677
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang H., 2014, AAAI, P2099
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Xing Eric P., 2003, ADV NEURAL INFORM PR, P521
   ZHANG Y, 2010, P 16 ACM SIGKDD INT, P1199, DOI DOI 10.1145/1835804.1835954
NR 25
TC 0
Z9 0
BN 978-1-5386-9569-2
PY 2018
BP 52
EP 57
ER

PT B
AU Taghiyarrenani, Z
   Fanian, A
   Mahdavi, E
   Mirzaei, A
   Farsi, H
AF Taghiyarrenani, Zahra
   Fanian, Ali
   Mahdavi, Ehsan
   Mirzaei, Abdolreza
   Farsi, Hamed
GP IEEE
TI Transfer Learning based Intrusion Detection
SO 2018 8TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING
   (ICCKE)
CT 8th International Conference on Computer and Knowledge Engineering
   (ICCKE)
CY OCT 25-26, 2018
CL Ferdowsi Univ Mashhad, Mashhad, IRAN
HO Ferdowsi Univ Mashhad
DE machine learning; intrusion detection; transfer learning; training
   samples
AB In the past decades, machine learning based intrusion detection systems have been developed. This paper discloses a new aspect of machine learning based intrusion detection systems. The proposed method detects normal and anomaly behaviors in the desired network where there are not any labeled samples as training dataset. That is while a plenty of labeled samples may exist in another network that is different from the desired network. Because of the difference between two networks, their samples produce in different manners. So, direct utilizing of labeled samples of a different network as training samples does not provide acceptable accuracy to detect anomaly behaviors in the desired network. In this paper, we propose a transfer learning based intrusion detection method which transfers knowledge between the networks and eliminates the problem of providing training samples that is a costly procedure. Comparing the experimental results with the results of a basic machine learning method (SVM) and also baseline method(DAMA) shows the effectiveness of the proposed method for transferring knowledge for intrusion detection systems.
CR Agrawal S, 2015, PROCEDIA COMPUT SCI, V60, P708, DOI 10.1016/j.procs.2015.08.220
   Blitzer J., 2006, P 2006 C EMP METH NA, P120
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57
   Gao J, 2008, KDD, P283, DOI DOI 10.1145/1401890.1401928
   Ham JH, 2003, LEARNING HIGH DIMENS
   Haq Nutan Farah, 2015, International Journal of Advanced Research in Artificial Intelligence, V4, P9
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Pan S., 2008, AAAI, P677
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Singh J., 2013, INT J ADV RES COMPUT, V2, P4349
   Tao JW, 2012, PATTERN RECOGN, V45, P3962, DOI 10.1016/j.patcog.2012.04.014
   Vapnik V, 2013, NATURE STAT LEARNING
   Wang C., 2011, IJCAI, P1541
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
NR 17
TC 0
Z9 0
BN 978-1-5386-9569-2
PY 2018
BP 92
EP 97
ER

PT S
AU Kloss, RB
   Ao, AJ
   Schwartz, WR
AF Kloss, Ricardo Barbosa
   Ao, Artur Jord
   Schwartz, William Robson
GP IEEE
TI Face Verification: Strategies for Employing Deep Models
SO PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE &
   GESTURE RECOGNITION (FG 2018)
SE IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops
CT 13th IEEE International Conference on Automatic Face & Gesture
   Recognition (FG)
CY MAY 15-19, 2018
CL Xi an, PEOPLES R CHINA
DE Transfer Learning; Artificial Neural Networks; Face Verification; Metric
   Learning
AB Features extracted with deep learning have now achieved state-of-the-art results in many tasks. However, to reuse a learned deep model, transfer learning with fine-tuning needs to be employed, which requires to re-train the whole model or part of it to extract useful features in the new domain. This step is burdensome and requires heavy computing power. Therefore, this work investigates alternatives in transfer-learning that do not involve performing fine-tuning for a model with the new domain. Namely, we explore the correlation of depth and scale in deep models, and look for the layer/scale that yields the best results for the new domain, we also explore metrics for the verification task, using locally connected convolutions to learn distance metrics. Our experiments use a model pre-trained in face identification and adapt it to the face verification task with different data, but still on the face domain. We achieve 96.65% mean accuracy on the Labeled Faces in the Wild dataset and 93.12% mean accuracy on the Youtube Faces dataset comparable to the state-of-the-art.
CR Chellappa R, 2010, COMPUTER, V43, P46, DOI 10.1109/MC.2010.37
   Haykin S, 2001, NEURAL NETWORKS COMP
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huang G. B., 2012, NIPS
   Huang GL, 2017, IEEE ICC
   Jain R., 1990, ART COMPUTER SYSTEMS
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Ouamane A., 2015, AUT FAC GEST REC FG, V02, P1, DOI DOI 10.1109/FG.2015.7284837
   Parkhi O. M., 2015, BRIT MACH VIS C
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wagner Andrew, 2012, T PATTERN ANAL MACHI
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wright John, 2009, T PATTERN ANAL MACHI
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
NR 23
TC 0
Z9 0
SN 2326-5396
BN 978-1-5386-2335-0
PY 2018
BP 258
EP 262
DI 10.1109/FG.2018.00045
ER

PT S
AU Yang, HY
   Zhang, Z
   Yin, LJ
AF Yang, Huiyuan
   Zhang, Zheng
   Yin, Lijun
GP IEEE
TI Identity-Adaptive Facial Expression Recognition Through Expression
   Regeneration Using Conditional Generative Adversarial Networks
SO PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE &
   GESTURE RECOGNITION (FG 2018)
SE IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops
CT 13th IEEE International Conference on Automatic Face & Gesture
   Recognition (FG)
CY MAY 15-19, 2018
CL Xi an, PEOPLES R CHINA
DE FER; GAN; Identity-adaptive; CNN
AB Subject variation is a challenging issue for facial expression recognition, especially when handling unseen subjects with small-scale lableled facial expression databases. Although transfer learning has been widely used to tackle the problem, the performance degrades on new data. In this paper, we present a novel approach (so-called LA-gen) to alleviate the issue of subject variations by regenerating expressions from any input facial images. First of all, we train conditional generative models to generate six prototypic facial expressions from any given query face image while keeping the identity related information unchanged. Generative Adversarial Networks are employed to train the conditional generative models, and each of them is designed to generate one of the prototypic facial expression images. Second, a regular CNN (FER-Net) is fine-tuned for expression classification. After the corresponding prototypic facial expressions are regenerated from each facial image, we output the last EC layer of FER-Net as features for both the input image and the generated images. Based on the minimum distance between the input image and the generated expression images in the feature space, the input image is classified as one of the prototypic expressions consequently. Our proposed method can not only alleviate the influence of inter-subject variations, but will also be flexible enough to integrate with any other FER CNNs for person-independent facial expression recognition. Our method has been evaluated on CK+, Oulu-CASIA, BU-3DFE and BU-4DFE databases, and the results demonstrate the effectiveness of our proposed method.
CR Abadi M., 2016, ARXIV160304467
   Berretti S, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4125, DOI 10.1109/ICPR.2010.1002
   Chen J., 2013, PATTERN RECOGNITION, V34
   Dapogny A., 2015, FG, V1
   Dapogny A, 2015, IEEE I CONF COMP VIS, P3783, DOI 10.1109/ICCV.2015.431
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   Gauthier J., 2014, CLASS PROJECT STANFO, V2014, P2
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR, P2672, DOI [DOI 10.1017/CBO9781139058452, DOI 10.1001/JAMAINTERNMED.2016.8245]
   Guo YM, 2012, LECT NOTES COMPUT SC, V7573, P631, DOI 10.1007/978-3-642-33709-3_45
   Isola Phillip, 2016, ARXIV161107004
   Jeni L. A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2168, DOI 10.1109/ICCVW.2011.6130516
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Klaser Alexander, 2008, BMVC 2008 19 BRIT MA, P275
   Ledig C., 2016, PHOTOREALISTIC SINGL
   Liu MY, 2015, LECT NOTES COMPUT SC, V9006, P143, DOI 10.1007/978-3-319-16817-3_10
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Lucey P., 2010, CVPR WORKSH
   Meng ZB, 2017, IEEE INT CONF AUTOMA, P558, DOI 10.1109/FG.2017.140
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Mollahosseini A., 2016, APPL COMPUTER VISION
   Pan ZL, 2016, LECT NOTES ARTIF INT, V10011, P369, DOI 10.1007/978-3-319-47665-0_35
   Radford A., 2015, ARXIV151106434
   Ronneberger O., 2015, INT C MED IM COMP CO
   Rudovic O, 2012, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2012.6247983
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Wang J., 2006, P IEEE INT C COMP VI, P1399
   Xudong Yang, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163090
   Yin L, 2008, AUTOMATIC FACE GESTU, P1, DOI DOI 10.1109/AFGR.2008.4813324
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 39
TC 2
Z9 2
SN 2326-5396
BN 978-1-5386-2335-0
PY 2018
BP 294
EP 301
DI 10.1109/FG.2018.00050
ER

PT S
AU Luo, ZM
   Hu, JN
   Deng, WH
   Shen, HF
AF Luo, Zimeng
   Hu, Jiani
   Deng, Weihong
   Shen, Haifeng
GP IEEE
TI Deep Unsupervised Domain Adaptation for Face Recognition
SO PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE &
   GESTURE RECOGNITION (FG 2018)
SE IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops
CT 13th IEEE International Conference on Automatic Face & Gesture
   Recognition (FG)
CY MAY 15-19, 2018
CL Xi an, PEOPLES R CHINA
AB Face recognition is challenge task which involves determining the identity of facial images. With availability of a massive amount of labeled facial images gathered from Internet, deep convolution neural networks(DCNNs) have achieved great success in face recognition tasks. Those images are gathered from unconstrain environment, which contain people with different ethnicity, age, gender and so on. However, in the actual application scenario, the target face database may be gathered under different conditions compered with source training dataset, e.g. different ethnicity, different age distribution, disparate shooting environment. These factors increase domain discrepancy between source training database and target application database and make the learnt model degenerate in target database. Meanwhile, for the target database where labeled data are lacking or unavailable, directly using target data to fine-tune pre-learnt model becomes intractable and impractical. In this paper, we adopt unsupervised transfer learning methods to address this issue. To alleviate the discrepancy between source and target face database and ensure the generalization ability of the model, we constrain the maximum mean discrepancy (MMD) between source database and target database and utilize the massive amount of labeled facial images of source database to training the deep neural network at the same time. We evaluate our method on two face recognition benchmarks and significantly enhance the performance without utilizing the target label.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chen B., 2017, IEEE C COMP VIS PATT
   Gretton A., 2012, ADV NEURAL INFORM PR, P1205
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Huang GB., 2007, 0749 U MASS
   Kan MN, 2015, IEEE I CONF COMP VIS, P3846, DOI 10.1109/ICCV.2015.438
   KEMELMACHERSHLIZER, 2016, P IEEE C COMP VIS PA, P4873, DOI DOI 10.1109/CVPR.2016.527
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Long M., 2015, INT C MACH LEARN, P97
   Parkhi O.M., 2015, P BR MACH VIS, V1, P6
   Phillips PJ, 2017, IEEE INT CONF AUTOMA, P705, DOI 10.1109/FG.2017.89
   Phillips PJ, 2012, IMAGE VISION COMPUT, V30, P177, DOI 10.1016/j.imavis.2012.01.004
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K, 2014, COMPUTER SCI
   Tzeng E., 2014, ARXIV14123474
   Wen  Y., 2016, DISCRIMINATIVE FEATU
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Yan H, 2017, IEEE GLOB COMM CONF
   Yi D., 2014, ARXIV14117923
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
   Zhang J, 2017, IEEE GLOB COMM CONF
NR 25
TC 0
Z9 0
SN 2326-5396
BN 978-1-5386-2335-0
PY 2018
BP 453
EP 457
DI 10.1109/FG.2018.00073
ER

PT S
AU Peng, M
   Wu, Z
   Zhang, ZH
   Chen, T
AF Peng, Min
   Wu, Zhan
   Zhang, Zhihao
   Chen, Tong
GP IEEE
TI From Macro to Micro Expression Recognition: Deep Learning on Small
   Datasets Using Transfer Learning
SO PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE &
   GESTURE RECOGNITION (FG 2018)
SE IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops
CT 13th IEEE International Conference on Automatic Face & Gesture
   Recognition (FG)
CY MAY 15-19, 2018
CL Xi an, PEOPLES R CHINA
DE micro expression recognition; deep learning; trasnfer learning
AB This paper presents the methods used in our submission to 2018 Facial Micro-Expression Grand Challenge (MEGC). The object of the challenge is to recognize micro-expression in two provided databases, including holdout-database recognition and composite database recognition. Considering the small size of the databases, we follow a rout of transfer learning to implement convolutional neural network to recognize the micro expression. ResNetlO pre-trained on ImageNet dataset was fine-tuned on macro-expression datasets with large size and then on the provided micro-expression datasets. Experimental results show that the method can achieve weighted average recall (WAR) of 0.561 and unweighted average recall (UAR) of 0.389 in Holdout-database Evaluation Task, and F1 Score of 0.64 in Composite Database Evaluation Task, which are much higher than what baseline methods (LBP-TOP, HOOF, HOG3D) can achieve.
CR Aifanti N., 2010, P 11 INT WORKSH AN M
   Cootes T. F., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P484
   Davison A. K., ARXIV170807549
   Davison AK, 2018, IEEE T AFFECT COMPUT, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Kamishima T., 2010, J JAPANESE SOC ARTIF, V25, P572
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li XB, 2018, IEEE T AFFECT COMPUT, V9, P563, DOI 10.1109/TAFFC.2017.2667642
   Liu W., ARXIV14094842
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Patel D, 2016, INT C PATT RECOG, P2258, DOI 10.1109/ICPR.2016.7899972
   Peng M, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01745
   Porter S, 2008, PSYCHOL SCI, V19, P508, DOI 10.1111/j.1467-9280.2008.02116.x
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Russell TA, 2006, BRIT J CLIN PSYCHOL, V45, P579, DOI 10.1348/014466505X90866
   Sherwood T, 2016, SOFTWARE PRACT EXPER, V46, P931, DOI 10.1002/spe.2339
   Simon M., ARXIV161201452
   Wang Y., 2014, P AS C COMP VIS SING, P21
   Weinberger S, 2010, NATURE, V465, P412, DOI 10.1038/465412a
   Xu F, 2017, IEEE T AFFECT COMPUT, V8, P254, DOI 10.1109/TAFFC.2016.2518162
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
NR 25
TC 1
Z9 1
SN 2326-5396
BN 978-1-5386-2335-0
PY 2018
BP 657
EP 661
DI 10.1109/FG.2018.00103
ER

PT S
AU Guo, X
   Polania, LF
   Barner, KE
AF Guo, Xin
   Polania, Luisa F.
   Barner, Kenneth E.
GP IEEE
TI Smile Detection in the Wild Based on Transfer Learning
SO PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE &
   GESTURE RECOGNITION (FG 2018)
SE IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops
CT 13th IEEE International Conference on Automatic Face & Gesture
   Recognition (FG)
CY MAY 15-19, 2018
CL Xi an, PEOPLES R CHINA
DE smile detection; CNN; deep learning; transfer learning
AB Smile detection from unconstrained facial images is a specialized and challenging problem. As one of the most informative expressions, smiles convey basic underlying emotions, such as happiness and satisfaction, and leads to multiple applications, such as human behavior analysis and interactive controlling. Compared to the size of databases for face recognition, far less labeled data is available for training smile detection systems. This paper proposes an efficient transfer learning-based smile detection approach to leverage the large amount of labeled data from face recognition datasets and to alleviate overfitting on smile detection. A well-trained deep face recognition model is explored and fine-tuned for smile detection in the wild, unlike previous works which use either hand-engineered features or train deep convolutional networks from scratch. Three different models are built as a result of fine-tuning the face recognition model with different inputs, including aligned, unaligned and grayscale images generated from the GENKI-4K dataset. Experiments show that the proposed approach achieves improved state-of-the-art performance. Robustness of the model to noise and blur artifacts is also evaluated in this paper.
CR Abel EL, 2010, PSYCHOL SCI, V21, P542, DOI 10.1177/0956797610363775
   An L, 2015, NEUROCOMPUTING, V149, P354, DOI 10.1016/j.neucom.2014.04.072
   Chen J., 2016, MICROCHIM ACTA, V76, P1
   Deniz O, 2008, LECT NOTES COMPUT SC, V5359, P602, DOI 10.1007/978-3-540-89646-3_59
   Dodge S., 2016, ARXIV160404004
   Ekman P., 1988, SELF DECEPTION ADAPT, P229
   Freire-Obregon D., 2009, P COMP VIS THEOR APP
   Gao Y, 2016, NEUROCOMPUTING, V174, P1077, DOI 10.1016/j.neucom.2015.10.022
   Glauner P.O., 2015, ARXIV150806535
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR, P2672, DOI [DOI 10.1017/CBO9781139058452, DOI 10.1001/JAMAINTERNMED.2016.8245]
   Harker LA, 2001, J PERS SOC PSYCHOL, V80, P112, DOI 10.1037//0022-3514.80.1.112
   Jain V., 2014, 12 WSEAS INT C SIGN
   Kazemi V., 2014, P IEEE C COMP VIS PA
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LeCun Yann, 1995, HDB BRAIN THEORY NEU, V3361, P10
   Liu M., 2012, LNCS, P577, DOI DOI 10.1007/978-3-642-37444-9_
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pantic M, 2007, LECT NOTES COMPUT SC, V4451, P47
   Parkhi  O.M., 2015, P BRIT MACH VIS C BM
   Potapova E., 2009, GRAPHICON 2009 P 19, P117
   Seder JP, 2012, SOC PSYCHOL PERS SCI, V3, P407, DOI 10.1177/1948550611424968
   Shan CF, 2012, IEEE T IMAGE PROCESS, V21, P431, DOI 10.1109/TIP.2011.2161587
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Tavares C., 2004, System and method for capturing and using biometrics to review a product, service, creative work or thing, Patent No. [uS Patent App. 10/876,848, 10876848]
   Whitehill J, 2009, IEEE T PATTERN ANAL, V31, P2106, DOI 10.1109/TPAMI.2009.42
   Yadappanavar H., 2012, INT J IMAGE PROCESSI, V1
   Zhang KH, 2015, Proceedings 3rd IAPR Asian Conference on Pattern Recognition ACPR 2015, P534, DOI 10.1109/ACPR.2015.7486560
NR 29
TC 0
Z9 0
SN 2326-5396
BN 978-1-5386-2335-0
PY 2018
BP 679
EP 686
DI 10.1109/FG.2018.00107
ER

PT S
AU Chen, DM
   Yang, S
   Zhou, FN
AF Chen, Danmin
   Yang, Shuai
   Zhou, Funa
GP IEEE
TI Incipient Fault Diagnosis Based on DNN with Transfer Learning
SO 2018 INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND INFORMATION
   SCIENCES (ICCAIS)
SE International Conference on Control Automation and Information Sciences
CT 7th International Conference on Control Automation and Information
   Sciences (ICCAIS)
CY OCT 24-27, 2018
CL Hangzhou Dianzi Univ, Hangzhou, PEOPLES R CHINA
HO Hangzhou Dianzi Univ
DE incipient fault diagnosis; DNN; transfer learning
AB Diagnosis of incipient fault is critical for safe operation of the system because it can prevent disastrous accidents from happening by diagnosing the early fault before deterioration. Deep learning is efficient in feature extraction but it requires a large number of samples to train traditional deep neural network (DNN). It is thus inevitable that the efficiency of DNN will be affected when it is applied to incipient fault diagnosis for there are usually a very limited number of incipient fault samples. Furthermore, a large amount of information involved in significant fault samples was not adequately used for incipient fault diagnosis. To solve this problem, this paper proposes an incipient fault diagnosis model with DNN-based transfer learning. The model can extract fault feature involved in a large number of significant fault samples and apply it to extract insignificant fault feature with a small number of incipient fault samples. In this way, the proposed transfer learning method can efficiently diagnose incipient fault in the case when only a limited number of incipient fault data is available. The efficiency of the proposed model is demonstrated by utilizing the Case Western Reserve University bearing data set.
CR Cao Hongliu, 2018, ICIAR 2018, P779
   Cao P, 2018, IEEE ACCESS, V6, P26241, DOI 10.1109/ACCESS.2018.2837621
   Delpha C, 2018, ENG APPL ARTIF INTEL, V73, P68, DOI 10.1016/j.engappai.2018.04.007
   Ding ZM, 2017, IEEE T IMAGE PROCESS, V26, P660, DOI 10.1109/TIP.2016.2631887
   Han DM, 2018, EXPERT SYST APPL, V95, P43, DOI 10.1016/j.eswa.2017.11.028
   Huang DR, 2018, IEEE ACCESS, V6, P26001, DOI 10.1109/ACCESS.2018.2829803
   Kaya H, 2017, IMAGE VISION COMPUT, V65, P66, DOI 10.1016/j.imavis.2017.01.012
   LI Q, 2017, ENTROPY-SWITZ, V19, DOI DOI 10.3390/E19080421
   Li WH, 2010, INT J MODEL IDENTIF, V10, P246, DOI 10.1504/IJMIC.2010.034577
   Li YB, 2017, IEEE T IND ELECTRON, V64, P6506, DOI 10.1109/TIE.2017.2650873
   Namdari M, 2014, ENG APPL ARTIF INTEL, V28, P22, DOI 10.1016/j.engappai.2013.11.013
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Turki T, 2017, IEEE ACCESS, V5, P7381, DOI 10.1109/ACCESS.2017.2696523
   Wagh N, 2014, APPL COMPUT INTELL S, DOI 10.1155/2014/845815
   Xiao Yang, 2017, ASONAM 2017, P341
   Xu LJ, 2010, CIRC SYST SIGNAL PR, V29, P577, DOI 10.1007/s00034-010-9160-1
   Yang Q, 2006, INT J INF TECH DECIS, V5, P597, DOI 10.1142/S0219622006002258
   Yue Du, ANN BIOMEDICAL ENG
   Zhang R, 2017, IEEE ACCESS, V5, P14347, DOI 10.1109/ACCESS.2017.2720965
   Zhao CH, 2017, P AMER CONTR CONF, P5430, DOI 10.23919/ACC.2017.7963799
   Zoph Barret, 2016, EMNLP 2016, P1568
NR 21
TC 0
Z9 0
SN 2475-7896
BN 978-1-5386-6020-1
PY 2018
BP 303
EP 308
ER

PT B
AU Wang, M
   Wu, ZY
   Wu, XX
   Meng, H
   Kang, SY
   Jia, J
   Cai, LH
AF Wang, Mu
   Wu, Zhiyong
   Wu, Xixin
   Meng, Helen
   Kang, Shiyin
   Jia, Jia
   Cai, Lianhong
GP IEEE
TI Emphatic Speech Synthesis and Control Based on Characteristic
   Transferring in End-to-End Speech Synthesis
SO 2018 FIRST ASIAN CONFERENCE ON AFFECTIVE COMPUTING AND INTELLIGENT
   INTERACTION (ACII ASIA)
CT 1st Asian Conference on Affective Computing and Intelligent Interaction
   (ACII Asia)
CY MAY 20-22, 2018
CL Beijing, PEOPLES R CHINA
DE end-to-end; expressive speech; multi-speaker speech synthesis; transfer
   learning; emphatic speech
AB End-to-end text-to-speech (E2E TTS) synthesis has achieved great success. This work investigates the emphatic speech synthesis and control mechanisms in the E2E framework and proposes an E2E-based method for transferring emphasis characteristic between speakers. Characteristic differences between emphatic and neutral speech are learned from a small-scale corpus containing parallel neutral and emphasis speech utterances recorded by one speaker and further transferred to another speaker so that we can generate emphatic speech with latter speakers voice. Emphasis embedding is injected to the encoder of the extended E2E TTS model to capture the aforementioned differences; while the decoder and attention module are used to decode those differences into synthetic neutral / emphatic speech. Speaker codes linked to the decoder and attention module provide the E2E model the ability for characteristic transferring between speakers. To control the emphatic strength, an encoder memory manipulation mechanism is proposed. Experimental results indicate the effectiveness of our proposed model.
CR Arik S., 2017, P 34 INT C MACH LEAR, P195
   Arik Sercan, 2017, P NIPS, P2966
   Chen Szu-wei, 2009, P ANN C INT SPEECH C
   Costa Francisco, 2004, P INT C SPEECH PROS
   GRIFFIN DW, 1984, IEEE T ACOUST SPEECH, V32, P236, DOI 10.1109/TASSP.1984.1164317
   Henter GE, 2017, INTERSPEECH, P3956, DOI 10.21437/Interspeech.2017-171
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Luong HT, 2017, INT CONF ACOUST SPEE, P4905, DOI 10.1109/ICASSP.2017.7953089
   Ning YS, 2015, INT CONF ACOUST SPEE, P4934, DOI 10.1109/ICASSP.2015.7178909
   Nose T, 2007, IEICE T INF SYST, VE90D, P1406, DOI 10.1093/ietisy/e90-d.9.1406
   Wang Yunjia, 2006, CHINESE TEACHING WOR, V2, P12
   Wang Yuxuan, 2017, ABS170310135 CORR
   Yamagishi J, 2007, IEICE T INF SYST, VE90D, P533, DOI 10.1093/ietisy/e90-d.2.533
NR 13
TC 0
Z9 0
BN 978-1-5386-5311-1
PY 2018
ER

PT B
AU Chen, CT
   Chen, AP
   Huang, SH
AF Chen, Chiao-Ting
   Chen, An-Pin
   Huang, Szu-Hao
GP IEEE
TI Cloning Strategies from Trading Records using Agent-based Reinforcement
   Learning Algorithm
SO 2018 IEEE INTERNATIONAL CONFERENCE ON AGENTS (ICA)
CT IEEE International Conference on Agents (ICA)
CY JUL 28-31, 2018
CL Nanyang Technolog Univ, Singapore, SINGAPORE
HO Nanyang Technolog Univ
DE reinforcement learning; policy gradient financial trading; transfer
   learning; strategy cloning
AB Investment decision making is considered as a series of complicated processes, which are difficult to be analyzed and imitated. Given large amounts of trading records with rich expert knowledge in financial domain, extracting its original decision logics and cloning the trading strategies are also quite challenging. In this paper, an agent-based reinforcement learning (RL) system is proposed to mimic professional trading strategies. The concept of continuous Markov decision process (MDP) in RL is similar to the trading decision making in financial time series data. With the specific-designed RL components, including states, actions, and rewards for financial applications, policy gradient method can successfully imitate the expert's strategies. In order to improve the convergence of RL agent in such highly dynamic environment, a pre-trained model based on supervised learning is transferred to the deep policy networks. The experimental results show that the proposed system can reproduce around eighty percent trading decisions both in training and testing stages. With the discussion of the tradeoff between explorations and model updating, this paper tried to fine-tuning the system parameters to get reasonable results. Finally, an advanced strategy is proposed to dynamically adjust the number of explorations in each episode to achieve better results.
CR Chen Y, 2008, J ADV COMPUT INTELL, V12, P383, DOI 10.20965/jaciii.2008.p0383
   Dempster MAH, 2006, EXPERT SYST APPL, V30, P543, DOI 10.1016/j.eswa.2005.10.012
   Deng Y., 2016, IEEE T NEUR NET LEAR, V99, P1
   Heaton JB, 2017, APPL STOCH MODEL BUS, V33, P3, DOI 10.1002/asmb.2209
   Lee JW, 2001, ISIE 2001: IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS PROCEEDINGS, VOLS I-III, P690, DOI 10.1109/ISIE.2001.931880
   Moody J, 2001, IEEE T NEURAL NETWOR, V12, P875, DOI 10.1109/72.935097
   Nevmyvaka Y, 2006, P 23 INT C MACH LEAR, P673, DOI DOI 10.1145/.1143929
   Takeuchi L., 2013, APPL DEEP LEARNING E
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 9
TC 0
Z9 0
BN 978-1-5386-8180-0
PY 2018
BP 34
EP 37
ER

PT B
AU Spryn, M
   Sharma, A
   Parkar, D
   Shrimal, M
AF Spryn, Mitchell
   Sharma, Aditya
   Parkar, Dhawal
   Shrimal, Madhur
GP IEEE
TI Distributed Deep Reinforcement Learning on the Cloud for Autonomous
   Driving
SO PROCEEDINGS 2018 IEEE/ACM 1ST INTERNATIONAL WORKSHOP ON SOFTWARE
   ENGINEERING FOR AI IN AUTONOMOUS SYSTEMS (SEFAIAS)
CT 1st IEEE/ACM International Workshop on Software Engineering for AI in
   Autonomous Systems (SEFAIAS)
CY MAY 28, 2018
CL Gothenburg, SWEDEN
DE Autonomous Driving; Deep Reinforcement Learning; Distributed Machine
   Learning; Cloud Computing; Simulation
AB This paper proposes an architecture for leveraging cloud computing technology to reduce training time for deep reinforcement learning models for autonomous driving by distributing the training process across a pool of virtual machines. By parallelizing the training process, careful design of the reward function and use of techniques like transfer learning, we demonstrate a decrease in training time for our example autonomous driving problem from 140 hours to less than 1 hour. We go over our network architecture, job distribution paradigm, reward function design and report results from experiments on small sized cluster (1-6 training nodes) of machines. We also discuss the limitations of our approach when trying to scale up to massive clusters.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Abdou M., 2016, END TO END DEEP REIN
   Bojarski M., 2016, CORR
   Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI 10.1109/ICCV.2015.312
   Chollet F., 2015, KERAS
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   John Christopher, 1989, THESIS
   Kalra N., 2016, DRIVING SAFETY MANY
   Melo Francisco S., CONVERGENCE Q LEARNI
   Mnih V., 2013, NIPS DEEP LEARN WORK
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nair A., 2015, CORR
   Shah S., 2017, FIELD SERVICE ROBOTI
   Shalev-Shwartz Shai, 2016, CORR
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Zhang Wei, 2015, CORR
NR 19
TC 0
Z9 0
BN 978-1-4503-5739-5
PY 2018
BP 16
EP 22
DI 10.1145/3194085.3194088
ER

PT S
AU Delas Penas, K
   Rivera, PT
   Naval, PC
AF Delas Penas, Kristofer
   Rivera, Pilarita T.
   Naval, Prospero C., Jr.
BE Nguyen, NT
   Hoang, DH
   Hong, TP
   Pham, H
   Trawinski, B
TI Analysis of Convolutional Neural Networks and Shape Features for
   Detection and Identification of Malaria Parasites on Thin Blood Smears
SO INTELLIGENT INFORMATION AND DATABASE SYSTEMS, ACIIDS 2018, PT II
SE Lecture Notes in Artificial Intelligence
CT 10th Asian Conference on Intelligent Information and Database Systems
   (ACIIDS)
CY MAR 19-21, 2018
CL Dong Hoi, VIETNAM
AB The gold standard for malaria diagnosis still remains to be microscopy. However, cases from remote areas needing immediate diagnosis and treatment can benefit from a faster diagnostic process. Several intelligent systems for malaria diagnosis have been proposed using different computer vision techniques. In this research, models using convolutional neural networks, and a model using extracted shape features are implemented and compared. The CNN models, one trained from scratch and the other utilizing transfer learning, with accuracies of 92.4% and 93.60%, both outperform the shape feature model in malaria parasite recognition.
CR [Anonymous], 2016, WORLD MAL REP 2016
   Centers for Disease Control Prevention, 2015, MAL
   Delas Penas KE, 2017, 2017 IEEE/ACM SECOND INTERNATIONAL CONFERENCE ON CONNECTED HEALTH - APPLICATIONS, SYSTEMS AND ENGINEERING TECHNOLOGIES (CHASE), P1, DOI 10.1109/CHASE.2017.51
   Makkapati VV, 2009, INT CONF ACOUST SPEE, P1361, DOI 10.1109/ICASSP.2009.4959845
   Math Works, 2014, MEAS PROP IM REG
   Nanoti A., 2016, P 2016 INT C INV COM, V1, P1
   Paniker C.J., 2013, PANIKERS TXB MED PAR
   Pinkaew A, 2015, 2015 8TH BIOMEDICAL ENGINEERING INTERNATIONAL CONFERENCE (BMEICON)
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
NR 9
TC 0
Z9 0
SN 0302-9743
EI 1611-3349
BN 978-3-319-75420-8; 978-3-319-75419-2
PY 2018
VL 10752
BP 472
EP 481
DI 10.1007/978-3-319-75420-8_45
ER

PT S
AU Moon, T
   Nakamura, N
   Gonsalves, T
AF Moon, TaeJun
   Nakamura, Noriyuki
   Gonsalves, Tad
BE Su, R
TI Obstacle detection and recognition using SSD
SO 2018 INTERNATIONAL CONFERENCE ON IMAGE AND VIDEO PROCESSING, AND
   ARTIFICIAL INTELLIGENCE
SE Proceedings of SPIE
CT International Conference on Image and Video Processing, and Artificial
   Intelligence (IVPAI)
CY AUG 15-17, 2018
CL Shanghai, PEOPLES R CHINA
DE Times Roman; image area; acronyms; references
AB Fast obstacle detection is essential for autonomous driving. In this research, we have developed an obstacle detection model using Single Shot Multi Box Detector. SSD is a regression-based object detecting convolutional neural network that takes images as an input to compute localization and classification at once. By using SSD, processing time is dramatically reduced compare to multi shot detector. SSD object detection model was trained using APIs provided by Google in different patterns of number of classes and availability of transfer learning. Increase of the number of classes tended to decrease the detection rate. Training with transfer learning increased the average precision in general. The effectiveness of transfer learning in image recognition can be confirmed. Also there is a difference in average precision depending on the class.
CR Everingham M, 2018, PASCAL          0512
   Liu W., 2015, ARXIV151202325
   Okaya Takayuki, 2015, MLP MACHINE LEARNING
   Ren  Shaoqing, 2016, ARXIV150601497V3
   Russakovsky O., 2014, ARXIV14090575
   Tatsuya Harada, 2017, MLP MACHINE LEARNING
NR 6
TC 0
Z9 0
SN 0277-786X
EI 1996-756X
BN 978-1-5106-2311-8
PY 2018
VL 10836
AR UNSP 108360M
DI 10.1117/12.2514269
ER

PT B
AU Silva, PH
   Luz, E
   Zanlorensi, LA
   Menotti, D
   Moreira, G
AF Silva, Pedro H.
   Luz, Eduardo
   Zanlorensi, Luiz A., Jr.
   Menotti, David
   Moreira, Gladston
GP IEEE
TI Multimodal Feature Level Fusion based on Particle Swarm Optimization
   with Deep Transfer Learning
SO 2018 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC)
SE IEEE Congress on Evolutionary Computation
CT IEEE Congress on Evolutionary Computation (IEEE CEC) as part of the IEEE
   World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
AB There are several biometric-based systems which rely on a single biometric modality, most of them focus on face, iris or fingerprint. Despite the good accuracies obtained with single modalities, these systems are more susceptible to attacks, i.e, spoofing attacks, and noises of all kinds, especially in non-cooperative (in-the-wild) environments. Since noncooperative environments are becoming more and more common, new approaches involving multi-modal biometrics have received more attention. One challenge in multimodal biometric systems is how to integrate the data from different modalities. Initially, we propose a deep transfer learning optimized from a model trained for face recognition achieving outstanding representation for only iris modality. Our feature level fusion by means of features selection targets the use of the Particle Swarm Optimization (PSO) for such aims. In our pool, we have the proposed iris fine-tuned representation and a periocular one from previous work of us. We compare this approach for fusion in feature level against three basic function rules for matching at score level: sum, multi, and min. Results are reported for iris and periocular region (NICE. II competition database) and also in an open-world scenario. The experiments in the NICE. II competition databases showed that our transfer learning representation for iris modality achieved a new state-of-the-art, i.e., decidability of 2.22 and 14.56% of EER. We also yielded a new state-of-the-art result when the fusion at feature level by PSO is done on periocular and iris modalities, i.e., decidability of 3.45 and 5.55% of EER.
CR Andersen-Hoppe E., 2017, 5 INT WORKSH BIOM FO
   Bharadwaj S., 2010, IEEE INT C BIOM THEO, P1, DOI DOI 10.1109/BTAS.2010.5634498
   Bowyer KW, 2008, COMPUT VIS IMAGE UND, V110, P281, DOI 10.1016/j.cviu.2007.08.005
   Crihalmeanu S, 2012, PATTERN RECOGN LETT, V33, P1860, DOI 10.1016/j.patrec.2011.11.006
   Daugman J. G., 1996, P CARDTECH SECURETEC, P223
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   Daugman J, 2007, IEEE T SYST MAN CY B, V37, P1167, DOI 10.1109/TSMCB.2607.903540
   Donahue  J., 2014, P 31 INT C MACH LEAR, P647
   Eberhart RC, 2001, IEEE C EVOL COMPUTAT, P81, DOI 10.1109/CEC.2001.934374
   Ghosh S, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P611, DOI 10.1145/2818346.2823313
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR, P2672, DOI [DOI 10.1017/CBO9781139058452, DOI 10.1001/JAMAINTERNMED.2016.8245]
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hollingsworth K., 2010, P IEEE INT C BIOM TH, P1
   Huang GB., 2007, 0749 U MASS
   Juefei-Xu F., 2011, P INT JOINT C BIOM I, P1, DOI DOI 10.1109/IJCB.2011.6117600
   Kennedy J, 1997, IEEE SYS MAN CYBERN, P4104, DOI 10.1109/ICSMC.1997.637339
   Luz E., 2011, INT C BIOINF COMP BI
   Luz E., 2017, PATTERN RECOGNITION
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Park U., 2009, BIOM THEOR APPL SYST, P1
   Parkhi O.M., 2015, P BR MACH VIS, V1, P6
   Proenca H., 2018, IEEE T INFORM FORENS, V13
   Proenca H, 2012, IEEE T INF FOREN SEC, V7, P798, DOI 10.1109/TIFS.2011.2177659
   Proenca H, 2010, IEEE T PATTERN ANAL, V32, P1529, DOI 10.1109/TPAMI.2009.66
   Santos G, 2012, PATTERN RECOGN LETT, V33, P984, DOI 10.1016/j.patrec.2011.08.017
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Tan CW, 2014, IEEE T IMAGE PROCESS, V23, P3962, DOI 10.1109/TIP.2014.2337714
   Tan CW, 2013, IEEE T IMAGE PROCESS, V22, P3751, DOI 10.1109/TIP.2013.2260165
   Tan TN, 2012, PATTERN RECOGN LETT, V33, P970, DOI 10.1016/j.patrec.2011.08.009
   Vajaria H, 2007, PATTERN RECOGN LETT, V28, P1572, DOI 10.1016/j.patrec.2007.03.019
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang Q, 2012, PATTERN RECOGN LETT, V33, P978, DOI 10.1016/j.patrec.2011.08.014
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Xu J, 2010, B ENTOMOL RES, V100, P359, DOI 10.1017/S0007485310000015
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 35
TC 0
Z9 0
BN 978-1-5090-6017-7
PY 2018
BP 2036
EP 2043
DI 10.1109/CEC.2018.8477817
ER

PT S
AU Liu, F
   Zhang, GQ
   Lu, J
AF Liu, Feng
   Zhang, Guangquan
   Lu, Jie
GP IEEE
TI Unconstrained fuzzy feature fusion for heterogeneous unsupervised domain
   adaptation
SO 2018 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS (FUZZ-IEEE)
SE IEEE International Conference on Fuzzy Systems
CT IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE transfer learning; domain adaptation; fuzzy features; machine learning
AB Domain adaptation can transfer knowledge from the source domain to improve pattern recognition accuracy in the target domain. However, it is rarely discussed when the target domain is unlabeled and heterogeneous with the source domain, which is a very challenging problem in the domain adaptation field. This paper presents a new feature reconstruction method: unconstrained fuzzy feature fusion. Through the reconstructed features of a source and a target domain, a geodesic flow kernel is applied to transfer knowledge between them. Furthermore, the original information of the target domain is also preserved when reconstructing the features of the two domains. Compared to the previous work, this work has two advantages: 1) the sum of the memberships of the original features to fuzzy features no longer must be one, and 2) the original information of the target domain is persevered. As a result of these advantages, this work delivers a better performance than previous studies using two public datasets.
CR Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504
   Behbood V, 2015, IEEE T FUZZY SYST, V23, P1917, DOI 10.1109/TFUZZ.2014.2387872
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Deng W., 2017, ARXIV1711070727CSCV, P1
   Ghifary M, 2017, IEEE T PATTERN ANAL, V39, P1414, DOI 10.1109/TPAMI.2016.2599532
   Gong BQ, 2014, INT J COMPUT VISION, V109, P3, DOI 10.1007/s11263-014-0718-4
   Gong Mingming, 2016, JMLR Workshop Conf Proc, V48, P2839
   Jiang Jing, 2007, P 16 ACM C INF KNOWL, P401
   Kanamori T, 2009, J MACH LEARN RES, V10, P1391
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167
   Liu F, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON CYBERNETICS AND INTELLIGENT SYSTEMS (CIS) AND IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P1, DOI [10.1109/ICCIS.2017.8274739, 10.1109/EMC-B.2017.8260468]
   Liu T., 2017, P INT JOINT C ART IN, P2365
   Long MS, 2016, IEEE T KNOWL DATA EN, V28, P2027, DOI 10.1109/TKDE.2016.2554549
   Lu J, 2015, KNOWL-BASED SYST, V80, P14, DOI 10.1016/j.knosys.2015.01.010
   Muller JS, 2011, J MACH LEARN RES, V12, P3065
   Nguyen HV, 2015, IEEE T IMAGE PROCESS, V24, P5479, DOI 10.1109/TIP.2015.2479405
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Rie K. Ando, 2006, P NEUR INF PROC SYST, P25
   Shi XX, 2013, IEEE T KNOWL DATA EN, V25, P906, DOI 10.1109/TKDE.2011.252
   Shi  Yuan, 2012, P INT C MACH LEARN, P1079
   Sun B., 2016, P 30 AAAI C ART INT, P2058
   Tan B., 2017, P 31 AAAI C ART INT, P2604
   Wang C., 2011, IJCAI, P1541
   Wu QX, 2013, IEEE T SYST MAN CY-S, V43, P875, DOI 10.1109/TSMCA.2012.2226575
   Xiao M, 2015, IEEE T PATTERN ANAL, V37, P54, DOI 10.1109/TPAMI.2014.2343216
   Xu JL, 2014, IEEE T PATTERN ANAL, V36, P2367, DOI 10.1109/TPAMI.2014.2327973
   Yang J., 2007, P 15 INT C MULT, P188, DOI DOI 10.1145/1291233.1291276
   Yeh YR, 2014, IEEE T IMAGE PROCESS, V23, P2009, DOI 10.1109/TIP.2014.2310992
   Zhong Z., 2017, ARXIV171110295CSCV, P1
   Zuo H, 2017, IEEE T FUZZY SYST, V25, P1795, DOI 10.1109/TFUZZ.2016.2633376
   Zuo H, 2016, WD SCI P COMP ENG, V10, P175
NR 33
TC 0
Z9 0
SN 1098-7584
BN 978-1-5090-6020-7
PY 2018
ER

PT S
AU Papez, M
   Quinn, A
AF Papez, Milan
   Quinn, Anthony
BE Pustelnik, N
   Ma, Z
   Tan, ZH
   Larsen, J
TI DYNAMIC BAYESIAN KNOWLEDGE TRANSFER BETWEEN A PAIR OF KALMAN FILTERS
SO 2018 IEEE 28TH INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL
   PROCESSING (MLSP)
SE IEEE International Workshop on Machine Learning for Signal Processing
CT IEEE 28th International Workshop on Machine Learning for Signal
   Processing (MLSP)
CY SEP 17-20, 2018
CL Aalborg, DENMARK
DE Bayesian transfer learning; fully probabilistic design; incomplete
   modelling; Kalman filtering
ID FULLY PROBABILISTIC DESIGN
AB Transfer learning is a framework that includes-among other topics-the design of knowledge transfer mechanisms between Bayesian filters. Transfer learning strategies in this context typically rely on a complete stochastic dependence structure being specified between the participating learning procedures (filters). This paper proposes a method that does not require such a restrictive assumption. The solution in this incomplete modelling case is based on the fully probabilistic design of an unknown probability distribution which conditions on knowledge in the form of an externally supplied distribution. We are specifically interested in the situation where the external distribution accumulates knowledge dynamically via Kalman filtering. Simulations illustrate that the proposed algorithm outperforms alternative methods for transferring this dynamic knowledge from the external Kalman filter.
CR Bengio Y., 2012, J MACHINE LEARNING R, P17
   Bernardo J. M., 1994, BAYESIAN THEORY
   Doucet A, 2009, OXFORD HDB NONLINEAR
   Faragher R, 2012, IEEE SIGNAL PROC MAG, V29, P128, DOI 10.1109/MSP.2012.2203621
   Foley C, 2018, IEEE SIGNAL PROC LET, V25, P487, DOI 10.1109/LSP.2017.2776223
   Isele D., 2017, ARXIV171201106
   Karbalayghareh A., 2018, ARXIV180100857
   Karny M, 1996, AUTOMATICA, V32, P1719, DOI 10.1016/S0005-1098(96)80009-4
   Karny M, 2012, INFORM SCIENCES, V186, P105, DOI 10.1016/j.ins.2011.09.018
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Mayne D.Q., 1966, Automatica, V4, P73, DOI 10.1016/0005-1098(66)90019-7
   Murphy K. P., 2012, MACHINE LEARNING PRO
   Pan S. J., 2015, DATA CLASSIFICATION, P537
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Quinn A, 2017, INT J APPROX REASON, V84, P150, DOI 10.1016/j.ijar.2017.02.001
   Quinn A, 2016, INFORM SCIENCES, V369, P532, DOI 10.1016/j.ins.2016.07.035
   Sarkka S., 2013, BAYESIAN FILTERING S
   SHORE JE, 1980, IEEE T INFORM THEORY, V26, P26, DOI 10.1109/TIT.1980.1056144
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   Torrey  L., 2010, HDB RES MACHINE LEAR, P242
   van Kasteren TLM, 2010, LECT NOTES COMPUT SC, V6030, P283, DOI 10.1007/978-3-642-12654-3_17
   Willner D., 1976, Proceedings of the 1976 IEEE Conference on Decision and Control including the 15th Symposium on Adaptive Processes, P570
   Wilson A., 2012, P ICML WORKSH UNS TR, P217
NR 23
TC 0
Z9 0
SN 2161-0363
BN 978-1-5386-5477-4
PY 2018
ER

PT S
AU Serrano, SA
   Benitez-Jimenez, R
   Nunez-Rosas, L
   Arizmendi, MD
   Greeney, H
   Reyes-Meza, V
   Morales, E
   Escalante, HJ
AF Serrano, Sergio A.
   Benitez-Jimenez, Ricardo
   Nunez-Rosas, Laura
   del Coro Arizmendi, Ma
   Greeney, Harold
   Reyes-Meza, Veronica
   Morales, Eduardo
   Jair Escalante, Hugo
BE MartinezTrinidad, JF
   CarrascoOchoa, JA
   OlveraLopez, JA
   Sarkar, S
TI Automated Detection of Hummingbirds in Images: A Deep Learning Approach
SO PATTERN RECOGNITION
SE Lecture Notes in Computer Science
CT 10th Mexican Conference on Pattern Recognition (MCPR)
CY JUN 27-30, 2018
CL Puebla, MEXICO
DE Image classification; Convolutional neural network; Transfer learning;
   Animal behavior analysis; Hummingbird detection
AB The analysis of natural images has been the topic of research in uncountable articles in computer vision and pattern recognition (e.g., natural images has been used as benchmarks for object recognition and image retrieval). However, despite the research progress in such field, there is a gap in the analysis of certain type of natural images, for instance, those in the context of animal behavior. In fact, biologists perform the analysis of natural images manually without the aid of techniques that were supposedly developed for this purpose. In this context, this paper presents a study on automated methods for the analysis of natural images of hummingbirds with the goal to assist biologists in the study of animal behavior. The automated analysis of hummingbird behavior is challenging mainly because of (1) the speed at which these birds move and interact; (2) the unpredictability of their trajectories; and (3) its camouflage skills. We report a comparative study of two deep learning approaches for the detection of hummingbirds in their nest. Two variants of transfer learning from convolutional neural networks (CNNs) are evaluated in real imagery for hummingbird behavior analysis. Transfer learning is adopted because not enough images are available for training a CNN from scratch, besides, transfer learning is less time consuming. Experimental results are encouraging, as acceptable classification performance is achieved with CNN-based features. Interestingly, a pretrained CNN without fine tunning and a standard classifier performed better in the considered data set.
CR Abadi M, 2016, ARXIV1603044067
   BALTOSSER WH, 1986, WILSON BULL, V98, P353
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Bleiweiss R, 1998, PHYLOGENY BODY MASS
   BROWN BT, 1992, J FIELD ORNITHOL, V63, P393
   Colwell RK, 2000, AM NAT, V156, P495, DOI 10.1086/303406
   Dai  W., 2007, P 24 INT C MACH LEAR, P193, DOI DOI 10.1145/1273496.1273521
   del Coro Arizmendi M, 2012, ORNITOLOGIA NEOTRO S, V23, P71
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Deng J., 2009, CVPR 2009
   Donahue  J., 2014, P 31 INT C MACH LEAR, P647
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Elliott A, 1999, HDB BIRDS WORLD, V5, P388
   Everingham M., 2006, PASCAL VISUAL OBJECT
   Fei-Fei L., 2004, P IEEE C COMP VIS PA, P178, DOI DOI 10.1109/CVPR.2004.109
   Fink M, 2008, INT J COMPUT VISION, V77, P143, DOI 10.1007/s11263-007-0066-8
   Greeney Harold F., 2008, Huitzil, V9, P35
   Greeney HF, 2009, WILSON J ORNITHOL, V121, P809, DOI 10.1676/08-174.1
   Griffin G., 2007, TECHNICAL REPORT
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Johnsgard P. A, 2016, HUMMINGBIRDS N AM
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Memisevic R, 2010, ADV NEURAL INFORM PR, P1603
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pasquale G, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4904, DOI 10.1109/IROS.2016.7759720
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F., 2015, CVPR
   Smith DM, 2009, CONDOR, V111, P641, DOI 10.1525/cond.2009.090089
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   VLECK CM, 1981, OECOLOGIA, V51, P199, DOI 10.1007/BF00540601
NR 35
TC 0
Z9 0
SN 0302-9743
EI 1611-3349
BN 978-3-319-92198-3; 978-3-319-92197-6
PY 2018
VL 10880
BP 155
EP 166
DI 10.1007/978-3-319-92198-3_16
ER

PT S
AU Chen, XY
   Lengelle, R
AF Chen, Xiaoyi
   Lengelle, Regis
BE DeMarsico, M
   DiBaja, GS
   Fred, A
TI Domain Adaptation Transfer Learning by Kernel Representation Adaptation
SO PATTERN RECOGNITION APPLICATIONS AND METHODS
SE Lecture Notes in Computer Science
CT 6th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM)
CY FEB 24-26, 2017
CL Porto, PORTUGAL
AB Domain adaptation, where no labeled target data is available, is a challenging task. To solve this problem, we first propose a new SVM based approach with a supplementary Maximum Mean Discrepancy (MMD)-like constraint. With this heuristic, source and target data are projected onto a common subspace of a Reproducing Kernel Hilbert Space (RKHS) where both data distributions are expected to become similar. Therefore, a classifier trained on source data might perform well on target data, if the conditional probabilities of labels are similar for source and target data, which is the main assumption of this paper. We demonstrate that adding this constraint does not change the quadratic nature of the optimization problem, so we can use common quadratic optimization tools. Secondly, using the same idea that rendering source and target data similar might ensure efficient transfer learning, and with the same assumption, a Kernel Principal Component Analysis (KPCA) based transfer learning method is proposed. Different from the first heuristic, this second method ensures other higher order moments to be aligned in the RKHS, which leads to better performances. Here again, we select MMD as the similarity measure. Then, a linear transformation is also applied to further improve the alignment between source and target data. We finally compare both methods with other transfer learning methods from the literature to show their efficiency on synthetic and real datasets.
CR Blobaum P., 2015, P 23 EUR S ART NEUR
   Chen X, 2017, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON RELIABILITY SYSTEMS ENGINEERING (ICRSE 2017)
   Dudley R. M., 2002, REAL ANAL PROBABILIT, V74
   DUDLEY RM, 1984, LECT NOTES MATH, V1097, P1, DOI DOI 10.1007/BFB0099432
   Fortet R, 1953, ANN SCI ECOLE NORM S, P266
   Gao J, 2008, KDD, P283, DOI DOI 10.1145/1401890.1401928
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Huang CH, 2012, LECT NOTES COMPUT SC, V7583, P342, DOI 10.1007/978-3-642-33863-2_34
   Huang J., 2006, ADV NEURAL INFORM PR, V19, P601
   Jiang J., 2008, LIT SURVEY DOMAIN AD
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Liang FD, 2014, MACH VISION APPL, V25, P1697, DOI 10.1007/s00138-013-0549-2
   Ling X, 2008, P 14 ACM SIGKDD INT, P488, DOI DOI 10.1145/1401890.1401951
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Pan S., 2008, AAAI, P677
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Paulsen V. I, 2009, INTRO THEORY REPROD
   Quanz  Brian, 2009, CIKM, P1327, DOI DOI 10.1145/1645953.1646121
   Ren JT, 2010, LECT NOTES ARTIF INT, V6441, P63, DOI 10.1007/978-3-642-17313-4_7
   Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416
   Serfling R J., 2009, APPROXIMATION THEORE, V162
   Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126
   Smola A, 2006, P 13 INT C ICONIP 20
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Steinwart I, 2002, J MACH LEARN RES, V2, P67
   Tan Q., 2012, ADV DATA MINING APPL, V7713, P223, DOI [10.1007/978-3-642-35527-119, DOI 10.1007/978-3-642-35527-119]
   Tu WT, 2011, PROC INT C TOOLS ART, P865, DOI 10.1109/ICTAI.2011.134
   Uguroglu S, 2011, LECT NOTES ARTIF INT, V6913, P430, DOI 10.1007/978-3-642-23808-6_28
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Z, 2008, LECT NOTES ARTIF INT, V5212, P550, DOI 10.1007/978-3-540-87481-2_36
   Yang SZ, 2012, NEURAL COMPUT APPL, V21, P1801, DOI 10.1007/s00521-012-1084-1
   Zhang P, 2009, IEEE DATA MINING, P627, DOI 10.1109/ICDM.2009.76
NR 35
TC 0
Z9 0
SN 0302-9743
EI 1611-3349
BN 978-3-319-93647-5; 978-3-319-93646-8
PY 2018
VL 10857
BP 45
EP 61
DI 10.1007/978-3-319-93647-5_3
ER

PT S
AU Tirumala, SS
AF Tirumala, Sreenivas Sremath
BE Chaki, N
   Cortesi, A
   Devarakonda, N
TI A Deep Autoencoder-Based Knowledge Transfer Approach
SO PROCEEDINGS OF INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE
   AND DATA ENGINEERING
SE Lecture Notes on Data Engineering and Communications Technologies
CT 1st International Conference on Computational Intelligence & Data
   Engineering (ICCIDE)
CY JUL 14-15, 2017
CL Lakireddy Bali Reddy Coll Engn, Mylavaram, INDIA
HO Lakireddy Bali Reddy Coll Engn
DE Deep autoencoders; Knowledge transfer; Hierarchical dataset; Corrupted
   dataset
AB Deep Transfer Learning or DTS has proven successful with deep neural networks and deep belief networks. However, there has been limited research on to using deep autoencoder (DAE)-based network to implement DTS. This paper for the first time attempts to identify transferable features in the form of learning and transfer them to another network implementing a simple DTS mechanism. In this paper, a transfer of knowledge process is proposed where in knowledge is transferred from one Deep autoencoder network to another. This knowledge transfer has helped to improve the classification accuracy of the receiving autoencoder, particularly when experimented using corrupted dataset. The experiments are carried out on a texa based hierarchical dataset. Firstly, a DAE is trained with regular undamaged dataset to achieve maximum accuracy. Then, a distorted dataset was used to train second DAEN for classification with which only 56.7% of the data is correctly classified. Then a set of weights are transferred from from first DAEN to the second DAEN which resulted in an an improvement of classification accuracy by about 22%. The key contribution of this paper is highlighting importance of knowledge transfer between two deep autoencoder networks which is proposed for the first time.
CR Bengio Y., 2007, P ADV NEUR INF PROC, P153
   Bengio Y, 2007, LARGE SCALE KERNEL M
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Ciresan Dan C., 2012, NEUR NETW IJCNN 2012, P1, DOI DOI 10.1109/IJCNN.2012.6252544
   Graves A., 2009, OFFLINE HANDWRITING, P545
   Gutstein S, 2008, INT J ARTIF INTELL T, V17, P555, DOI 10.1142/S0218213008004059
   He K., 2015, P INT C COMP VIS, P1
   Hingu Dharmendra, 2015, 2015 International Conference on Communication, Information & Computing Technology (ICCICT), P1, DOI 10.1109/ICCICT.2015.7045732
   Kandaswamy Chetak, 2014, Artificial Neural Networks and Machine Learning - ICANN 2014. 24th International Conference on Artificial Neural Networks. Proceedings: LNCS 8681, P265, DOI 10.1007/978-3-319-11179-7_34
   LI EY, 1994, INFORM MANAGE, V27, P303, DOI 10.1016/0378-7206(94)90024-8
   Long M., 2015, INT C MACH LEARN, P97
   Milligan D. K., 1990, FUNDAMENTAL STRUCTUR, P997
   Terekhov A. V., 2015, KNOWLEDGE TRANSFER D, P268
   Tirumala SS, 2015, LECT NOTES COMPUT SC, V9489, P492, DOI 10.1007/978-3-319-26532-2_54
   Tirumala SS, 2014, P 2 INT WORKSH ART I, P164
   Waszczyszyn Z., 1999, FUNDAMENTALS ARTIFIC, P1
   Xiong C., 2015, CIRCUITS SYSTEMS VID, P1
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 18
TC 0
Z9 0
SN 2367-4512
BN 978-981-10-6319-0; 978-981-10-6318-3
PY 2018
VL 9
BP 277
EP 284
DI 10.1007/978-981-10-6319-0_23
ER

PT B
AU Al Mufti, M
   Al Hadhrami, E
   Taha, B
   Werghi, N
AF Al Mufti, Maha
   Al Hadhrami, Esra
   Taha, Bilal
   Werghi, Naoufel
GP IEEE
TI SAR Automatic Target Recognition Using Transfer Learning Approach
SO 2018 INTERNATIONAL CONFERENCE ON INTELLIGENT AUTONOMOUS SYSTEMS (ICOIAS)
CT International Conference on Intelligent Autonomous Systems (ICoIAS)
CY MAR 01-03, 2018
CL Singapore, SINGAPORE
DE component; deep learning; synthetic aperture radar; automatic target
   recognition
ID SUPPORT VECTOR MACHINES; IMAGES; CLASSIFICATION; MODELS
AB In this paper we propose a new approach for Synthetic Aperture Radar (SAR) automatic target recognition (ATR). One of the main obstacles in SAR ATR is the limited availability of datasets that are used for training. In this paper, a deep learning approach is employed for ATR. The proposed scheme is based on employing a pre-trained convolutional neural network (CNNs) as transfer learning. A pre-trained CNN namely AlexNet is utilized as a feature extractor whereas the output features are used to train a multiclass support vector machine (SVM) classifier. The effectiveness of the proposed framework is verified on a public database where the final result using three target classes attain an accuracy of 99.4%.
CR Cao ZJ, 2012, IEEE GLOBE WORK, P1450, DOI 10.1109/GLOCOMW.2012.6477798
   Chen SZ, 2016, IEEE T GEOSCI REMOTE, V54, P4806, DOI 10.1109/TGRS.2016.2551720
   Ding J, 2016, IEEE GEOSCI REMOTE S, V13, P364, DOI 10.1109/LGRS.2015.2513754
   Dong GG, 2017, IEEE T IMAGE PROCESS, V26, P2892, DOI 10.1109/TIP.2017.2692524
   Dong GG, 2015, IEEE GEOSCI REMOTE S, V12, P199, DOI 10.1109/LGRS.2014.2332076
   Huang ZL, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090907
   Kang CY, 2016, INT GEOSCI REMOTE SE, P1146, DOI 10.1109/IGARSS.2016.7729290
   Keydel ER, 1996, P SOC PHOTO-OPT INS, V2757, P228, DOI 10.1117/12.242059
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lin Zhao, 2017, IEEE GEOSCIENCE REMO
   Malmgren-Hansen D, 2017, IEEE GEOSCI REMOTE S, V14, P1484, DOI 10.1109/LGRS.2017.2717486
   NOVAK LM, 1997, LINCOLN LAB J, V10
   Pradhan B, 2013, COMPUT GEOSCI-UK, V51, P350, DOI 10.1016/j.cageo.2012.08.023
   Song HB, 2016, APPL SCI-BASEL, V6, DOI 10.3390/app6010026
   Tison C, 2007, INT GEOSCI REMOTE SE, P456, DOI 10.1109/IGARSS.2007.4422829
   VASUKI P, 2012, RES J APPL SCI ENG T, V4, P5510
   Wang HP, 2015, INT GEOSCI REMOTE SE, P3743, DOI 10.1109/IGARSS.2015.7326637
   Wilmanski Michael, 2016, SPIE DEFENSE SECURIT
   Yang Y, 2005, Sixth International Conference on Software Engineerng, Artificial Intelligence, Networking and Parallel/Distributed Computing and First AICS International Workshop on Self-Assembling Wireless Networks, Proceedings, P2
   Zhao Q, 2001, IEEE T AERO ELEC SYS, V37, P643, DOI 10.1109/7.937475
NR 20
TC 0
Z9 0
BN 978-1-5386-6331-8
PY 2018
BP 1
EP 4
ER

PT B
AU Xie, ZP
   Lv, WF
   Ali, SMA
   Du, BW
   Huang, RH
AF Xie, Zhipu
   Lv, Weifeng
   Ali, Syed Muhammad Asim
   Du, Bowen
   Huang, Runhe
GP IEEE
TI Anomaly Prediction in Passenger Flow with Knowledge Transfer Method
SO 2018 16TH IEEE INT CONF ON DEPENDABLE, AUTONOM AND SECURE COMP, 16TH
   IEEE INT CONF ON PERVAS INTELLIGENCE AND COMP, 4TH IEEE INT CONF ON BIG
   DATA INTELLIGENCE AND COMP, 3RD IEEE CYBER SCI AND TECHNOL CONGRESS
   (DASC/PICOM/DATACOM/CYBERSCITECH)
CT 16th IEEE Int Conf on Dependable, Autonom and Secure Comp/16th IEEE Int
   Conf on Pervas Intelligence and Comp/4th IEEE Int Conf on Big Data
   Intelligence and Comp/3rd IEEE Cyber Sci and Technol Congress
   (DASC/PiCom/DataCom/CyberSciTech)
CY AUG 12-15, 2018
CL Athens, GREECE
DE Passenger Flow; Prediction Model; Knowledge Transfer
AB Predicting anomaly in travel demand is a crucial finding from smart card data analytics. The output of these predictions is a significant contribution to planning sustainable public transport system and generating possible knowledge for transportation learning models. This paper investigates the anomaly effects of the surge in bus passengers demand and compare it with an increase in taxi demand. Indeed, both short-term and long-term demands reveal different patterns of passengers in uncertain situations. In pursuit of our goal, we estimated the similarity in stations by both selected and latent features where pre trained knowledge are combined as an ensemble with different weights. We present Surge Prediction and Knowledge Transfer (SPKT) model that uses Seq2Seq method combined with Multi-source Transfer Learning method on travel patterns extracted from smart card data to classify source stations and target station. To illustrate the demands blueprint, we considered multiple source stations as input to the predictor, to develop a mechanism that bridges the knowledge transfer learning with the targeted stations. To exemplify our method, we use a case study of an event with passenger surge. From experiments, we found that transferring knowledge can make the surge prediction better compared to only limited training data for the target stations. The results have proved the effectiveness of surge predictions and knowledge transfer for learning models.
CR Anvari S, 2016, J ADV TRANSPORT, V50, P25, DOI 10.1002/atr.1332
   Bai Y, 2017, APPL SOFT COMPUT, V58, P669, DOI 10.1016/j.asoc.2017.05.011
   Bai Y, 2015, J HYDROL ENG, V20, DOI 10.1061/(ASCE)HE.1943-5584.0001101
   Box G., 2013, TIME SERIES ANAL FOR
   Chen JH, 2016, WEB INF SYST ENG INT, P27, DOI 10.1007/978-3-319-44198-6_2
   Chen YH, 2012, APPL SOFT COMPUT, V12, P274, DOI 10.1016/j.asoc.2011.08.045
   Christodoulidis S, 2017, IEEE J BIOMED HEALTH, V21, P76, DOI 10.1109/JBHI.2016.2636929
   Dai W., 2007, P 24 INT C MACH LEAR
   Ding ZM, 2018, IEEE T NEUR NET LEAR, V29, P310, DOI 10.1109/TNNLS.2016.2618765
   Drucker Harris, 1997, INT C MACH LEARN ICM
   Du BW, 2016, IEEE T COMPUT, V65, P3524, DOI 10.1109/TC.2016.2529623
   Feng Kaiyu, 2017, ARXIV170909287
   Gan M, 2014, APPL SOFT COMPUT, V24, P13, DOI 10.1016/j.asoc.2014.06.047
   He J., 2011, P 28 INT C MACH LEAR, P25
   Hu YR, 2015, INT C INTEL HUM MACH, DOI 10.1109/IHMSC.2015.112
   Lee Roy Ka-Wei, 2014, International Journal of Engineering and Technology, V6, P431, DOI 10.7763/IJET.2014.V6.737
   Li Linchao, 2017, KSCE J CIV ENG, P1
   Mikolov Tomas, 2013, ADV NEURAL INFORM PR
   Milenkovi'c M., 2016, TRANSPORT
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pang SN, 2014, COGN COMPUT, V6, P304, DOI 10.1007/s12559-013-9238-8
   Pardoe David, 2010, P 27 INT C INT C MAC
   Sato A, 2015, IEEE 12TH INT CONF UBIQUITOUS INTELLIGENCE & COMP/IEEE 12TH INT CONF ADV & TRUSTED COMP/IEEE 15TH INT CONF SCALABLE COMP & COMMUN/IEEE INT CONF CLOUD & BIG DATA COMP/IEEE INT CONF INTERNET PEOPLE AND ASSOCIATED SYMPOSIA/WORKSHOPS, P609, DOI 10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.120
   Shekhar S, 2013, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.2013.53
   Susanty A, 2012, PROC ECON FINANC, V4, P23, DOI 10.1016/S2212-5671(12)00317-6
   Tang Tao, 2018, IET INTELLIGENT TRAN
   Vapnik Vladimir, 2015, J MACHINE LEARNING R, V16, P55
   Wang L, 2018, ARXIV180200386
   Wei Ying, 2016, P 22 ACM SIGKDD INT
   Wei Y, 2012, TRANSPORT RES C-EMER, V21, P148, DOI 10.1016/j.trc.2011.06.009
   Williams C, 2007, STRATEG MANAGE J, V28, P867, DOI 10.1002/smj.614
   Yang Yang, 2015, CICTP 2015. Efficient, Safe and Green Multimodal Transportation. 15th COTA International Conference of Transportation Professionals. Proceedings, P1143, DOI 10.1061/9780784479292.106
   Yao Y, 2010, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2010.5539857
   Yuan Zhixiang, 2017, COMP INT INF SYST CI
   Zhang K., 2015, P 29 AAAI C ART INT, P3150
   Zhen Jiangjie, 2017, ROB BIOM ROBIO 2017
NR 36
TC 0
Z9 0
BN 978-1-5386-7518-2
PY 2018
BP 270
EP 277
DI 10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00054
ER

PT S
AU Adama, DA
   Lotfi, A
   Langensiepen, C
   Lee, K
AF Adama, David Ada
   Lotfi, Ahmad
   Langensiepen, Caroline
   Lee, Kevin
BE Chao, F
   Schockaert, S
   Zhang, Q
TI Human Activities Transfer Learning for Assistive Robotics
SO ADVANCES IN COMPUTATIONAL INTELLIGENCE SYSTEMS
SE Advances in Intelligent Systems and Computing
CT 17th Annual UK Workshop on Computational Intelligence (UKCI)
CY SEP 06-08, 2017
CL Cardiff, ENGLAND
DE Activity recognition; Activity classification; Assistive robotics
ID RECOGNITION
AB Assisted living homes aim to deploy tools to promote better living of elderly population. One of such tools is assistive robotics to perform tasks a human carer would normally be required to perform. For assistive robots to perform activities without explicit programming, a major requirement is learning and classifying activities while it observes a human carry out the activities. This work proposes a human activity learning and classification system from features obtained using 3D RGB-D data. Different classifiers are explored in this approach and the system is evaluated on a publicly available data set, showing promising results which is capable of improving assistive robots performance in living environments.
CR Adama D. A., 2017, P 10 C PERV TECHN RE
   Iglesias JA, 2010, INT J NEURAL SYST, V20, P355, DOI 10.1142/S0129065710002462
   Bezdek J. C., 1981, PATTERN RECOGNITION
   Cippitelli E., 2016, COMPUT INTEL NEUROSC, V2016, DOI [10.1155/2016/4351435, DOI 10.1155/2016/4351435]
   Faria DR, 2014, IEEE ROMAN, P732, DOI 10.1109/ROMAN.2014.6926340
   Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011
   Huiquan Zhang, 2012, 2012 International Conference on Machine Learning and Cybernetics (ICMLC 2012), P1058, DOI 10.1109/ICMLC.2012.6359501
   Hussein M. E., 2013, IJCAI, P2466
   Jalal A, 2014, 2014 11TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P74, DOI 10.1109/AVSS.2014.6918647
   Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446
   Kviatkovsky I., 2014, IEEE C COMP VIS PATT
   Li SZ, 2015, NEUROCOMPUTING, V151, P565, DOI 10.1016/j.neucom.2014.06.086
   Lu J, 2015, KNOWL-BASED SYST, V80, P14, DOI 10.1016/j.knosys.2015.01.010
   Microsoft, DEV KIN WIND
   Shell J, 2015, INFORM SCIENCES, V293, P59, DOI 10.1016/j.ins.2014.09.004
   Sung J., 2011, P AAAI WORKSH PATT A, V64, P47
   Wei P, 2013, IEEE I CONF COMP VIS, P3136, DOI 10.1109/ICCV.2013.389
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Ye Gu, 2012, Proceedings of the 2012 IEEE International Conference on Robotics and Biomimetics (ROBIO), P1379, DOI 10.1109/ROBIO.2012.6491161
NR 19
TC 0
Z9 0
SN 2194-5357
EI 2194-5365
BN 978-3-319-66939-7; 978-3-319-66938-0
PY 2018
VL 650
BP 253
EP 264
DI 10.1007/978-3-319-66939-7_22
ER

PT B
AU Al Hadhrami, E
   Al Mufti, M
   Taha, B
   Werghi, N
AF Al Hadhrami, Esra
   Al Mufti, Maha
   Taha, Bilal
   Werghi, Naoufel
GP IEEE
TI Transfer Learning with Convolutional Neural Networks for Moving Target
   Classification with Micro-Doppler Radar Spectrograms
SO 2018 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND BIG DATA
   (ICAIBD)
CT International Conference on Artificial Intelligence and Big Data
   (ICAIBD)
CY MAY 26-28, 2018
CL Chengdu, PEOPLES R CHINA
DE convolutional neural network; transfer learning; AlexNet; micro-doppler;
   radar classification; automatic target recognition
AB In this work, we propose a transfer learning approach with Convolutional Neural Networks (CNNs) for radar Automatic Target Recognition (ATR). Radar echo signals of moving targets introduce micro-Doppler signatures that are widely used in classifying moving targets. Spectrograms have the advantage of expressing the distinctive micro-Doppler signatures of different targets, and thus fed as 2D images to a CNN model. A pre-trained CNN model namely AlexNet is employed as a feature extractor in which feature maps can be extracted from any of the layers to train a classical classifier. SoftMax classifier have been used in this approach. The efficiency of the presented framework is demonstrated on the public RadEch database of 8 ground moving target classes, in which the experimental results indicate that our methodology significantly outperforms other competitive state-of-the-art methods with an accuracy of 99.9%.
CR Andric M., 2010, 10 S NEUR NETW APPL
   Andric M., 2010, 18 TEL FOR TELFOR BE
   Andric M, 2014, RADIOENGINEERING, V23, P11
   Bjorklund S., 2012, 2012 IEEE Radar Conference (RadarCon), P934, DOI 10.1109/RADAR.2012.6212271
   Bjorklund S, 2015, IET RADAR SONAR NAV, V9, P1181, DOI 10.1049/iet-rsn.2015.0084
   Chen VC, 2006, IEEE T AERO ELEC SYS, V42, P2, DOI 10.1109/TAES.2006.1603402
   Clemente C, 2015, IEEE T AERO ELEC SYS, V51, P417, DOI 10.1109/TAES.2014.130762
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Kim BK, 2017, IEEE GEOSCI REMOTE S, V14, P38, DOI 10.1109/LGRS.2016.2624820
   Kim Y, 2016, IEEE GEOSCI REMOTE S, V13, P8, DOI 10.1109/LGRS.2015.2491329
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Molchanov P., 2012, 2012 IEEE Radar Conference (RadarCon), P366, DOI 10.1109/RADAR.2012.6212166
   Nogueira K, 2017, PATTERN RECOGN, V61, P539, DOI 10.1016/j.patcog.2016.07.001
   Park J, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16121990
   Serir A., 2012, 2012 11th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA), P995, DOI 10.1109/ISSPA.2012.6310701
   Simonyan K., 2014, CORR
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   van Dorp P, 2003, IEE P-RADAR SON NAV, V150, P356, DOI 10.1049/ip-rsn:20030568
   Zabalza J, 2014, IEEE T AERO ELEC SYS, V50, P2304, DOI 10.1109/TAES.2014.130082
NR 19
TC 0
Z9 0
BN 978-1-5386-6987-7
PY 2018
BP 148
EP 154
ER

PT B
AU Al Mufti, M
   Al Hadhrami, E
   Taha, B
   Werghi, N
AF Al Mufti, Maha
   Al Hadhrami, Esra
   Taha, Bilal
   Werghi, Naoufel
GP IEEE
TI Automatic Target Recognition in SAR Images Comparison Between
   Pre-trained CNNs in a Tranfer Learning Based Approach
SO 2018 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND BIG DATA
   (ICAIBD)
CT International Conference on Artificial Intelligence and Big Data
   (ICAIBD)
CY MAY 26-28, 2018
CL Chengdu, PEOPLES R CHINA
DE deep learning; synthetic aperture radar; transfer learning; automatic
   target recognition
AB Synthetic aperture radar (SAR) are high resolution imaging radar systems. In many SAR applications classifying objects that are detected within the SAR image is important. In this paper an approach is proposed to tackle the Synthetic SAR Automatic Target Recognition (ATR) problem. The proposed scheme is based on a transfer leaning approach where three different pre-trained Convolutional Neural Networks (CNNs) are used as feature extractors in combination with a Support Vector Machine classifier (SVM). The CNNs used in this paper are AlexNet, VGG16 and GoogLeNet. The performance of these three CNNs is compared in regards to the SAR-ATR problem; where it is observed that AlexNet gives the best performance accuracy of 99.27%.
CR Agarap A. F. M., 2017, ARXIV171203541
   Alalshekmubarak A., 2013, INN INF TECHN IIT 20, P42
   Anagnostopoulos GC, 2009, NONLINEAR ANAL-THEOR, V71, pE2934, DOI 10.1016/j.na.2009.07.030
   Chen SZ, 2014, 2014 INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS (DSAA), P541, DOI 10.1109/DSAA.2014.7058124
   Chen SZ, 2016, IEEE T GEOSCI REMOTE, V54, P4806, DOI 10.1109/TGRS.2016.2551720
   Clemente C, 2015, IET RADAR SONAR NAV, V9, P457, DOI 10.1049/iet-rsn.2014.0296
   Ding J, 2016, IEEE GEOSCI REMOTE S, V13, P364, DOI 10.1109/LGRS.2015.2513754
   Han XB, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9080848
   Huang ZL, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090907
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lin Z, 2017, IEEE GEOSCI REMOTE S, V14, P1091, DOI 10.1109/LGRS.2017.2698213
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Park JI, 2013, IEEE GEOSCI REMOTE S, V10, P476, DOI 10.1109/LGRS.2012.2210385
   Simonyan K., 2014, ARXIV14091556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang  Y., 2013, ARXIV13060239
   Wagner S., 2014, P 17 INT C INF FUS F, P1
   Wilmanski Michael, 2016, SPIE DEFENSE SECURIT
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
   Zeng Q, 2017, IEEE CONF IMAGING SY, P122
   Zhu Z. Q., 2017, 20 INT C EC VECH REN, P1
NR 21
TC 0
Z9 0
BN 978-1-5386-6987-7
PY 2018
BP 160
EP 164
ER

PT B
AU Xue, YJ
   Beauseroy, P
AF Xue, Yongjian
   Beauseroy, Pierre
BE DeMarsico, M
   DiBaja, GS
   Fred, A
TI Transfer Learning to Adapt One Class SVM Detection to Additional
   Features
SO PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION
   APPLICATIONS AND METHODS (ICPRAM 2018)
CT 7th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM)
CY JAN 16-18, 2018
CL Funchal, PORTUGAL
DE Transfer Learning; Multi-task Learning; Outliers Detection; One Class
   Classification
ID SUPPORT
AB In this paper, we use the multi-task learning idea to solve a problem of detection with one class SVM when new sensors are added to the system. The main idea is to adapt the detection system to the upgraded sensor system. To solve that problem, the kernel matrix of multi-task learning model can be divided into two parts, one part is based on the former features and the other part is based on the new features. Typical estimation methods can be used to fill the corresponding new features in the old detection system, and a variable kernel is used for the new features in order to balance the importance of the new features with the number of observed samples. Experimental results show that it can keep the false alarm rate relatively stable and decrease the miss alarm rate rapidly as the number of samples increases in the target task.
CR Chang CC, 2001, NEURAL COMPUT, V13, P2119, DOI 10.1162/089976601750399335
   Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109, DOI DOI 10.1145/1014052.1014067
   He XY, 2014, NEUROCOMPUTING, V133, P416, DOI 10.1016/j.neucom.2013.12.022
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Silverman B.W., 1986, DENSITY ESTIMATION S, V26
   Tax DMJ, 1999, PATTERN RECOGN LETT, V20, P1191, DOI 10.1016/S0167-8655(99)00087-2
   Xue Y, 2017, PATTERN RECOGNITION
   Xue YJ, 2016, INT C PATT RECOG, P1571, DOI 10.1109/ICPR.2016.7899861
   Yang HC, 2011, NEUROSURGERY, V68, P682, DOI 10.1227/NEU.0b013e318207a58b
NR 10
TC 0
Z9 0
BN 978-989-758-276-9
PY 2018
BP 78
EP 85
DI 10.5220/0006553200780085
ER

PT B
AU Sakurai, S
   Uchiyama, H
   Shimada, A
   Arita, D
   Taniguchi, R
AF Sakurai, Shunsuke
   Uchiyama, Hideaki
   Shimada, Atsushi
   Arita, Daisaku
   Taniguchi, Rin-ichiro
BE DeMarsico, M
   DiBaja, GS
   Fred, A
TI Two-step Transfer Learning for Semantic Plant Segmentation
SO PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION
   APPLICATIONS AND METHODS (ICPRAM 2018)
CT 7th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM)
CY JAN 16-18, 2018
CL Funchal, PORTUGAL
DE Semantic Segmentation; Transfer Learning; Deep Learning; CNN; Plant
   Segmentation
AB We discuss the applicability of a fully convolutional network (FCN), which provides promising performance in semantic segmentation tasks, to plant segmentation tasks. The challenge lies in training the network with a small dataset because there are not many samples in plant image datasets, as compared to object image datasets such as ImageNet and PASCAL VOC datasets. The proposed method is inspired by transfer learning, but involves a two-step adaptation. In the first step, we apply transfer learning from a source domain that contains many objects with a large amount of labeled data to a major category in the plant domain. Then, in the second step, category adaptation is performed from the major category to a minor category with a few samples within the plant domain. With leaf segmentation challenge (LSC) dataset, the experimental results confirm the effectiveness of the proposed method such that F-measure criterion was, for instance, 0.953 for the A2 dataset, which was 0.355 higher than that of direct adaptation, and 0.527 higher than that of non-adaptation.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Minervini M, 2014, ECOL INFORM, V23, P35, DOI 10.1016/j.ecoinf.2013.07.004
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pape J.-M., 2014, P 13 EUR C COMP VIS, P61
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scharr H., 2014, EUR C COMP VIS, P6
   Scharr H, 2015, MACHI VISION, P1
   van Opbroek A, 2015, IEEE T MED IMAGING, V34, P1018, DOI 10.1109/TMI.2014.2366792
   Yin X, 2014, P IEEE C IMAG PROC
NR 11
TC 0
Z9 0
BN 978-989-758-276-9
PY 2018
BP 332
EP 339
DI 10.5220/0006576303320339
ER

PT B
AU Roman-Jimenez, G
   Viard-Gaudin, C
   Granet, A
   Mouchere, H
AF Roman-Jimenez, Geoffrey
   Viard-Gaudin, Christian
   Granet, Adeline
   Mouchere, Harold
BE DeMarsico, M
   DiBaja, GS
   Fred, A
TI Transfer Learning for Structures Spotting in Unlabeled Handwritten
   Documents using Randomly Generated Documents
SO PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION
   APPLICATIONS AND METHODS (ICPRAM 2018)
CT 7th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM)
CY JAN 16-18, 2018
CL Funchal, PORTUGAL
DE Handwritting Recognition; Image Generation; Digit Detection; Deep Neural
   Networks; Knowledge Transfer
AB Despite recent achievements in handwritten text recognition due to major advances in deep neural networks, historical handwritten documents analysis is still a challenging problem because of the requirement of large annotated training database. In this context, knowledge transfer of neural networks pre-trained on already available labeled data could allow us to process new collections of documents. In this study, we focus on localization of structures at the word-level, distinguishing words from numbers, in unlabeled handwritten documents. We based our approach on a transductive transfer learning paradigm using a deep convolutional neural network pre-trained on artificial labeled images randomly generated with strokes, word and number patches. We designed our model to predict a mask of the structures positions at the pixel-level, directly from the pixel values. The model has been trained using 100,000 generated images. The classification performances of our model were assessed by using randomly generated images coming from a different set of images of words and digits. At the pixel level, the averaged accuracy of the proposed structures detection system reach 96.1%. We evaluated the transfer capability of our model on two datasets of real handwritten documents unseen during the training. Results show that our model is able to distinguish most "digits" structures from "word" structures while avoiding other various structures present in the documents, showing the good transferability of the system to real documents.
CR Augustin E., 2006, P WORKSH FRONT HANDW
   Bergstra J., 2010, P 9 PYTH SCI C, P1
   Butt UM, 2016, INT CONF FRONT HAND, P19, DOI [10.1109/ICFHR.2016.14, 10.1109/ICFHR.2016.0017]
   Cethefi T, 2016, ANR14CE310017
   Delalandre M, 2010, INT J DOC ANAL RECOG, V13, P187, DOI 10.1007/s10032-010-0120-x
   Dieleman S, 2015, LASAGNE 1 RELEASE
   Dumoulin V., 2016, ARXIV160307285
   Gorodkin J, 2004, COMPUT BIOL CHEM, V28, P367, DOI 10.1016/j.compbiolchem.2004.09.006
   Grosicki E, 2011, PROC INT CONF DOC, P1459, DOI 10.1109/ICDAR.2011.290
   Kieu V. C, 2013, 12 INT C DOC AN REC
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Luca E. D, 2011, REPERTOIRE COMEDIE I
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9
   Moysset B, 2016, INT CONF FRONT HAND, P1, DOI [10.1109/ICFHR.2016.0014, 10.1109/ICFHR.2016.11]
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Viard-Gaudin C., 1999, Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318), P455, DOI 10.1109/ICDAR.1999.791823
NR 20
TC 0
Z9 0
BN 978-989-758-276-9
PY 2018
BP 417
EP 425
DI 10.5220/0006598204170425
ER

PT B
AU Granet, A
   Morin, E
   Mouchere, H
   Quiniou, S
   Viard-Gaudin, C
AF Granet, Adeline
   Morin, Emmanuel
   Mouchere, Harold
   Quiniou, Solen
   Viard-Gaudin, Christian
BE DeMarsico, M
   DiBaja, GS
   Fred, A
TI Transfer Learning for Handwriting Recognition on Historical Documents
SO PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION
   APPLICATIONS AND METHODS (ICPRAM 2018)
CT 7th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM)
CY JAN 16-18, 2018
CL Funchal, PORTUGAL
DE Handwriting Recognition; Historical Document; Transfer Learning; Deep
   Neural Network; Unlabeled Data
AB In this work, we investigate handwriting recognition on new historical handwritten documents using transfer learning. Establishing a manual ground-truth of a new collection of handwritten documents is time consuming but needed to train and to test recognition systems. We want to implement a recognition system without performing this annotation step. Our research deals with transfer learning from heterogeneous datasets with a ground-truth and sharing common properties with a new dataset that has no ground-truth. The main difficulties of transfer learning lie in changes in the writing style, the vocabulary, and the named entities over centuries and datasets. In our experiment, we show how a CNN-BLSTM-CTC neural network behaves, for the task of transcribing handwritten titles of plays of the Italian Comedy, when trained on combinations of various datasets such as RIMES, Georges Washington, and Los Esposalles. We show that the choice of the training datasets and the merging methods are determinant to the results of the transfer learning task.
CR Arvanitopoulos N, 2014, INT CONF FRONT HAND, P726, DOI 10.1109/ICFHR.2014.127
   Augustin E, 2006, ICFHR
   BUNKE H, 1995, PATTERN RECOGN, V28, P1399, DOI 10.1016/0031-3203(95)00013-P
   Cloppet F, 2016, INT CONF FRONT HAND, P590, DOI [10.1109/ICFHR.2016.106, 10.1109/ICFHR.2016.0113]
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fischer A, 2012, PATTERN RECOGN LETT, V33, P934, DOI 10.1016/j.patrec.2011.09.009
   Fischer A, 2009, 2009 15TH INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA PROCEEDINGS (VSMM 2009), P137, DOI 10.1109/VSMM.2009.26
   Frinken V, 2010, Proceedings 2010 12th International Conference on Frontiers in Handwriting Recognition (ICFHR 2010), P352, DOI 10.1109/ICFHR.2010.61
   Frinken V, 2012, IEEE T PATTERN ANAL, V34, P211, DOI 10.1109/TPAMI.2011.113
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI 10.1007/978-3-642-24797-2
   Graves A., 2009, ADV NEURAL INFORM PR, V22, P545
   Grosicki Emmanuele, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1398, DOI 10.1109/ICDAR.2009.184
   Grosicki E, 2011, PROC INT CONF DOC, P1459, DOI 10.1109/ICDAR.2011.290
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Koerich AL, 2002, EIGHTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION: PROCEEDINGS, P99, DOI 10.1109/IWFHR.2002.1030893
   Kozielski M, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P121, DOI 10.1109/DAS.2014.8
   Lavrenko V, 2004, FIRST INTERNATIONAL WORKSHOP ON DOCUMENT IMAGE ANALYSIS FOR LIBRARIES, PROCEEDINGS, P278, DOI 10.1109/DIAL.2004.1263256
   Llados J, 2012, IJPRAI, V26
   Moysset B, 2014, INT CONF FRONT HAND, P297, DOI 10.1109/ICFHR.2014.57
   Murdock M, 2015, 2015 13TH IAPR INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION (ICDAR), P1171, DOI 10.1109/ICDAR.2015.7333945
   Oprean C, 2013, PROC INT CONF DOC, P989, DOI 10.1109/ICDAR.2013.199
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Puigcerver J, 2015, 2015 13TH IAPR INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION (ICDAR), P1176, DOI 10.1109/ICDAR.2015.7333946
   Rath TM, 2007, INT J DOC ANAL RECOG, V9, P139, DOI [10.1007/s10032-006-0027-8, 10.1007/s10032-007-0027-8]
   Romero V, 2013, PATTERN RECOGN, V46, P1658, DOI 10.1016/j.patcog.2012.11.024
   Senior AW, 1998, IEEE T PATTERN ANAL, V20, P309, DOI 10.1109/34.667887
   Suryani D, 2016, INT CONF FRONT HAND, P193, DOI [10.1109/ICFHR.2016.0046, 10.1109/ICFHR.2016.43]
   Terasawa Kengo, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P116, DOI 10.1109/ICDAR.2009.118
   Voigtlaender P, 2016, ICFHR, P2228
   Pham V, 2014, INT CONF FRONT HAND, P285, DOI 10.1109/ICFHR.2014.55
NR 30
TC 1
Z9 1
BN 978-989-758-276-9
PY 2018
BP 432
EP 439
DI 10.5220/0006598804320439
ER

PT B
AU Jia, KG
   Liu, ZY
   Wei, Q
   Qiao, F
   Liu, XJ
   Yang, Y
   Fan, H
   Yang, HZ
AF Jia, Kaige
   Liu, Zheyu
   Wei, Qi
   Qiao, Fei
   Liu, Xinjun
   Yang, Yi
   Fan, Hua
   Yang, Huazhong
GP IEEE
TI Calibrating Process Variation at System Level with In-Situ Low-Precision
   Transfer Learning for Analog Neural Network Processors
SO 2018 55TH ACM/ESDA/IEEE DESIGN AUTOMATION CONFERENCE (DAC)
CT 55th ACM/ESDA/IEEE Design Automation Conference (DAC)
CY JUN 24-28, 2018
CL San Francisco, CA
DE Analog Neural Network; Process Variation; Low-Precision; In-Situ
   Transfer Learning
ID ERROR RESILIENCE; IMPLEMENTATION
AB Process Variation (PV) may cause accuracy loss of the analog neural network (ANN) processors, and make it hard to be scaled down, as well as feasibility degrading. This paper first analyses the impact of PV on the performance of ANN dhips. Then proposes an in-situ transfer learning method at system level to reduce PV's influence with low-precision back-propagation. Simulation results show the proposed method could increase 50% tolerance of operating point drift and 70% 100% tolerance of mismatch with less than I% accuracy loss of benchmarks. It also reduces 66.7% memories and has about 50x energy-efficiency improvement of multiplication in the learning stage, compared with the conventional full-precision (32bit float) training system.
CR Bathen LAD, 2012, DES AUT TEST EUROPE, P284
   Bojnordi MN, 2016, INT S HIGH PERF COMP, P1, DOI 10.1109/HPCA.2016.7446049
   Chen YJ, 2014, INT SYMP MICROARCH, P609, DOI 10.1109/MICRO.2014.58
   Dighe S, 2011, IEEE J SOLID-ST CIRC, V46, P184, DOI 10.1109/JSSC.2010.2080550
   Du ZD, 2014, ASIA S PACIF DES AUT, P201, DOI 10.1109/ASPDAC.2014.6742890
   Fick L, 2017, IEEE CUST INTEGR CIR
   Gatet L, 2008, IEEE SENS J, V8, P1413, DOI 10.1109/JSEN.2008.920713
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   Jia Y., 2014, ARXIV14085093
   Kang K, 2010, IEEE T CIRCUITS-I, V57, P1513, DOI 10.1109/TCSI.2009.2034234
   Krizhevsky Alex, ALEXAAZS CIFAR 10 TU
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lei Jiang, 2011, 2011 International Symposium on Low Power Electronics and Design (ISLPED 2011), P127, DOI 10.1109/ISLPED.2011.5993624
   LikamWa R, 2016, CONF PROC INT SYMP C, P255, DOI 10.1109/ISCA.2016.31
   Lu J, 2015, IEEE J SOLID-ST CIRC, V50, P270, DOI 10.1109/JSSC.2014.2356197
   Paul S, 2011, IEEE T COMPUT, V60, P20, DOI 10.1109/TC.2010.203
   PELGROM MJM, 1989, IEEE J SOLID-ST CIRC, V24, P1433, DOI 10.1109/JSSC.1989.572629
   Valle M, 2002, ANALOG INTEGR CIRC S, V33, P263, DOI 10.1023/A:1020717929709
   Zhang LD, 2009, DES AUT CON, P694
   Zhao M., 2014, P 51 ANN DES AUT C, P1
NR 20
TC 0
Z9 0
BN 978-1-4503-5700-5
PY 2018
DI 10.1145/3195970.3196004
ER

PT S
AU Bu, SJ
   Cho, SB
AF Bu, Seok-Jun
   Cho, Sung-Bae
BE Juez, FJD
   Villar, JR
   DeLaCal, EA
   Herrero, A
   Quintian, H
   Saez, JA
   Corchado, E
TI A Hybrid Deep Learning System of CNN and LRCN to Detect Cyberbullying
   from SNS Comments
SO HYBRID ARTIFICIAL INTELLIGENT SYSTEMS (HAIS 2018)
SE Lecture Notes in Artificial Intelligence
CT 13th International Conference on Hybrid Artificial Intelligent Systems
   (HAIS)
CY JUN 20-22, 2018
CL Oviedo, SPAIN
AB The cyberbullying is becoming a significant social issue, in proportion to the proliferation of Social Network Service (SNS). The cyberbullying commentaries can be categorized into syntactic and semantic subsets. In this paper, we propose an ensemble method of the two deep learning models: One is character-level CNN which captures low-level syntactic information from the sequence of characters and is robust to noise using the transfer learning. The other is word-level LRCN which captures high-level semantic information from the sequence of words, complementing the CNN model. Empirical results show that the performance of the ensemble method is significantly enhanced, outperforming the state-of-the-art methods for detecting cyberbullying comment. The model is analyzed by t-SNE algorithm to investigate the mutually cooperative relations between syntactic and semantic models.
CR Abadi M., 2016, P 12 USENIX S OP SYS
   Bengio Y., 2012, J MACHINE LEARNING R, P17
   Bu SJ, 2017, LECT NOTES ARTIF INT, V10334, P615, DOI 10.1007/978-3-319-59650-1_52
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [DOI 10.1145/1390156.1390177, 10.1145/1390156.1390177]
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Forman G., 2008, P 17 ACM C INF KNOWL, P263, DOI DOI 10.1145/1458082.1458119
   Goldberg  Y., 2014, ARXIV14023722
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Mikolov T., 2013, COMPUTING RES REPOSI, V1301, P3781, DOI DOI 10.1109/TNN.2003.820440]
   Mikolov T., 2013, ADV NEURAL INFORM PR, P1, DOI 10.1162/jmlr.2003.3.4-5.951
   Olweus D., 1995, BULLYING SCH WHAT WE
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79
   Patchin JW, 2006, YOUTH VIOLENCE JUV J, V4, P148, DOI DOI 10.1177/1541204006286288
   Reynolds K., 2011, Proceedings of the 2011 Tenth International Conference on Machine Learning and Applications (ICMLA 2011), P241, DOI 10.1109/ICMLA.2011.152
   Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Turney PD, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P417
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Ybarra M., 2010, NATL SUMMIT INTERPER
   Zhang X., 2015, ADV NEURAL INFORM PR, V28, P649
   Zhang Yun-tao, 2005, Journal of Zhejiang University (Science), V6A, P49, DOI 10.1631/jzus.2005.A0049
NR 21
TC 0
Z9 0
SN 0302-9743
EI 1611-3349
BN 978-3-319-92639-1; 978-3-319-92638-4
PY 2018
VL 10870
BP 561
EP 572
DI 10.1007/978-3-319-92639-1_47
ER

PT S
AU Peng, B
   Li, W
   He, JP
AF Peng, Bo
   Li, Wei
   He, Jiping
BE Tseng, J
   Kotenko, I
TI Online Calibration of Intracortical Neural Interface Based on Transfer
   Learning
SO 3RD ANNUAL INTERNATIONAL CONFERENCE ON INFORMATION SYSTEM AND ARTIFICIAL
   INTELLIGENCE (ISAI2018)
SE Journal of Physics Conference Series
CT 3rd Annual International Conference on Information System and Artificial
   Intelligence (ISAI)
CY JUN 22-24, 2018
CL Suzhou, PEOPLES R CHINA
ID GRASP; REACH
AB In the application of neural interface, the neural activity of neurons and neuronal groups is not fixed even under the same task conditions. Meanwhile, the recording conditions of neural signals are also very unstable, with a high degree of within-and across-day variability. This results in a very unstable firing pattern for the recorded neural spike signals. In order to get better performance, the decoder often requires a lot of online calibration samples. This brings a heavy training burden to neural interface users. To solve this problem, this paper proposes to apply transfer learning (TL) to online calibration of intracortical neural interface to reduce the dependence of decoder on a large number of online calibration samples. Experimental results show that through transferring from a large amount of historical data, decoder can achieve satisfactory classification accuracy with only a small amount of online data.
CR Aflalo T, 2015, SCIENCE, V348, P906, DOI 10.1126/science.aaa5417
   Bamdadian A, 2013, IEEE ENG MED BIO, P2188, DOI 10.1109/EMBC.2013.6609969
   Collinger JL, 2013, LANCET, V381, P557, DOI 10.1016/S0140-6736(12)61816-9
   Devlaminck D, 2011, COMPUT INTEL NEUROSC, DOI 10.1155/2011/217987
   Guo YY, 2014, IEEE ENG MED BIO, P2322, DOI 10.1109/EMBC.2014.6944085
   Hochberg LR, 2006, NATURE, V442, P164, DOI 10.1038/nature04970
   Hochberg LR, 2012, NATURE, V485, P372, DOI 10.1038/nature11076
   Li Y, 2010, IEEE T BIO-MED ENG, V57, P1318, DOI 10.1109/TBME.2009.2039997
   McKhann GM, 2008, NEUROSURGERY, V62
   Musallam S, 2004, SCIENCE, V305, P258, DOI 10.1126/science.1097938
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Samek W, 2013, IEEE T BIO-MED ENG, V60, P2289, DOI 10.1109/TBME.2013.2253608
   Tu WT, 2012, NEUROCOMPUTING, V82, P109, DOI 10.1016/j.neucom.2011.10.024
   Vidaurre C, 2011, IEEE T BIO-MED ENG, V58, P587, DOI 10.1109/TBME.2010.2093133
   Wang PT, 2015, INT CONF INFO SCI, P315, DOI 10.1109/ICIST.2015.7288989
   Wu D, 2015, IEEE SYS MAN CYBERN, P3209, DOI 10.1109/SMC.2015.557
   Wu DR, 2014, IEEE SYS MAN CYBERN, P2801, DOI 10.1109/SMC.2014.6974353
   Wu W, 2009, IEEE T NEUR SYS REH, V17, P370, DOI 10.1109/TNSRE.2009.2023307
NR 18
TC 0
Z9 0
SN 1742-6588
EI 1742-6596
PY 2018
VL 1069
AR UNSP 012090
DI 10.1088/1742-6596/1069/1/012090
ER

PT S
AU Sato, M
   Orihara, R
   Sei, YC
   Tahara, Y
   Ohsuga, A
AF Sato, Minato
   Orihara, Ryohei
   Sei, Yuichi
   Tahara, Yasuyuki
   Ohsuga, Akihiko
BE VanDenHerik, J
   Rocha, AP
   Filipe, J
TI Text Classification and Transfer Learning Based on Character-Level Deep
   Convolutional Neural Networks
SO AGENTS AND ARTIFICIAL INTELLIGENCE (ICAART 2017)
SE Lecture Notes in Artificial Intelligence
CT 9th International Conference on Agents and Artificial Intelligence
   (ICAART)
CY FEB 24-26, 2017
CL Porto, PORTUGAL
DE Deep learning; Temporal ConvNets; Transfer learning Text classification;
   Sentiment analysis
AB Temporal (one-dimensional) Convolutional Neural Network (Temporal CNN, ConvNet) is an emergent technology for text understanding. The input for the ConvNets could be either a sequence of words or a sequence of characters. In the latter case there are no needs for natural language processing. Past studies showed that the character-level ConvNets worked well for text classification in English and romanized Chinese corpus. In this article we apply the character-level ConvNets to Japanese corpus. We confirmed that meaningful representations are extracted by the ConvNets in English corpus and Japanese corpus. We attempt to reuse the meaningful representations that are learned in the ConvNets from a large-scale dataset in the form of transfer learning. As for the application to the news categorization and the sentiment analysis tasks in Japanese corpus, the ConvNets outperformed N-gram-based classifiers. In addition, our ConvNets transfer learning frameworks worked well for a task which is similar to one used for pre-training.
CR Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22
   Bengio Y., 2013, P 2013 IEEE INT C AC
   Del Corso G. M., 2005, P 14 INT C WORLD WID, P97
   Deng J., 2009, P 2009 IEEE C COMP V
   Dos Santos C. N., 2014, COLING, V2014, P69
   Girshick R, 2014, P 2014 IEEE C COMP V
   Glorot X, 2010, P 13 INT C ART INT S
   Glorot X, 2011, P 28 INT C MACH LEAR
   Gulli A., 2005, INT C WORLD WID WEB, P880
   Kim Y., 2014, P 2014 C EMP METH NA, P1746, DOI DOI 10.3115/V1/D14-1181
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kudo T, 2004, P 2004 C EMP METH NA, P230
   Maas A. L., 2011, P 49 ANN M ASS COMP, V1, P142
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   McAuley Julian, 2015, P 21 ACM SIGKDD INT, P785, DOI [DOI 10.1145/2783258.2783381, 10.1145/2783258.2783381]
   Mikolov T., 2013, P NAACL 2013, P746
   Mikolov T., 2013, ADV NEURAL INFORM PR, P1, DOI 10.1162/jmlr.2003.3.4-5.951
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Santos C. N. dos, 2015, P ANN M ASS COMP LIN, V1, P626
   Sato M, 2017, ICAART: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE, VOL 2, P175, DOI 10.5220/0006193401750184
   Severyn A., 2015, P 9 INT WORKSH SEM E, P464
   Severyn A, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P959, DOI 10.1145/2766462.2767830
   Sharif Razavian  A., 2014, IEEE C COMP VIS PATT
   Socher R., 2013, P C EMP METH NAT LAN, V2013, P1631
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Zhang X., 2015, ADV NEURAL INFORM PR, V28, P649
   Zhang Xucong, 2015, CORR
NR 27
TC 0
Z9 0
SN 0302-9743
EI 1611-3349
BN 978-3-319-93581-2; 978-3-319-93580-5
PY 2018
VL 10839
BP 62
EP 81
DI 10.1007/978-3-319-93581-2_4
ER

PT S
AU Perre, A
   Alexandre, LA
   Freire, LC
AF Perre, Ana
   Alexandre, Luis A.
   Freire, Luis C.
BE Tavares, JMRS
   Jorge, RMN
TI Lesion Classification in Mammograms Using Convolutional Neural Networks
   and Transfer Learning
SO VIPIMAGE 2017
SE Lecture Notes in Computational Vision and Biomechanics
CT 6th ECCOMAS Thematic Conference on Computational Vision and Medical
   Image Processing (VipIMAGE)
CY OCT 18-20, 2017
CL Porto, PORTUGAL
AB Computer-Aided Detection/Diagnosis (CAD) tools were created to assist the detection and diagnosis of early stage cancers, decreasing false negative rate and improving radiologists' efficiency. Convolutional Neural Networks (CNNs) are one example of deep learning algorithms that proved to be successful in image classification. In this paper we aim to study the application of CNNs to the classification of lesions in mammograms. One major problem in the training of CNNs for medical applications is the large dataset of images that is often required but seldom available. To solve this problem, we use a transfer learning approach, which is based on three different networks that were pre-trained on the Imagenet dataset. We then investigate the performance of these pretrained CNNs and two types of image normalization to classify lesions in mammograms. The best results were obtained using the Caffe reference model for the CNN with no image normalization.
CR Arevalo J, 2016, COMPUT METH PROG BIO, V127, P248, DOI 10.1016/j.cmpb.2015.12.014
   Chatfield K., 2014, AW RET DEV DET DELV
   Ganesan Karthikeyan, 2013, IEEE Rev Biomed Eng, V6, P77, DOI 10.1109/RBME.2012.2232289
   Jalalian A, 2013, CLIN IMAG, V37, P420, DOI 10.1016/j.clinimag.2012.09.024
   Jia Y., 2014, ARXIV14085093
   Jiao ZC, 2016, NEUROCOMPUTING, V197, P221, DOI 10.1016/j.neucom.2016.02.060
   Pisco J.M., 2003, IMAGIOLOGIA BSICA TE
   Sampat MP, 2005, HANDBOOK OF IMAGE AND VIDEO PROCESSING, 2ND EDITION, P1195, DOI 10.1016/B978-012119792-6/50130-3
   Tang JS, 2014, IEEE SYST J, V8, P907, DOI 10.1109/JSYST.2014.2317378
   Vedaldi A., 2015, P ACM INT C MULT
   Wang  D., 2016, DEEP LEARNING IDENTI
   Wichakam I, 2016, INT CONF KNOWL SMART, P239, DOI 10.1109/KST.2016.7440527
NR 12
TC 0
Z9 0
SN 2212-9391
BN 978-3-319-68195-5; 978-3-319-68194-8
PY 2018
VL 27
BP 360
EP 368
DI 10.1007/978-3-319-68195-5_40
ER

PT S
AU Chang, YJ
   Smedby, O
AF Chang, Yongjun
   Smedby, Orjan
BE Tavares, JMRS
   Jorge, RMN
TI Effects of Preprocessing in Slice-Level Classification of Interstitial
   Lung Disease Based on Deep Convolutional Networks
SO VIPIMAGE 2017
SE Lecture Notes in Computational Vision and Biomechanics
CT 6th ECCOMAS Thematic Conference on Computational Vision and Medical
   Image Processing (VipIMAGE)
CY OCT 18-20, 2017
CL Porto, PORTUGAL
DE Deep learning; Preprocessing; Transfer learning; Deep convolutional
   network
ID RESOLUTION COMPUTED-TOMOGRAPHY
AB Several preprocessing methods are applied to the automatic classification of interstitial lung disease (ILD). The proposed methods are used for the inputs to an established convolutional neural network in order to investigate the effect of those preprocessing techniques to slice-level classification accuracy. Experimental results demonstrate that the proposed preprocessing methods and a deep learning approach outperformed the case of the original images input to deep learning without preprocessing.
CR Anthimopoulos M, 2016, IEEE T MED IMAGING, V35, P1207, DOI 10.1109/TMI.2016.2535865
   Bartholmai BJ, 2013, J THORAC IMAG, V28, P298, DOI 10.1097/RTI.0b013e3182a21969
   Delorme S, 1997, INVEST RADIOL, V32, P566, DOI 10.1097/00004424-199709000-00009
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Uchiyama Y, 2003, MED PHYS, V30, P2440, DOI 10.1118/1.1597431
   Xu Y, 2006, ACAD RADIOL, V13, P969, DOI 10.1016/j.acra.2006.04.017
NR 6
TC 0
Z9 0
SN 2212-9391
BN 978-3-319-68195-5; 978-3-319-68194-8
PY 2018
VL 27
BP 624
EP 629
DI 10.1007/978-3-319-68195-5_67
ER

PT S
AU Utkin, LV
   Ryabinin, MA
AF Utkin, Lev V.
   Ryabinin, Mikhail A.
BE Filchenkov, A
   Pivovarova, L
   Zizka, J
TI A Deep Forest for Transductive Transfer Learning by Using a Consensus
   Measure
SO ARTIFICIAL INTELLIGENCE AND NATURAL LANGUAGE
SE Communications in Computer and Information Science
CT 6th Conference on Artificial Intelligence and Natural Language (AINL)
CY SEP 20-23, 2017
CL Saint Petersburg, RUSSIA
DE Classification; Random forest; Decision tree; Transfer learning;
   Quadratic optimization
AB A Transfer Learning Deep Forest (TLDF) is proposed in the paper. It is based on the Deep Forest or gcForest proposed by Zhou and Feng and can be viewed as a gcForest modification whose aim is to implement the transductive transfer learning. The transfer learning is based on introducing weights of trees in forests which impact on the forest class probability distributions. The weights can be regarded as training parameters of the deep forest and are determined in order to maximize the agreement on target and source domains. The convex quadratic optimization problem with linear constraints is obtained to compute optimal weights for every forest taking into account the consensus principle. The numerical experiments illustrate the proposed distance metric method.
CR Arnold A., 2007, 7 IEEE INT C DAT MIN, P77, DOI DOI 10.1109/ICDMW.2007.109
   Ben-David Shai, 2007, ADV NEURAL INFORM PR, P137, DOI DOI 10.1007/S10994-009-5152-4
   Chen  Minmin, 2011, P ADV NEUR INF PROC, P2456
   Ding ZM, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3453
   Duan L, 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411
   Epstein B., 2017, ARXIV170510494V1
   Farajidavar N., 2014, BRIT MACH VIS C, V25, P1
   Fuzhen Zhuang, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P417, DOI 10.1007/978-3-662-44845-8_27
   Gong T, 2012, IEEE C EVOL COMPUTAT
   Habrard A, 2016, KNOWL INF SYST, V47, P45, DOI 10.1007/s10115-015-0839-2
   Hu J, 2015, PROC CVPR IEEE, P325, DOI 10.1109/CVPR.2015.7298629
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Lu J, 2015, INFORMATION, V6, P388, DOI 10.3390/info6030388
   Luo L, 2017, ARXIV170508620V1
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Reddi S., 2016, ARXIV160708254V2
   Sun SL, 2013, NEURAL COMPUT APPL, V23, P2031, DOI 10.1007/s00521-013-1362-6
   Utkin L., 2017, ARXIV170408715V1
   Utkin L., 2017, ARXIV170509620V1
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Xu C., 2013, ARXIV13045634, P1
   Xu ZJ, 2012, LECT NOTES COMPUT SC, V7665, P332, DOI 10.1007/978-3-642-34487-9_41
   Zhang X., 2015, ARXIV150300591V1
   Zhou Z., 2017, ARXIV170208835V2
   Zhuang FZ, 2010, IEEE T KNOWL DATA EN, V22, P1664, DOI 10.1109/TKDE.2009.205
NR 27
TC 1
Z9 1
SN 1865-0929
EI 1865-0937
BN 978-3-319-71746-3; 978-3-319-71745-6
PY 2018
VL 789
BP 194
EP 208
DI 10.1007/978-3-319-71746-3_17
ER

PT S
AU Gandarias, JM
   Gomez-de-Gabriel, JM
   Garcia-Cerezo, AJ
AF Gandarias, Juan M.
   Gomez-de-Gabriel, Jesus M.
   Garcia-Cerezo, Alfonso J.
BE Ollero, A
   Sanfeliu, A
   Montano, L
   Lau, N
   Cardeira, C
TI Tactile Sensing and Machine Learning for Human and Object Recognition in
   Disaster Scenarios
SO ROBOT 2017: THIRD IBERIAN ROBOTICS CONFERENCE, VOL 2
SE Advances in Intelligent Systems and Computing
CT 3rd Iberian Robotics Conference (Robot)
CY NOV 22-24, 2017
CL Seville, SPAIN
DE Tactile sensors; Rescue robotics; Machine learning; Deep learning;
   Transfer learning; Object recognition
AB This paper presents the application of machine learning to tactile sensing for rescue robotics. Disaster situations often exhibit low-visibility scenarios where haptic feedback provides a valuable information for the search of potential victims. To extract haptic information from the environment, a tactile sensor attached to a lightweight robotic arm is used. Then, methods based on the SURF descriptor, support vector machines (SVM), Deep Convolutional Neural Networks (DCNN) and transfer learning are implemented to classify the data. Besides, experiments have been carried out, to compare those procedures, using different contact elements, such as human parts and objects that could be found in catastrophe scenarios. The best achieved accuracy of 92.22%, results from the application of the transfer learning procedure using a pre-trained DCNN and fine-tuning the classification layer of the network.
CR Abdelrahman W, 2012, IEEE SYS MAN CYBERN, P2213, DOI 10.1109/ICSMC.2012.6378069
   Ang QZ, 2015, IEEE SYST J, V9, P86, DOI 10.1109/JSYST.2013.2283955
   Baishya SS, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P8, DOI 10.1109/IROS.2016.7758088
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Espes D., 2013, ARXIV13124162
   Garcia-Cerezo A., 2007, IEEE INT WORKSH SAF, P1, DOI DOI 10.1109/SSRR2007.4381269
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Khasnobish A., 2012, 2012 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2012.6252593
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2
   Likas A, 2003, PATTERN RECOGN, V36, P451
   Liu HB, 2012, IEEE INT CONF ROBOT, P1410, DOI 10.1109/ICRA.2012.6224872
   Liu HP, 2016, IEEE T INSTRUM MEAS, V65, P656, DOI 10.1109/TIM.2016.2514779
   Liu YG, 2013, J INTELL ROBOT SYST, V72, P147, DOI 10.1007/s10846-013-9822-x
   Luo S, 2015, LECT NOTES ARTIF INT, V9245, P15, DOI 10.1007/978-3-319-22876-1_2
   Luo S, 2015, IEEE SENS J, V15, P5001, DOI 10.1109/JSEN.2015.2432127
   Luo S, 2016, IEEE ICC
   Madry M, 2014, IEEE INT CONF ROBOT, P2262, DOI 10.1109/ICRA.2014.6907172
   Murphy R. R., 2008, SPRINGER HDB ROBOTIC, P1151, DOI DOI 10.1007/978-3-540-30301-5_51
   Navarro S. E., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P497, DOI 10.1109/HAPTIC.2012.6183837
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Ranasinghe A, 2016, IEEE T CYBERNETICS, V46, P568, DOI 10.1109/TCYB.2015.2409772
   Schmitz A, 2014, IEEE-RAS INT C HUMAN, P1044, DOI 10.1109/HUMANOIDS.2014.7041493
   Serrano-Cuerda J., 2011, 2011 7th International Conference on Intelligent Environments, P354, DOI 10.1109/IE.2011.21
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Vidal-Verdu F, 2011, SENSORS-BASEL, V11, P5489, DOI 10.3390/s110505489
NR 28
TC 1
Z9 1
SN 2194-5357
EI 2194-5365
BN 978-3-319-70836-2; 978-3-319-70835-5
PY 2018
VL 694
BP 165
EP 175
DI 10.1007/978-3-319-70836-2_14
ER

PT S
AU Kim, D
   Cho, D
   Yoo, D
   Kweon, IS
AF Kim, Dahun
   Cho, Donghyeon
   Yoo, Donggeun
   Kweon, In So
GP IEEE
TI Learning Image Representations by Completing Damaged Jigsaw Puzzles
SO 2018 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV
   2018)
SE IEEE Winter Conference on Applications of Computer Vision
CT 18th IEEE Winter Conference on Applications of Computer Vision (WACV)
CY MAR 12-15, 2018
CL NV
AB In this paper, we explore methods of complicating self-supervised tasks for representation learning. That is, we do severe damage to data and encourage a network to recover them. First, we complicate each of three powerful self-supervised task candidates: jigsaw puzzle, inpainting, and colorization. In addition, we introduce a novel complicated self-supervised task called "Completing damaged jigsaw puzzles" which is puzzles with one piece missing and the other pieces without color. We train a convolutional neural network not only to solve the puzzles, but also generate the missing content and colorize the puzzles. The recovery of the aforementioned damage pushes the network to obtain robust and general-purpose representations. We demonstrate that complicating the self-supervised tasks improves their original versions and that our final task learns more robust and transferable representations compared to the previous methods, as well as the simple combination of our candidate tasks. Our approach achieves state-of-the-art performance in transfer learning on PASCAL classification and semantic segmentation.
CR Bilen H., 2016, P COMP VIS PATT REC
   Diederik M. W., 2014, P INT C LEARN REPR I
   Doersch C., 2015, P INT C COMP VIS ICC
   Doersch C., 2017, P INT C COMP VIS ICC
   Donahue J., 2017, P INT C LEARN REPR I
   Everingham M., PASCAL VISUAL OBJECT
   Girshick R., 2015, P INT C COMP VIS ICC
   He K., 2016, P COMP VIS PATT REC
   Isola P., 2015, ICLR WORKSH
   Jayaraman D, 2017, INT J COMPUT VISION, V125, P136, DOI 10.1007/s11263-017-1001-2
   Jia Y., 2014, ARXIV14085093
   Jie Z., 2017, P COMP VIS PATT REC
   Kantorov V., 2016, P EUR C COMP VIS ECC
   Kim D., 2017, P INT C COMP VIS ICC
   Kingma D., 2015, P INT C LEARN REPR I
   Krahenbuhl P., 2016, P INT C LEARN REPR I
   Krizhevsky A., 2012, P NEUR INF PROC SYST
   Larsson G., 2017, P COMP VIS PATT REC
   Lee H.-Y., 2017, P INT C COMP VIS ICC
   Long Jonathan, 2015, P COMP VIS PATT REC
   Misra I., 2016, P EUR C COMP VIS ECC
   Noroozi M., 2017, P INT C COMP VIS ICC
   Noroozi M., 2016, P EUR C COMP VIS E
   Oquab M., 2015, P COMP VIS PATT REC
   Owens A., 2016, P EUR C COMP VIS ECC
   Pathak D., 2017, P COMP VIS PATT REC
   Pathak D., 2016, P COMP VIS PATT REC
   Radford A, 2016, P INT C LEARN REPR I
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, ABS14091556 CORR
   Szegedy C., 2015, P COMP VIS PATT REC
   Tang MF, 2015, IEEE INT CON MULTI
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang X., 2015, P INT C COMP VIS ICC
   Wang X., 2017, P INT C COMP VIS ICC
   Zhang R., 2017, P COMP VIS PATT REC
   Zhang R., 2016, P EUR C COMP VIS ECC
   Zisserman A., 2017, P INT C COMP VIS ICC
NR 38
TC 0
Z9 0
SN 2472-6737
BN 978-1-5386-4886-5
PY 2018
BP 793
EP 802
DI 10.1109/WACV.2018.00092
ER

PT S
AU Dominguez, M
   Dhamdhere, R
   Petkar, A
   Jain, S
   Sah, S
   Ptucha, R
AF Dominguez, Miguel
   Dhamdhere, Rohan
   Petkar, Atir
   Jain, Saloni
   Sah, Shagan
   Ptucha, Raymond
GP IEEE
TI General-Purpose Deep Point Cloud Feature Extractor
SO 2018 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV
   2018)
SE IEEE Winter Conference on Applications of Computer Vision
CT 18th IEEE Winter Conference on Applications of Computer Vision (WACV)
CY MAR 12-15, 2018
CL NV
AB Depth sensors used in autonomous driving and gaming systems often report back 3D point clouds. The lack of structure from these sensors does not allow these systems to take advantage of recent advances in convolutional neural networks which are dependent upon traditional filtering and pooling operations. Analogous to image based convolutional architectures, recently introduced graph based architectures afford similar filtering and pooling operations on arbitrary graphs. We adopt these graph based methods to 3D point clouds to introduce a generic vector representation of 3D graphs, we call graph 3D (G3D). We believe we are the first to use large scale transfer learning on 3D point cloud data and demonstrate the discriminant power of our salient latent representation of 3D point clouds on unforeseen test sets. By using our G3D network (G3DNet) as a feature extractor, and then pairing G3D feature vectors with a standard classifier, we achieve the best accuracy on ModelNet10 (93.1%) and ModelNet 40 (91.7%) for a graph network, and comparable performance on the Sydney Urban Objects dataset to other methods. This general-purpose feature extractor can be used as an off-the-shelf component in other 3D scene understanding or object tracking works.
CR Amenta N, 1999, DISCRETE COMPUT GEOM, V22, P481, DOI 10.1007/PL00009475
   Arvind V., 2017, ARXIV E PRINTS
   Atwood J, 2016, ADV NEURAL INFORM PR, P1993
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Bell W. N., 2015, PYAMG ALGEBRAIC MULT
   Bell W. N., 2008, THESIS
   Ben-Shabat Y., 2017, ARXIV E PRINTS
   Brock A., GENERATIVE DISCRIMIN
   Bruna J., 2013, ARXIV13126203
   Chang A. X, 2015, ARXIV151203012
   Chen TT, 2014, IEEE INT VEH SYM, P667, DOI 10.1109/IVS.2014.6856425
   Chen XG, 2015, LECT N MECH ENG, P1, DOI 10.1007/978-3-319-09507-3_1
   Cignoni P, 2008, ERCIM NEWS, V73, P45, DOI DOI 10.2312/L0CALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPC0NF2008/129-136
   Defferrard M., ADV NEURAL INFORM PR, V29, P3844
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Deuge M. D., 2013, AUSTR C ROB AUT, V2
   Dominguez M., 2017, INT C IM PROC
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Edwards M., BRIT MACH VIS C
   Gomez-Donoso F, 2017, IEEE IJCNN, P412, DOI 10.1109/IJCNN.2017.7965883
   Henaff Mikael, 2015, ARXIV150605163
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Johns E., 2016, IEEE C COMP VIS PATT
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma Diederik P., 2014, CORR
   Kipf T. N., P ICLR 2017
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Maturana D, 2015, IEEE RSJ INT C INT R
   Monti F., 2016, ARXIV161108402
   Niepert M., 2016, INT C MACH LEARN, P2014
   Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732
   Qi C. R., 2017, IEEE C COMP VIS PATT
   Ravanbakhsh S., P ICLR 2017, V1611
   Riemenschneider Hayko, 2014, LEARNING CLASSIFY MU, P516
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandryhaila A, 2013, IEEE T SIGNAL PROCES, V61, P1644, DOI 10.1109/TSP.2013.2238935
   Sedaghat N., 2016, CORR
   Sedaghat N., 2017, BRIT MACH VIS C BMVC
   Sedaghat N, 2015, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2015.155
   Serna A., 3 INT C PATT REC APP
   Sfikas K., 2017, EUR WORKSH 3D OBJ RE
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Simonovsky M., 2017, IEEE C COMP VIS PATT
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Sinha A., 2016, DEEP LEARNING 3D SHA, P223
   Soltani A. A., 2017, IEEE C COMP VIS PATT
   Su H., 2015, P ICCV
   Such FP, 2017, IEEE J-STSP, V11, P884, DOI 10.1109/JSTSP.2017.2726981
   Trottenberg U., 2000, MULTIGRID
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang C., 2017, BRIT MACH VIS C
   Wu J., 2016, ADV NEURAL INFORM PR, P82
   Xu K., 2015, INT C MACH LEARN, P2048
   Xu X, 2016, INT C PATT RECOG, P3506, DOI 10.1109/ICPR.2016.7900177
   Yi L., 2017, IEEE C COMP VIS PATT
   Zamir AR, 2016, LECT NOTES COMPUT SC, V9907, P535, DOI 10.1007/978-3-319-46487-9_33
   Zanuttigh P., 2017, INT C IM PROC
   Zhi S., 2017, COMPUTERS GRAPHICS
   Zhi S., 2017, EUR WORKSH 3D OBJ RE
   Zhirong W., 2015 IEEE C COMP VIS, P1912
NR 63
TC 0
Z9 0
SN 2472-6737
BN 978-1-5386-4886-5
PY 2018
BP 1972
EP 1981
DI 10.1109/WACV.2018.00218
ER

PT B
AU Hong, YX
   Ling, C
   Ye, ZC
AF Hong, Yuxi
   Ling, Chen
   Ye, Zuochang
GP IEEE
TI End-to-End Soccer Video Scene and Event Classification with Deep
   Transfer Learning
SO 2018 INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS AND COMPUTER VISION
   (ISCV2018)
CT International Conference on Intelligent Systems and Computer Vision
   (ISCV)
CY APR 02-04, 2018
CL Fac Sci Dhar Mahraz, Fez, MOROCCO
HO Fac Sci Dhar Mahraz
DE Soccer video scene and event classification; Deep transfer learning; CNN
AB Soccer video scene and event classification are two essential tasks for the soccer video semantic analysis and have attracted many interests of researchers because of their importance and practicability. However most proposed methods solve these two tasks separately. In order to solve two tasks at the same time and improve the efficiency of video processing, we treat them as one end-to-end classification task. We introduce a new Soccer Video Scene and Event Dataset (SVSED) with six categories from the scenes and events, which contains 600 video clips. Then, we show that frame features extracted from pretrained CNN model of different categories are separable in 3-D space. Finally, we construct a CNN model for the classification task and deep transfer learning method is used for optimizing classification task result considering relative small training datasets. We fine-tuned several state-of-art CNN models and achieves accuracy above 89% within several minutes training.
CR DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Jiang HH, 2016, PROC INT C TOOLS ART, P490, DOI [10.1109/ICTAI.2016.78, 10.1109/ICTAI.2016.0081]
   Kolekar M. H., 2008, TENCON 2008 2008 IEE, P1
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Li B., 2003, P IEEE INT C AC SPEE, V3, P169
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Simonyan K., 2014, ARXIV14091556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tavassolipour M, 2014, IEEE T CIRC SYST VID, V24, P291, DOI 10.1109/TCSVT.2013.2243640
   Tingxi Liu, 2017, Neural Information Processing. 24th International Conference, ICONIP 2017. Proceedings: LNCS 10635, P440, DOI 10.1007/978-3-319-70096-0_46
   Tjondronegoro DW, 2010, IEEE T SYST MAN CY A, V40, P1009, DOI 10.1109/TSMCA.2010.2046729
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
NR 15
TC 0
Z9 0
BN 978-1-5386-4396-9
PY 2018
ER

PT S
AU Habibzadeh, M
   Jannesari, M
   Rezaei, Z
   Baharvand, H
   Totonchi, M
AF Habibzadeh, Mehdi
   Jannesari, Mahboobeh
   Rezaei, Zahra
   Baharvand, Hossein
   Totonchi, Mehdi
BE Verikas, A
   Radeva, P
   Nikolaev, D
   Zhou, J
TI Automatic White Blood Cell Classification Using Pre-trained Deep
   Learning Models: ResNet and Inception
SO TENTH INTERNATIONAL CONFERENCE ON MACHINE VISION (ICMV 2017)
SE Proceedings of SPIE
CT 10th International Conference on Machine Vision (ICMV)
CY NOV 13-15, 2017
CL Vienna, AUSTRIA
DE Deep learning; Inception; ResNet; transfer learning; fine-tuning; white
   blood cell classification
AB This works gives an account of evaluation of white blood cell differential counts via computer aided diagnosis (CAD) system and hematology rules. Leukocytes, also called white blood cells (WBCs) play main role of the immune system. Leukocyte is responsible for phagocytosis and immunity and therefore in defense against infection involving the fatal diseases incidence and mortality related issues. Admittedly, microscopic examination of blood samples is a time consuming, expensive and error-prone task. A manual diagnosis would search for specific Leukocytes and number abnormalities in the blood slides while complete blood count (CBC) examination is performed. Complications may arise from the large number of varying samples including different types of Leukocytes, related sub-types and concentration in blood, which makes the analysis prone to human error. This process can be automated by computerized techniques which are more reliable and economical. In essence, we seek to determine a fast, accurate mechanism for classification and gather information about distribution of white blood evidences which may help to diagnose the degree of any abnormalities during CBC test. In this work, we consider the problem of pre-processing and supervised classification of white blood cells into their four primary types including Neutrophils, Eosinophils, Lymphocytes, and Monocytes using a consecutive proposed deep learning framework. For first step, this research proposes three consecutive pre-processing calculations namely are color distortion; bounding box distortion (crop) and image flipping mirroring. In second phase, white blood cell recognition performed with hierarchy topological feature extraction using Inception and ResNet architectures. Finally, the results obtained from the preliminary analysis of cell classification with (11200) training samples and 1244 white blood cells evaluation data set are presented in confusion matrices and interpreted using accuracy rate, and false positive with the classification framework being validated with experiments conducted on poor quality blood images sized 320 x 240 pixels. The deferential outcomes in the challenging cell detection task, as shown in result section, indicate that there is a significant achievement in using Inception and ResNet architecture with proposed settings. Our framework detects on average 100% of the four main white blood cell types using ResNet V1 50 while also alternative promising result with 99.84% and 99.46% accuracy rate obtained with ResNet V1 152 and ResNet 101, respectively with 3000 epochs and fine-tuning all layers. Further statistical confusion matrix tests revealed that this work achieved 1, 0.9979, 0.9989 sensitivity values when area under the curve (AUC) scores above 1, 0.9992, 0.9833 on three proposed techniques. In addition, current work shows negligible and small false negative 0, 2, 1 and substantial false positive with 0, 0, 5 values in Leukocytes detection.
CR Aubreville M., 2017, ARXIV170301622
   Bahadori MT, 2014, KNOWL INF SYST, V38, P61, DOI 10.1007/s10115-013-0647-5
   Bruegel M, 2015, CLIN CHEM LAB MED, V53, P1057, DOI 10.1515/cclm-2014-0945
   Chang H., 2017, ARXIV170300534
   Chen CL, 2016, SCI REP-UK, V6, DOI 10.1038/srep21471
   Cruz-Roa A, 2017, SCI REP-UK, V7, DOI 10.1038/srep46450
   Daume H, 2006, J ARTIF INTELL RES, V26, P101, DOI 10.1613/jair.1872
   Dong YH, 2017, 2017 IEEE EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL & HEALTH INFORMATICS (BHI), P101, DOI 10.1109/BHI.2017.7897215
   Dorini LB, 2013, IEEE J BIOMED HEALTH, V17, P250, DOI 10.1109/TITB.2012.2207398
   Grimaldi E, 2000, AM J CLIN PATHOL, V113, P497
   Gui L., 2017, INT J MACHINE LEARNI, P1
   Habibzadeh Mehdi, 2014, Artificial Neural Networks in Pattern Recognition. 6th IAPR TC 3 International Workshop, ANNPR 2014. Proceedings: LNCS 8774, P216, DOI 10.1007/978-3-319-11656-3_20
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton G., 2012, NEURAL NETWORKS MACH
   Ioffe S., 2015, CORR
   Joshi M. D., 2013, INT J EMERGING TREND, V2
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1145/1830483.1830503
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Lee C.-Y., 2015, ARTIF INTELL, P562
   Liu Y., 2017, ARXIV170302442
   Naz S., 2017, IEEE C COMP VIS PATT
   Nazlibilek S., 2015, BIOMEDICAL RES, V26
   Parthasarathy D., 2017, WBC CLASSIFICATION
   Ramesh N., 2012, J PATHOLOGY INFORN, V3
   Ramoser H., 2006, P IEEE EMBS, P3371, DOI DOI 10.1109/IEMBS.2005.1617200
   Rezatofighi SH, 2011, COMPUT MED IMAG GRAP, V35, P333, DOI 10.1016/j.compmedimag.2011.01.003
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruzicka K, 2001, ARCH PATHOL LAB MED, V125, P391
   Sajjad M, 2016, INT CONF FRONT INFO, P99, DOI [10.1109/FIT.2016.24, 10.1109/FIT.2016.026]
   Schaul T., 2013, P 30 INT C MACH LEAR, P343
   Sirinukunwattana K, 2016, IEEE T MED IMAGING, V35, P1196, DOI 10.1109/TMI.2016.2525803
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   Szegedy C., 2017, AAAI, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   T. G. Developers, 2017, DISTR TENS
   T. G. Developers, 2017, INST TENS UB
   T. N. Developers, 2017, NVIDIA GPUS ENG DEEP
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Wang D., 2016, ARXIV160605718
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
   Zadrozny B, 2004, ICML, V2004, P114, DOI DOI 10.1145/1015330.1015425
   Zeiler M. D., 2012, ARXIV12125701
NR 43
TC 1
Z9 1
SN 0277-786X
EI 1996-756X
BN 978-1-5106-1942-5
PY 2018
VL 10696
AR UNSP 1069612
DI 10.1117/12.2311282
ER

PT S
AU Patel, VA
   Joshi, MV
AF Patel, Vaibhav Amit
   Joshi, Manjunath V.
BE Verikas, A
   Radeva, P
   Nikolaev, D
   Zhou, J
TI Convolutional Neural Network with Transfer Learning for Rice Type
   Classification
SO TENTH INTERNATIONAL CONFERENCE ON MACHINE VISION (ICMV 2017)
SE Proceedings of SPIE
CT 10th International Conference on Machine Vision (ICMV)
CY NOV 13-15, 2017
CL Vienna, AUSTRIA
DE Convolutional neural network; computer vision; transfer learning
AB Presently, rice type is identified manually by humans, which is time consuming and error prone. Therefore, there is a need to do this by machine which makes it faster with greater accuracy. This paper proposes a deep learning based method for classification of rice types. We propose two methods to classify the rice types. In the first method, we train a deep convolutional neural network (CNN) using the given segmented rice images. In the second method, we train a combination of a pretrained VGG16 network and the proposed method, while using transfer learning in which the weights of a pretrained network are used to achieve better accuracy. Our approach can also be used for classification of rice grain as broken or fine. We train a 5-class model for classifying rice types using 4000 training images and another 2- class model for the classification of broken and normal rice using 1600 training images. We observe that despite having distinct rice images, our architecture, pretrained on ImageNet data boosts classification accuracy significantly.
CR Guzman J. D., 2008, World conference on agricultural information and IT, IAALD AFITA WCCA 2008, Tokyo University of Agriculture, Tokyo, Japan, 24 - 27 August, 2008, P41
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Huang Xing-yi, 2004, J JIANGSU U NATL SCI, V2
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LeCun Yann, 1995, HDB BRAIN THEORY NEU, V3361, P10
   Liu Zhao-yan, 2005, J Zhejiang Univ Sci B, V6, P1095, DOI 10.1631/jzus.2005.B1095
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   Singh B., 2013, INT J SCI RES PUBL, V3, P1
   Sonnadara Upul, 2013, P TECHNICAL SESSIONS, V29, P9
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 17
TC 0
Z9 0
SN 0277-786X
EI 1996-756X
BN 978-1-5106-1942-5
PY 2018
VL 10696
AR UNSP 1069613
DI 10.1117/12.2309482
ER

PT S
AU Li, XD
   Mao, WJ
   Jiang, W
   Yao, Y
AF Li, Xiaodong
   Mao, Weijie
   Jiang, Wei
   Yao, Ye
BE Cao, J
   Cambria, E
   Lendasse, A
   Miche, Y
   Vong, CM
TI Multi-kernel Transfer Extreme Learning Classification
SO PROCEEDINGS OF ELM-2016
SE Proceedings in Adaptation Learning and Optimization
CT 7th International Conference on Extreme Learning Machines (ELM)
CY DEC 13-15, 2016
CL Singapore, SINGAPORE
DE Extreme learning machine; Transfer learning (TL); Multiple kernel
   learning
ID FEEDFORWARD NETWORKS; MACHINE; ALGORITHM
AB In this paper, a novel transfer extreme learning machine (TELM) algorithm based on multi-kernel (MK) framework has been proposed for classification. In this case, the problem is transformed into a semi-supervised learning problem, which allows multi-kernel extreme learning machine (MK-TELM) classifiers to be trained for the data categorization. Compared with many popular algorithms, the proposed method, named as MK-TELM, shows its satisfactorily experimental results on the variety of data sets, which highlights the robustness and effectiveness for classification applications.
CR CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Dai  W., 2007, P 24 INT C MACH LEAR, P193, DOI DOI 10.1145/1273496.1273521
   Hecht-Nielsen R., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P593, DOI 10.1109/IJCNN.1989.118638
   Huang G, 2014, IEEE T CYBERNETICS, V44, P2405, DOI 10.1109/TCYB.2014.2307349
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Huang GB, 2015, IEEE COMPUT INTELL M, V10, P18, DOI 10.1109/MCI.2015.2405316
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Kim CT, 2008, IEEE T NEURAL NETWOR, V19, P371, DOI 10.1109/TNN.2007.911739
   Lam D., 2013, NEUR NETW IJCNN 2013, P1
   Li GH, 2010, COMPUT MATH APPL, V60, P377, DOI 10.1016/j.camwa.2010.03.023
   Li XD, 2016, NEURAL COMPUT APPL, V27, P175, DOI 10.1007/s00521-014-1709-7
   Liu XW, 2015, NEUROCOMPUTING, V149, P253, DOI 10.1016/j.neucom.2013.09.072
   Pan J, 2014, NEUROCOMPUTING, V137, P57, DOI 10.1016/j.neucom.2013.04.045
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Peng Y, 2015, NEUROCOMPUTING, V149, P340, DOI 10.1016/j.neucom.2013.12.065
   Rong HJ, 2008, NEUROCOMPUTING, V72, P359, DOI 10.1016/j.neucom.2008.01.005
   Scardapane S, 2014, RECENT ADV NEURAL NE, P25, DOI [10.1007/978-3-319-04129-2, DOI 10.1007/978-3-319-04129-2.]
   Wang YG, 2011, NEUROCOMPUTING, V74, P2483, DOI 10.1016/j.neucom.2010.11.030
   Zeng GQ, 2016, INFORM SCIENCES, V330, P49, DOI 10.1016/j.ins.2015.10.010
   Zong WW, 2013, NEUROCOMPUTING, V101, P229, DOI 10.1016/j.neucom.2012.08.010
NR 20
TC 0
Z9 0
SN 2363-6084
BN 978-3-319-57421-9; 978-3-319-57420-2
PY 2018
VL 9
BP 159
EP 170
DI 10.1007/978-3-319-57421-9_13
ER

PT S
AU Lopez-Sanchez, D
   Arrieta, AG
   Corchado, JM
AF Lopez-Sanchez, Daniel
   Gonzalez Arrieta, Angelica
   Corchado, Juan M.
BE Omatu, S
   Rodriguez, S
   Villarrubia, G
   Faria, P
   Sitek, P
   Prieto, J
TI Deep neural networks and transfer learning applied to multimedia web
   mining
SO DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE
SE Advances in Intelligent Systems and Computing
CT 14th International Symposium on Distributed Computing and Artificial
   Intelligence (DCAI)
CY JUN 21-23, 2017
CL Porto, PORTUGAL
DE Web mining; deep learning; transfer learning
AB The growth in the amount of multimedia content available online supposes a challenge for search and recommender systems. This information in the form of visual elements is of great value to a variety of web mining tasks; however, the mining of these resources is a difficult task due to the complexity and variability of the images. In this paper, we propose applying a deep learning model to the problem of web categorization. In addition, we make use of a technique known as transfer or inductive learning to drastically reduce the computational cost of the training phase. Finally, we report experimental results on the effectiveness of the proposed method using different classification methods and features from various depths of the deep model.
CR Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Masoudnia S, 2014, ARTIF INTELL REV, V42, P275, DOI 10.1007/s10462-012-9338-y
   Nair V.G., 2014, GETTING STARTED BEAU
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1109/TNN.2010.2066286
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Yosinski J., 2014, ADV NEURAL INFORM PR, V27, P3320
NR 12
TC 2
Z9 2
SN 2194-5357
EI 2194-5365
BN 978-3-319-62410-5
PY 2018
VL 620
BP 124
EP 131
DI 10.1007/978-3-319-62410-5_15
ER

EF